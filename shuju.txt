After 1 training step(s), loss on training batch is 0.587876.
After 2 training step(s), loss on training batch is 0.457988.
After 3 training step(s), loss on training batch is 0.99322.
After 4 training step(s), loss on training batch is 0.955372.
After 5 training step(s), loss on training batch is 0.858253.
After 6 training step(s), loss on training batch is 0.352705.
After 7 training step(s), loss on training batch is 0.98011.
After 8 training step(s), loss on training batch is 0.372401.
After 9 training step(s), loss on training batch is 0.651292.
After 10 training step(s), loss on training batch is 0.0901754.
After 11 training step(s), loss on training batch is 0.0715696.
After 12 training step(s), loss on training batch is 0.0670665.
After 13 training step(s), loss on training batch is 0.0599285.
After 14 training step(s), loss on training batch is 0.0649797.
After 15 training step(s), loss on training batch is 0.0585508.
After 16 training step(s), loss on training batch is 0.0518702.
After 17 training step(s), loss on training batch is 0.0526193.
After 18 training step(s), loss on training batch is 0.0539813.
After 19 training step(s), loss on training batch is 0.0580237.
After 20 training step(s), loss on training batch is 0.0676054.
After 21 training step(s), loss on training batch is 0.056759.
After 22 training step(s), loss on training batch is 0.0532488.
After 23 training step(s), loss on training batch is 0.0535933.
After 24 training step(s), loss on training batch is 0.0542236.
After 25 training step(s), loss on training batch is 0.0515394.
After 26 training step(s), loss on training batch is 0.049607.
After 27 training step(s), loss on training batch is 0.0539238.
After 28 training step(s), loss on training batch is 0.0505418.
After 29 training step(s), loss on training batch is 0.0454669.
After 30 training step(s), loss on training batch is 0.04673.
After 31 training step(s), loss on training batch is 0.0483473.
After 32 training step(s), loss on training batch is 0.0512806.
After 33 training step(s), loss on training batch is 0.0608028.
After 34 training step(s), loss on training batch is 0.0500767.
After 35 training step(s), loss on training batch is 0.0473788.
After 36 training step(s), loss on training batch is 0.0483035.
After 37 training step(s), loss on training batch is 0.0490008.
After 38 training step(s), loss on training batch is 0.0454434.
After 39 training step(s), loss on training batch is 0.0445429.
After 40 training step(s), loss on training batch is 0.0488411.
After 41 training step(s), loss on training batch is 0.0455837.
After 42 training step(s), loss on training batch is 0.0402987.
After 43 training step(s), loss on training batch is 0.0419546.
After 44 training step(s), loss on training batch is 0.0437798.
After 45 training step(s), loss on training batch is 0.0461134.
After 46 training step(s), loss on training batch is 0.0555414.
After 47 training step(s), loss on training batch is 0.04474.
After 48 training step(s), loss on training batch is 0.042097.
After 49 training step(s), loss on training batch is 0.043373.
After 50 training step(s), loss on training batch is 0.0441732.
After 51 training step(s), loss on training batch is 0.0397845.
After 52 training step(s), loss on training batch is 0.0398494.
After 53 training step(s), loss on training batch is 0.0441428.
After 54 training step(s), loss on training batch is 0.0413441.
After 55 training step(s), loss on training batch is 0.0355495.
After 56 training step(s), loss on training batch is 0.0377378.
After 57 training step(s), loss on training batch is 0.0396645.
After 58 training step(s), loss on training batch is 0.0414535.
After 59 training step(s), loss on training batch is 0.0507433.
After 60 training step(s), loss on training batch is 0.0398175.
After 61 training step(s), loss on training batch is 0.0374797.
After 62 training step(s), loss on training batch is 0.0389019.
After 63 training step(s), loss on training batch is 0.0399044.
After 64 training step(s), loss on training batch is 0.0349072.
After 65 training step(s), loss on training batch is 0.03583.
After 66 training step(s), loss on training batch is 0.0398337.
After 67 training step(s), loss on training batch is 0.0376172.
After 68 training step(s), loss on training batch is 0.031429.
After 69 training step(s), loss on training batch is 0.0342471.
After 70 training step(s), loss on training batch is 0.0360657.
After 71 training step(s), loss on training batch is 0.0374255.
After 72 training step(s), loss on training batch is 0.0463787.
After 73 training step(s), loss on training batch is 0.0353627.
After 74 training step(s), loss on training batch is 0.0335424.
After 75 training step(s), loss on training batch is 0.0353156.
After 76 training step(s), loss on training batch is 0.0364132.
After 77 training step(s), loss on training batch is 0.0313989.
After 78 training step(s), loss on training batch is 0.0328949.
After 79 training step(s), loss on training batch is 0.036176.
After 80 training step(s), loss on training batch is 0.0346555.
After 81 training step(s), loss on training batch is 0.0283489.
After 82 training step(s), loss on training batch is 0.0319395.
After 83 training step(s), loss on training batch is 0.0335174.
After 84 training step(s), loss on training batch is 0.0338497.
After 85 training step(s), loss on training batch is 0.042453.
After 86 training step(s), loss on training batch is 0.0315081.
After 87 training step(s), loss on training batch is 0.0305844.
After 88 training step(s), loss on training batch is 0.032796.
After 89 training step(s), loss on training batch is 0.033836.
After 90 training step(s), loss on training batch is 0.0290686.
After 91 training step(s), loss on training batch is 0.0307845.
After 92 training step(s), loss on training batch is 0.0333471.
After 93 training step(s), loss on training batch is 0.0322489.
After 94 training step(s), loss on training batch is 0.0261769.
After 95 training step(s), loss on training batch is 0.0302032.
After 96 training step(s), loss on training batch is 0.0315037.
After 97 training step(s), loss on training batch is 0.0311186.
After 98 training step(s), loss on training batch is 0.039473.
After 99 training step(s), loss on training batch is 0.0287058.
After 100 training step(s), loss on training batch is 0.0283541.
After 101 training step(s), loss on training batch is 0.0307904.
After 102 training step(s), loss on training batch is 0.0317999.
After 103 training step(s), loss on training batch is 0.0271995.
After 104 training step(s), loss on training batch is 0.0290615.
After 105 training step(s), loss on training batch is 0.0312838.
After 106 training step(s), loss on training batch is 0.0304237.
After 107 training step(s), loss on training batch is 0.0244574.
After 108 training step(s), loss on training batch is 0.028815.
After 109 training step(s), loss on training batch is 0.0298524.
After 110 training step(s), loss on training batch is 0.0288827.
After 111 training step(s), loss on training batch is 0.0368411.
After 112 training step(s), loss on training batch is 0.026462.
After 113 training step(s), loss on training batch is 0.0265917.
After 114 training step(s), loss on training batch is 0.0291928.
After 115 training step(s), loss on training batch is 0.0301923.
After 116 training step(s), loss on training batch is 0.025721.
After 117 training step(s), loss on training batch is 0.0276851.
After 118 training step(s), loss on training batch is 0.029844.
After 119 training step(s), loss on training batch is 0.0287577.
After 120 training step(s), loss on training batch is 0.0232178.
After 121 training step(s), loss on training batch is 0.0276081.
After 122 training step(s), loss on training batch is 0.0285096.
After 123 training step(s), loss on training batch is 0.0271954.
After 124 training step(s), loss on training batch is 0.0348851.
After 125 training step(s), loss on training batch is 0.0247491.
After 126 training step(s), loss on training batch is 0.0252742.
After 127 training step(s), loss on training batch is 0.0279663.
After 128 training step(s), loss on training batch is 0.0288997.
After 129 training step(s), loss on training batch is 0.024478.
After 130 training step(s), loss on training batch is 0.0265008.
After 131 training step(s), loss on training batch is 0.0284441.
After 132 training step(s), loss on training batch is 0.0275468.
After 133 training step(s), loss on training batch is 0.0221108.
After 134 training step(s), loss on training batch is 0.0266405.
After 135 training step(s), loss on training batch is 0.0273547.
After 136 training step(s), loss on training batch is 0.0257936.
After 137 training step(s), loss on training batch is 0.0331306.
After 138 training step(s), loss on training batch is 0.0234433.
After 139 training step(s), loss on training batch is 0.0241349.
After 140 training step(s), loss on training batch is 0.026892.
After 141 training step(s), loss on training batch is 0.0277611.
After 142 training step(s), loss on training batch is 0.0235714.
After 143 training step(s), loss on training batch is 0.0254914.
After 144 training step(s), loss on training batch is 0.0273223.
After 145 training step(s), loss on training batch is 0.0264659.
After 146 training step(s), loss on training batch is 0.0212153.
After 147 training step(s), loss on training batch is 0.0257726.
After 148 training step(s), loss on training batch is 0.0264148.
After 149 training step(s), loss on training batch is 0.0246356.
After 150 training step(s), loss on training batch is 0.0316604.
After 151 training step(s), loss on training batch is 0.0223462.
After 152 training step(s), loss on training batch is 0.0232149.
After 153 training step(s), loss on training batch is 0.0259697.
After 154 training step(s), loss on training batch is 0.0268296.
After 155 training step(s), loss on training batch is 0.0225786.
After 156 training step(s), loss on training batch is 0.0246152.
After 157 training step(s), loss on training batch is 0.0265292.
After 158 training step(s), loss on training batch is 0.0255968.
After 159 training step(s), loss on training batch is 0.0204704.
After 160 training step(s), loss on training batch is 0.0250438.
After 161 training step(s), loss on training batch is 0.0256352.
After 162 training step(s), loss on training batch is 0.0236471.
After 163 training step(s), loss on training batch is 0.0304548.
After 164 training step(s), loss on training batch is 0.021449.
After 165 training step(s), loss on training batch is 0.0223924.
After 166 training step(s), loss on training batch is 0.0251601.
After 167 training step(s), loss on training batch is 0.0259303.
After 168 training step(s), loss on training batch is 0.0218378.
After 169 training step(s), loss on training batch is 0.0238501.
After 170 training step(s), loss on training batch is 0.025736.
After 171 training step(s), loss on training batch is 0.0248047.
After 172 training step(s), loss on training batch is 0.0199028.
After 173 training step(s), loss on training batch is 0.0243846.
After 174 training step(s), loss on training batch is 0.0248906.
After 175 training step(s), loss on training batch is 0.0227958.
After 176 training step(s), loss on training batch is 0.0293517.
After 177 training step(s), loss on training batch is 0.0206435.
After 178 training step(s), loss on training batch is 0.0216627.
After 179 training step(s), loss on training batch is 0.0244261.
After 180 training step(s), loss on training batch is 0.0251715.
After 181 training step(s), loss on training batch is 0.0212794.
After 182 training step(s), loss on training batch is 0.0231806.
After 183 training step(s), loss on training batch is 0.0250535.
After 184 training step(s), loss on training batch is 0.0241426.
After 185 training step(s), loss on training batch is 0.0192523.
After 186 training step(s), loss on training batch is 0.0238622.
After 187 training step(s), loss on training batch is 0.0242923.
After 188 training step(s), loss on training batch is 0.0220764.
After 189 training step(s), loss on training batch is 0.0283751.
After 190 training step(s), loss on training batch is 0.0199124.
After 191 training step(s), loss on training batch is 0.0209403.
After 192 training step(s), loss on training batch is 0.0237609.
After 193 training step(s), loss on training batch is 0.0244848.
After 194 training step(s), loss on training batch is 0.0207101.
After 195 training step(s), loss on training batch is 0.0225379.
After 196 training step(s), loss on training batch is 0.0244447.
After 197 training step(s), loss on training batch is 0.023483.
After 198 training step(s), loss on training batch is 0.0187118.
After 199 training step(s), loss on training batch is 0.0232933.
After 200 training step(s), loss on training batch is 0.0236942.
After 201 training step(s), loss on training batch is 0.0212946.
After 202 training step(s), loss on training batch is 0.0272839.
After 203 training step(s), loss on training batch is 0.0193577.
After 204 training step(s), loss on training batch is 0.0204588.
After 205 training step(s), loss on training batch is 0.0231026.
After 206 training step(s), loss on training batch is 0.0238126.
After 207 training step(s), loss on training batch is 0.0201431.
After 208 training step(s), loss on training batch is 0.0219578.
After 209 training step(s), loss on training batch is 0.0240049.
After 210 training step(s), loss on training batch is 0.0228864.
After 211 training step(s), loss on training batch is 0.0182257.
After 212 training step(s), loss on training batch is 0.0227828.
After 213 training step(s), loss on training batch is 0.0231639.
After 214 training step(s), loss on training batch is 0.0207129.
After 215 training step(s), loss on training batch is 0.0264729.
After 216 training step(s), loss on training batch is 0.0188368.
After 217 training step(s), loss on training batch is 0.019915.
After 218 training step(s), loss on training batch is 0.0224862.
After 219 training step(s), loss on training batch is 0.0232065.
After 220 training step(s), loss on training batch is 0.019533.
After 221 training step(s), loss on training batch is 0.0213972.
After 222 training step(s), loss on training batch is 0.023478.
After 223 training step(s), loss on training batch is 0.0223703.
After 224 training step(s), loss on training batch is 0.0177908.
After 225 training step(s), loss on training batch is 0.0223131.
After 226 training step(s), loss on training batch is 0.0226485.
After 227 training step(s), loss on training batch is 0.020231.
After 228 training step(s), loss on training batch is 0.0258787.
After 229 training step(s), loss on training batch is 0.0182522.
After 230 training step(s), loss on training batch is 0.019397.
After 231 training step(s), loss on training batch is 0.0219435.
After 232 training step(s), loss on training batch is 0.0226159.
After 233 training step(s), loss on training batch is 0.0190847.
After 234 training step(s), loss on training batch is 0.0209594.
After 235 training step(s), loss on training batch is 0.0230932.
After 236 training step(s), loss on training batch is 0.0218762.
After 237 training step(s), loss on training batch is 0.0174011.
After 238 training step(s), loss on training batch is 0.0218924.
After 239 training step(s), loss on training batch is 0.0221902.
After 240 training step(s), loss on training batch is 0.0196497.
After 241 training step(s), loss on training batch is 0.025091.
After 242 training step(s), loss on training batch is 0.0177979.
After 243 training step(s), loss on training batch is 0.0189543.
After 244 training step(s), loss on training batch is 0.02146.
After 245 training step(s), loss on training batch is 0.0221268.
After 246 training step(s), loss on training batch is 0.0186087.
After 247 training step(s), loss on training batch is 0.0204956.
After 248 training step(s), loss on training batch is 0.0226987.
After 249 training step(s), loss on training batch is 0.0214412.
After 250 training step(s), loss on training batch is 0.0170355.
After 251 training step(s), loss on training batch is 0.0215137.
After 252 training step(s), loss on training batch is 0.0217975.
After 253 training step(s), loss on training batch is 0.0191938.
After 254 training step(s), loss on training batch is 0.0244695.
After 255 training step(s), loss on training batch is 0.0173842.
After 256 training step(s), loss on training batch is 0.0185427.
After 257 training step(s), loss on training batch is 0.0209813.
After 258 training step(s), loss on training batch is 0.0215847.
After 259 training step(s), loss on training batch is 0.0182589.
After 260 training step(s), loss on training batch is 0.0200842.
After 261 training step(s), loss on training batch is 0.0224066.
After 262 training step(s), loss on training batch is 0.0210353.
After 263 training step(s), loss on training batch is 0.0166994.
After 264 training step(s), loss on training batch is 0.0211268.
After 265 training step(s), loss on training batch is 0.021347.
After 266 training step(s), loss on training batch is 0.0187327.
After 267 training step(s), loss on training batch is 0.0237525.
After 268 training step(s), loss on training batch is 0.0170386.
After 269 training step(s), loss on training batch is 0.0181475.
After 270 training step(s), loss on training batch is 0.020528.
After 271 training step(s), loss on training batch is 0.0211517.
After 272 training step(s), loss on training batch is 0.0178044.
After 273 training step(s), loss on training batch is 0.0196917.
After 274 training step(s), loss on training batch is 0.0221385.
After 275 training step(s), loss on training batch is 0.0206611.
After 276 training step(s), loss on training batch is 0.0163646.
After 277 training step(s), loss on training batch is 0.0207448.
After 278 training step(s), loss on training batch is 0.0209572.
After 279 training step(s), loss on training batch is 0.018374.
After 280 training step(s), loss on training batch is 0.0232983.
After 281 training step(s), loss on training batch is 0.0166866.
After 282 training step(s), loss on training batch is 0.0178094.
After 283 training step(s), loss on training batch is 0.0201341.
After 284 training step(s), loss on training batch is 0.0207257.
After 285 training step(s), loss on training batch is 0.0175233.
After 286 training step(s), loss on training batch is 0.01934.
After 287 training step(s), loss on training batch is 0.021859.
After 288 training step(s), loss on training batch is 0.020383.
After 289 training step(s), loss on training batch is 0.016135.
After 290 training step(s), loss on training batch is 0.0204995.
After 291 training step(s), loss on training batch is 0.0206939.
After 292 training step(s), loss on training batch is 0.0179908.
After 293 training step(s), loss on training batch is 0.0226795.
After 294 training step(s), loss on training batch is 0.0163746.
After 295 training step(s), loss on training batch is 0.0174555.
After 296 training step(s), loss on training batch is 0.01978.
After 297 training step(s), loss on training batch is 0.020425.
After 298 training step(s), loss on training batch is 0.0170919.
After 299 training step(s), loss on training batch is 0.0189601.
After 300 training step(s), loss on training batch is 0.0214483.
After 301 training step(s), loss on training batch is 0.0199504.
After 302 training step(s), loss on training batch is 0.015806.
After 303 training step(s), loss on training batch is 0.0201167.
After 304 training step(s), loss on training batch is 0.0203342.
After 305 training step(s), loss on training batch is 0.0176893.
After 306 training step(s), loss on training batch is 0.0222881.
After 307 training step(s), loss on training batch is 0.0160424.
After 308 training step(s), loss on training batch is 0.0170471.
After 309 training step(s), loss on training batch is 0.0194245.
After 310 training step(s), loss on training batch is 0.0200369.
After 311 training step(s), loss on training batch is 0.0167507.
After 312 training step(s), loss on training batch is 0.0186285.
After 313 training step(s), loss on training batch is 0.0213258.
After 314 training step(s), loss on training batch is 0.0196356.
After 315 training step(s), loss on training batch is 0.0155493.
After 316 training step(s), loss on training batch is 0.0198293.
After 317 training step(s), loss on training batch is 0.0200072.
After 318 training step(s), loss on training batch is 0.0174598.
After 319 training step(s), loss on training batch is 0.0220762.
After 320 training step(s), loss on training batch is 0.015782.
After 321 training step(s), loss on training batch is 0.0168221.
After 322 training step(s), loss on training batch is 0.0190327.
After 323 training step(s), loss on training batch is 0.0196445.
After 324 training step(s), loss on training batch is 0.0164609.
After 325 training step(s), loss on training batch is 0.0182854.
After 326 training step(s), loss on training batch is 0.0211135.
After 327 training step(s), loss on training batch is 0.0193975.
After 328 training step(s), loss on training batch is 0.0153883.
After 329 training step(s), loss on training batch is 0.0196043.
After 330 training step(s), loss on training batch is 0.0197394.
After 331 training step(s), loss on training batch is 0.017102.
After 332 training step(s), loss on training batch is 0.0214061.
After 333 training step(s), loss on training batch is 0.0154778.
After 334 training step(s), loss on training batch is 0.0165651.
After 335 training step(s), loss on training batch is 0.0187061.
After 336 training step(s), loss on training batch is 0.0192585.
After 337 training step(s), loss on training batch is 0.0162231.
After 338 training step(s), loss on training batch is 0.0180233.
After 339 training step(s), loss on training batch is 0.0208317.
After 340 training step(s), loss on training batch is 0.0192908.
After 341 training step(s), loss on training batch is 0.015268.
After 342 training step(s), loss on training batch is 0.019435.
After 343 training step(s), loss on training batch is 0.0195621.
After 344 training step(s), loss on training batch is 0.0168108.
After 345 training step(s), loss on training batch is 0.0209187.
After 346 training step(s), loss on training batch is 0.0154387.
After 347 training step(s), loss on training batch is 0.0163982.
After 348 training step(s), loss on training batch is 0.0183911.
After 349 training step(s), loss on training batch is 0.0189241.
After 350 training step(s), loss on training batch is 0.015924.
After 351 training step(s), loss on training batch is 0.0177468.
After 352 training step(s), loss on training batch is 0.0205204.
After 353 training step(s), loss on training batch is 0.0187915.
After 354 training step(s), loss on training batch is 0.014829.
After 355 training step(s), loss on training batch is 0.0190027.
After 356 training step(s), loss on training batch is 0.0191323.
After 357 training step(s), loss on training batch is 0.0166027.
After 358 training step(s), loss on training batch is 0.0207666.
After 359 training step(s), loss on training batch is 0.015112.
After 360 training step(s), loss on training batch is 0.0160932.
After 361 training step(s), loss on training batch is 0.018064.
After 362 training step(s), loss on training batch is 0.0185823.
After 363 training step(s), loss on training batch is 0.0157204.
After 364 training step(s), loss on training batch is 0.0174621.
After 365 training step(s), loss on training batch is 0.0204722.
After 366 training step(s), loss on training batch is 0.0185259.
After 367 training step(s), loss on training batch is 0.0146207.
After 368 training step(s), loss on training batch is 0.0187403.
After 369 training step(s), loss on training batch is 0.01889.
After 370 training step(s), loss on training batch is 0.0163331.
After 371 training step(s), loss on training batch is 0.0203895.
After 372 training step(s), loss on training batch is 0.0148701.
After 373 training step(s), loss on training batch is 0.0158283.
After 374 training step(s), loss on training batch is 0.0177548.
After 375 training step(s), loss on training batch is 0.0182981.
After 376 training step(s), loss on training batch is 0.0153625.
After 377 training step(s), loss on training batch is 0.0171937.
After 378 training step(s), loss on training batch is 0.0202568.
After 379 training step(s), loss on training batch is 0.018277.
After 380 training step(s), loss on training batch is 0.0144221.
After 381 training step(s), loss on training batch is 0.0184883.
After 382 training step(s), loss on training batch is 0.0186184.
After 383 training step(s), loss on training batch is 0.0160994.
After 384 training step(s), loss on training batch is 0.0200303.
After 385 training step(s), loss on training batch is 0.0145803.
After 386 training step(s), loss on training batch is 0.0155379.
After 387 training step(s), loss on training batch is 0.0174404.
After 388 training step(s), loss on training batch is 0.0179429.
After 389 training step(s), loss on training batch is 0.0151252.
After 390 training step(s), loss on training batch is 0.0169502.
After 391 training step(s), loss on training batch is 0.0206107.
After 392 training step(s), loss on training batch is 0.01802.
After 393 training step(s), loss on training batch is 0.0141816.
After 394 training step(s), loss on training batch is 0.018251.
After 395 training step(s), loss on training batch is 0.0183329.
After 396 training step(s), loss on training batch is 0.0158234.
After 397 training step(s), loss on training batch is 0.0196074.
After 398 training step(s), loss on training batch is 0.0144667.
After 399 training step(s), loss on training batch is 0.0153641.
After 400 training step(s), loss on training batch is 0.0171281.
After 401 training step(s), loss on training batch is 0.0176023.
After 402 training step(s), loss on training batch is 0.0149507.
After 403 training step(s), loss on training batch is 0.0167004.
After 404 training step(s), loss on training batch is 0.0201541.
After 405 training step(s), loss on training batch is 0.017813.
After 406 training step(s), loss on training batch is 0.0139968.
After 407 training step(s), loss on training batch is 0.0180294.
After 408 training step(s), loss on training batch is 0.0181045.
After 409 training step(s), loss on training batch is 0.0156008.
After 410 training step(s), loss on training batch is 0.019291.
After 411 training step(s), loss on training batch is 0.0142447.
After 412 training step(s), loss on training batch is 0.0151247.
After 413 training step(s), loss on training batch is 0.0168447.
After 414 training step(s), loss on training batch is 0.0173133.
After 415 training step(s), loss on training batch is 0.014732.
After 416 training step(s), loss on training batch is 0.0164587.
After 417 training step(s), loss on training batch is 0.020089.
After 418 training step(s), loss on training batch is 0.0175722.
After 419 training step(s), loss on training batch is 0.0138054.
After 420 training step(s), loss on training batch is 0.0178037.
After 421 training step(s), loss on training batch is 0.0178666.
After 422 training step(s), loss on training batch is 0.0153748.
After 423 training step(s), loss on training batch is 0.0188849.
After 424 training step(s), loss on training batch is 0.0140788.
After 425 training step(s), loss on training batch is 0.0149187.
After 426 training step(s), loss on training batch is 0.0165656.
After 427 training step(s), loss on training batch is 0.0170022.
After 428 training step(s), loss on training batch is 0.014504.
After 429 training step(s), loss on training batch is 0.0162686.
After 430 training step(s), loss on training batch is 0.0200306.
After 431 training step(s), loss on training batch is 0.0173483.
After 432 training step(s), loss on training batch is 0.0135952.
After 433 training step(s), loss on training batch is 0.0175692.
After 434 training step(s), loss on training batch is 0.0176232.
After 435 training step(s), loss on training batch is 0.015196.
After 436 training step(s), loss on training batch is 0.0187353.
After 437 training step(s), loss on training batch is 0.0137452.
After 438 training step(s), loss on training batch is 0.0146426.
After 439 training step(s), loss on training batch is 0.016302.
After 440 training step(s), loss on training batch is 0.0168019.
After 441 training step(s), loss on training batch is 0.0141815.
After 442 training step(s), loss on training batch is 0.0160531.
After 443 training step(s), loss on training batch is 0.0198308.
After 444 training step(s), loss on training batch is 0.017126.
After 445 training step(s), loss on training batch is 0.0134109.
After 446 training step(s), loss on training batch is 0.017364.
After 447 training step(s), loss on training batch is 0.0173775.
After 448 training step(s), loss on training batch is 0.0149697.
After 449 training step(s), loss on training batch is 0.0183971.
After 450 training step(s), loss on training batch is 0.0136268.
After 451 training step(s), loss on training batch is 0.014472.
After 452 training step(s), loss on training batch is 0.0160295.
After 453 training step(s), loss on training batch is 0.0164872.
After 454 training step(s), loss on training batch is 0.0140577.
After 455 training step(s), loss on training batch is 0.015776.
After 456 training step(s), loss on training batch is 0.0195925.
After 457 training step(s), loss on training batch is 0.016913.
After 458 training step(s), loss on training batch is 0.0132463.
After 459 training step(s), loss on training batch is 0.0171577.
After 460 training step(s), loss on training batch is 0.0171453.
After 461 training step(s), loss on training batch is 0.0148372.
After 462 training step(s), loss on training batch is 0.0182752.
After 463 training step(s), loss on training batch is 0.0133607.
After 464 training step(s), loss on training batch is 0.0141879.
After 465 training step(s), loss on training batch is 0.0158295.
After 466 training step(s), loss on training batch is 0.016294.
After 467 training step(s), loss on training batch is 0.0137059.
After 468 training step(s), loss on training batch is 0.0155486.
After 469 training step(s), loss on training batch is 0.0195318.
After 470 training step(s), loss on training batch is 0.0167109.
After 471 training step(s), loss on training batch is 0.0130596.
After 472 training step(s), loss on training batch is 0.0169398.
After 473 training step(s), loss on training batch is 0.0169244.
After 474 training step(s), loss on training batch is 0.0146043.
After 475 training step(s), loss on training batch is 0.0178537.
After 476 training step(s), loss on training batch is 0.0132513.
After 477 training step(s), loss on training batch is 0.0140486.
After 478 training step(s), loss on training batch is 0.0155328.
After 479 training step(s), loss on training batch is 0.0159552.
After 480 training step(s), loss on training batch is 0.0136153.
After 481 training step(s), loss on training batch is 0.0153487.
After 482 training step(s), loss on training batch is 0.0192779.
After 483 training step(s), loss on training batch is 0.0165005.
After 484 training step(s), loss on training batch is 0.0129097.
After 485 training step(s), loss on training batch is 0.0167418.
After 486 training step(s), loss on training batch is 0.0167055.
After 487 training step(s), loss on training batch is 0.0144156.
After 488 training step(s), loss on training batch is 0.0175759.
After 489 training step(s), loss on training batch is 0.0130764.
After 490 training step(s), loss on training batch is 0.0138587.
After 491 training step(s), loss on training batch is 0.0152629.
After 492 training step(s), loss on training batch is 0.0156519.
After 493 training step(s), loss on training batch is 0.0134545.
After 494 training step(s), loss on training batch is 0.0152022.
After 495 training step(s), loss on training batch is 0.0192599.
After 496 training step(s), loss on training batch is 0.0163355.
After 497 training step(s), loss on training batch is 0.0127787.
After 498 training step(s), loss on training batch is 0.0165523.
After 499 training step(s), loss on training batch is 0.0164859.
After 500 training step(s), loss on training batch is 0.0142252.
After 501 training step(s), loss on training batch is 0.0172056.
After 502 training step(s), loss on training batch is 0.0129539.
After 503 training step(s), loss on training batch is 0.0136791.
After 504 training step(s), loss on training batch is 0.0150157.
After 505 training step(s), loss on training batch is 0.0153994.
After 506 training step(s), loss on training batch is 0.0132796.
After 507 training step(s), loss on training batch is 0.0149361.
After 508 training step(s), loss on training batch is 0.018987.
After 509 training step(s), loss on training batch is 0.0161343.
After 510 training step(s), loss on training batch is 0.0126019.
After 511 training step(s), loss on training batch is 0.016365.
After 512 training step(s), loss on training batch is 0.0163046.
After 513 training step(s), loss on training batch is 0.0140489.
After 514 training step(s), loss on training batch is 0.0169369.
After 515 training step(s), loss on training batch is 0.0127473.
After 516 training step(s), loss on training batch is 0.013496.
After 517 training step(s), loss on training batch is 0.0147834.
After 518 training step(s), loss on training batch is 0.0151463.
After 519 training step(s), loss on training batch is 0.0131523.
After 520 training step(s), loss on training batch is 0.0147215.
After 521 training step(s), loss on training batch is 0.0187242.
After 522 training step(s), loss on training batch is 0.0159617.
After 523 training step(s), loss on training batch is 0.0124979.
After 524 training step(s), loss on training batch is 0.0161889.
After 525 training step(s), loss on training batch is 0.0161325.
After 526 training step(s), loss on training batch is 0.0138862.
After 527 training step(s), loss on training batch is 0.0166061.
After 528 training step(s), loss on training batch is 0.0126282.
After 529 training step(s), loss on training batch is 0.0133004.
After 530 training step(s), loss on training batch is 0.0145439.
After 531 training step(s), loss on training batch is 0.014929.
After 532 training step(s), loss on training batch is 0.0128989.
After 533 training step(s), loss on training batch is 0.0145721.
After 534 training step(s), loss on training batch is 0.0188711.
After 535 training step(s), loss on training batch is 0.0157879.
After 536 training step(s), loss on training batch is 0.0123526.
After 537 training step(s), loss on training batch is 0.0160141.
After 538 training step(s), loss on training batch is 0.0159415.
After 539 training step(s), loss on training batch is 0.0137358.
After 540 training step(s), loss on training batch is 0.0163377.
After 541 training step(s), loss on training batch is 0.0124918.
After 542 training step(s), loss on training batch is 0.0131492.
After 543 training step(s), loss on training batch is 0.0143363.
After 544 training step(s), loss on training batch is 0.0146763.
After 545 training step(s), loss on training batch is 0.0127252.
After 546 training step(s), loss on training batch is 0.0143703.
After 547 training step(s), loss on training batch is 0.0187772.
After 548 training step(s), loss on training batch is 0.0157972.
After 549 training step(s), loss on training batch is 0.0123802.
After 550 training step(s), loss on training batch is 0.0159362.
After 551 training step(s), loss on training batch is 0.0158979.
After 552 training step(s), loss on training batch is 0.013633.
After 553 training step(s), loss on training batch is 0.0159622.
After 554 training step(s), loss on training batch is 0.0124865.
After 555 training step(s), loss on training batch is 0.0130563.
After 556 training step(s), loss on training batch is 0.0141963.
After 557 training step(s), loss on training batch is 0.0145527.
After 558 training step(s), loss on training batch is 0.012636.
After 559 training step(s), loss on training batch is 0.0141859.
After 560 training step(s), loss on training batch is 0.0182555.
After 561 training step(s), loss on training batch is 0.015438.
After 562 training step(s), loss on training batch is 0.012088.
After 563 training step(s), loss on training batch is 0.0156792.
After 564 training step(s), loss on training batch is 0.0155979.
After 565 training step(s), loss on training batch is 0.0134305.
After 566 training step(s), loss on training batch is 0.0159112.
After 567 training step(s), loss on training batch is 0.0122447.
After 568 training step(s), loss on training batch is 0.0128693.
After 569 training step(s), loss on training batch is 0.0139792.
After 570 training step(s), loss on training batch is 0.0143084.
After 571 training step(s), loss on training batch is 0.0124682.
After 572 training step(s), loss on training batch is 0.0140274.
After 573 training step(s), loss on training batch is 0.0181779.
After 574 training step(s), loss on training batch is 0.0152781.
After 575 training step(s), loss on training batch is 0.0119485.
After 576 training step(s), loss on training batch is 0.0155007.
After 577 training step(s), loss on training batch is 0.0153848.
After 578 training step(s), loss on training batch is 0.0132672.
After 579 training step(s), loss on training batch is 0.0157782.
After 580 training step(s), loss on training batch is 0.0120518.
After 581 training step(s), loss on training batch is 0.012701.
After 582 training step(s), loss on training batch is 0.0137816.
After 583 training step(s), loss on training batch is 0.0140934.
After 584 training step(s), loss on training batch is 0.0123177.
After 585 training step(s), loss on training batch is 0.0138611.
After 586 training step(s), loss on training batch is 0.0185622.
After 587 training step(s), loss on training batch is 0.0151041.
After 588 training step(s), loss on training batch is 0.0118046.
After 589 training step(s), loss on training batch is 0.0153394.
After 590 training step(s), loss on training batch is 0.0152229.
After 591 training step(s), loss on training batch is 0.0131353.
After 592 training step(s), loss on training batch is 0.015553.
After 593 training step(s), loss on training batch is 0.0119046.
After 594 training step(s), loss on training batch is 0.0125385.
After 595 training step(s), loss on training batch is 0.0135987.
After 596 training step(s), loss on training batch is 0.0139053.
After 597 training step(s), loss on training batch is 0.0121992.
After 598 training step(s), loss on training batch is 0.0137091.
After 599 training step(s), loss on training batch is 0.0183571.
After 600 training step(s), loss on training batch is 0.0149384.
After 601 training step(s), loss on training batch is 0.0116632.
After 602 training step(s), loss on training batch is 0.0151801.
After 603 training step(s), loss on training batch is 0.0150479.
After 604 training step(s), loss on training batch is 0.012985.
After 605 training step(s), loss on training batch is 0.0153667.
After 606 training step(s), loss on training batch is 0.0117437.
After 607 training step(s), loss on training batch is 0.0123781.
After 608 training step(s), loss on training batch is 0.0134207.
After 609 training step(s), loss on training batch is 0.0137116.
After 610 training step(s), loss on training batch is 0.0119403.
After 611 training step(s), loss on training batch is 0.013537.
After 612 training step(s), loss on training batch is 0.0185438.
After 613 training step(s), loss on training batch is 0.0147691.
After 614 training step(s), loss on training batch is 0.0115244.
After 615 training step(s), loss on training batch is 0.0150284.
After 616 training step(s), loss on training batch is 0.0148814.
After 617 training step(s), loss on training batch is 0.0128829.
After 618 training step(s), loss on training batch is 0.0152011.
After 619 training step(s), loss on training batch is 0.0115835.
After 620 training step(s), loss on training batch is 0.0122084.
After 621 training step(s), loss on training batch is 0.0132328.
After 622 training step(s), loss on training batch is 0.0135261.
After 623 training step(s), loss on training batch is 0.011872.
After 624 training step(s), loss on training batch is 0.0133104.
After 625 training step(s), loss on training batch is 0.0180015.
After 626 training step(s), loss on training batch is 0.0146095.
After 627 training step(s), loss on training batch is 0.0114459.
After 628 training step(s), loss on training batch is 0.0148646.
After 629 training step(s), loss on training batch is 0.0147215.
After 630 training step(s), loss on training batch is 0.0127141.
After 631 training step(s), loss on training batch is 0.014877.
After 632 training step(s), loss on training batch is 0.0115041.
After 633 training step(s), loss on training batch is 0.0120722.
After 634 training step(s), loss on training batch is 0.0130146.
After 635 training step(s), loss on training batch is 0.0132898.
After 636 training step(s), loss on training batch is 0.0116478.
After 637 training step(s), loss on training batch is 0.0131617.
After 638 training step(s), loss on training batch is 0.0180829.
After 639 training step(s), loss on training batch is 0.0144735.
After 640 training step(s), loss on training batch is 0.0113485.
After 641 training step(s), loss on training batch is 0.0147161.
After 642 training step(s), loss on training batch is 0.0145718.
After 643 training step(s), loss on training batch is 0.0126005.
After 644 training step(s), loss on training batch is 0.0145503.
After 645 training step(s), loss on training batch is 0.0113742.
After 646 training step(s), loss on training batch is 0.0119099.
After 647 training step(s), loss on training batch is 0.0128321.
After 648 training step(s), loss on training batch is 0.0131055.
After 649 training step(s), loss on training batch is 0.0115203.
After 650 training step(s), loss on training batch is 0.0129816.
After 651 training step(s), loss on training batch is 0.0180966.
After 652 training step(s), loss on training batch is 0.0142906.
After 653 training step(s), loss on training batch is 0.0111902.
After 654 training step(s), loss on training batch is 0.0145377.
After 655 training step(s), loss on training batch is 0.0144089.
After 656 training step(s), loss on training batch is 0.0124704.
After 657 training step(s), loss on training batch is 0.0143939.
After 658 training step(s), loss on training batch is 0.0112787.
After 659 training step(s), loss on training batch is 0.0117548.
After 660 training step(s), loss on training batch is 0.0126293.
After 661 training step(s), loss on training batch is 0.0129025.
After 662 training step(s), loss on training batch is 0.0113499.
After 663 training step(s), loss on training batch is 0.0127877.
After 664 training step(s), loss on training batch is 0.0179284.
After 665 training step(s), loss on training batch is 0.0141468.
After 666 training step(s), loss on training batch is 0.0111003.
After 667 training step(s), loss on training batch is 0.0143863.
After 668 training step(s), loss on training batch is 0.0142516.
After 669 training step(s), loss on training batch is 0.012341.
After 670 training step(s), loss on training batch is 0.0141244.
After 671 training step(s), loss on training batch is 0.0111795.
After 672 training step(s), loss on training batch is 0.0116373.
After 673 training step(s), loss on training batch is 0.0124437.
After 674 training step(s), loss on training batch is 0.012701.
After 675 training step(s), loss on training batch is 0.0112604.
After 676 training step(s), loss on training batch is 0.01264.
After 677 training step(s), loss on training batch is 0.0179098.
After 678 training step(s), loss on training batch is 0.0139809.
After 679 training step(s), loss on training batch is 0.0109226.
After 680 training step(s), loss on training batch is 0.0142096.
After 681 training step(s), loss on training batch is 0.0140402.
After 682 training step(s), loss on training batch is 0.0121909.
After 683 training step(s), loss on training batch is 0.0140893.
After 684 training step(s), loss on training batch is 0.0108961.
After 685 training step(s), loss on training batch is 0.0114277.
After 686 training step(s), loss on training batch is 0.0122601.
After 687 training step(s), loss on training batch is 0.0125144.
After 688 training step(s), loss on training batch is 0.0110288.
After 689 training step(s), loss on training batch is 0.0124848.
After 690 training step(s), loss on training batch is 0.0178351.
After 691 training step(s), loss on training batch is 0.0138167.
After 692 training step(s), loss on training batch is 0.0107915.
After 693 training step(s), loss on training batch is 0.0140485.
After 694 training step(s), loss on training batch is 0.0138963.
After 695 training step(s), loss on training batch is 0.0120682.
After 696 training step(s), loss on training batch is 0.0137784.
After 697 training step(s), loss on training batch is 0.0108257.
After 698 training step(s), loss on training batch is 0.0112684.
After 699 training step(s), loss on training batch is 0.0120813.
After 700 training step(s), loss on training batch is 0.0123314.
After 701 training step(s), loss on training batch is 0.0108986.
After 702 training step(s), loss on training batch is 0.012298.
After 703 training step(s), loss on training batch is 0.0176786.
After 704 training step(s), loss on training batch is 0.0136708.
After 705 training step(s), loss on training batch is 0.010681.
After 706 training step(s), loss on training batch is 0.0138962.
After 707 training step(s), loss on training batch is 0.0137393.
After 708 training step(s), loss on training batch is 0.0119428.
After 709 training step(s), loss on training batch is 0.0136291.
After 710 training step(s), loss on training batch is 0.010734.
After 711 training step(s), loss on training batch is 0.0111663.
After 712 training step(s), loss on training batch is 0.0118925.
After 713 training step(s), loss on training batch is 0.0121477.
After 714 training step(s), loss on training batch is 0.0107015.
After 715 training step(s), loss on training batch is 0.0121494.
After 716 training step(s), loss on training batch is 0.0175559.
After 717 training step(s), loss on training batch is 0.0135264.
After 718 training step(s), loss on training batch is 0.01056.
After 719 training step(s), loss on training batch is 0.0137524.
After 720 training step(s), loss on training batch is 0.0135904.
After 721 training step(s), loss on training batch is 0.0118207.
After 722 training step(s), loss on training batch is 0.0133761.
After 723 training step(s), loss on training batch is 0.0106007.
After 724 training step(s), loss on training batch is 0.0110153.
After 725 training step(s), loss on training batch is 0.0117227.
After 726 training step(s), loss on training batch is 0.0119485.
After 727 training step(s), loss on training batch is 0.0107018.
After 728 training step(s), loss on training batch is 0.011983.
After 729 training step(s), loss on training batch is 0.0173579.
After 730 training step(s), loss on training batch is 0.0133921.
After 731 training step(s), loss on training batch is 0.0104817.
After 732 training step(s), loss on training batch is 0.0136142.
After 733 training step(s), loss on training batch is 0.0134465.
After 734 training step(s), loss on training batch is 0.0117014.
After 735 training step(s), loss on training batch is 0.0132209.
After 736 training step(s), loss on training batch is 0.0104996.
After 737 training step(s), loss on training batch is 0.0108784.
After 738 training step(s), loss on training batch is 0.0115728.
After 739 training step(s), loss on training batch is 0.0118038.
After 740 training step(s), loss on training batch is 0.0105368.
After 741 training step(s), loss on training batch is 0.0118705.
After 742 training step(s), loss on training batch is 0.0173915.
After 743 training step(s), loss on training batch is 0.013272.
After 744 training step(s), loss on training batch is 0.0103324.
After 745 training step(s), loss on training batch is 0.0134789.
After 746 training step(s), loss on training batch is 0.0132946.
After 747 training step(s), loss on training batch is 0.0115876.
After 748 training step(s), loss on training batch is 0.0131501.
After 749 training step(s), loss on training batch is 0.0103219.
After 750 training step(s), loss on training batch is 0.0107246.
After 751 training step(s), loss on training batch is 0.0114004.
After 752 training step(s), loss on training batch is 0.0116382.
After 753 training step(s), loss on training batch is 0.0103094.
After 754 training step(s), loss on training batch is 0.0117371.
After 755 training step(s), loss on training batch is 0.0172466.
After 756 training step(s), loss on training batch is 0.0131284.
After 757 training step(s), loss on training batch is 0.0102446.
After 758 training step(s), loss on training batch is 0.0133402.
After 759 training step(s), loss on training batch is 0.0131638.
After 760 training step(s), loss on training batch is 0.0114595.
After 761 training step(s), loss on training batch is 0.0128917.
After 762 training step(s), loss on training batch is 0.0102345.
After 763 training step(s), loss on training batch is 0.0106.
After 764 training step(s), loss on training batch is 0.0112615.
After 765 training step(s), loss on training batch is 0.011495.
After 766 training step(s), loss on training batch is 0.0102655.
After 767 training step(s), loss on training batch is 0.0115693.
After 768 training step(s), loss on training batch is 0.0170497.
After 769 training step(s), loss on training batch is 0.0129856.
After 770 training step(s), loss on training batch is 0.0101447.
After 771 training step(s), loss on training batch is 0.0132116.
After 772 training step(s), loss on training batch is 0.0130411.
After 773 training step(s), loss on training batch is 0.0113363.
After 774 training step(s), loss on training batch is 0.0126781.
After 775 training step(s), loss on training batch is 0.0101367.
After 776 training step(s), loss on training batch is 0.0105067.
After 777 training step(s), loss on training batch is 0.0110958.
After 778 training step(s), loss on training batch is 0.0113359.
After 779 training step(s), loss on training batch is 0.0101543.
After 780 training step(s), loss on training batch is 0.0114231.
After 781 training step(s), loss on training batch is 0.0168919.
After 782 training step(s), loss on training batch is 0.0128537.
After 783 training step(s), loss on training batch is 0.0100474.
After 784 training step(s), loss on training batch is 0.0130864.
After 785 training step(s), loss on training batch is 0.0129139.
After 786 training step(s), loss on training batch is 0.0112166.
After 787 training step(s), loss on training batch is 0.0125412.
After 788 training step(s), loss on training batch is 0.00998973.
After 789 training step(s), loss on training batch is 0.0103733.
After 790 training step(s), loss on training batch is 0.0109512.
After 791 training step(s), loss on training batch is 0.0111873.
After 792 training step(s), loss on training batch is 0.0100028.
After 793 training step(s), loss on training batch is 0.0112935.
After 794 training step(s), loss on training batch is 0.0167768.
After 795 training step(s), loss on training batch is 0.0127247.
After 796 training step(s), loss on training batch is 0.00995493.
After 797 training step(s), loss on training batch is 0.0129654.
After 798 training step(s), loss on training batch is 0.0127943.
After 799 training step(s), loss on training batch is 0.0111127.
After 800 training step(s), loss on training batch is 0.0122814.
After 801 training step(s), loss on training batch is 0.00994179.
After 802 training step(s), loss on training batch is 0.0102778.
After 803 training step(s), loss on training batch is 0.0108034.
After 804 training step(s), loss on training batch is 0.0110349.
After 805 training step(s), loss on training batch is 0.00997094.
After 806 training step(s), loss on training batch is 0.0111527.
After 807 training step(s), loss on training batch is 0.0166201.
After 808 training step(s), loss on training batch is 0.0126063.
After 809 training step(s), loss on training batch is 0.00986058.
After 810 training step(s), loss on training batch is 0.0128505.
After 811 training step(s), loss on training batch is 0.0126672.
After 812 training step(s), loss on training batch is 0.0109923.
After 813 training step(s), loss on training batch is 0.0120908.
After 814 training step(s), loss on training batch is 0.00977201.
After 815 training step(s), loss on training batch is 0.0101285.
After 816 training step(s), loss on training batch is 0.0106749.
After 817 training step(s), loss on training batch is 0.0109068.
After 818 training step(s), loss on training batch is 0.00981009.
After 819 training step(s), loss on training batch is 0.011035.
After 820 training step(s), loss on training batch is 0.0165287.
After 821 training step(s), loss on training batch is 0.0124886.
After 822 training step(s), loss on training batch is 0.00976662.
After 823 training step(s), loss on training batch is 0.0127322.
After 824 training step(s), loss on training batch is 0.0125439.
After 825 training step(s), loss on training batch is 0.0108969.
After 826 training step(s), loss on training batch is 0.0119881.
After 827 training step(s), loss on training batch is 0.00968863.
After 828 training step(s), loss on training batch is 0.0100121.
After 829 training step(s), loss on training batch is 0.010538.
After 830 training step(s), loss on training batch is 0.0107715.
After 831 training step(s), loss on training batch is 0.00972994.
After 832 training step(s), loss on training batch is 0.0109135.
After 833 training step(s), loss on training batch is 0.0164005.
After 834 training step(s), loss on training batch is 0.0123717.
After 835 training step(s), loss on training batch is 0.00968577.
After 836 training step(s), loss on training batch is 0.0126155.
After 837 training step(s), loss on training batch is 0.0124246.
After 838 training step(s), loss on training batch is 0.0107931.
After 839 training step(s), loss on training batch is 0.0118028.
After 840 training step(s), loss on training batch is 0.00961532.
After 841 training step(s), loss on training batch is 0.00990811.
After 842 training step(s), loss on training batch is 0.0104038.
After 843 training step(s), loss on training batch is 0.0106544.
After 844 training step(s), loss on training batch is 0.00958059.
After 845 training step(s), loss on training batch is 0.0107923.
After 846 training step(s), loss on training batch is 0.0163337.
After 847 training step(s), loss on training batch is 0.0122508.
After 848 training step(s), loss on training batch is 0.00959464.
After 849 training step(s), loss on training batch is 0.0125016.
After 850 training step(s), loss on training batch is 0.0123122.
After 851 training step(s), loss on training batch is 0.0107039.
After 852 training step(s), loss on training batch is 0.0115787.
After 853 training step(s), loss on training batch is 0.00956722.
After 854 training step(s), loss on training batch is 0.00984096.
After 855 training step(s), loss on training batch is 0.0102828.
After 856 training step(s), loss on training batch is 0.0105107.
After 857 training step(s), loss on training batch is 0.00950581.
After 858 training step(s), loss on training batch is 0.0106749.
After 859 training step(s), loss on training batch is 0.0162044.
After 860 training step(s), loss on training batch is 0.012142.
After 861 training step(s), loss on training batch is 0.00950257.
After 862 training step(s), loss on training batch is 0.0123924.
After 863 training step(s), loss on training batch is 0.012199.
After 864 training step(s), loss on training batch is 0.0105871.
After 865 training step(s), loss on training batch is 0.0114677.
After 866 training step(s), loss on training batch is 0.0094405.
After 867 training step(s), loss on training batch is 0.00971208.
After 868 training step(s), loss on training batch is 0.0101436.
After 869 training step(s), loss on training batch is 0.0104287.
After 870 training step(s), loss on training batch is 0.00937179.
After 871 training step(s), loss on training batch is 0.0105987.
After 872 training step(s), loss on training batch is 0.0161863.
After 873 training step(s), loss on training batch is 0.0120456.
After 874 training step(s), loss on training batch is 0.00941047.
After 875 training step(s), loss on training batch is 0.0122981.
After 876 training step(s), loss on training batch is 0.0120873.
After 877 training step(s), loss on training batch is 0.0104865.
After 878 training step(s), loss on training batch is 0.0113695.
After 879 training step(s), loss on training batch is 0.00932244.
After 880 training step(s), loss on training batch is 0.00961765.
After 881 training step(s), loss on training batch is 0.0100207.
After 882 training step(s), loss on training batch is 0.0102545.
After 883 training step(s), loss on training batch is 0.00931474.
After 884 training step(s), loss on training batch is 0.0104418.
After 885 training step(s), loss on training batch is 0.015998.
After 886 training step(s), loss on training batch is 0.0119235.
After 887 training step(s), loss on training batch is 0.00933887.
After 888 training step(s), loss on training batch is 0.0121848.
After 889 training step(s), loss on training batch is 0.0119838.
After 890 training step(s), loss on training batch is 0.0104034.
After 891 training step(s), loss on training batch is 0.0112471.
After 892 training step(s), loss on training batch is 0.00920129.
After 893 training step(s), loss on training batch is 0.00950832.
After 894 training step(s), loss on training batch is 0.00991305.
After 895 training step(s), loss on training batch is 0.0101436.
After 896 training step(s), loss on training batch is 0.00924703.
After 897 training step(s), loss on training batch is 0.0103389.
After 898 training step(s), loss on training batch is 0.0158849.
After 899 training step(s), loss on training batch is 0.011818.
After 900 training step(s), loss on training batch is 0.00925336.
After 901 training step(s), loss on training batch is 0.0120816.
After 902 training step(s), loss on training batch is 0.0118843.
After 903 training step(s), loss on training batch is 0.010322.
After 904 training step(s), loss on training batch is 0.0110318.
After 905 training step(s), loss on training batch is 0.00919473.
After 906 training step(s), loss on training batch is 0.00944226.
After 907 training step(s), loss on training batch is 0.00979773.
After 908 training step(s), loss on training batch is 0.0100359.
After 909 training step(s), loss on training batch is 0.00913587.
After 910 training step(s), loss on training batch is 0.0102275.
After 911 training step(s), loss on training batch is 0.0158019.
After 912 training step(s), loss on training batch is 0.0117099.
After 913 training step(s), loss on training batch is 0.00916195.
After 914 training step(s), loss on training batch is 0.0119864.
After 915 training step(s), loss on training batch is 0.0117899.
After 916 training step(s), loss on training batch is 0.0102496.
After 917 training step(s), loss on training batch is 0.0109293.
After 918 training step(s), loss on training batch is 0.00908148.
After 919 training step(s), loss on training batch is 0.00930477.
After 920 training step(s), loss on training batch is 0.00970836.
After 921 training step(s), loss on training batch is 0.00993721.
After 922 training step(s), loss on training batch is 0.0090462.
After 923 training step(s), loss on training batch is 0.0101136.
After 924 training step(s), loss on training batch is 0.0157667.
After 925 training step(s), loss on training batch is 0.0116208.
After 926 training step(s), loss on training batch is 0.00909909.
After 927 training step(s), loss on training batch is 0.0118882.
After 928 training step(s), loss on training batch is 0.0116895.
After 929 training step(s), loss on training batch is 0.0101527.
After 930 training step(s), loss on training batch is 0.010811.
After 931 training step(s), loss on training batch is 0.00897682.
After 932 training step(s), loss on training batch is 0.00924836.
After 933 training step(s), loss on training batch is 0.00957507.
After 934 training step(s), loss on training batch is 0.00980622.
After 935 training step(s), loss on training batch is 0.00900226.
After 936 training step(s), loss on training batch is 0.0100169.
After 937 training step(s), loss on training batch is 0.0155979.
After 938 training step(s), loss on training batch is 0.0115297.
After 939 training step(s), loss on training batch is 0.00904992.
After 940 training step(s), loss on training batch is 0.0117869.
After 941 training step(s), loss on training batch is 0.0116001.
After 942 training step(s), loss on training batch is 0.0100874.
After 943 training step(s), loss on training batch is 0.010581.
After 944 training step(s), loss on training batch is 0.00893599.
After 945 training step(s), loss on training batch is 0.00915292.
After 946 training step(s), loss on training batch is 0.00948336.
After 947 training step(s), loss on training batch is 0.00970695.
After 948 training step(s), loss on training batch is 0.00889408.
After 949 training step(s), loss on training batch is 0.0099256.
After 950 training step(s), loss on training batch is 0.0155148.
After 951 training step(s), loss on training batch is 0.0114341.
After 952 training step(s), loss on training batch is 0.00896668.
After 953 training step(s), loss on training batch is 0.0117016.
After 954 training step(s), loss on training batch is 0.0114909.
After 955 training step(s), loss on training batch is 0.00998751.
After 956 training step(s), loss on training batch is 0.0105045.
After 957 training step(s), loss on training batch is 0.00886059.
After 958 training step(s), loss on training batch is 0.00907871.
After 959 training step(s), loss on training batch is 0.00938092.
After 960 training step(s), loss on training batch is 0.00960718.
After 961 training step(s), loss on training batch is 0.00881714.
After 962 training step(s), loss on training batch is 0.00981454.
After 963 training step(s), loss on training batch is 0.0154137.
After 964 training step(s), loss on training batch is 0.0113389.
After 965 training step(s), loss on training batch is 0.00890574.
After 966 training step(s), loss on training batch is 0.0115975.
After 967 training step(s), loss on training batch is 0.0114121.
After 968 training step(s), loss on training batch is 0.0099311.
After 969 training step(s), loss on training batch is 0.0103683.
After 970 training step(s), loss on training batch is 0.00878738.
After 971 training step(s), loss on training batch is 0.00900294.
After 972 training step(s), loss on training batch is 0.00926817.
After 973 training step(s), loss on training batch is 0.00949773.
After 974 training step(s), loss on training batch is 0.00877245.
After 975 training step(s), loss on training batch is 0.00976023.
After 976 training step(s), loss on training batch is 0.0154006.
After 977 training step(s), loss on training batch is 0.0112656.
After 978 training step(s), loss on training batch is 0.0088432.
After 979 training step(s), loss on training batch is 0.0115154.
After 980 training step(s), loss on training batch is 0.0113162.
After 981 training step(s), loss on training batch is 0.00984773.
After 982 training step(s), loss on training batch is 0.0103185.
After 983 training step(s), loss on training batch is 0.00869554.
After 984 training step(s), loss on training batch is 0.00891669.
After 985 training step(s), loss on training batch is 0.00917086.
After 986 training step(s), loss on training batch is 0.00940476.
After 987 training step(s), loss on training batch is 0.00858647.
After 988 training step(s), loss on training batch is 0.00964496.
After 989 training step(s), loss on training batch is 0.0153036.
After 990 training step(s), loss on training batch is 0.0111741.
After 991 training step(s), loss on training batch is 0.00877386.
After 992 training step(s), loss on training batch is 0.0114191.
After 993 training step(s), loss on training batch is 0.0112285.
After 994 training step(s), loss on training batch is 0.00978371.
After 995 training step(s), loss on training batch is 0.010156.
After 996 training step(s), loss on training batch is 0.00865161.
After 997 training step(s), loss on training batch is 0.00882954.
After 998 training step(s), loss on training batch is 0.00907962.
After 999 training step(s), loss on training batch is 0.00933171.
After 1000 training step(s), loss on training batch is 0.00855674.
After 1001 training step(s), loss on training batch is 0.00955413.
After 1002 training step(s), loss on training batch is 0.0152434.
After 1003 training step(s), loss on training batch is 0.0110938.
After 1004 training step(s), loss on training batch is 0.0087101.
After 1005 training step(s), loss on training batch is 0.0113356.
After 1006 training step(s), loss on training batch is 0.0111509.
After 1007 training step(s), loss on training batch is 0.00971792.
After 1008 training step(s), loss on training batch is 0.0100555.
After 1009 training step(s), loss on training batch is 0.00859811.
After 1010 training step(s), loss on training batch is 0.00876333.
After 1011 training step(s), loss on training batch is 0.00897735.
After 1012 training step(s), loss on training batch is 0.00920418.
After 1013 training step(s), loss on training batch is 0.00849717.
After 1014 training step(s), loss on training batch is 0.00947295.
After 1015 training step(s), loss on training batch is 0.0151343.
After 1016 training step(s), loss on training batch is 0.0110194.
After 1017 training step(s), loss on training batch is 0.00866294.
After 1018 training step(s), loss on training batch is 0.0112545.
After 1019 training step(s), loss on training batch is 0.0110755.
After 1020 training step(s), loss on training batch is 0.00965022.
After 1021 training step(s), loss on training batch is 0.00991686.
After 1022 training step(s), loss on training batch is 0.00855208.
After 1023 training step(s), loss on training batch is 0.00872408.
After 1024 training step(s), loss on training batch is 0.00887533.
After 1025 training step(s), loss on training batch is 0.00910586.
After 1026 training step(s), loss on training batch is 0.00848094.
After 1027 training step(s), loss on training batch is 0.00938099.
After 1028 training step(s), loss on training batch is 0.015024.
After 1029 training step(s), loss on training batch is 0.0109421.
After 1030 training step(s), loss on training batch is 0.00860753.
After 1031 training step(s), loss on training batch is 0.0111768.
After 1032 training step(s), loss on training batch is 0.0109983.
After 1033 training step(s), loss on training batch is 0.0095848.
After 1034 training step(s), loss on training batch is 0.00982805.
After 1035 training step(s), loss on training batch is 0.00847389.
After 1036 training step(s), loss on training batch is 0.00864051.
After 1037 training step(s), loss on training batch is 0.00878839.
After 1038 training step(s), loss on training batch is 0.00901908.
After 1039 training step(s), loss on training batch is 0.00836453.
After 1040 training step(s), loss on training batch is 0.00933789.
After 1041 training step(s), loss on training batch is 0.0150176.
After 1042 training step(s), loss on training batch is 0.0108719.
After 1043 training step(s), loss on training batch is 0.00868502.
After 1044 training step(s), loss on training batch is 0.0111059.
After 1045 training step(s), loss on training batch is 0.0109161.
After 1046 training step(s), loss on training batch is 0.00950088.
After 1047 training step(s), loss on training batch is 0.00976393.
After 1048 training step(s), loss on training batch is 0.00837029.
After 1049 training step(s), loss on training batch is 0.00856312.
After 1050 training step(s), loss on training batch is 0.00869997.
After 1051 training step(s), loss on training batch is 0.00893145.
After 1052 training step(s), loss on training batch is 0.00834174.
After 1053 training step(s), loss on training batch is 0.00924318.
After 1054 training step(s), loss on training batch is 0.0149113.
After 1055 training step(s), loss on training batch is 0.0107894.
After 1056 training step(s), loss on training batch is 0.00862045.
After 1057 training step(s), loss on training batch is 0.0110222.
After 1058 training step(s), loss on training batch is 0.0108448.
After 1059 training step(s), loss on training batch is 0.00943852.
After 1060 training step(s), loss on training batch is 0.00963887.
After 1061 training step(s), loss on training batch is 0.00832335.
After 1062 training step(s), loss on training batch is 0.00850056.
After 1063 training step(s), loss on training batch is 0.00862559.
After 1064 training step(s), loss on training batch is 0.00886338.
After 1065 training step(s), loss on training batch is 0.00826043.
After 1066 training step(s), loss on training batch is 0.00915586.
After 1067 training step(s), loss on training batch is 0.0148423.
After 1068 training step(s), loss on training batch is 0.010712.
After 1069 training step(s), loss on training batch is 0.0085553.
After 1070 training step(s), loss on training batch is 0.0109459.
After 1071 training step(s), loss on training batch is 0.0107772.
After 1072 training step(s), loss on training batch is 0.00938705.
After 1073 training step(s), loss on training batch is 0.00949928.
After 1074 training step(s), loss on training batch is 0.00829664.
After 1075 training step(s), loss on training batch is 0.0084487.
After 1076 training step(s), loss on training batch is 0.00853574.
After 1077 training step(s), loss on training batch is 0.00877356.
After 1078 training step(s), loss on training batch is 0.00826473.
After 1079 training step(s), loss on training batch is 0.00904335.
After 1080 training step(s), loss on training batch is 0.0145783.
After 1081 training step(s), loss on training batch is 0.0106469.
After 1082 training step(s), loss on training batch is 0.00840193.
After 1083 training step(s), loss on training batch is 0.0108685.
After 1084 training step(s), loss on training batch is 0.0107219.
After 1085 training step(s), loss on training batch is 0.0093307.
After 1086 training step(s), loss on training batch is 0.00943008.
After 1087 training step(s), loss on training batch is 0.00821769.
After 1088 training step(s), loss on training batch is 0.00836993.
After 1089 training step(s), loss on training batch is 0.00846426.
After 1090 training step(s), loss on training batch is 0.00871113.
After 1091 training step(s), loss on training batch is 0.00815031.
After 1092 training step(s), loss on training batch is 0.00896969.
After 1093 training step(s), loss on training batch is 0.0145259.
After 1094 training step(s), loss on training batch is 0.0105709.
After 1095 training step(s), loss on training batch is 0.00846261.
After 1096 training step(s), loss on training batch is 0.0107911.
After 1097 training step(s), loss on training batch is 0.0106473.
After 1098 training step(s), loss on training batch is 0.0092838.
After 1099 training step(s), loss on training batch is 0.00930648.
After 1100 training step(s), loss on training batch is 0.0082023.
After 1101 training step(s), loss on training batch is 0.00833204.
After 1102 training step(s), loss on training batch is 0.00837963.
After 1103 training step(s), loss on training batch is 0.00862544.
After 1104 training step(s), loss on training batch is 0.00813303.
After 1105 training step(s), loss on training batch is 0.00891044.
After 1106 training step(s), loss on training batch is 0.0144838.
After 1107 training step(s), loss on training batch is 0.010501.
After 1108 training step(s), loss on training batch is 0.00839196.
After 1109 training step(s), loss on training batch is 0.0107278.
After 1110 training step(s), loss on training batch is 0.0105662.
After 1111 training step(s), loss on training batch is 0.00920635.
After 1112 training step(s), loss on training batch is 0.00927404.
After 1113 training step(s), loss on training batch is 0.00809806.
After 1114 training step(s), loss on training batch is 0.00824224.
After 1115 training step(s), loss on training batch is 0.00830617.
After 1116 training step(s), loss on training batch is 0.00855194.
After 1117 training step(s), loss on training batch is 0.00806205.
After 1118 training step(s), loss on training batch is 0.00881367.
After 1119 training step(s), loss on training batch is 0.0143609.
After 1120 training step(s), loss on training batch is 0.010436.
After 1121 training step(s), loss on training batch is 0.00835833.
After 1122 training step(s), loss on training batch is 0.0106487.
After 1123 training step(s), loss on training batch is 0.0105093.
After 1124 training step(s), loss on training batch is 0.00916219.
After 1125 training step(s), loss on training batch is 0.00912934.
After 1126 training step(s), loss on training batch is 0.00808077.
After 1127 training step(s), loss on training batch is 0.00819055.
After 1128 training step(s), loss on training batch is 0.00822972.
After 1129 training step(s), loss on training batch is 0.00847854.
After 1130 training step(s), loss on training batch is 0.00798334.
After 1131 training step(s), loss on training batch is 0.00875903.
After 1132 training step(s), loss on training batch is 0.0143194.
After 1133 training step(s), loss on training batch is 0.0103691.
After 1134 training step(s), loss on training batch is 0.00830309.
After 1135 training step(s), loss on training batch is 0.0105802.
After 1136 training step(s), loss on training batch is 0.0104429.
After 1137 training step(s), loss on training batch is 0.00910284.
After 1138 training step(s), loss on training batch is 0.00905336.
After 1139 training step(s), loss on training batch is 0.00798183.
After 1140 training step(s), loss on training batch is 0.0081248.
After 1141 training step(s), loss on training batch is 0.00816337.
After 1142 training step(s), loss on training batch is 0.00841298.
After 1143 training step(s), loss on training batch is 0.0079354.
After 1144 training step(s), loss on training batch is 0.00867857.
After 1145 training step(s), loss on training batch is 0.0142475.
After 1146 training step(s), loss on training batch is 0.0103045.
After 1147 training step(s), loss on training batch is 0.00825285.
After 1148 training step(s), loss on training batch is 0.0105156.
After 1149 training step(s), loss on training batch is 0.0103803.
After 1150 training step(s), loss on training batch is 0.0090477.
After 1151 training step(s), loss on training batch is 0.00901413.
After 1152 training step(s), loss on training batch is 0.00789882.
After 1153 training step(s), loss on training batch is 0.00806117.
After 1154 training step(s), loss on training batch is 0.00809637.
After 1155 training step(s), loss on training batch is 0.00833668.
After 1156 training step(s), loss on training batch is 0.0078724.
After 1157 training step(s), loss on training batch is 0.00861828.
After 1158 training step(s), loss on training batch is 0.0141789.
After 1159 training step(s), loss on training batch is 0.0102462.
After 1160 training step(s), loss on training batch is 0.0082127.
After 1161 training step(s), loss on training batch is 0.010454.
After 1162 training step(s), loss on training batch is 0.0103268.
After 1163 training step(s), loss on training batch is 0.00900574.
After 1164 training step(s), loss on training batch is 0.00886435.
After 1165 training step(s), loss on training batch is 0.00793718.
After 1166 training step(s), loss on training batch is 0.00804096.
After 1167 training step(s), loss on training batch is 0.00801591.
After 1168 training step(s), loss on training batch is 0.00826301.
After 1169 training step(s), loss on training batch is 0.00786738.
After 1170 training step(s), loss on training batch is 0.00854372.
After 1171 training step(s), loss on training batch is 0.0141012.
After 1172 training step(s), loss on training batch is 0.010184.
After 1173 training step(s), loss on training batch is 0.00816134.
After 1174 training step(s), loss on training batch is 0.0103938.
After 1175 training step(s), loss on training batch is 0.0102612.
After 1176 training step(s), loss on training batch is 0.00894785.
After 1177 training step(s), loss on training batch is 0.00882983.
After 1178 training step(s), loss on training batch is 0.00786364.
After 1179 training step(s), loss on training batch is 0.00796458.
After 1180 training step(s), loss on training batch is 0.00795952.
After 1181 training step(s), loss on training batch is 0.00820655.
After 1182 training step(s), loss on training batch is 0.00775626.
After 1183 training step(s), loss on training batch is 0.00848584.
After 1184 training step(s), loss on training batch is 0.0140682.
After 1185 training step(s), loss on training batch is 0.0101309.
After 1186 training step(s), loss on training batch is 0.00812446.
After 1187 training step(s), loss on training batch is 0.0103291.
After 1188 training step(s), loss on training batch is 0.0102109.
After 1189 training step(s), loss on training batch is 0.00891383.
After 1190 training step(s), loss on training batch is 0.00872986.
After 1191 training step(s), loss on training batch is 0.00783518.
After 1192 training step(s), loss on training batch is 0.00792107.
After 1193 training step(s), loss on training batch is 0.00788917.
After 1194 training step(s), loss on training batch is 0.00812919.
After 1195 training step(s), loss on training batch is 0.00774998.
After 1196 training step(s), loss on training batch is 0.00843043.
After 1197 training step(s), loss on training batch is 0.0140361.
After 1198 training step(s), loss on training batch is 0.0100806.
After 1199 training step(s), loss on training batch is 0.0080809.
After 1200 training step(s), loss on training batch is 0.0102795.
After 1201 training step(s), loss on training batch is 0.0101482.
After 1202 training step(s), loss on training batch is 0.00883889.
After 1203 training step(s), loss on training batch is 0.00865653.
After 1204 training step(s), loss on training batch is 0.00775535.
After 1205 training step(s), loss on training batch is 0.00786923.
After 1206 training step(s), loss on training batch is 0.00782208.
After 1207 training step(s), loss on training batch is 0.00805702.
After 1208 training step(s), loss on training batch is 0.00771533.
After 1209 training step(s), loss on training batch is 0.00836696.
After 1210 training step(s), loss on training batch is 0.0139875.
After 1211 training step(s), loss on training batch is 0.0100279.
After 1212 training step(s), loss on training batch is 0.00802934.
After 1213 training step(s), loss on training batch is 0.0102246.
After 1214 training step(s), loss on training batch is 0.0100947.
After 1215 training step(s), loss on training batch is 0.00879142.
After 1216 training step(s), loss on training batch is 0.00859705.
After 1217 training step(s), loss on training batch is 0.00770771.
After 1218 training step(s), loss on training batch is 0.00781055.
After 1219 training step(s), loss on training batch is 0.0077689.
After 1220 training step(s), loss on training batch is 0.00800252.
After 1221 training step(s), loss on training batch is 0.00763506.
After 1222 training step(s), loss on training batch is 0.00830986.
After 1223 training step(s), loss on training batch is 0.0138919.
After 1224 training step(s), loss on training batch is 0.00997144.
After 1225 training step(s), loss on training batch is 0.00799686.
After 1226 training step(s), loss on training batch is 0.0101539.
After 1227 training step(s), loss on training batch is 0.0100455.
After 1228 training step(s), loss on training batch is 0.00874546.
After 1229 training step(s), loss on training batch is 0.00851808.
After 1230 training step(s), loss on training batch is 0.00766628.
After 1231 training step(s), loss on training batch is 0.0077759.
After 1232 training step(s), loss on training batch is 0.00771248.
After 1233 training step(s), loss on training batch is 0.00794734.
After 1234 training step(s), loss on training batch is 0.00759574.
After 1235 training step(s), loss on training batch is 0.00824759.
After 1236 training step(s), loss on training batch is 0.0138499.
After 1237 training step(s), loss on training batch is 0.00991454.
After 1238 training step(s), loss on training batch is 0.00794474.
After 1239 training step(s), loss on training batch is 0.0100997.
After 1240 training step(s), loss on training batch is 0.00998924.
After 1241 training step(s), loss on training batch is 0.00869553.
After 1242 training step(s), loss on training batch is 0.00847542.
After 1243 training step(s), loss on training batch is 0.00759781.
After 1244 training step(s), loss on training batch is 0.00771333.
After 1245 training step(s), loss on training batch is 0.00765263.
After 1246 training step(s), loss on training batch is 0.0078903.
After 1247 training step(s), loss on training batch is 0.00751586.
After 1248 training step(s), loss on training batch is 0.00821297.
After 1249 training step(s), loss on training batch is 0.013826.
After 1250 training step(s), loss on training batch is 0.0098693.
After 1251 training step(s), loss on training batch is 0.0079049.
After 1252 training step(s), loss on training batch is 0.0100528.
After 1253 training step(s), loss on training batch is 0.00993788.
After 1254 training step(s), loss on training batch is 0.00864802.
After 1255 training step(s), loss on training batch is 0.00837316.
After 1256 training step(s), loss on training batch is 0.0075844.
After 1257 training step(s), loss on training batch is 0.0076862.
After 1258 training step(s), loss on training batch is 0.00758243.
After 1259 training step(s), loss on training batch is 0.00781351.
After 1260 training step(s), loss on training batch is 0.00753264.
After 1261 training step(s), loss on training batch is 0.0081356.
After 1262 training step(s), loss on training batch is 0.0137347.
After 1263 training step(s), loss on training batch is 0.00981851.
After 1264 training step(s), loss on training batch is 0.00786584.
After 1265 training step(s), loss on training batch is 0.00999176.
After 1266 training step(s), loss on training batch is 0.00988525.
After 1267 training step(s), loss on training batch is 0.00861531.
After 1268 training step(s), loss on training batch is 0.00832246.
After 1269 training step(s), loss on training batch is 0.00749749.
After 1270 training step(s), loss on training batch is 0.00762026.
After 1271 training step(s), loss on training batch is 0.00753383.
After 1272 training step(s), loss on training batch is 0.00776581.
After 1273 training step(s), loss on training batch is 0.00745304.
After 1274 training step(s), loss on training batch is 0.00808859.
After 1275 training step(s), loss on training batch is 0.0136967.
After 1276 training step(s), loss on training batch is 0.00977378.
After 1277 training step(s), loss on training batch is 0.00782697.
After 1278 training step(s), loss on training batch is 0.0099452.
After 1279 training step(s), loss on training batch is 0.00983802.
After 1280 training step(s), loss on training batch is 0.00856147.
After 1281 training step(s), loss on training batch is 0.00824801.
After 1282 training step(s), loss on training batch is 0.0074926.
After 1283 training step(s), loss on training batch is 0.00758385.
After 1284 training step(s), loss on training batch is 0.00747104.
After 1285 training step(s), loss on training batch is 0.00770126.
After 1286 training step(s), loss on training batch is 0.00742408.
After 1287 training step(s), loss on training batch is 0.00803124.
After 1288 training step(s), loss on training batch is 0.0136387.
After 1289 training step(s), loss on training batch is 0.00972347.
After 1290 training step(s), loss on training batch is 0.00779456.
After 1291 training step(s), loss on training batch is 0.00988376.
After 1292 training step(s), loss on training batch is 0.0097982.
After 1293 training step(s), loss on training batch is 0.00853218.
After 1294 training step(s), loss on training batch is 0.00820401.
After 1295 training step(s), loss on training batch is 0.00745564.
After 1296 training step(s), loss on training batch is 0.0075445.
After 1297 training step(s), loss on training batch is 0.00742395.
After 1298 training step(s), loss on training batch is 0.00765792.
After 1299 training step(s), loss on training batch is 0.00736245.
After 1300 training step(s), loss on training batch is 0.00798231.
After 1301 training step(s), loss on training batch is 0.013648.
After 1302 training step(s), loss on training batch is 0.00966121.
After 1303 training step(s), loss on training batch is 0.0077361.
After 1304 training step(s), loss on training batch is 0.00983274.
After 1305 training step(s), loss on training batch is 0.00974271.
After 1306 training step(s), loss on training batch is 0.0084948.
After 1307 training step(s), loss on training batch is 0.00816258.
After 1308 training step(s), loss on training batch is 0.00739212.
After 1309 training step(s), loss on training batch is 0.00747984.
After 1310 training step(s), loss on training batch is 0.00739802.
After 1311 training step(s), loss on training batch is 0.00764905.
After 1312 training step(s), loss on training batch is 0.00727911.
After 1313 training step(s), loss on training batch is 0.00796804.
After 1314 training step(s), loss on training batch is 0.0136429.
After 1315 training step(s), loss on training batch is 0.00961779.
After 1316 training step(s), loss on training batch is 0.00769926.
After 1317 training step(s), loss on training batch is 0.00979224.
After 1318 training step(s), loss on training batch is 0.00969256.
After 1319 training step(s), loss on training batch is 0.00845032.
After 1320 training step(s), loss on training batch is 0.00810884.
After 1321 training step(s), loss on training batch is 0.0073619.
After 1322 training step(s), loss on training batch is 0.00745353.
After 1323 training step(s), loss on training batch is 0.00731827.
After 1324 training step(s), loss on training batch is 0.00755409.
After 1325 training step(s), loss on training batch is 0.0072722.
After 1326 training step(s), loss on training batch is 0.00787437.
After 1327 training step(s), loss on training batch is 0.0135042.
After 1328 training step(s), loss on training batch is 0.00957471.
After 1329 training step(s), loss on training batch is 0.00767406.
After 1330 training step(s), loss on training batch is 0.00973281.
After 1331 training step(s), loss on training batch is 0.00965045.
After 1332 training step(s), loss on training batch is 0.00841104.
After 1333 training step(s), loss on training batch is 0.00802035.
After 1334 training step(s), loss on training batch is 0.00734873.
After 1335 training step(s), loss on training batch is 0.00741076.
After 1336 training step(s), loss on training batch is 0.00726297.
After 1337 training step(s), loss on training batch is 0.00749324.
After 1338 training step(s), loss on training batch is 0.00725485.
After 1339 training step(s), loss on training batch is 0.00782247.
After 1340 training step(s), loss on training batch is 0.0134556.
After 1341 training step(s), loss on training batch is 0.00953322.
After 1342 training step(s), loss on training batch is 0.00764152.
After 1343 training step(s), loss on training batch is 0.00968006.
After 1344 training step(s), loss on training batch is 0.00960824.
After 1345 training step(s), loss on training batch is 0.00837674.
After 1346 training step(s), loss on training batch is 0.00795507.
After 1347 training step(s), loss on training batch is 0.00731902.
After 1348 training step(s), loss on training batch is 0.00738443.
After 1349 training step(s), loss on training batch is 0.00721361.
After 1350 training step(s), loss on training batch is 0.00743908.
After 1351 training step(s), loss on training batch is 0.00723512.
After 1352 training step(s), loss on training batch is 0.00777337.
After 1353 training step(s), loss on training batch is 0.0133893.
After 1354 training step(s), loss on training batch is 0.00949101.
After 1355 training step(s), loss on training batch is 0.00761301.
After 1356 training step(s), loss on training batch is 0.00963029.
After 1357 training step(s), loss on training batch is 0.00956141.
After 1358 training step(s), loss on training batch is 0.00832513.
After 1359 training step(s), loss on training batch is 0.00790949.
After 1360 training step(s), loss on training batch is 0.00724337.
After 1361 training step(s), loss on training batch is 0.00733316.
After 1362 training step(s), loss on training batch is 0.00716979.
After 1363 training step(s), loss on training batch is 0.00740715.
After 1364 training step(s), loss on training batch is 0.00711873.
After 1365 training step(s), loss on training batch is 0.00775895.
After 1366 training step(s), loss on training batch is 0.0134217.
After 1367 training step(s), loss on training batch is 0.00944213.
After 1368 training step(s), loss on training batch is 0.00756943.
After 1369 training step(s), loss on training batch is 0.00959028.
After 1370 training step(s), loss on training batch is 0.00951662.
After 1371 training step(s), loss on training batch is 0.00828799.
After 1372 training step(s), loss on training batch is 0.00786414.
After 1373 training step(s), loss on training batch is 0.00721012.
After 1374 training step(s), loss on training batch is 0.00729201.
After 1375 training step(s), loss on training batch is 0.00711992.
After 1376 training step(s), loss on training batch is 0.00734692.
After 1377 training step(s), loss on training batch is 0.00713728.
After 1378 training step(s), loss on training batch is 0.00768808.
After 1379 training step(s), loss on training batch is 0.0133686.
After 1380 training step(s), loss on training batch is 0.0094.
After 1381 training step(s), loss on training batch is 0.00753768.
After 1382 training step(s), loss on training batch is 0.00954805.
After 1383 training step(s), loss on training batch is 0.00947756.
After 1384 training step(s), loss on training batch is 0.00825194.
After 1385 training step(s), loss on training batch is 0.00780952.
After 1386 training step(s), loss on training batch is 0.00716263.
After 1387 training step(s), loss on training batch is 0.00724683.
After 1388 training step(s), loss on training batch is 0.00707602.
After 1389 training step(s), loss on training batch is 0.00729849.
After 1390 training step(s), loss on training batch is 0.00708664.
After 1391 training step(s), loss on training batch is 0.00764425.
After 1392 training step(s), loss on training batch is 0.0132933.
After 1393 training step(s), loss on training batch is 0.00936044.
After 1394 training step(s), loss on training batch is 0.00750688.
After 1395 training step(s), loss on training batch is 0.00950192.
After 1396 training step(s), loss on training batch is 0.00943536.
After 1397 training step(s), loss on training batch is 0.00821952.
After 1398 training step(s), loss on training batch is 0.00773862.
After 1399 training step(s), loss on training batch is 0.00715398.
After 1400 training step(s), loss on training batch is 0.00722322.
After 1401 training step(s), loss on training batch is 0.00702991.
After 1402 training step(s), loss on training batch is 0.00724892.
After 1403 training step(s), loss on training batch is 0.00703954.
After 1404 training step(s), loss on training batch is 0.00761209.
After 1405 training step(s), loss on training batch is 0.0132724.
After 1406 training step(s), loss on training batch is 0.00932121.
After 1407 training step(s), loss on training batch is 0.00747624.
After 1408 training step(s), loss on training batch is 0.00945885.
After 1409 training step(s), loss on training batch is 0.00940154.
After 1410 training step(s), loss on training batch is 0.00817956.
After 1411 training step(s), loss on training batch is 0.00767877.
After 1412 training step(s), loss on training batch is 0.00710906.
After 1413 training step(s), loss on training batch is 0.00718263.
After 1414 training step(s), loss on training batch is 0.00698273.
After 1415 training step(s), loss on training batch is 0.00719744.
After 1416 training step(s), loss on training batch is 0.00703521.
After 1417 training step(s), loss on training batch is 0.00756075.
After 1418 training step(s), loss on training batch is 0.0132027.
After 1419 training step(s), loss on training batch is 0.00928268.
After 1420 training step(s), loss on training batch is 0.00743528.
After 1421 training step(s), loss on training batch is 0.00941423.
After 1422 training step(s), loss on training batch is 0.00935455.
After 1423 training step(s), loss on training batch is 0.00814775.
After 1424 training step(s), loss on training batch is 0.00766171.
After 1425 training step(s), loss on training batch is 0.00704158.
After 1426 training step(s), loss on training batch is 0.00712684.
After 1427 training step(s), loss on training batch is 0.00695624.
After 1428 training step(s), loss on training batch is 0.00718144.
After 1429 training step(s), loss on training batch is 0.00697259.
After 1430 training step(s), loss on training batch is 0.00750812.
After 1431 training step(s), loss on training batch is 0.0131549.
After 1432 training step(s), loss on training batch is 0.00922693.
After 1433 training step(s), loss on training batch is 0.00739814.
After 1434 training step(s), loss on training batch is 0.00936309.
After 1435 training step(s), loss on training batch is 0.00931561.
After 1436 training step(s), loss on training batch is 0.0081152.
After 1437 training step(s), loss on training batch is 0.00759677.
After 1438 training step(s), loss on training batch is 0.00703777.
After 1439 training step(s), loss on training batch is 0.00710653.
After 1440 training step(s), loss on training batch is 0.00691746.
After 1441 training step(s), loss on training batch is 0.00713812.
After 1442 training step(s), loss on training batch is 0.00694533.
After 1443 training step(s), loss on training batch is 0.0074643.
After 1444 training step(s), loss on training batch is 0.0131188.
After 1445 training step(s), loss on training batch is 0.00918461.
After 1446 training step(s), loss on training batch is 0.00736736.
After 1447 training step(s), loss on training batch is 0.00931963.
After 1448 training step(s), loss on training batch is 0.0092791.
After 1449 training step(s), loss on training batch is 0.00808986.
After 1450 training step(s), loss on training batch is 0.00754213.
After 1451 training step(s), loss on training batch is 0.00702052.
After 1452 training step(s), loss on training batch is 0.00707533.
After 1453 training step(s), loss on training batch is 0.00687363.
After 1454 training step(s), loss on training batch is 0.00708726.
After 1455 training step(s), loss on training batch is 0.00692688.
After 1456 training step(s), loss on training batch is 0.00742533.
After 1457 training step(s), loss on training batch is 0.0130724.
After 1458 training step(s), loss on training batch is 0.00915427.
After 1459 training step(s), loss on training batch is 0.00734709.
After 1460 training step(s), loss on training batch is 0.0092815.
After 1461 training step(s), loss on training batch is 0.00923848.
After 1462 training step(s), loss on training batch is 0.0080523.
After 1463 training step(s), loss on training batch is 0.00752379.
After 1464 training step(s), loss on training batch is 0.006942.
After 1465 training step(s), loss on training batch is 0.00701676.
After 1466 training step(s), loss on training batch is 0.0068283.
After 1467 training step(s), loss on training batch is 0.00705014.
After 1468 training step(s), loss on training batch is 0.00683104.
After 1469 training step(s), loss on training batch is 0.00740125.
After 1470 training step(s), loss on training batch is 0.0130805.
After 1471 training step(s), loss on training batch is 0.00911895.
After 1472 training step(s), loss on training batch is 0.00731493.
After 1473 training step(s), loss on training batch is 0.00924617.
After 1474 training step(s), loss on training batch is 0.00919922.
After 1475 training step(s), loss on training batch is 0.00801423.
After 1476 training step(s), loss on training batch is 0.00746588.
After 1477 training step(s), loss on training batch is 0.00693045.
After 1478 training step(s), loss on training batch is 0.00699086.
After 1479 training step(s), loss on training batch is 0.00678038.
After 1480 training step(s), loss on training batch is 0.00699588.
After 1481 training step(s), loss on training batch is 0.006819.
After 1482 training step(s), loss on training batch is 0.00736206.
After 1483 training step(s), loss on training batch is 0.0130502.
After 1484 training step(s), loss on training batch is 0.00908211.
After 1485 training step(s), loss on training batch is 0.0072842.
After 1486 training step(s), loss on training batch is 0.00920781.
After 1487 training step(s), loss on training batch is 0.00916197.
After 1488 training step(s), loss on training batch is 0.00798234.
After 1489 training step(s), loss on training batch is 0.00740088.
After 1490 training step(s), loss on training batch is 0.00692371.
After 1491 training step(s), loss on training batch is 0.00697952.
After 1492 training step(s), loss on training batch is 0.00674278.
After 1493 training step(s), loss on training batch is 0.00694671.
After 1494 training step(s), loss on training batch is 0.00683288.
After 1495 training step(s), loss on training batch is 0.00731466.
After 1496 training step(s), loss on training batch is 0.0129716.
After 1497 training step(s), loss on training batch is 0.0090525.
After 1498 training step(s), loss on training batch is 0.00726461.
After 1499 training step(s), loss on training batch is 0.00916291.
After 1500 training step(s), loss on training batch is 0.00912934.
After 1501 training step(s), loss on training batch is 0.00794912.
After 1502 training step(s), loss on training batch is 0.00735506.
After 1503 training step(s), loss on training batch is 0.00688093.
After 1504 training step(s), loss on training batch is 0.0069394.
After 1505 training step(s), loss on training batch is 0.00670184.
After 1506 training step(s), loss on training batch is 0.00690648.
After 1507 training step(s), loss on training batch is 0.00678243.
After 1508 training step(s), loss on training batch is 0.00728453.
After 1509 training step(s), loss on training batch is 0.0129499.
After 1510 training step(s), loss on training batch is 0.00901685.
After 1511 training step(s), loss on training batch is 0.0072396.
After 1512 training step(s), loss on training batch is 0.00912463.
After 1513 training step(s), loss on training batch is 0.00909583.
After 1514 training step(s), loss on training batch is 0.00792094.
After 1515 training step(s), loss on training batch is 0.00730679.
After 1516 training step(s), loss on training batch is 0.00686335.
After 1517 training step(s), loss on training batch is 0.0069156.
After 1518 training step(s), loss on training batch is 0.00666254.
After 1519 training step(s), loss on training batch is 0.00686339.
After 1520 training step(s), loss on training batch is 0.00675891.
After 1521 training step(s), loss on training batch is 0.00725011.
After 1522 training step(s), loss on training batch is 0.0129194.
After 1523 training step(s), loss on training batch is 0.00898712.
After 1524 training step(s), loss on training batch is 0.00721369.
After 1525 training step(s), loss on training batch is 0.00910328.
After 1526 training step(s), loss on training batch is 0.00906057.
After 1527 training step(s), loss on training batch is 0.00788237.
After 1528 training step(s), loss on training batch is 0.00726747.
After 1529 training step(s), loss on training batch is 0.00683235.
After 1530 training step(s), loss on training batch is 0.00688764.
After 1531 training step(s), loss on training batch is 0.00662151.
After 1532 training step(s), loss on training batch is 0.00682079.
After 1533 training step(s), loss on training batch is 0.00673798.
After 1534 training step(s), loss on training batch is 0.00720888.
After 1535 training step(s), loss on training batch is 0.0128666.
After 1536 training step(s), loss on training batch is 0.00895398.
After 1537 training step(s), loss on training batch is 0.00718875.
After 1538 training step(s), loss on training batch is 0.00905516.
After 1539 training step(s), loss on training batch is 0.00902733.
After 1540 training step(s), loss on training batch is 0.00785543.
After 1541 training step(s), loss on training batch is 0.00723236.
After 1542 training step(s), loss on training batch is 0.00679057.
After 1543 training step(s), loss on training batch is 0.00685259.
After 1544 training step(s), loss on training batch is 0.00658105.
After 1545 training step(s), loss on training batch is 0.00678801.
After 1546 training step(s), loss on training batch is 0.00665139.
After 1547 training step(s), loss on training batch is 0.00720244.
After 1548 training step(s), loss on training batch is 0.0128867.
After 1549 training step(s), loss on training batch is 0.00893069.
After 1550 training step(s), loss on training batch is 0.00716995.
After 1551 training step(s), loss on training batch is 0.00903969.
After 1552 training step(s), loss on training batch is 0.00899551.
After 1553 training step(s), loss on training batch is 0.00781664.
After 1554 training step(s), loss on training batch is 0.00718315.
After 1555 training step(s), loss on training batch is 0.0067728.
After 1556 training step(s), loss on training batch is 0.00683514.
After 1557 training step(s), loss on training batch is 0.00654281.
After 1558 training step(s), loss on training batch is 0.00674262.
After 1559 training step(s), loss on training batch is 0.00666609.
After 1560 training step(s), loss on training batch is 0.00714663.
After 1561 training step(s), loss on training batch is 0.0128237.
After 1562 training step(s), loss on training batch is 0.00889181.
After 1563 training step(s), loss on training batch is 0.00713996.
After 1564 training step(s), loss on training batch is 0.00898447.
After 1565 training step(s), loss on training batch is 0.00896938.
After 1566 training step(s), loss on training batch is 0.00779948.
After 1567 training step(s), loss on training batch is 0.00714227.
After 1568 training step(s), loss on training batch is 0.0067469.
After 1569 training step(s), loss on training batch is 0.00679271.
After 1570 training step(s), loss on training batch is 0.00650367.
After 1571 training step(s), loss on training batch is 0.00670093.
After 1572 training step(s), loss on training batch is 0.00662862.
After 1573 training step(s), loss on training batch is 0.00711.
After 1574 training step(s), loss on training batch is 0.0127907.
After 1575 training step(s), loss on training batch is 0.008863.
After 1576 training step(s), loss on training batch is 0.00712544.
After 1577 training step(s), loss on training batch is 0.00896731.
After 1578 training step(s), loss on training batch is 0.00892952.
After 1579 training step(s), loss on training batch is 0.00776062.
After 1580 training step(s), loss on training batch is 0.00710318.
After 1581 training step(s), loss on training batch is 0.00670276.
After 1582 training step(s), loss on training batch is 0.00676159.
After 1583 training step(s), loss on training batch is 0.00647067.
After 1584 training step(s), loss on training batch is 0.0066749.
After 1585 training step(s), loss on training batch is 0.00652325.
After 1586 training step(s), loss on training batch is 0.0071257.
After 1587 training step(s), loss on training batch is 0.0128412.
After 1588 training step(s), loss on training batch is 0.00883057.
After 1589 training step(s), loss on training batch is 0.00709132.
After 1590 training step(s), loss on training batch is 0.00894149.
After 1591 training step(s), loss on training batch is 0.00889587.
After 1592 training step(s), loss on training batch is 0.0077338.
After 1593 training step(s), loss on training batch is 0.007097.
After 1594 training step(s), loss on training batch is 0.00665336.
After 1595 training step(s), loss on training batch is 0.00671844.
After 1596 training step(s), loss on training batch is 0.00644039.
After 1597 training step(s), loss on training batch is 0.00664891.
After 1598 training step(s), loss on training batch is 0.00652689.
After 1599 training step(s), loss on training batch is 0.00705629.
After 1600 training step(s), loss on training batch is 0.012785.
After 1601 training step(s), loss on training batch is 0.00878923.
After 1602 training step(s), loss on training batch is 0.00704516.
After 1603 training step(s), loss on training batch is 0.00888905.
After 1604 training step(s), loss on training batch is 0.00885991.
After 1605 training step(s), loss on training batch is 0.00772654.
After 1606 training step(s), loss on training batch is 0.00704708.
After 1607 training step(s), loss on training batch is 0.00666437.
After 1608 training step(s), loss on training batch is 0.00670237.
After 1609 training step(s), loss on training batch is 0.00641205.
After 1610 training step(s), loss on training batch is 0.00661515.
After 1611 training step(s), loss on training batch is 0.00652842.
After 1612 training step(s), loss on training batch is 0.00699701.
After 1613 training step(s), loss on training batch is 0.012695.
After 1614 training step(s), loss on training batch is 0.00875004.
After 1615 training step(s), loss on training batch is 0.00701488.
After 1616 training step(s), loss on training batch is 0.00885021.
After 1617 training step(s), loss on training batch is 0.00883285.
After 1618 training step(s), loss on training batch is 0.00770595.
After 1619 training step(s), loss on training batch is 0.00701562.
After 1620 training step(s), loss on training batch is 0.00663734.
After 1621 training step(s), loss on training batch is 0.00667304.
After 1622 training step(s), loss on training batch is 0.00638902.
After 1623 training step(s), loss on training batch is 0.00659284.
After 1624 training step(s), loss on training batch is 0.00651732.
After 1625 training step(s), loss on training batch is 0.00695931.
After 1626 training step(s), loss on training batch is 0.0126564.
After 1627 training step(s), loss on training batch is 0.00872007.
After 1628 training step(s), loss on training batch is 0.00699861.
After 1629 training step(s), loss on training batch is 0.00880733.
After 1630 training step(s), loss on training batch is 0.00881122.
After 1631 training step(s), loss on training batch is 0.00769096.
After 1632 training step(s), loss on training batch is 0.0069574.
After 1633 training step(s), loss on training batch is 0.00664691.
After 1634 training step(s), loss on training batch is 0.0066604.
After 1635 training step(s), loss on training batch is 0.00636025.
After 1636 training step(s), loss on training batch is 0.0065576.
After 1637 training step(s), loss on training batch is 0.00649724.
After 1638 training step(s), loss on training batch is 0.00692974.
After 1639 training step(s), loss on training batch is 0.0126247.
After 1640 training step(s), loss on training batch is 0.00869267.
After 1641 training step(s), loss on training batch is 0.00697629.
After 1642 training step(s), loss on training batch is 0.00877712.
After 1643 training step(s), loss on training batch is 0.00878159.
After 1644 training step(s), loss on training batch is 0.00766075.
After 1645 training step(s), loss on training batch is 0.00692626.
After 1646 training step(s), loss on training batch is 0.00660512.
After 1647 training step(s), loss on training batch is 0.00662923.
After 1648 training step(s), loss on training batch is 0.00632613.
After 1649 training step(s), loss on training batch is 0.00652865.
After 1650 training step(s), loss on training batch is 0.00647038.
After 1651 training step(s), loss on training batch is 0.00689714.
After 1652 training step(s), loss on training batch is 0.0125866.
After 1653 training step(s), loss on training batch is 0.00866619.
After 1654 training step(s), loss on training batch is 0.00695521.
After 1655 training step(s), loss on training batch is 0.00875214.
After 1656 training step(s), loss on training batch is 0.00875175.
After 1657 training step(s), loss on training batch is 0.00763544.
After 1658 training step(s), loss on training batch is 0.00688902.
After 1659 training step(s), loss on training batch is 0.00658502.
After 1660 training step(s), loss on training batch is 0.00660505.
After 1661 training step(s), loss on training batch is 0.00629551.
After 1662 training step(s), loss on training batch is 0.00649511.
After 1663 training step(s), loss on training batch is 0.00645253.
After 1664 training step(s), loss on training batch is 0.00687667.
After 1665 training step(s), loss on training batch is 0.0125845.
After 1666 training step(s), loss on training batch is 0.00864118.
After 1667 training step(s), loss on training batch is 0.00693164.
After 1668 training step(s), loss on training batch is 0.00872802.
After 1669 training step(s), loss on training batch is 0.00872409.
After 1670 training step(s), loss on training batch is 0.00760393.
After 1671 training step(s), loss on training batch is 0.0068597.
After 1672 training step(s), loss on training batch is 0.0065367.
After 1673 training step(s), loss on training batch is 0.00657179.
After 1674 training step(s), loss on training batch is 0.00626142.
After 1675 training step(s), loss on training batch is 0.0064582.
After 1676 training step(s), loss on training batch is 0.00641231.
After 1677 training step(s), loss on training batch is 0.00684892.
After 1678 training step(s), loss on training batch is 0.0125438.
After 1679 training step(s), loss on training batch is 0.00861562.
After 1680 training step(s), loss on training batch is 0.00691528.
After 1681 training step(s), loss on training batch is 0.00869385.
After 1682 training step(s), loss on training batch is 0.00870074.
After 1683 training step(s), loss on training batch is 0.00757881.
After 1684 training step(s), loss on training batch is 0.00681672.
After 1685 training step(s), loss on training batch is 0.00652648.
After 1686 training step(s), loss on training batch is 0.00654454.
After 1687 training step(s), loss on training batch is 0.00622456.
After 1688 training step(s), loss on training batch is 0.00642689.
After 1689 training step(s), loss on training batch is 0.00634506.
After 1690 training step(s), loss on training batch is 0.00685006.
After 1691 training step(s), loss on training batch is 0.0125856.
After 1692 training step(s), loss on training batch is 0.0085964.
After 1693 training step(s), loss on training batch is 0.00690691.
After 1694 training step(s), loss on training batch is 0.00868159.
After 1695 training step(s), loss on training batch is 0.00867243.
After 1696 training step(s), loss on training batch is 0.00754375.
After 1697 training step(s), loss on training batch is 0.00680101.
After 1698 training step(s), loss on training batch is 0.00647691.
After 1699 training step(s), loss on training batch is 0.00652202.
After 1700 training step(s), loss on training batch is 0.0061933.
After 1701 training step(s), loss on training batch is 0.00639435.
After 1702 training step(s), loss on training batch is 0.00632727.
After 1703 training step(s), loss on training batch is 0.00680346.
After 1704 training step(s), loss on training batch is 0.0125255.
After 1705 training step(s), loss on training batch is 0.00856772.
After 1706 training step(s), loss on training batch is 0.00687866.
After 1707 training step(s), loss on training batch is 0.00865445.
After 1708 training step(s), loss on training batch is 0.00864514.
After 1709 training step(s), loss on training batch is 0.00751967.
After 1710 training step(s), loss on training batch is 0.00675794.
After 1711 training step(s), loss on training batch is 0.00646509.
After 1712 training step(s), loss on training batch is 0.00650256.
After 1713 training step(s), loss on training batch is 0.00616304.
After 1714 training step(s), loss on training batch is 0.00635715.
After 1715 training step(s), loss on training batch is 0.00631817.
After 1716 training step(s), loss on training batch is 0.0067871.
After 1717 training step(s), loss on training batch is 0.01251.
After 1718 training step(s), loss on training batch is 0.00854433.
After 1719 training step(s), loss on training batch is 0.00686741.
After 1720 training step(s), loss on training batch is 0.00862552.
After 1721 training step(s), loss on training batch is 0.00862105.
After 1722 training step(s), loss on training batch is 0.00748874.
After 1723 training step(s), loss on training batch is 0.00672856.
After 1724 training step(s), loss on training batch is 0.00642789.
After 1725 training step(s), loss on training batch is 0.00648102.
After 1726 training step(s), loss on training batch is 0.00613295.
After 1727 training step(s), loss on training batch is 0.00633238.
After 1728 training step(s), loss on training batch is 0.00630617.
After 1729 training step(s), loss on training batch is 0.00674865.
After 1730 training step(s), loss on training batch is 0.0124599.
After 1731 training step(s), loss on training batch is 0.00851769.
After 1732 training step(s), loss on training batch is 0.0068466.
After 1733 training step(s), loss on training batch is 0.00859003.
After 1734 training step(s), loss on training batch is 0.00859907.
After 1735 training step(s), loss on training batch is 0.00747169.
After 1736 training step(s), loss on training batch is 0.00668792.
After 1737 training step(s), loss on training batch is 0.00642494.
After 1738 training step(s), loss on training batch is 0.00646176.
After 1739 training step(s), loss on training batch is 0.00611131.
After 1740 training step(s), loss on training batch is 0.00631029.
After 1741 training step(s), loss on training batch is 0.00629834.
After 1742 training step(s), loss on training batch is 0.00671882.
After 1743 training step(s), loss on training batch is 0.0124103.
After 1744 training step(s), loss on training batch is 0.00849255.
After 1745 training step(s), loss on training batch is 0.00682089.
After 1746 training step(s), loss on training batch is 0.00855453.
After 1747 training step(s), loss on training batch is 0.00857743.
After 1748 training step(s), loss on training batch is 0.00745376.
After 1749 training step(s), loss on training batch is 0.00665033.
After 1750 training step(s), loss on training batch is 0.00640191.
After 1751 training step(s), loss on training batch is 0.00643853.
After 1752 training step(s), loss on training batch is 0.00607979.
After 1753 training step(s), loss on training batch is 0.00627764.
After 1754 training step(s), loss on training batch is 0.00626526.
After 1755 training step(s), loss on training batch is 0.00670156.
After 1756 training step(s), loss on training batch is 0.0124095.
After 1757 training step(s), loss on training batch is 0.00846861.
After 1758 training step(s), loss on training batch is 0.00681884.
After 1759 training step(s), loss on training batch is 0.00854309.
After 1760 training step(s), loss on training batch is 0.008555.
After 1761 training step(s), loss on training batch is 0.00741556.
After 1762 training step(s), loss on training batch is 0.00664183.
After 1763 training step(s), loss on training batch is 0.00633904.
After 1764 training step(s), loss on training batch is 0.00641117.
After 1765 training step(s), loss on training batch is 0.00605228.
After 1766 training step(s), loss on training batch is 0.00625237.
After 1767 training step(s), loss on training batch is 0.00621021.
After 1768 training step(s), loss on training batch is 0.00669257.
After 1769 training step(s), loss on training batch is 0.0124205.
After 1770 training step(s), loss on training batch is 0.00844482.
After 1771 training step(s), loss on training batch is 0.00679754.
After 1772 training step(s), loss on training batch is 0.00852427.
After 1773 training step(s), loss on training batch is 0.00852927.
After 1774 training step(s), loss on training batch is 0.00740112.
After 1775 training step(s), loss on training batch is 0.00659847.
After 1776 training step(s), loss on training batch is 0.00634875.
After 1777 training step(s), loss on training batch is 0.00639181.
After 1778 training step(s), loss on training batch is 0.00603399.
After 1779 training step(s), loss on training batch is 0.00623266.
After 1780 training step(s), loss on training batch is 0.00622139.
After 1781 training step(s), loss on training batch is 0.00665184.
After 1782 training step(s), loss on training batch is 0.0123555.
After 1783 training step(s), loss on training batch is 0.00841742.
After 1784 training step(s), loss on training batch is 0.00676751.
After 1785 training step(s), loss on training batch is 0.00848527.
After 1786 training step(s), loss on training batch is 0.00850934.
After 1787 training step(s), loss on training batch is 0.0073846.
After 1788 training step(s), loss on training batch is 0.00656123.
After 1789 training step(s), loss on training batch is 0.00634893.
After 1790 training step(s), loss on training batch is 0.00637874.
After 1791 training step(s), loss on training batch is 0.00601306.
After 1792 training step(s), loss on training batch is 0.00620677.
After 1793 training step(s), loss on training batch is 0.00617975.
After 1794 training step(s), loss on training batch is 0.00663645.
After 1795 training step(s), loss on training batch is 0.0123448.
After 1796 training step(s), loss on training batch is 0.00839761.
After 1797 training step(s), loss on training batch is 0.00675458.
After 1798 training step(s), loss on training batch is 0.00846338.
After 1799 training step(s), loss on training batch is 0.00848854.
After 1800 training step(s), loss on training batch is 0.00735615.
After 1801 training step(s), loss on training batch is 0.00653009.
After 1802 training step(s), loss on training batch is 0.00631428.
After 1803 training step(s), loss on training batch is 0.00635032.
After 1804 training step(s), loss on training batch is 0.00597924.
After 1805 training step(s), loss on training batch is 0.00617904.
After 1806 training step(s), loss on training batch is 0.00617151.
After 1807 training step(s), loss on training batch is 0.00660386.
After 1808 training step(s), loss on training batch is 0.012311.
After 1809 training step(s), loss on training batch is 0.0083709.
After 1810 training step(s), loss on training batch is 0.00672456.
After 1811 training step(s), loss on training batch is 0.00843277.
After 1812 training step(s), loss on training batch is 0.00846334.
After 1813 training step(s), loss on training batch is 0.00734478.
After 1814 training step(s), loss on training batch is 0.00650558.
After 1815 training step(s), loss on training batch is 0.00629652.
After 1816 training step(s), loss on training batch is 0.0063301.
After 1817 training step(s), loss on training batch is 0.00596201.
After 1818 training step(s), loss on training batch is 0.00616032.
After 1819 training step(s), loss on training batch is 0.00615415.
After 1820 training step(s), loss on training batch is 0.00657804.
After 1821 training step(s), loss on training batch is 0.0122867.
After 1822 training step(s), loss on training batch is 0.00835062.
After 1823 training step(s), loss on training batch is 0.00670869.
After 1824 training step(s), loss on training batch is 0.0084063.
After 1825 training step(s), loss on training batch is 0.00844336.
After 1826 training step(s), loss on training batch is 0.00732716.
After 1827 training step(s), loss on training batch is 0.00647355.
After 1828 training step(s), loss on training batch is 0.00627748.
After 1829 training step(s), loss on training batch is 0.00631076.
After 1830 training step(s), loss on training batch is 0.0059374.
After 1831 training step(s), loss on training batch is 0.00613688.
After 1832 training step(s), loss on training batch is 0.00613117.
After 1833 training step(s), loss on training batch is 0.0065566.
After 1834 training step(s), loss on training batch is 0.012282.
After 1835 training step(s), loss on training batch is 0.00833153.
After 1836 training step(s), loss on training batch is 0.00669402.
After 1837 training step(s), loss on training batch is 0.00838393.
After 1838 training step(s), loss on training batch is 0.00842136.
After 1839 training step(s), loss on training batch is 0.00730086.
After 1840 training step(s), loss on training batch is 0.00645362.
After 1841 training step(s), loss on training batch is 0.00625833.
After 1842 training step(s), loss on training batch is 0.00629134.
After 1843 training step(s), loss on training batch is 0.00591211.
After 1844 training step(s), loss on training batch is 0.00611399.
After 1845 training step(s), loss on training batch is 0.00611394.
After 1846 training step(s), loss on training batch is 0.00653087.
After 1847 training step(s), loss on training batch is 0.0122552.
After 1848 training step(s), loss on training batch is 0.00830911.
After 1849 training step(s), loss on training batch is 0.00667353.
After 1850 training step(s), loss on training batch is 0.00835868.
After 1851 training step(s), loss on training batch is 0.00839895.
After 1852 training step(s), loss on training batch is 0.00728924.
After 1853 training step(s), loss on training batch is 0.00642991.
After 1854 training step(s), loss on training batch is 0.00625459.
After 1855 training step(s), loss on training batch is 0.00627422.
After 1856 training step(s), loss on training batch is 0.00589497.
After 1857 training step(s), loss on training batch is 0.00609675.
After 1858 training step(s), loss on training batch is 0.00611098.
After 1859 training step(s), loss on training batch is 0.00650726.
After 1860 training step(s), loss on training batch is 0.012216.
After 1861 training step(s), loss on training batch is 0.00828979.
After 1862 training step(s), loss on training batch is 0.00665313.
After 1863 training step(s), loss on training batch is 0.00833747.
After 1864 training step(s), loss on training batch is 0.00837933.
After 1865 training step(s), loss on training batch is 0.00727079.
After 1866 training step(s), loss on training batch is 0.00640258.
After 1867 training step(s), loss on training batch is 0.00622967.
After 1868 training step(s), loss on training batch is 0.00625983.
After 1869 training step(s), loss on training batch is 0.00586594.
After 1870 training step(s), loss on training batch is 0.0060665.
After 1871 training step(s), loss on training batch is 0.00608648.
After 1872 training step(s), loss on training batch is 0.00648805.
After 1873 training step(s), loss on training batch is 0.0121942.
After 1874 training step(s), loss on training batch is 0.00827386.
After 1875 training step(s), loss on training batch is 0.00665845.
After 1876 training step(s), loss on training batch is 0.00831601.
After 1877 training step(s), loss on training batch is 0.00836153.
After 1878 training step(s), loss on training batch is 0.0072453.
After 1879 training step(s), loss on training batch is 0.0063817.
After 1880 training step(s), loss on training batch is 0.00620377.
After 1881 training step(s), loss on training batch is 0.00623875.
After 1882 training step(s), loss on training batch is 0.00584517.
After 1883 training step(s), loss on training batch is 0.00604269.
After 1884 training step(s), loss on training batch is 0.00605343.
After 1885 training step(s), loss on training batch is 0.00647226.
After 1886 training step(s), loss on training batch is 0.0121878.
After 1887 training step(s), loss on training batch is 0.00825466.
After 1888 training step(s), loss on training batch is 0.00663731.
After 1889 training step(s), loss on training batch is 0.00829919.
After 1890 training step(s), loss on training batch is 0.00834303.
After 1891 training step(s), loss on training batch is 0.00722824.
After 1892 training step(s), loss on training batch is 0.00635009.
After 1893 training step(s), loss on training batch is 0.00619662.
After 1894 training step(s), loss on training batch is 0.00622327.
After 1895 training step(s), loss on training batch is 0.00582501.
After 1896 training step(s), loss on training batch is 0.00602348.
After 1897 training step(s), loss on training batch is 0.00605678.
After 1898 training step(s), loss on training batch is 0.00644556.
After 1899 training step(s), loss on training batch is 0.0121455.
After 1900 training step(s), loss on training batch is 0.00823609.
After 1901 training step(s), loss on training batch is 0.00661764.
After 1902 training step(s), loss on training batch is 0.00826693.
After 1903 training step(s), loss on training batch is 0.008326.
After 1904 training step(s), loss on training batch is 0.00721131.
After 1905 training step(s), loss on training batch is 0.00632613.
After 1906 training step(s), loss on training batch is 0.00617665.
After 1907 training step(s), loss on training batch is 0.00619918.
After 1908 training step(s), loss on training batch is 0.0058045.
After 1909 training step(s), loss on training batch is 0.00600229.
After 1910 training step(s), loss on training batch is 0.00601085.
After 1911 training step(s), loss on training batch is 0.00643703.
After 1912 training step(s), loss on training batch is 0.0121785.
After 1913 training step(s), loss on training batch is 0.00821657.
After 1914 training step(s), loss on training batch is 0.00660792.
After 1915 training step(s), loss on training batch is 0.00826767.
After 1916 training step(s), loss on training batch is 0.00830107.
After 1917 training step(s), loss on training batch is 0.00719689.
After 1918 training step(s), loss on training batch is 0.00632433.
After 1919 training step(s), loss on training batch is 0.00614914.
After 1920 training step(s), loss on training batch is 0.00617872.
After 1921 training step(s), loss on training batch is 0.00579197.
After 1922 training step(s), loss on training batch is 0.00599068.
After 1923 training step(s), loss on training batch is 0.005985.
After 1924 training step(s), loss on training batch is 0.00641127.
After 1925 training step(s), loss on training batch is 0.0121535.
After 1926 training step(s), loss on training batch is 0.00819727.
After 1927 training step(s), loss on training batch is 0.00659104.
After 1928 training step(s), loss on training batch is 0.00824004.
After 1929 training step(s), loss on training batch is 0.00828138.
After 1930 training step(s), loss on training batch is 0.00718404.
After 1931 training step(s), loss on training batch is 0.00630343.
After 1932 training step(s), loss on training batch is 0.00613699.
After 1933 training step(s), loss on training batch is 0.00616353.
After 1934 training step(s), loss on training batch is 0.00577043.
After 1935 training step(s), loss on training batch is 0.00597011.
After 1936 training step(s), loss on training batch is 0.00597226.
After 1937 training step(s), loss on training batch is 0.00638488.
After 1938 training step(s), loss on training batch is 0.0121154.
After 1939 training step(s), loss on training batch is 0.00817879.
After 1940 training step(s), loss on training batch is 0.00657257.
After 1941 training step(s), loss on training batch is 0.00821196.
After 1942 training step(s), loss on training batch is 0.00826495.
After 1943 training step(s), loss on training batch is 0.00716476.
After 1944 training step(s), loss on training batch is 0.00626339.
After 1945 training step(s), loss on training batch is 0.00613636.
After 1946 training step(s), loss on training batch is 0.0061518.
After 1947 training step(s), loss on training batch is 0.00575023.
After 1948 training step(s), loss on training batch is 0.00595206.
After 1949 training step(s), loss on training batch is 0.00598019.
After 1950 training step(s), loss on training batch is 0.0063625.
After 1951 training step(s), loss on training batch is 0.012075.
After 1952 training step(s), loss on training batch is 0.00815771.
After 1953 training step(s), loss on training batch is 0.00655064.
After 1954 training step(s), loss on training batch is 0.00819297.
After 1955 training step(s), loss on training batch is 0.00824661.
After 1956 training step(s), loss on training batch is 0.00714798.
After 1957 training step(s), loss on training batch is 0.00624939.
After 1958 training step(s), loss on training batch is 0.00611687.
After 1959 training step(s), loss on training batch is 0.00613537.
After 1960 training step(s), loss on training batch is 0.00573153.
After 1961 training step(s), loss on training batch is 0.00593828.
After 1962 training step(s), loss on training batch is 0.00595236.
After 1963 training step(s), loss on training batch is 0.00634632.
After 1964 training step(s), loss on training batch is 0.0120673.
After 1965 training step(s), loss on training batch is 0.00814088.
After 1966 training step(s), loss on training batch is 0.00653352.
After 1967 training step(s), loss on training batch is 0.00816792.
After 1968 training step(s), loss on training batch is 0.00822873.
After 1969 training step(s), loss on training batch is 0.00712644.
After 1970 training step(s), loss on training batch is 0.00622492.
After 1971 training step(s), loss on training batch is 0.00609402.
After 1972 training step(s), loss on training batch is 0.0061169.
After 1973 training step(s), loss on training batch is 0.00570893.
After 1974 training step(s), loss on training batch is 0.00590888.
After 1975 training step(s), loss on training batch is 0.00592575.
After 1976 training step(s), loss on training batch is 0.00633917.
After 1977 training step(s), loss on training batch is 0.0120625.
After 1978 training step(s), loss on training batch is 0.0081262.
After 1979 training step(s), loss on training batch is 0.00653242.
After 1980 training step(s), loss on training batch is 0.00815031.
After 1981 training step(s), loss on training batch is 0.00821602.
After 1982 training step(s), loss on training batch is 0.0071104.
After 1983 training step(s), loss on training batch is 0.00619938.
After 1984 training step(s), loss on training batch is 0.00607557.
After 1985 training step(s), loss on training batch is 0.00609858.
After 1986 training step(s), loss on training batch is 0.00568429.
After 1987 training step(s), loss on training batch is 0.00588615.
After 1988 training step(s), loss on training batch is 0.00591892.
After 1989 training step(s), loss on training batch is 0.00631742.
After 1990 training step(s), loss on training batch is 0.0120385.
After 1991 training step(s), loss on training batch is 0.00811086.
After 1992 training step(s), loss on training batch is 0.00652442.
After 1993 training step(s), loss on training batch is 0.00813308.
After 1994 training step(s), loss on training batch is 0.00819758.
After 1995 training step(s), loss on training batch is 0.00709027.
After 1996 training step(s), loss on training batch is 0.00617475.
After 1997 training step(s), loss on training batch is 0.00607287.
After 1998 training step(s), loss on training batch is 0.00609107.
After 1999 training step(s), loss on training batch is 0.00567231.
After 2000 training step(s), loss on training batch is 0.0058734.

