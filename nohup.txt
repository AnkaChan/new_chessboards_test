After 1 training step(s), loss on training batch is 2.38478.
After 2 training step(s), loss on training batch is 2.47992.
After 3 training step(s), loss on training batch is 2.41294.
After 4 training step(s), loss on training batch is 2.2878.
After 5 training step(s), loss on training batch is 2.19454.
After 6 training step(s), loss on training batch is 2.12822.
After 7 training step(s), loss on training batch is 2.11123.
After 8 training step(s), loss on training batch is 2.22155.
After 9 training step(s), loss on training batch is 2.71744.
After 10 training step(s), loss on training batch is 3.03662.
After 11 training step(s), loss on training batch is 2.70156.
After 12 training step(s), loss on training batch is 2.39022.
After 13 training step(s), loss on training batch is 2.0708.
After 14 training step(s), loss on training batch is 2.03109.
After 15 training step(s), loss on training batch is 2.05807.
After 16 training step(s), loss on training batch is 1.90737.
After 17 training step(s), loss on training batch is 1.5852.
After 18 training step(s), loss on training batch is 1.61183.
After 19 training step(s), loss on training batch is 1.89417.
After 20 training step(s), loss on training batch is 2.38797.
After 21 training step(s), loss on training batch is 1.88259.
After 22 training step(s), loss on training batch is 1.55429.
After 23 training step(s), loss on training batch is 0.9954.
After 24 training step(s), loss on training batch is 0.850249.
After 25 training step(s), loss on training batch is 1.10664.
After 26 training step(s), loss on training batch is 1.34692.
After 27 training step(s), loss on training batch is 0.939868.
After 28 training step(s), loss on training batch is 0.705867.
After 29 training step(s), loss on training batch is 0.78734.
After 30 training step(s), loss on training batch is 0.963033.
After 31 training step(s), loss on training batch is 1.12215.
After 32 training step(s), loss on training batch is 1.40114.
After 33 training step(s), loss on training batch is 1.30089.
After 34 training step(s), loss on training batch is 1.3996.
After 35 training step(s), loss on training batch is 0.981062.
After 36 training step(s), loss on training batch is 0.682623.
After 37 training step(s), loss on training batch is 0.605171.
After 38 training step(s), loss on training batch is 0.536749.
After 39 training step(s), loss on training batch is 0.464627.
After 40 training step(s), loss on training batch is 0.35511.
After 41 training step(s), loss on training batch is 0.506145.
After 42 training step(s), loss on training batch is 0.245382.
After 43 training step(s), loss on training batch is 0.245484.
After 44 training step(s), loss on training batch is 0.337848.
After 45 training step(s), loss on training batch is 0.309729.
After 46 training step(s), loss on training batch is 0.371258.
After 47 training step(s), loss on training batch is 0.479721.
After 48 training step(s), loss on training batch is 0.345913.
After 49 training step(s), loss on training batch is 0.578628.
After 50 training step(s), loss on training batch is 0.60768.
After 51 training step(s), loss on training batch is 0.560882.
After 52 training step(s), loss on training batch is 0.284579.
After 53 training step(s), loss on training batch is 0.261605.
After 54 training step(s), loss on training batch is 0.309552.
After 55 training step(s), loss on training batch is 0.359381.
After 56 training step(s), loss on training batch is 0.328383.
After 57 training step(s), loss on training batch is 0.268728.
After 58 training step(s), loss on training batch is 0.372836.
After 59 training step(s), loss on training batch is 0.243821.
After 60 training step(s), loss on training batch is 0.207515.
After 61 training step(s), loss on training batch is 0.191099.
After 62 training step(s), loss on training batch is 0.193886.
After 63 training step(s), loss on training batch is 0.29578.
After 64 training step(s), loss on training batch is 0.316614.
After 65 training step(s), loss on training batch is 0.271712.
After 66 training step(s), loss on training batch is 0.183668.
After 67 training step(s), loss on training batch is 0.157955.
After 68 training step(s), loss on training batch is 0.241458.
After 69 training step(s), loss on training batch is 0.213457.
After 70 training step(s), loss on training batch is 0.0961427.
After 71 training step(s), loss on training batch is 0.243254.
After 72 training step(s), loss on training batch is 0.264568.
After 73 training step(s), loss on training batch is 0.187179.
After 74 training step(s), loss on training batch is 0.117602.
After 75 training step(s), loss on training batch is 0.157903.
After 76 training step(s), loss on training batch is 0.256383.
After 77 training step(s), loss on training batch is 0.183681.
After 78 training step(s), loss on training batch is 0.126294.
After 79 training step(s), loss on training batch is 0.286983.
After 80 training step(s), loss on training batch is 0.270777.
After 81 training step(s), loss on training batch is 0.166302.
After 82 training step(s), loss on training batch is 0.187322.
After 83 training step(s), loss on training batch is 0.301685.
After 84 training step(s), loss on training batch is 0.245197.
After 85 training step(s), loss on training batch is 0.159199.
After 86 training step(s), loss on training batch is 0.173426.
After 87 training step(s), loss on training batch is 0.174176.
After 88 training step(s), loss on training batch is 0.262534.
After 89 training step(s), loss on training batch is 0.19707.
After 90 training step(s), loss on training batch is 0.11745.
After 91 training step(s), loss on training batch is 0.1713.
After 92 training step(s), loss on training batch is 0.0817034.
After 93 training step(s), loss on training batch is 0.128723.
After 94 training step(s), loss on training batch is 0.172414.
After 95 training step(s), loss on training batch is 0.148228.
After 96 training step(s), loss on training batch is 0.131617.
After 97 training step(s), loss on training batch is 0.132591.
After 98 training step(s), loss on training batch is 0.164163.
After 99 training step(s), loss on training batch is 0.291731.
After 100 training step(s), loss on training batch is 0.372719.
After 101 training step(s), loss on training batch is 0.846095.
After 102 training step(s), loss on training batch is 0.942432.
After 103 training step(s), loss on training batch is 0.398368.
After 104 training step(s), loss on training batch is 0.236587.
After 105 training step(s), loss on training batch is 0.209095.
After 106 training step(s), loss on training batch is 0.107965.
After 107 training step(s), loss on training batch is 0.163712.
After 108 training step(s), loss on training batch is 0.198247.
After 109 training step(s), loss on training batch is 0.261454.
After 110 training step(s), loss on training batch is 0.159627.
After 111 training step(s), loss on training batch is 0.169399.
After 112 training step(s), loss on training batch is 0.140782.
After 113 training step(s), loss on training batch is 0.167104.
After 114 training step(s), loss on training batch is 0.139069.
After 115 training step(s), loss on training batch is 0.13339.
After 116 training step(s), loss on training batch is 0.13222.
After 117 training step(s), loss on training batch is 0.160606.
After 118 training step(s), loss on training batch is 0.16679.
After 119 training step(s), loss on training batch is 0.297142.
After 120 training step(s), loss on training batch is 0.241621.
After 121 training step(s), loss on training batch is 0.214018.
After 122 training step(s), loss on training batch is 0.169746.
After 123 training step(s), loss on training batch is 0.158206.
After 124 training step(s), loss on training batch is 0.139886.
After 125 training step(s), loss on training batch is 0.220391.
After 126 training step(s), loss on training batch is 0.143882.
After 127 training step(s), loss on training batch is 0.0902895.
After 128 training step(s), loss on training batch is 0.146198.
After 129 training step(s), loss on training batch is 0.174625.
After 130 training step(s), loss on training batch is 0.149434.
After 131 training step(s), loss on training batch is 0.219512.
After 132 training step(s), loss on training batch is 0.198121.
After 133 training step(s), loss on training batch is 0.154287.
After 134 training step(s), loss on training batch is 0.192697.
After 135 training step(s), loss on training batch is 0.105833.
After 136 training step(s), loss on training batch is 0.149213.
After 137 training step(s), loss on training batch is 0.14441.
After 138 training step(s), loss on training batch is 0.182406.
After 139 training step(s), loss on training batch is 0.111996.
After 140 training step(s), loss on training batch is 0.0936334.
After 141 training step(s), loss on training batch is 0.113074.
After 142 training step(s), loss on training batch is 0.143672.
After 143 training step(s), loss on training batch is 0.0911257.
After 144 training step(s), loss on training batch is 0.150994.
After 145 training step(s), loss on training batch is 0.0525425.
After 146 training step(s), loss on training batch is 0.103642.
After 147 training step(s), loss on training batch is 0.070885.
After 148 training step(s), loss on training batch is 0.159505.
After 149 training step(s), loss on training batch is 0.137873.
After 150 training step(s), loss on training batch is 0.190544.
After 151 training step(s), loss on training batch is 0.107546.
After 152 training step(s), loss on training batch is 0.107226.
After 153 training step(s), loss on training batch is 0.133972.
After 154 training step(s), loss on training batch is 0.105987.
After 155 training step(s), loss on training batch is 0.140104.
After 156 training step(s), loss on training batch is 0.16615.
After 157 training step(s), loss on training batch is 0.0999514.
After 158 training step(s), loss on training batch is 0.0902983.
After 159 training step(s), loss on training batch is 0.105542.
After 160 training step(s), loss on training batch is 0.138301.
After 161 training step(s), loss on training batch is 0.132886.
After 162 training step(s), loss on training batch is 0.143489.
After 163 training step(s), loss on training batch is 0.16576.
After 164 training step(s), loss on training batch is 0.112152.
After 165 training step(s), loss on training batch is 0.0970708.
After 166 training step(s), loss on training batch is 0.0864405.
After 167 training step(s), loss on training batch is 0.0793292.
After 168 training step(s), loss on training batch is 0.118448.
After 169 training step(s), loss on training batch is 0.146045.
After 170 training step(s), loss on training batch is 0.0829511.
After 171 training step(s), loss on training batch is 0.0942366.
After 172 training step(s), loss on training batch is 0.178475.
After 173 training step(s), loss on training batch is 0.126768.
After 174 training step(s), loss on training batch is 0.154577.
After 175 training step(s), loss on training batch is 0.120845.
After 176 training step(s), loss on training batch is 0.0737071.
After 177 training step(s), loss on training batch is 0.110816.
After 178 training step(s), loss on training batch is 0.0720849.
After 179 training step(s), loss on training batch is 0.106681.
After 180 training step(s), loss on training batch is 0.10739.
After 181 training step(s), loss on training batch is 0.090936.
After 182 training step(s), loss on training batch is 0.172794.
After 183 training step(s), loss on training batch is 0.158568.
After 184 training step(s), loss on training batch is 0.0745008.
After 185 training step(s), loss on training batch is 0.11824.
After 186 training step(s), loss on training batch is 0.0893813.
After 187 training step(s), loss on training batch is 0.0867873.
After 188 training step(s), loss on training batch is 0.166185.
After 189 training step(s), loss on training batch is 0.0757553.
After 190 training step(s), loss on training batch is 0.194245.
After 191 training step(s), loss on training batch is 0.0854027.
After 192 training step(s), loss on training batch is 0.0992173.
After 193 training step(s), loss on training batch is 0.118234.
After 194 training step(s), loss on training batch is 0.130664.
After 195 training step(s), loss on training batch is 0.121582.
After 196 training step(s), loss on training batch is 0.109386.
After 197 training step(s), loss on training batch is 0.139667.
After 198 training step(s), loss on training batch is 0.146595.
After 199 training step(s), loss on training batch is 0.0740408.
After 200 training step(s), loss on training batch is 0.129257.
After 201 training step(s), loss on training batch is 0.139931.
After 202 training step(s), loss on training batch is 0.0566702.
After 203 training step(s), loss on training batch is 0.123621.
After 204 training step(s), loss on training batch is 0.0948603.
After 205 training step(s), loss on training batch is 0.125255.
After 206 training step(s), loss on training batch is 0.112421.
After 207 training step(s), loss on training batch is 0.221636.
After 208 training step(s), loss on training batch is 0.205836.
After 209 training step(s), loss on training batch is 0.197114.
After 210 training step(s), loss on training batch is 0.0811128.
After 211 training step(s), loss on training batch is 0.140326.
After 212 training step(s), loss on training batch is 0.209803.
After 213 training step(s), loss on training batch is 0.15699.
After 214 training step(s), loss on training batch is 0.112307.
After 215 training step(s), loss on training batch is 0.0920609.
After 216 training step(s), loss on training batch is 0.110914.
After 217 training step(s), loss on training batch is 0.117594.
After 218 training step(s), loss on training batch is 0.0674594.
After 219 training step(s), loss on training batch is 0.0581975.
After 220 training step(s), loss on training batch is 0.19096.
After 221 training step(s), loss on training batch is 0.270477.
After 222 training step(s), loss on training batch is 0.121744.
After 223 training step(s), loss on training batch is 0.419859.
After 224 training step(s), loss on training batch is 0.111609.
After 225 training step(s), loss on training batch is 0.181426.
After 226 training step(s), loss on training batch is 0.149371.
After 227 training step(s), loss on training batch is 0.234025.
After 228 training step(s), loss on training batch is 0.160952.
After 229 training step(s), loss on training batch is 0.100172.
After 230 training step(s), loss on training batch is 0.0683708.
After 231 training step(s), loss on training batch is 0.0672354.
After 232 training step(s), loss on training batch is 0.120004.
After 233 training step(s), loss on training batch is 0.0785406.
After 234 training step(s), loss on training batch is 0.0783085.
After 235 training step(s), loss on training batch is 0.123291.
After 236 training step(s), loss on training batch is 0.113296.
After 237 training step(s), loss on training batch is 0.103048.
After 238 training step(s), loss on training batch is 0.0438381.
After 239 training step(s), loss on training batch is 0.106557.
After 240 training step(s), loss on training batch is 0.207032.
After 241 training step(s), loss on training batch is 0.0802342.
After 242 training step(s), loss on training batch is 0.0868446.
After 243 training step(s), loss on training batch is 0.0882187.
After 244 training step(s), loss on training batch is 0.113355.
After 245 training step(s), loss on training batch is 0.134856.
After 246 training step(s), loss on training batch is 0.0967855.
After 247 training step(s), loss on training batch is 0.0662952.
After 248 training step(s), loss on training batch is 0.0967801.
After 249 training step(s), loss on training batch is 0.071382.
After 250 training step(s), loss on training batch is 0.093918.
After 251 training step(s), loss on training batch is 0.0606487.
After 252 training step(s), loss on training batch is 0.117226.
After 253 training step(s), loss on training batch is 0.109046.
After 254 training step(s), loss on training batch is 0.0828439.
After 255 training step(s), loss on training batch is 0.0948011.
After 256 training step(s), loss on training batch is 0.063197.
After 257 training step(s), loss on training batch is 0.102078.
After 258 training step(s), loss on training batch is 0.126779.
After 259 training step(s), loss on training batch is 0.0598217.
After 260 training step(s), loss on training batch is 0.0803764.
After 261 training step(s), loss on training batch is 0.0680989.
After 262 training step(s), loss on training batch is 0.0669601.
After 263 training step(s), loss on training batch is 0.0896317.
After 264 training step(s), loss on training batch is 0.13941.
After 265 training step(s), loss on training batch is 0.046957.
After 266 training step(s), loss on training batch is 0.0647893.
After 267 training step(s), loss on training batch is 0.0473346.
After 268 training step(s), loss on training batch is 0.0386462.
After 269 training step(s), loss on training batch is 0.027062.
After 270 training step(s), loss on training batch is 0.0785917.
After 271 training step(s), loss on training batch is 0.019161.
After 272 training step(s), loss on training batch is 0.137805.
After 273 training step(s), loss on training batch is 0.059432.
After 274 training step(s), loss on training batch is 0.250287.
After 275 training step(s), loss on training batch is 0.149403.
After 276 training step(s), loss on training batch is 0.10319.
After 277 training step(s), loss on training batch is 0.134725.
After 278 training step(s), loss on training batch is 0.0666049.
After 279 training step(s), loss on training batch is 0.10249.
After 280 training step(s), loss on training batch is 0.0662437.
After 281 training step(s), loss on training batch is 0.0948907.
After 282 training step(s), loss on training batch is 0.137915.
After 283 training step(s), loss on training batch is 0.0772054.
After 284 training step(s), loss on training batch is 0.128909.
After 285 training step(s), loss on training batch is 0.0955551.
After 286 training step(s), loss on training batch is 0.104988.
After 287 training step(s), loss on training batch is 0.0758144.
After 288 training step(s), loss on training batch is 0.0897173.
After 289 training step(s), loss on training batch is 0.108684.
After 290 training step(s), loss on training batch is 0.0573355.
After 291 training step(s), loss on training batch is 0.0710228.
After 292 training step(s), loss on training batch is 0.0782063.
After 293 training step(s), loss on training batch is 0.107972.
After 294 training step(s), loss on training batch is 0.171529.
After 295 training step(s), loss on training batch is 0.0913544.
After 296 training step(s), loss on training batch is 0.0818899.
After 297 training step(s), loss on training batch is 0.0635344.
After 298 training step(s), loss on training batch is 0.129825.
After 299 training step(s), loss on training batch is 0.047943.
After 300 training step(s), loss on training batch is 0.0925131.
After 301 training step(s), loss on training batch is 0.0547418.
After 302 training step(s), loss on training batch is 0.0961388.
After 303 training step(s), loss on training batch is 0.0906905.
After 304 training step(s), loss on training batch is 0.0537493.
After 305 training step(s), loss on training batch is 0.0759417.
After 306 training step(s), loss on training batch is 0.0611092.
After 307 training step(s), loss on training batch is 0.0699034.
After 308 training step(s), loss on training batch is 0.165861.
After 309 training step(s), loss on training batch is 0.0838426.
After 310 training step(s), loss on training batch is 0.0770811.
After 311 training step(s), loss on training batch is 0.0494073.
After 312 training step(s), loss on training batch is 0.0664167.
After 313 training step(s), loss on training batch is 0.0820621.
After 314 training step(s), loss on training batch is 0.0796413.
After 315 training step(s), loss on training batch is 0.0676736.
After 316 training step(s), loss on training batch is 0.0685841.
After 317 training step(s), loss on training batch is 0.0809674.
After 318 training step(s), loss on training batch is 0.0811175.
After 319 training step(s), loss on training batch is 0.0520426.
After 320 training step(s), loss on training batch is 0.0980859.
After 321 training step(s), loss on training batch is 0.10857.
After 322 training step(s), loss on training batch is 0.0518677.
After 323 training step(s), loss on training batch is 0.126337.
After 324 training step(s), loss on training batch is 0.0715767.
After 325 training step(s), loss on training batch is 0.121452.
After 326 training step(s), loss on training batch is 0.0788853.
After 327 training step(s), loss on training batch is 0.0764125.
After 328 training step(s), loss on training batch is 0.0752346.
After 329 training step(s), loss on training batch is 0.0519432.
After 330 training step(s), loss on training batch is 0.0848124.
After 331 training step(s), loss on training batch is 0.0937124.
After 332 training step(s), loss on training batch is 0.0722113.
After 333 training step(s), loss on training batch is 0.064373.
After 334 training step(s), loss on training batch is 0.0590344.
After 335 training step(s), loss on training batch is 0.0981706.
After 336 training step(s), loss on training batch is 0.0642295.
After 337 training step(s), loss on training batch is 0.0908717.
After 338 training step(s), loss on training batch is 0.0801831.
After 339 training step(s), loss on training batch is 0.0933171.
After 340 training step(s), loss on training batch is 0.113083.
After 341 training step(s), loss on training batch is 0.0610718.
After 342 training step(s), loss on training batch is 0.119522.
After 343 training step(s), loss on training batch is 0.0589264.
After 344 training step(s), loss on training batch is 0.0616916.
After 345 training step(s), loss on training batch is 0.0809592.
After 346 training step(s), loss on training batch is 0.097875.
After 347 training step(s), loss on training batch is 0.0628224.
After 348 training step(s), loss on training batch is 0.0660178.
After 349 training step(s), loss on training batch is 0.0850587.
After 350 training step(s), loss on training batch is 0.0765051.
After 351 training step(s), loss on training batch is 0.0951969.
After 352 training step(s), loss on training batch is 0.0540644.
After 353 training step(s), loss on training batch is 0.0623705.
After 354 training step(s), loss on training batch is 0.0955148.
After 355 training step(s), loss on training batch is 0.0980697.
After 356 training step(s), loss on training batch is 0.124309.
After 357 training step(s), loss on training batch is 0.093681.
After 358 training step(s), loss on training batch is 0.0594288.
After 359 training step(s), loss on training batch is 0.0799703.
After 360 training step(s), loss on training batch is 0.0496978.
After 361 training step(s), loss on training batch is 0.064279.
After 362 training step(s), loss on training batch is 0.0822053.
After 363 training step(s), loss on training batch is 0.0615871.
After 364 training step(s), loss on training batch is 0.100364.
After 365 training step(s), loss on training batch is 0.0590508.
After 366 training step(s), loss on training batch is 0.0499768.
After 367 training step(s), loss on training batch is 0.069586.
After 368 training step(s), loss on training batch is 0.0409282.
After 369 training step(s), loss on training batch is 0.064259.
After 370 training step(s), loss on training batch is 0.0731316.
After 371 training step(s), loss on training batch is 0.0944617.
After 372 training step(s), loss on training batch is 0.108816.
After 373 training step(s), loss on training batch is 0.0876761.
After 374 training step(s), loss on training batch is 0.0865537.
After 375 training step(s), loss on training batch is 0.0591727.
After 376 training step(s), loss on training batch is 0.0820128.
After 377 training step(s), loss on training batch is 0.132478.
After 378 training step(s), loss on training batch is 0.10371.
After 379 training step(s), loss on training batch is 0.083206.
After 380 training step(s), loss on training batch is 0.0712572.
After 381 training step(s), loss on training batch is 0.0505901.
After 382 training step(s), loss on training batch is 0.0576315.
After 383 training step(s), loss on training batch is 0.0649231.
After 384 training step(s), loss on training batch is 0.122172.
After 385 training step(s), loss on training batch is 0.0992699.
After 386 training step(s), loss on training batch is 0.0880981.
After 387 training step(s), loss on training batch is 0.0888321.
After 388 training step(s), loss on training batch is 0.102467.
After 389 training step(s), loss on training batch is 0.057799.
After 390 training step(s), loss on training batch is 0.0969726.
After 391 training step(s), loss on training batch is 0.091807.
After 392 training step(s), loss on training batch is 0.0469512.
After 393 training step(s), loss on training batch is 0.0588392.
After 394 training step(s), loss on training batch is 0.0521946.
After 395 training step(s), loss on training batch is 0.107644.
After 396 training step(s), loss on training batch is 0.109217.
After 397 training step(s), loss on training batch is 0.0895917.
After 398 training step(s), loss on training batch is 0.0749809.
After 399 training step(s), loss on training batch is 0.112521.
After 400 training step(s), loss on training batch is 0.0643315.
After 401 training step(s), loss on training batch is 0.0339472.
After 402 training step(s), loss on training batch is 0.0993533.
After 403 training step(s), loss on training batch is 0.0680045.
After 404 training step(s), loss on training batch is 0.14117.
After 405 training step(s), loss on training batch is 0.0612346.
After 406 training step(s), loss on training batch is 0.0695233.
After 407 training step(s), loss on training batch is 0.0860677.
After 408 training step(s), loss on training batch is 0.080861.
After 409 training step(s), loss on training batch is 0.070932.
After 410 training step(s), loss on training batch is 0.0759569.
After 411 training step(s), loss on training batch is 0.0610059.
After 412 training step(s), loss on training batch is 0.0551882.
After 413 training step(s), loss on training batch is 0.0510778.
After 414 training step(s), loss on training batch is 0.0743913.
After 415 training step(s), loss on training batch is 0.121921.
After 416 training step(s), loss on training batch is 0.112478.
After 417 training step(s), loss on training batch is 0.0657008.
After 418 training step(s), loss on training batch is 0.0504171.
After 419 training step(s), loss on training batch is 0.125213.
After 420 training step(s), loss on training batch is 0.0451201.
After 421 training step(s), loss on training batch is 0.0539994.
After 422 training step(s), loss on training batch is 0.0661236.
After 423 training step(s), loss on training batch is 0.066413.
After 424 training step(s), loss on training batch is 0.0695795.
After 425 training step(s), loss on training batch is 0.0764533.
After 426 training step(s), loss on training batch is 0.0749969.
After 427 training step(s), loss on training batch is 0.0510351.
After 428 training step(s), loss on training batch is 0.0493937.
After 429 training step(s), loss on training batch is 0.0466264.
After 430 training step(s), loss on training batch is 0.0558612.
After 431 training step(s), loss on training batch is 0.135595.
After 432 training step(s), loss on training batch is 0.0535857.
After 433 training step(s), loss on training batch is 0.0527577.
After 434 training step(s), loss on training batch is 0.0818725.
After 435 training step(s), loss on training batch is 0.0938133.
After 436 training step(s), loss on training batch is 0.0603779.
After 437 training step(s), loss on training batch is 0.0742438.
After 438 training step(s), loss on training batch is 0.0463376.
After 439 training step(s), loss on training batch is 0.0409705.
After 440 training step(s), loss on training batch is 0.0603561.
After 441 training step(s), loss on training batch is 0.102582.
After 442 training step(s), loss on training batch is 0.079059.
After 443 training step(s), loss on training batch is 0.0553443.
After 444 training step(s), loss on training batch is 0.083542.
After 445 training step(s), loss on training batch is 0.166096.
After 446 training step(s), loss on training batch is 0.0972488.
After 447 training step(s), loss on training batch is 0.059897.
After 448 training step(s), loss on training batch is 0.0916353.
After 449 training step(s), loss on training batch is 0.0886819.
After 450 training step(s), loss on training batch is 0.0564344.
After 451 training step(s), loss on training batch is 0.0726992.
After 452 training step(s), loss on training batch is 0.0839911.
After 453 training step(s), loss on training batch is 0.0766922.
After 454 training step(s), loss on training batch is 0.0863442.
After 455 training step(s), loss on training batch is 0.0482554.
After 456 training step(s), loss on training batch is 0.064463.
After 457 training step(s), loss on training batch is 0.0451578.
After 458 training step(s), loss on training batch is 0.0703722.
After 459 training step(s), loss on training batch is 0.0457159.
After 460 training step(s), loss on training batch is 0.0645661.
After 461 training step(s), loss on training batch is 0.0569714.
After 462 training step(s), loss on training batch is 0.0959849.
After 463 training step(s), loss on training batch is 0.0849077.
After 464 training step(s), loss on training batch is 0.121077.
After 465 training step(s), loss on training batch is 0.0484951.
After 466 training step(s), loss on training batch is 0.0566942.
After 467 training step(s), loss on training batch is 0.0919256.
After 468 training step(s), loss on training batch is 0.0690859.
After 469 training step(s), loss on training batch is 0.057852.
After 470 training step(s), loss on training batch is 0.092023.
After 471 training step(s), loss on training batch is 0.0657941.
After 472 training step(s), loss on training batch is 0.0485053.
After 473 training step(s), loss on training batch is 0.0951977.
After 474 training step(s), loss on training batch is 0.0494763.
After 475 training step(s), loss on training batch is 0.0543208.
After 476 training step(s), loss on training batch is 0.162314.
After 477 training step(s), loss on training batch is 0.0608026.
After 478 training step(s), loss on training batch is 0.074142.
After 479 training step(s), loss on training batch is 0.0863112.
After 480 training step(s), loss on training batch is 0.0926907.
After 481 training step(s), loss on training batch is 0.0559675.
After 482 training step(s), loss on training batch is 0.0784105.
After 483 training step(s), loss on training batch is 0.0590974.
After 484 training step(s), loss on training batch is 0.0356085.
After 485 training step(s), loss on training batch is 0.0594379.
After 486 training step(s), loss on training batch is 0.0715597.
After 487 training step(s), loss on training batch is 0.0868896.
After 488 training step(s), loss on training batch is 0.0445372.
After 489 training step(s), loss on training batch is 0.0412078.
After 490 training step(s), loss on training batch is 0.0394401.
After 491 training step(s), loss on training batch is 0.0752565.
After 492 training step(s), loss on training batch is 0.0586166.
After 493 training step(s), loss on training batch is 0.129311.
After 494 training step(s), loss on training batch is 0.049362.
After 495 training step(s), loss on training batch is 0.0762532.
After 496 training step(s), loss on training batch is 0.0572953.
After 497 training step(s), loss on training batch is 0.0341644.
After 498 training step(s), loss on training batch is 0.0344919.
After 499 training step(s), loss on training batch is 0.0727159.
After 500 training step(s), loss on training batch is 0.0524533.
After 501 training step(s), loss on training batch is 0.0293263.
After 502 training step(s), loss on training batch is 0.0684656.
After 503 training step(s), loss on training batch is 0.0474771.
After 504 training step(s), loss on training batch is 0.0488484.
After 505 training step(s), loss on training batch is 0.0895286.
After 506 training step(s), loss on training batch is 0.0607995.
After 507 training step(s), loss on training batch is 0.0525631.
After 508 training step(s), loss on training batch is 0.146277.
After 509 training step(s), loss on training batch is 0.11218.
After 510 training step(s), loss on training batch is 0.0582036.
After 511 training step(s), loss on training batch is 0.0830101.
After 512 training step(s), loss on training batch is 0.090883.
After 513 training step(s), loss on training batch is 0.0271874.
After 514 training step(s), loss on training batch is 0.061242.
After 515 training step(s), loss on training batch is 0.0906232.
After 516 training step(s), loss on training batch is 0.0552778.
After 517 training step(s), loss on training batch is 0.0466911.
After 518 training step(s), loss on training batch is 0.0737972.
After 519 training step(s), loss on training batch is 0.0583174.
After 520 training step(s), loss on training batch is 0.0818536.
After 521 training step(s), loss on training batch is 0.0446542.
After 522 training step(s), loss on training batch is 0.0613821.
After 523 training step(s), loss on training batch is 0.0562437.
After 524 training step(s), loss on training batch is 0.0677722.
After 525 training step(s), loss on training batch is 0.0700696.
After 526 training step(s), loss on training batch is 0.0504962.
After 527 training step(s), loss on training batch is 0.0505349.
After 528 training step(s), loss on training batch is 0.0823438.
After 529 training step(s), loss on training batch is 0.133635.
After 530 training step(s), loss on training batch is 0.104549.
After 531 training step(s), loss on training batch is 0.0861931.
After 532 training step(s), loss on training batch is 0.106997.
After 533 training step(s), loss on training batch is 0.0415106.
After 534 training step(s), loss on training batch is 0.0585832.
After 535 training step(s), loss on training batch is 0.0515215.
After 536 training step(s), loss on training batch is 0.0853445.
After 537 training step(s), loss on training batch is 0.104004.
After 538 training step(s), loss on training batch is 0.155066.
After 539 training step(s), loss on training batch is 0.0466991.
After 540 training step(s), loss on training batch is 0.0646737.
After 541 training step(s), loss on training batch is 0.0354025.
After 542 training step(s), loss on training batch is 0.0703855.
After 543 training step(s), loss on training batch is 0.0760635.
After 544 training step(s), loss on training batch is 0.0472084.
After 545 training step(s), loss on training batch is 0.0542921.
After 546 training step(s), loss on training batch is 0.0840745.
After 547 training step(s), loss on training batch is 0.114185.
After 548 training step(s), loss on training batch is 0.0671358.
After 549 training step(s), loss on training batch is 0.0510108.
After 550 training step(s), loss on training batch is 0.0954806.
After 551 training step(s), loss on training batch is 0.0981812.
After 552 training step(s), loss on training batch is 0.0487635.
After 553 training step(s), loss on training batch is 0.0454766.
After 554 training step(s), loss on training batch is 0.052465.
After 555 training step(s), loss on training batch is 0.0920061.
After 556 training step(s), loss on training batch is 0.0385463.
After 557 training step(s), loss on training batch is 0.0442754.
After 558 training step(s), loss on training batch is 0.0324844.
After 559 training step(s), loss on training batch is 0.0592139.
After 560 training step(s), loss on training batch is 0.0697944.
After 561 training step(s), loss on training batch is 0.0477247.
After 562 training step(s), loss on training batch is 0.0256152.
After 563 training step(s), loss on training batch is 0.0788678.
After 564 training step(s), loss on training batch is 0.0553963.
After 565 training step(s), loss on training batch is 0.0512943.
After 566 training step(s), loss on training batch is 0.0808281.
After 567 training step(s), loss on training batch is 0.0565126.
After 568 training step(s), loss on training batch is 0.118053.
After 569 training step(s), loss on training batch is 0.0858824.
After 570 training step(s), loss on training batch is 0.0466742.
After 571 training step(s), loss on training batch is 0.0419522.
After 572 training step(s), loss on training batch is 0.0442488.
After 573 training step(s), loss on training batch is 0.0453208.
After 574 training step(s), loss on training batch is 0.0497534.
After 575 training step(s), loss on training batch is 0.0490397.
After 576 training step(s), loss on training batch is 0.0694701.
After 577 training step(s), loss on training batch is 0.0666331.
After 578 training step(s), loss on training batch is 0.0407234.
After 579 training step(s), loss on training batch is 0.0426468.
After 580 training step(s), loss on training batch is 0.0616589.
After 581 training step(s), loss on training batch is 0.131391.
After 582 training step(s), loss on training batch is 0.0886078.
After 583 training step(s), loss on training batch is 0.0381993.
After 584 training step(s), loss on training batch is 0.049284.
After 585 training step(s), loss on training batch is 0.0483467.
After 586 training step(s), loss on training batch is 0.045324.
After 587 training step(s), loss on training batch is 0.0566638.
After 588 training step(s), loss on training batch is 0.0520185.
After 589 training step(s), loss on training batch is 0.0659457.
After 590 training step(s), loss on training batch is 0.0590911.
After 591 training step(s), loss on training batch is 0.0636774.
After 592 training step(s), loss on training batch is 0.0314846.
After 593 training step(s), loss on training batch is 0.0243614.
After 594 training step(s), loss on training batch is 0.0316122.
After 595 training step(s), loss on training batch is 0.0332476.
After 596 training step(s), loss on training batch is 0.0394212.
After 597 training step(s), loss on training batch is 0.0461206.
After 598 training step(s), loss on training batch is 0.059622.
After 599 training step(s), loss on training batch is 0.0545194.
After 600 training step(s), loss on training batch is 0.059482.
After 601 training step(s), loss on training batch is 0.068777.
After 602 training step(s), loss on training batch is 0.02397.
After 603 training step(s), loss on training batch is 0.073866.
After 604 training step(s), loss on training batch is 0.0488334.
After 605 training step(s), loss on training batch is 0.107434.
After 606 training step(s), loss on training batch is 0.0488644.
After 607 training step(s), loss on training batch is 0.128776.
After 608 training step(s), loss on training batch is 0.0582891.
After 609 training step(s), loss on training batch is 0.0936655.
After 610 training step(s), loss on training batch is 0.069973.
After 611 training step(s), loss on training batch is 0.0282641.
After 612 training step(s), loss on training batch is 0.045168.
After 613 training step(s), loss on training batch is 0.0780366.
After 614 training step(s), loss on training batch is 0.061716.
After 615 training step(s), loss on training batch is 0.0526651.
After 616 training step(s), loss on training batch is 0.0435508.
After 617 training step(s), loss on training batch is 0.0674651.
After 618 training step(s), loss on training batch is 0.0256212.
After 619 training step(s), loss on training batch is 0.0386696.
After 620 training step(s), loss on training batch is 0.0436181.
After 621 training step(s), loss on training batch is 0.0640136.
After 622 training step(s), loss on training batch is 0.0300528.
After 623 training step(s), loss on training batch is 0.0476091.
After 624 training step(s), loss on training batch is 0.0414853.
After 625 training step(s), loss on training batch is 0.0281169.
After 626 training step(s), loss on training batch is 0.0337734.
After 627 training step(s), loss on training batch is 0.0317941.
After 628 training step(s), loss on training batch is 0.0340323.
After 629 training step(s), loss on training batch is 0.0255628.
After 630 training step(s), loss on training batch is 0.0700637.
After 631 training step(s), loss on training batch is 0.123152.
After 632 training step(s), loss on training batch is 0.0329715.
After 633 training step(s), loss on training batch is 0.0538222.
After 634 training step(s), loss on training batch is 0.0700826.
After 635 training step(s), loss on training batch is 0.0561141.
After 636 training step(s), loss on training batch is 0.0597266.
After 637 training step(s), loss on training batch is 0.0416112.
After 638 training step(s), loss on training batch is 0.090962.
After 639 training step(s), loss on training batch is 0.0547506.
After 640 training step(s), loss on training batch is 0.0470022.
After 641 training step(s), loss on training batch is 0.0416756.
After 642 training step(s), loss on training batch is 0.156901.
After 643 training step(s), loss on training batch is 0.03962.
After 644 training step(s), loss on training batch is 0.0787962.
After 645 training step(s), loss on training batch is 0.0875521.
After 646 training step(s), loss on training batch is 0.0829062.
After 647 training step(s), loss on training batch is 0.061553.
After 648 training step(s), loss on training batch is 0.0749676.
After 649 training step(s), loss on training batch is 0.0412473.
After 650 training step(s), loss on training batch is 0.0323845.
After 651 training step(s), loss on training batch is 0.0805108.
After 652 training step(s), loss on training batch is 0.0558991.
After 653 training step(s), loss on training batch is 0.0326232.
After 654 training step(s), loss on training batch is 0.07954.
After 655 training step(s), loss on training batch is 0.0676156.
After 656 training step(s), loss on training batch is 0.0377602.
After 657 training step(s), loss on training batch is 0.0333137.
After 658 training step(s), loss on training batch is 0.0375743.
After 659 training step(s), loss on training batch is 0.0937783.
After 660 training step(s), loss on training batch is 0.0683592.
After 661 training step(s), loss on training batch is 0.0420329.
After 662 training step(s), loss on training batch is 0.0637953.
After 663 training step(s), loss on training batch is 0.0925243.
After 664 training step(s), loss on training batch is 0.0557662.
After 665 training step(s), loss on training batch is 0.0328321.
After 666 training step(s), loss on training batch is 0.0597995.
After 667 training step(s), loss on training batch is 0.0425781.
After 668 training step(s), loss on training batch is 0.0472322.
After 669 training step(s), loss on training batch is 0.0317401.
After 670 training step(s), loss on training batch is 0.0464037.
After 671 training step(s), loss on training batch is 0.0428353.
After 672 training step(s), loss on training batch is 0.0502247.
After 673 training step(s), loss on training batch is 0.0555423.
After 674 training step(s), loss on training batch is 0.0872578.
After 675 training step(s), loss on training batch is 0.0382812.
After 676 training step(s), loss on training batch is 0.0436833.
After 677 training step(s), loss on training batch is 0.05395.
After 678 training step(s), loss on training batch is 0.0327538.
After 679 training step(s), loss on training batch is 0.0407851.
After 680 training step(s), loss on training batch is 0.0795666.
After 681 training step(s), loss on training batch is 0.0575367.
After 682 training step(s), loss on training batch is 0.0450515.
After 683 training step(s), loss on training batch is 0.0449229.
After 684 training step(s), loss on training batch is 0.0664543.
After 685 training step(s), loss on training batch is 0.0940609.
After 686 training step(s), loss on training batch is 0.110228.
After 687 training step(s), loss on training batch is 0.0768955.
After 688 training step(s), loss on training batch is 0.0624265.
After 689 training step(s), loss on training batch is 0.0699925.
After 690 training step(s), loss on training batch is 0.0724145.
After 691 training step(s), loss on training batch is 0.0574225.
After 692 training step(s), loss on training batch is 0.0809709.
After 693 training step(s), loss on training batch is 0.0383799.
After 694 training step(s), loss on training batch is 0.0836834.
After 695 training step(s), loss on training batch is 0.0813385.
After 696 training step(s), loss on training batch is 0.0415384.
After 697 training step(s), loss on training batch is 0.0898304.
After 698 training step(s), loss on training batch is 0.0872137.
After 699 training step(s), loss on training batch is 0.0793162.
After 700 training step(s), loss on training batch is 0.0652787.
After 701 training step(s), loss on training batch is 0.0647009.
After 702 training step(s), loss on training batch is 0.0369676.
After 703 training step(s), loss on training batch is 0.0243569.
After 704 training step(s), loss on training batch is 0.036445.
After 705 training step(s), loss on training batch is 0.0494948.
After 706 training step(s), loss on training batch is 0.0442444.
After 707 training step(s), loss on training batch is 0.0454342.
After 708 training step(s), loss on training batch is 0.0272166.
After 709 training step(s), loss on training batch is 0.0702838.
After 710 training step(s), loss on training batch is 0.0460086.
After 711 training step(s), loss on training batch is 0.0555602.
After 712 training step(s), loss on training batch is 0.0517182.
After 713 training step(s), loss on training batch is 0.0477119.
After 714 training step(s), loss on training batch is 0.0670018.
After 715 training step(s), loss on training batch is 0.0309848.
After 716 training step(s), loss on training batch is 0.0989802.
After 717 training step(s), loss on training batch is 0.0763753.
After 718 training step(s), loss on training batch is 0.05846.
After 719 training step(s), loss on training batch is 0.0343715.
After 720 training step(s), loss on training batch is 0.0732387.
After 721 training step(s), loss on training batch is 0.0515839.
After 722 training step(s), loss on training batch is 0.0494252.
After 723 training step(s), loss on training batch is 0.0504724.
After 724 training step(s), loss on training batch is 0.0759977.
After 725 training step(s), loss on training batch is 0.0506795.
After 726 training step(s), loss on training batch is 0.0281129.
After 727 training step(s), loss on training batch is 0.0459346.
After 728 training step(s), loss on training batch is 0.0303916.
After 729 training step(s), loss on training batch is 0.0538882.
After 730 training step(s), loss on training batch is 0.0489597.
After 731 training step(s), loss on training batch is 0.0580489.
After 732 training step(s), loss on training batch is 0.0659974.
After 733 training step(s), loss on training batch is 0.0362062.
After 734 training step(s), loss on training batch is 0.0785618.
After 735 training step(s), loss on training batch is 0.0386453.
After 736 training step(s), loss on training batch is 0.0301985.
After 737 training step(s), loss on training batch is 0.041079.
After 738 training step(s), loss on training batch is 0.0433537.
After 739 training step(s), loss on training batch is 0.0814498.
After 740 training step(s), loss on training batch is 0.0852465.
After 741 training step(s), loss on training batch is 0.0524377.
After 742 training step(s), loss on training batch is 0.0706339.
After 743 training step(s), loss on training batch is 0.0419818.
After 744 training step(s), loss on training batch is 0.0581179.
After 745 training step(s), loss on training batch is 0.0560701.
After 746 training step(s), loss on training batch is 0.0698794.
After 747 training step(s), loss on training batch is 0.0624669.
After 748 training step(s), loss on training batch is 0.0400733.
After 749 training step(s), loss on training batch is 0.0885869.
After 750 training step(s), loss on training batch is 0.0483985.
After 751 training step(s), loss on training batch is 0.0524448.
After 752 training step(s), loss on training batch is 0.052108.
After 753 training step(s), loss on training batch is 0.0444903.
After 754 training step(s), loss on training batch is 0.0286117.
After 755 training step(s), loss on training batch is 0.051442.
After 756 training step(s), loss on training batch is 0.0518523.
After 757 training step(s), loss on training batch is 0.0329636.
After 758 training step(s), loss on training batch is 0.0447672.
After 759 training step(s), loss on training batch is 0.0505996.
After 760 training step(s), loss on training batch is 0.0307452.
After 761 training step(s), loss on training batch is 0.0310865.
After 762 training step(s), loss on training batch is 0.06055.
After 763 training step(s), loss on training batch is 0.0367217.
After 764 training step(s), loss on training batch is 0.0311714.
After 765 training step(s), loss on training batch is 0.0545422.
After 766 training step(s), loss on training batch is 0.0346038.
After 767 training step(s), loss on training batch is 0.0256761.
After 768 training step(s), loss on training batch is 0.058019.
After 769 training step(s), loss on training batch is 0.098056.
After 770 training step(s), loss on training batch is 0.0462297.
After 771 training step(s), loss on training batch is 0.0291796.
After 772 training step(s), loss on training batch is 0.0655416.
After 773 training step(s), loss on training batch is 0.0430027.
After 774 training step(s), loss on training batch is 0.08765.
After 775 training step(s), loss on training batch is 0.120381.
After 776 training step(s), loss on training batch is 0.0500964.
After 777 training step(s), loss on training batch is 0.0590677.
After 778 training step(s), loss on training batch is 0.0637621.
After 779 training step(s), loss on training batch is 0.0583042.
After 780 training step(s), loss on training batch is 0.0516698.
After 781 training step(s), loss on training batch is 0.0264778.
After 782 training step(s), loss on training batch is 0.0226714.
After 783 training step(s), loss on training batch is 0.0362095.
After 784 training step(s), loss on training batch is 0.0789396.
After 785 training step(s), loss on training batch is 0.0508994.
After 786 training step(s), loss on training batch is 0.0466703.
After 787 training step(s), loss on training batch is 0.040969.
After 788 training step(s), loss on training batch is 0.0605773.
After 789 training step(s), loss on training batch is 0.0417921.
After 790 training step(s), loss on training batch is 0.0418964.
After 791 training step(s), loss on training batch is 0.0546296.
After 792 training step(s), loss on training batch is 0.0730876.
After 793 training step(s), loss on training batch is 0.0708553.
After 794 training step(s), loss on training batch is 0.0292049.
After 795 training step(s), loss on training batch is 0.0311047.
After 796 training step(s), loss on training batch is 0.0369122.
After 797 training step(s), loss on training batch is 0.0920493.
After 798 training step(s), loss on training batch is 0.0461168.
After 799 training step(s), loss on training batch is 0.0397733.
After 800 training step(s), loss on training batch is 0.0966563.
After 801 training step(s), loss on training batch is 0.0503283.
After 802 training step(s), loss on training batch is 0.101657.
After 803 training step(s), loss on training batch is 0.0492931.
After 804 training step(s), loss on training batch is 0.0402454.
After 805 training step(s), loss on training batch is 0.0499433.
After 806 training step(s), loss on training batch is 0.0612309.
After 807 training step(s), loss on training batch is 0.0637437.
After 808 training step(s), loss on training batch is 0.0494209.
After 809 training step(s), loss on training batch is 0.0376408.
After 810 training step(s), loss on training batch is 0.0318903.
After 811 training step(s), loss on training batch is 0.0458075.
After 812 training step(s), loss on training batch is 0.0745296.
After 813 training step(s), loss on training batch is 0.0475842.
After 814 training step(s), loss on training batch is 0.0752955.
After 815 training step(s), loss on training batch is 0.0661291.
After 816 training step(s), loss on training batch is 0.0472664.
After 817 training step(s), loss on training batch is 0.0805349.
After 818 training step(s), loss on training batch is 0.0574911.
After 819 training step(s), loss on training batch is 0.0238298.
After 820 training step(s), loss on training batch is 0.046591.
After 821 training step(s), loss on training batch is 0.0213838.
After 822 training step(s), loss on training batch is 0.0437917.
After 823 training step(s), loss on training batch is 0.0464918.
After 824 training step(s), loss on training batch is 0.0401652.
After 825 training step(s), loss on training batch is 0.061908.
After 826 training step(s), loss on training batch is 0.0615604.
After 827 training step(s), loss on training batch is 0.0713115.
After 828 training step(s), loss on training batch is 0.0347069.
After 829 training step(s), loss on training batch is 0.0339949.
After 830 training step(s), loss on training batch is 0.036808.
After 831 training step(s), loss on training batch is 0.0342709.
After 832 training step(s), loss on training batch is 0.0353693.
After 833 training step(s), loss on training batch is 0.0334521.
After 834 training step(s), loss on training batch is 0.0521567.
After 835 training step(s), loss on training batch is 0.0427959.
After 836 training step(s), loss on training batch is 0.0367736.
After 837 training step(s), loss on training batch is 0.0321391.
After 838 training step(s), loss on training batch is 0.0762804.
After 839 training step(s), loss on training batch is 0.0400162.
After 840 training step(s), loss on training batch is 0.0550656.
After 841 training step(s), loss on training batch is 0.0519993.
After 842 training step(s), loss on training batch is 0.0596512.
After 843 training step(s), loss on training batch is 0.0491788.
After 844 training step(s), loss on training batch is 0.0451214.
After 845 training step(s), loss on training batch is 0.0272095.
After 846 training step(s), loss on training batch is 0.077977.
After 847 training step(s), loss on training batch is 0.0637878.
After 848 training step(s), loss on training batch is 0.0237263.
After 849 training step(s), loss on training batch is 0.0382649.
After 850 training step(s), loss on training batch is 0.0244847.
After 851 training step(s), loss on training batch is 0.0340671.
After 852 training step(s), loss on training batch is 0.0513866.
After 853 training step(s), loss on training batch is 0.0276767.
After 854 training step(s), loss on training batch is 0.0526735.
After 855 training step(s), loss on training batch is 0.0475344.
After 856 training step(s), loss on training batch is 0.0667908.
After 857 training step(s), loss on training batch is 0.031606.
After 858 training step(s), loss on training batch is 0.0354275.
After 859 training step(s), loss on training batch is 0.0529789.
After 860 training step(s), loss on training batch is 0.0515478.
After 861 training step(s), loss on training batch is 0.0438618.
After 862 training step(s), loss on training batch is 0.0426341.
After 863 training step(s), loss on training batch is 0.0535977.
After 864 training step(s), loss on training batch is 0.0429379.
After 865 training step(s), loss on training batch is 0.033055.
After 866 training step(s), loss on training batch is 0.0547814.
After 867 training step(s), loss on training batch is 0.0491018.
After 868 training step(s), loss on training batch is 0.0411599.
After 869 training step(s), loss on training batch is 0.0673574.
After 870 training step(s), loss on training batch is 0.0597032.
After 871 training step(s), loss on training batch is 0.0987034.
After 872 training step(s), loss on training batch is 0.063953.
After 873 training step(s), loss on training batch is 0.0532166.
After 874 training step(s), loss on training batch is 0.0516659.
After 875 training step(s), loss on training batch is 0.0495355.
After 876 training step(s), loss on training batch is 0.0717574.
After 877 training step(s), loss on training batch is 0.0446186.
After 878 training step(s), loss on training batch is 0.0260598.
After 879 training step(s), loss on training batch is 0.0416665.
After 880 training step(s), loss on training batch is 0.0274856.
After 881 training step(s), loss on training batch is 0.0214862.
After 882 training step(s), loss on training batch is 0.0272434.
After 883 training step(s), loss on training batch is 0.0463589.
After 884 training step(s), loss on training batch is 0.041524.
After 885 training step(s), loss on training batch is 0.0531484.
After 886 training step(s), loss on training batch is 0.0445162.
After 887 training step(s), loss on training batch is 0.0548105.
After 888 training step(s), loss on training batch is 0.0221338.
After 889 training step(s), loss on training batch is 0.0389701.
After 890 training step(s), loss on training batch is 0.0342053.
After 891 training step(s), loss on training batch is 0.0534086.
After 892 training step(s), loss on training batch is 0.0490819.
After 893 training step(s), loss on training batch is 0.0445596.
After 894 training step(s), loss on training batch is 0.0606603.
After 895 training step(s), loss on training batch is 0.12294.
After 896 training step(s), loss on training batch is 0.105849.
After 897 training step(s), loss on training batch is 0.0498594.
After 898 training step(s), loss on training batch is 0.0353033.
After 899 training step(s), loss on training batch is 0.0427094.
After 900 training step(s), loss on training batch is 0.0447794.
After 901 training step(s), loss on training batch is 0.039676.
After 902 training step(s), loss on training batch is 0.0767737.
After 903 training step(s), loss on training batch is 0.0330757.
After 904 training step(s), loss on training batch is 0.0423809.
After 905 training step(s), loss on training batch is 0.0480372.
After 906 training step(s), loss on training batch is 0.071061.
After 907 training step(s), loss on training batch is 0.0465362.
After 908 training step(s), loss on training batch is 0.0418755.
After 909 training step(s), loss on training batch is 0.0310013.
After 910 training step(s), loss on training batch is 0.0314684.
After 911 training step(s), loss on training batch is 0.0272727.
After 912 training step(s), loss on training batch is 0.0374176.
After 913 training step(s), loss on training batch is 0.0279842.
After 914 training step(s), loss on training batch is 0.0382279.
After 915 training step(s), loss on training batch is 0.0692307.
After 916 training step(s), loss on training batch is 0.0483192.
After 917 training step(s), loss on training batch is 0.0652459.
After 918 training step(s), loss on training batch is 0.0296863.
After 919 training step(s), loss on training batch is 0.0242142.
After 920 training step(s), loss on training batch is 0.0389558.
After 921 training step(s), loss on training batch is 0.0397702.
After 922 training step(s), loss on training batch is 0.031024.
After 923 training step(s), loss on training batch is 0.0237554.
After 924 training step(s), loss on training batch is 0.051941.
After 925 training step(s), loss on training batch is 0.0648977.
After 926 training step(s), loss on training batch is 0.0335123.
After 927 training step(s), loss on training batch is 0.0304716.
After 928 training step(s), loss on training batch is 0.0211506.
After 929 training step(s), loss on training batch is 0.0363475.
After 930 training step(s), loss on training batch is 0.0434168.
After 931 training step(s), loss on training batch is 0.0246641.
After 932 training step(s), loss on training batch is 0.0449666.
After 933 training step(s), loss on training batch is 0.0402592.
After 934 training step(s), loss on training batch is 0.0442453.
After 935 training step(s), loss on training batch is 0.0229911.
After 936 training step(s), loss on training batch is 0.0493151.
After 937 training step(s), loss on training batch is 0.0450142.
After 938 training step(s), loss on training batch is 0.0598447.
After 939 training step(s), loss on training batch is 0.0218366.
After 940 training step(s), loss on training batch is 0.0307258.
After 941 training step(s), loss on training batch is 0.0284884.
After 942 training step(s), loss on training batch is 0.0452875.
After 943 training step(s), loss on training batch is 0.0341052.
After 944 training step(s), loss on training batch is 0.0284404.
After 945 training step(s), loss on training batch is 0.0647206.
After 946 training step(s), loss on training batch is 0.0328907.
After 947 training step(s), loss on training batch is 0.0466287.
After 948 training step(s), loss on training batch is 0.0566687.
After 949 training step(s), loss on training batch is 0.0360043.
After 950 training step(s), loss on training batch is 0.069164.
After 951 training step(s), loss on training batch is 0.0448029.
After 952 training step(s), loss on training batch is 0.0379375.
After 953 training step(s), loss on training batch is 0.06477.
After 954 training step(s), loss on training batch is 0.0404535.
After 955 training step(s), loss on training batch is 0.0486708.
After 956 training step(s), loss on training batch is 0.0404754.
After 957 training step(s), loss on training batch is 0.0269296.
After 958 training step(s), loss on training batch is 0.0337937.
After 959 training step(s), loss on training batch is 0.0263081.
After 960 training step(s), loss on training batch is 0.0193502.
After 961 training step(s), loss on training batch is 0.0441002.
After 962 training step(s), loss on training batch is 0.0290822.
After 963 training step(s), loss on training batch is 0.060677.
After 964 training step(s), loss on training batch is 0.031392.
After 965 training step(s), loss on training batch is 0.0871063.
After 966 training step(s), loss on training batch is 0.0455616.
After 967 training step(s), loss on training batch is 0.0338462.
After 968 training step(s), loss on training batch is 0.0378261.
After 969 training step(s), loss on training batch is 0.0549893.
After 970 training step(s), loss on training batch is 0.0282718.
After 971 training step(s), loss on training batch is 0.0318951.
After 972 training step(s), loss on training batch is 0.0259257.
After 973 training step(s), loss on training batch is 0.0550009.
After 974 training step(s), loss on training batch is 0.0346734.
After 975 training step(s), loss on training batch is 0.0300916.
After 976 training step(s), loss on training batch is 0.0305529.
After 977 training step(s), loss on training batch is 0.0281479.
After 978 training step(s), loss on training batch is 0.0988417.
After 979 training step(s), loss on training batch is 0.0402325.
After 980 training step(s), loss on training batch is 0.051125.
After 981 training step(s), loss on training batch is 0.0523172.
After 982 training step(s), loss on training batch is 0.0667065.
After 983 training step(s), loss on training batch is 0.0317358.
After 984 training step(s), loss on training batch is 0.0614912.
After 985 training step(s), loss on training batch is 0.0261951.
After 986 training step(s), loss on training batch is 0.04625.
After 987 training step(s), loss on training batch is 0.0562148.
After 988 training step(s), loss on training batch is 0.0267463.
After 989 training step(s), loss on training batch is 0.0453264.
After 990 training step(s), loss on training batch is 0.0414603.
After 991 training step(s), loss on training batch is 0.0354822.
After 992 training step(s), loss on training batch is 0.0387751.
After 993 training step(s), loss on training batch is 0.0362349.
After 994 training step(s), loss on training batch is 0.0184692.
After 995 training step(s), loss on training batch is 0.0274569.
After 996 training step(s), loss on training batch is 0.0365342.
After 997 training step(s), loss on training batch is 0.0568035.
After 998 training step(s), loss on training batch is 0.0824654.
After 999 training step(s), loss on training batch is 0.0797872.
After 1000 training step(s), loss on training batch is 0.0300349.
After 1001 training step(s), loss on training batch is 0.0608214.
After 1002 training step(s), loss on training batch is 0.04025.
After 1003 training step(s), loss on training batch is 0.0533133.
After 1004 training step(s), loss on training batch is 0.0361927.
After 1005 training step(s), loss on training batch is 0.0716219.
After 1006 training step(s), loss on training batch is 0.0398983.
After 1007 training step(s), loss on training batch is 0.0469937.
After 1008 training step(s), loss on training batch is 0.0295338.
After 1009 training step(s), loss on training batch is 0.0411396.
After 1010 training step(s), loss on training batch is 0.027753.
After 1011 training step(s), loss on training batch is 0.0312197.
After 1012 training step(s), loss on training batch is 0.0448406.
After 1013 training step(s), loss on training batch is 0.0329405.
After 1014 training step(s), loss on training batch is 0.0394972.
After 1015 training step(s), loss on training batch is 0.0350432.
After 1016 training step(s), loss on training batch is 0.033915.
After 1017 training step(s), loss on training batch is 0.0307733.
After 1018 training step(s), loss on training batch is 0.0210447.
After 1019 training step(s), loss on training batch is 0.0551881.
After 1020 training step(s), loss on training batch is 0.0722147.
After 1021 training step(s), loss on training batch is 0.0646365.
After 1022 training step(s), loss on training batch is 0.0272751.
After 1023 training step(s), loss on training batch is 0.0609274.
After 1024 training step(s), loss on training batch is 0.0182638.
After 1025 training step(s), loss on training batch is 0.0327785.
After 1026 training step(s), loss on training batch is 0.0246511.
After 1027 training step(s), loss on training batch is 0.029271.
After 1028 training step(s), loss on training batch is 0.0430716.
After 1029 training step(s), loss on training batch is 0.079767.
After 1030 training step(s), loss on training batch is 0.101125.
After 1031 training step(s), loss on training batch is 0.058864.
After 1032 training step(s), loss on training batch is 0.0668941.
After 1033 training step(s), loss on training batch is 0.0443463.
After 1034 training step(s), loss on training batch is 0.0459215.
After 1035 training step(s), loss on training batch is 0.0567913.
After 1036 training step(s), loss on training batch is 0.0186824.
After 1037 training step(s), loss on training batch is 0.0315577.
After 1038 training step(s), loss on training batch is 0.035313.
After 1039 training step(s), loss on training batch is 0.0413719.
After 1040 training step(s), loss on training batch is 0.040092.
After 1041 training step(s), loss on training batch is 0.0318698.
After 1042 training step(s), loss on training batch is 0.0285014.
After 1043 training step(s), loss on training batch is 0.106046.
After 1044 training step(s), loss on training batch is 0.0356958.
After 1045 training step(s), loss on training batch is 0.0407339.
After 1046 training step(s), loss on training batch is 0.0300959.
After 1047 training step(s), loss on training batch is 0.056865.
After 1048 training step(s), loss on training batch is 0.0276564.
After 1049 training step(s), loss on training batch is 0.0262289.
After 1050 training step(s), loss on training batch is 0.0467205.
After 1051 training step(s), loss on training batch is 0.0232904.
After 1052 training step(s), loss on training batch is 0.0572547.
After 1053 training step(s), loss on training batch is 0.0263731.
After 1054 training step(s), loss on training batch is 0.0502108.
After 1055 training step(s), loss on training batch is 0.0261236.
After 1056 training step(s), loss on training batch is 0.0303216.
After 1057 training step(s), loss on training batch is 0.0397275.
After 1058 training step(s), loss on training batch is 0.0526683.
After 1059 training step(s), loss on training batch is 0.0531923.
After 1060 training step(s), loss on training batch is 0.0253147.
After 1061 training step(s), loss on training batch is 0.0457729.
After 1062 training step(s), loss on training batch is 0.0434222.
After 1063 training step(s), loss on training batch is 0.0322657.
After 1064 training step(s), loss on training batch is 0.0377671.
After 1065 training step(s), loss on training batch is 0.0689983.
After 1066 training step(s), loss on training batch is 0.0319411.
After 1067 training step(s), loss on training batch is 0.0208558.
After 1068 training step(s), loss on training batch is 0.0373599.
After 1069 training step(s), loss on training batch is 0.0300127.
After 1070 training step(s), loss on training batch is 0.0542343.
After 1071 training step(s), loss on training batch is 0.0285399.
After 1072 training step(s), loss on training batch is 0.0224557.
After 1073 training step(s), loss on training batch is 0.0180369.
After 1074 training step(s), loss on training batch is 0.0308974.
After 1075 training step(s), loss on training batch is 0.0186309.
After 1076 training step(s), loss on training batch is 0.0455246.
After 1077 training step(s), loss on training batch is 0.0549674.
After 1078 training step(s), loss on training batch is 0.0393378.
After 1079 training step(s), loss on training batch is 0.0238309.
After 1080 training step(s), loss on training batch is 0.0371327.
After 1081 training step(s), loss on training batch is 0.0185503.
After 1082 training step(s), loss on training batch is 0.0527202.
After 1083 training step(s), loss on training batch is 0.0381277.
After 1084 training step(s), loss on training batch is 0.0324228.
After 1085 training step(s), loss on training batch is 0.0467183.
After 1086 training step(s), loss on training batch is 0.0305485.
After 1087 training step(s), loss on training batch is 0.0258819.
After 1088 training step(s), loss on training batch is 0.0193643.
After 1089 training step(s), loss on training batch is 0.0432487.
After 1090 training step(s), loss on training batch is 0.0581057.
After 1091 training step(s), loss on training batch is 0.0534484.
After 1092 training step(s), loss on training batch is 0.0661085.
After 1093 training step(s), loss on training batch is 0.0738474.
After 1094 training step(s), loss on training batch is 0.161383.
After 1095 training step(s), loss on training batch is 0.036352.
After 1096 training step(s), loss on training batch is 0.0295663.
After 1097 training step(s), loss on training batch is 0.0443761.
After 1098 training step(s), loss on training batch is 0.0234048.
After 1099 training step(s), loss on training batch is 0.0487888.
After 1100 training step(s), loss on training batch is 0.0360059.
After 1101 training step(s), loss on training batch is 0.0235985.
After 1102 training step(s), loss on training batch is 0.0687598.
After 1103 training step(s), loss on training batch is 0.0329114.
After 1104 training step(s), loss on training batch is 0.0270365.
After 1105 training step(s), loss on training batch is 0.0302572.
After 1106 training step(s), loss on training batch is 0.0336491.
After 1107 training step(s), loss on training batch is 0.0284427.
After 1108 training step(s), loss on training batch is 0.0217767.
After 1109 training step(s), loss on training batch is 0.0243858.
After 1110 training step(s), loss on training batch is 0.0208916.
After 1111 training step(s), loss on training batch is 0.0307865.
After 1112 training step(s), loss on training batch is 0.02736.
After 1113 training step(s), loss on training batch is 0.0784482.
After 1114 training step(s), loss on training batch is 0.0381598.
After 1115 training step(s), loss on training batch is 0.0278849.
After 1116 training step(s), loss on training batch is 0.0485016.
After 1117 training step(s), loss on training batch is 0.0345343.
After 1118 training step(s), loss on training batch is 0.030904.
After 1119 training step(s), loss on training batch is 0.0457509.
After 1120 training step(s), loss on training batch is 0.0747124.
After 1121 training step(s), loss on training batch is 0.044293.
After 1122 training step(s), loss on training batch is 0.0269428.
After 1123 training step(s), loss on training batch is 0.0508857.
After 1124 training step(s), loss on training batch is 0.022849.
After 1125 training step(s), loss on training batch is 0.0328644.
After 1126 training step(s), loss on training batch is 0.0266989.
After 1127 training step(s), loss on training batch is 0.0231174.
After 1128 training step(s), loss on training batch is 0.0185503.
After 1129 training step(s), loss on training batch is 0.0381412.
After 1130 training step(s), loss on training batch is 0.0331952.
After 1131 training step(s), loss on training batch is 0.0282216.
After 1132 training step(s), loss on training batch is 0.0499876.
After 1133 training step(s), loss on training batch is 0.0303757.
After 1134 training step(s), loss on training batch is 0.0349588.
After 1135 training step(s), loss on training batch is 0.0311482.
After 1136 training step(s), loss on training batch is 0.0291142.
After 1137 training step(s), loss on training batch is 0.0334575.
After 1138 training step(s), loss on training batch is 0.0533011.
After 1139 training step(s), loss on training batch is 0.0258174.
After 1140 training step(s), loss on training batch is 0.0224496.
After 1141 training step(s), loss on training batch is 0.0447984.
After 1142 training step(s), loss on training batch is 0.0261989.
After 1143 training step(s), loss on training batch is 0.0391354.
After 1144 training step(s), loss on training batch is 0.0455888.
After 1145 training step(s), loss on training batch is 0.018649.
After 1146 training step(s), loss on training batch is 0.0346613.
After 1147 training step(s), loss on training batch is 0.0380066.
After 1148 training step(s), loss on training batch is 0.0204798.
After 1149 training step(s), loss on training batch is 0.030616.
After 1150 training step(s), loss on training batch is 0.028369.
After 1151 training step(s), loss on training batch is 0.0314208.
After 1152 training step(s), loss on training batch is 0.0240797.
After 1153 training step(s), loss on training batch is 0.0242813.
After 1154 training step(s), loss on training batch is 0.023195.
After 1155 training step(s), loss on training batch is 0.0631434.
After 1156 training step(s), loss on training batch is 0.0445393.
After 1157 training step(s), loss on training batch is 0.0472569.
After 1158 training step(s), loss on training batch is 0.0238436.
After 1159 training step(s), loss on training batch is 0.0266539.
After 1160 training step(s), loss on training batch is 0.0157324.
After 1161 training step(s), loss on training batch is 0.0211887.
After 1162 training step(s), loss on training batch is 0.0285358.
After 1163 training step(s), loss on training batch is 0.0384368.
After 1164 training step(s), loss on training batch is 0.03654.
After 1165 training step(s), loss on training batch is 0.0357516.
After 1166 training step(s), loss on training batch is 0.0404951.
After 1167 training step(s), loss on training batch is 0.0363483.
After 1168 training step(s), loss on training batch is 0.0265509.
After 1169 training step(s), loss on training batch is 0.0321443.
After 1170 training step(s), loss on training batch is 0.0343774.
After 1171 training step(s), loss on training batch is 0.0208558.
After 1172 training step(s), loss on training batch is 0.0266428.
After 1173 training step(s), loss on training batch is 0.0282816.
After 1174 training step(s), loss on training batch is 0.0302926.
After 1175 training step(s), loss on training batch is 0.0156852.
After 1176 training step(s), loss on training batch is 0.0346098.
After 1177 training step(s), loss on training batch is 0.0348475.
After 1178 training step(s), loss on training batch is 0.0290495.
After 1179 training step(s), loss on training batch is 0.0594342.
After 1180 training step(s), loss on training batch is 0.0591989.
After 1181 training step(s), loss on training batch is 0.0290642.
After 1182 training step(s), loss on training batch is 0.0276393.
After 1183 training step(s), loss on training batch is 0.0289791.
After 1184 training step(s), loss on training batch is 0.036359.
After 1185 training step(s), loss on training batch is 0.0422601.
After 1186 training step(s), loss on training batch is 0.0663789.
After 1187 training step(s), loss on training batch is 0.047959.
After 1188 training step(s), loss on training batch is 0.0326809.
After 1189 training step(s), loss on training batch is 0.0447735.
After 1190 training step(s), loss on training batch is 0.0336429.
After 1191 training step(s), loss on training batch is 0.0178658.
After 1192 training step(s), loss on training batch is 0.0330491.
After 1193 training step(s), loss on training batch is 0.0264178.
After 1194 training step(s), loss on training batch is 0.037241.
After 1195 training step(s), loss on training batch is 0.0494331.
After 1196 training step(s), loss on training batch is 0.0474752.
After 1197 training step(s), loss on training batch is 0.0215322.
After 1198 training step(s), loss on training batch is 0.0291038.
After 1199 training step(s), loss on training batch is 0.0485613.
After 1200 training step(s), loss on training batch is 0.0162908.
After 1201 training step(s), loss on training batch is 0.0191535.
After 1202 training step(s), loss on training batch is 0.0247858.
After 1203 training step(s), loss on training batch is 0.0531261.
After 1204 training step(s), loss on training batch is 0.0295117.
After 1205 training step(s), loss on training batch is 0.0884214.
After 1206 training step(s), loss on training batch is 0.0265949.
After 1207 training step(s), loss on training batch is 0.034582.
After 1208 training step(s), loss on training batch is 0.0576617.
After 1209 training step(s), loss on training batch is 0.0279353.
After 1210 training step(s), loss on training batch is 0.0268136.
After 1211 training step(s), loss on training batch is 0.0321718.
After 1212 training step(s), loss on training batch is 0.0246852.
After 1213 training step(s), loss on training batch is 0.0192896.
After 1214 training step(s), loss on training batch is 0.0495594.
After 1215 training step(s), loss on training batch is 0.0331925.
After 1216 training step(s), loss on training batch is 0.0695807.
After 1217 training step(s), loss on training batch is 0.0367472.
After 1218 training step(s), loss on training batch is 0.0509327.
After 1219 training step(s), loss on training batch is 0.0236222.
After 1220 training step(s), loss on training batch is 0.0339041.
After 1221 training step(s), loss on training batch is 0.121398.
After 1222 training step(s), loss on training batch is 0.0238172.
After 1223 training step(s), loss on training batch is 0.0613112.
After 1224 training step(s), loss on training batch is 0.0555225.
After 1225 training step(s), loss on training batch is 0.0222134.
After 1226 training step(s), loss on training batch is 0.0188917.
After 1227 training step(s), loss on training batch is 0.021479.
After 1228 training step(s), loss on training batch is 0.0301151.
After 1229 training step(s), loss on training batch is 0.0342998.
After 1230 training step(s), loss on training batch is 0.0331275.
After 1231 training step(s), loss on training batch is 0.0188258.
After 1232 training step(s), loss on training batch is 0.0545307.
After 1233 training step(s), loss on training batch is 0.0766329.
After 1234 training step(s), loss on training batch is 0.0291597.
After 1235 training step(s), loss on training batch is 0.0269131.
After 1236 training step(s), loss on training batch is 0.0296224.
After 1237 training step(s), loss on training batch is 0.0660096.
After 1238 training step(s), loss on training batch is 0.0393473.
After 1239 training step(s), loss on training batch is 0.0586891.
After 1240 training step(s), loss on training batch is 0.0307276.
After 1241 training step(s), loss on training batch is 0.0207425.
After 1242 training step(s), loss on training batch is 0.0353249.
After 1243 training step(s), loss on training batch is 0.0279944.
After 1244 training step(s), loss on training batch is 0.0235212.
After 1245 training step(s), loss on training batch is 0.0756741.
After 1246 training step(s), loss on training batch is 0.0326557.
After 1247 training step(s), loss on training batch is 0.0229876.
After 1248 training step(s), loss on training batch is 0.0345202.
After 1249 training step(s), loss on training batch is 0.0350715.
After 1250 training step(s), loss on training batch is 0.026983.
After 1251 training step(s), loss on training batch is 0.032504.
After 1252 training step(s), loss on training batch is 0.0302193.
After 1253 training step(s), loss on training batch is 0.0858718.
After 1254 training step(s), loss on training batch is 0.0448738.
After 1255 training step(s), loss on training batch is 0.0232567.
After 1256 training step(s), loss on training batch is 0.0300045.
After 1257 training step(s), loss on training batch is 0.0209284.
After 1258 training step(s), loss on training batch is 0.0327713.
After 1259 training step(s), loss on training batch is 0.030569.
After 1260 training step(s), loss on training batch is 0.0308919.
After 1261 training step(s), loss on training batch is 0.0197111.
After 1262 training step(s), loss on training batch is 0.0189573.
After 1263 training step(s), loss on training batch is 0.0278569.
After 1264 training step(s), loss on training batch is 0.0245692.
After 1265 training step(s), loss on training batch is 0.0579936.
After 1266 training step(s), loss on training batch is 0.0758227.
After 1267 training step(s), loss on training batch is 0.0373987.
After 1268 training step(s), loss on training batch is 0.0704924.
After 1269 training step(s), loss on training batch is 0.0714961.
After 1270 training step(s), loss on training batch is 0.0397526.
After 1271 training step(s), loss on training batch is 0.0260559.
After 1272 training step(s), loss on training batch is 0.0301811.
After 1273 training step(s), loss on training batch is 0.0551541.
After 1274 training step(s), loss on training batch is 0.0673947.
After 1275 training step(s), loss on training batch is 0.0712734.
After 1276 training step(s), loss on training batch is 0.018263.
After 1277 training step(s), loss on training batch is 0.0750481.
After 1278 training step(s), loss on training batch is 0.0231277.
After 1279 training step(s), loss on training batch is 0.033092.
After 1280 training step(s), loss on training batch is 0.0404777.
After 1281 training step(s), loss on training batch is 0.0909809.
After 1282 training step(s), loss on training batch is 0.0677932.
After 1283 training step(s), loss on training batch is 0.0325205.
After 1284 training step(s), loss on training batch is 0.0530102.
After 1285 training step(s), loss on training batch is 0.024944.
After 1286 training step(s), loss on training batch is 0.0302239.
After 1287 training step(s), loss on training batch is 0.0418468.
After 1288 training step(s), loss on training batch is 0.11541.
After 1289 training step(s), loss on training batch is 0.0271366.
After 1290 training step(s), loss on training batch is 0.0532165.
After 1291 training step(s), loss on training batch is 0.0171094.
After 1292 training step(s), loss on training batch is 0.0322126.
After 1293 training step(s), loss on training batch is 0.0174496.
After 1294 training step(s), loss on training batch is 0.0264868.
After 1295 training step(s), loss on training batch is 0.0269097.
After 1296 training step(s), loss on training batch is 0.0186585.
After 1297 training step(s), loss on training batch is 0.0191539.
After 1298 training step(s), loss on training batch is 0.0327645.
After 1299 training step(s), loss on training batch is 0.0677939.
After 1300 training step(s), loss on training batch is 0.0512151.
After 1301 training step(s), loss on training batch is 0.0296437.
After 1302 training step(s), loss on training batch is 0.0172957.
After 1303 training step(s), loss on training batch is 0.0491664.
After 1304 training step(s), loss on training batch is 0.0431277.
After 1305 training step(s), loss on training batch is 0.0833543.
After 1306 training step(s), loss on training batch is 0.0509014.
After 1307 training step(s), loss on training batch is 0.0357583.
After 1308 training step(s), loss on training batch is 0.0180776.
After 1309 training step(s), loss on training batch is 0.0504697.
After 1310 training step(s), loss on training batch is 0.0155134.
After 1311 training step(s), loss on training batch is 0.020743.
After 1312 training step(s), loss on training batch is 0.0342688.
After 1313 training step(s), loss on training batch is 0.0504861.
After 1314 training step(s), loss on training batch is 0.0210021.
After 1315 training step(s), loss on training batch is 0.05137.
After 1316 training step(s), loss on training batch is 0.025018.
After 1317 training step(s), loss on training batch is 0.0253138.
After 1318 training step(s), loss on training batch is 0.0299372.
After 1319 training step(s), loss on training batch is 0.0493419.
After 1320 training step(s), loss on training batch is 0.0377766.
After 1321 training step(s), loss on training batch is 0.0388172.
After 1322 training step(s), loss on training batch is 0.0523162.
After 1323 training step(s), loss on training batch is 0.0381901.
After 1324 training step(s), loss on training batch is 0.0251797.
After 1325 training step(s), loss on training batch is 0.0286203.
After 1326 training step(s), loss on training batch is 0.0174915.
After 1327 training step(s), loss on training batch is 0.0185905.
After 1328 training step(s), loss on training batch is 0.071682.
After 1329 training step(s), loss on training batch is 0.0291439.
After 1330 training step(s), loss on training batch is 0.0342339.
After 1331 training step(s), loss on training batch is 0.0257072.
After 1332 training step(s), loss on training batch is 0.0308551.
After 1333 training step(s), loss on training batch is 0.0524378.
After 1334 training step(s), loss on training batch is 0.0521607.
After 1335 training step(s), loss on training batch is 0.0316298.
After 1336 training step(s), loss on training batch is 0.0377766.
After 1337 training step(s), loss on training batch is 0.0508574.
After 1338 training step(s), loss on training batch is 0.0302153.
After 1339 training step(s), loss on training batch is 0.0595516.
After 1340 training step(s), loss on training batch is 0.0667434.
After 1341 training step(s), loss on training batch is 0.0256209.
After 1342 training step(s), loss on training batch is 0.0212357.
After 1343 training step(s), loss on training batch is 0.0303223.
After 1344 training step(s), loss on training batch is 0.0217841.
After 1345 training step(s), loss on training batch is 0.0441915.
After 1346 training step(s), loss on training batch is 0.0249.
After 1347 training step(s), loss on training batch is 0.0264099.
After 1348 training step(s), loss on training batch is 0.0327769.
After 1349 training step(s), loss on training batch is 0.021866.
After 1350 training step(s), loss on training batch is 0.0434161.
After 1351 training step(s), loss on training batch is 0.0320493.
After 1352 training step(s), loss on training batch is 0.0389798.
After 1353 training step(s), loss on training batch is 0.0993648.
After 1354 training step(s), loss on training batch is 0.0196894.
After 1355 training step(s), loss on training batch is 0.0165169.
After 1356 training step(s), loss on training batch is 0.0387988.
After 1357 training step(s), loss on training batch is 0.0451384.
After 1358 training step(s), loss on training batch is 0.0262372.
After 1359 training step(s), loss on training batch is 0.0234634.
After 1360 training step(s), loss on training batch is 0.0177488.
After 1361 training step(s), loss on training batch is 0.0373985.
After 1362 training step(s), loss on training batch is 0.0461258.
After 1363 training step(s), loss on training batch is 0.02034.
After 1364 training step(s), loss on training batch is 0.0226968.
After 1365 training step(s), loss on training batch is 0.050586.
After 1366 training step(s), loss on training batch is 0.0480719.
After 1367 training step(s), loss on training batch is 0.0264771.
After 1368 training step(s), loss on training batch is 0.0387316.
After 1369 training step(s), loss on training batch is 0.0315453.
After 1370 training step(s), loss on training batch is 0.0198705.
After 1371 training step(s), loss on training batch is 0.036336.
After 1372 training step(s), loss on training batch is 0.0195875.
After 1373 training step(s), loss on training batch is 0.0455308.
After 1374 training step(s), loss on training batch is 0.0221771.
After 1375 training step(s), loss on training batch is 0.0302759.
After 1376 training step(s), loss on training batch is 0.0249432.
After 1377 training step(s), loss on training batch is 0.028139.
After 1378 training step(s), loss on training batch is 0.0318628.
After 1379 training step(s), loss on training batch is 0.0183798.
After 1380 training step(s), loss on training batch is 0.055193.
After 1381 training step(s), loss on training batch is 0.0218392.
After 1382 training step(s), loss on training batch is 0.0176448.
After 1383 training step(s), loss on training batch is 0.0270858.
After 1384 training step(s), loss on training batch is 0.0836423.
After 1385 training step(s), loss on training batch is 0.028031.
After 1386 training step(s), loss on training batch is 0.021364.
After 1387 training step(s), loss on training batch is 0.022783.
After 1388 training step(s), loss on training batch is 0.0258102.
After 1389 training step(s), loss on training batch is 0.0246851.
After 1390 training step(s), loss on training batch is 0.031596.
After 1391 training step(s), loss on training batch is 0.0299319.
After 1392 training step(s), loss on training batch is 0.0290448.
After 1393 training step(s), loss on training batch is 0.0289902.
After 1394 training step(s), loss on training batch is 0.0215432.
After 1395 training step(s), loss on training batch is 0.0198274.
After 1396 training step(s), loss on training batch is 0.0312152.
After 1397 training step(s), loss on training batch is 0.0381617.
After 1398 training step(s), loss on training batch is 0.0234113.
After 1399 training step(s), loss on training batch is 0.0311901.
After 1400 training step(s), loss on training batch is 0.0243628.
After 1401 training step(s), loss on training batch is 0.0212806.
After 1402 training step(s), loss on training batch is 0.0307541.
After 1403 training step(s), loss on training batch is 0.0228887.
After 1404 training step(s), loss on training batch is 0.0394971.
After 1405 training step(s), loss on training batch is 0.0468033.
After 1406 training step(s), loss on training batch is 0.0169717.
After 1407 training step(s), loss on training batch is 0.0207917.
After 1408 training step(s), loss on training batch is 0.0278506.
After 1409 training step(s), loss on training batch is 0.0551841.
After 1410 training step(s), loss on training batch is 0.0368505.
After 1411 training step(s), loss on training batch is 0.015545.
After 1412 training step(s), loss on training batch is 0.0213359.
After 1413 training step(s), loss on training batch is 0.0194083.
After 1414 training step(s), loss on training batch is 0.017882.
After 1415 training step(s), loss on training batch is 0.0195866.
After 1416 training step(s), loss on training batch is 0.0241552.
After 1417 training step(s), loss on training batch is 0.0224992.
After 1418 training step(s), loss on training batch is 0.0154121.
After 1419 training step(s), loss on training batch is 0.0187038.
After 1420 training step(s), loss on training batch is 0.0179316.
After 1421 training step(s), loss on training batch is 0.0167426.
After 1422 training step(s), loss on training batch is 0.0261633.
After 1423 training step(s), loss on training batch is 0.0198913.
After 1424 training step(s), loss on training batch is 0.0231263.
After 1425 training step(s), loss on training batch is 0.025016.
After 1426 training step(s), loss on training batch is 0.0220769.
After 1427 training step(s), loss on training batch is 0.0217437.
After 1428 training step(s), loss on training batch is 0.0402732.
After 1429 training step(s), loss on training batch is 0.0171092.
After 1430 training step(s), loss on training batch is 0.0323905.
After 1431 training step(s), loss on training batch is 0.0307424.
After 1432 training step(s), loss on training batch is 0.024547.
After 1433 training step(s), loss on training batch is 0.0257389.
After 1434 training step(s), loss on training batch is 0.0153391.
After 1435 training step(s), loss on training batch is 0.0346874.
After 1436 training step(s), loss on training batch is 0.0180117.
After 1437 training step(s), loss on training batch is 0.035031.
After 1438 training step(s), loss on training batch is 0.0341728.
After 1439 training step(s), loss on training batch is 0.0278917.
After 1440 training step(s), loss on training batch is 0.0440647.
After 1441 training step(s), loss on training batch is 0.0343332.
After 1442 training step(s), loss on training batch is 0.029591.
After 1443 training step(s), loss on training batch is 0.0274904.
After 1444 training step(s), loss on training batch is 0.0669115.
After 1445 training step(s), loss on training batch is 0.033766.
After 1446 training step(s), loss on training batch is 0.0537711.
After 1447 training step(s), loss on training batch is 0.0252778.
After 1448 training step(s), loss on training batch is 0.0214532.
After 1449 training step(s), loss on training batch is 0.0269078.
After 1450 training step(s), loss on training batch is 0.0274934.
After 1451 training step(s), loss on training batch is 0.0342113.
After 1452 training step(s), loss on training batch is 0.0378377.
After 1453 training step(s), loss on training batch is 0.0175544.
After 1454 training step(s), loss on training batch is 0.0614065.
After 1455 training step(s), loss on training batch is 0.0351859.
After 1456 training step(s), loss on training batch is 0.023887.
After 1457 training step(s), loss on training batch is 0.0144398.
After 1458 training step(s), loss on training batch is 0.0583425.
After 1459 training step(s), loss on training batch is 0.033406.
After 1460 training step(s), loss on training batch is 0.0366645.
After 1461 training step(s), loss on training batch is 0.0206667.
After 1462 training step(s), loss on training batch is 0.0179971.
After 1463 training step(s), loss on training batch is 0.0172206.
After 1464 training step(s), loss on training batch is 0.040188.
After 1465 training step(s), loss on training batch is 0.033619.
After 1466 training step(s), loss on training batch is 0.0206054.
After 1467 training step(s), loss on training batch is 0.031332.
After 1468 training step(s), loss on training batch is 0.0356346.
After 1469 training step(s), loss on training batch is 0.0195379.
After 1470 training step(s), loss on training batch is 0.0236951.
After 1471 training step(s), loss on training batch is 0.0207594.
After 1472 training step(s), loss on training batch is 0.0200489.
After 1473 training step(s), loss on training batch is 0.01444.
After 1474 training step(s), loss on training batch is 0.0230474.
After 1475 training step(s), loss on training batch is 0.0279448.
After 1476 training step(s), loss on training batch is 0.0186447.
After 1477 training step(s), loss on training batch is 0.0145439.
After 1478 training step(s), loss on training batch is 0.0153573.
After 1479 training step(s), loss on training batch is 0.0463721.
After 1480 training step(s), loss on training batch is 0.0204578.
After 1481 training step(s), loss on training batch is 0.0181571.
After 1482 training step(s), loss on training batch is 0.0240108.
After 1483 training step(s), loss on training batch is 0.021345.
After 1484 training step(s), loss on training batch is 0.0237837.
After 1485 training step(s), loss on training batch is 0.0307689.
After 1486 training step(s), loss on training batch is 0.0167571.
After 1487 training step(s), loss on training batch is 0.0165161.
After 1488 training step(s), loss on training batch is 0.0419857.
After 1489 training step(s), loss on training batch is 0.045235.
After 1490 training step(s), loss on training batch is 0.0308695.
After 1491 training step(s), loss on training batch is 0.0453905.
After 1492 training step(s), loss on training batch is 0.0333627.
After 1493 training step(s), loss on training batch is 0.0375897.
After 1494 training step(s), loss on training batch is 0.0379055.
After 1495 training step(s), loss on training batch is 0.0365938.
After 1496 training step(s), loss on training batch is 0.0213996.
After 1497 training step(s), loss on training batch is 0.0181706.
After 1498 training step(s), loss on training batch is 0.0777215.
After 1499 training step(s), loss on training batch is 0.0214043.
After 1500 training step(s), loss on training batch is 0.0257389.
After 1501 training step(s), loss on training batch is 0.0320644.
After 1502 training step(s), loss on training batch is 0.0288399.
After 1503 training step(s), loss on training batch is 0.0305121.
After 1504 training step(s), loss on training batch is 0.0252315.
After 1505 training step(s), loss on training batch is 0.0276591.
After 1506 training step(s), loss on training batch is 0.0216813.
After 1507 training step(s), loss on training batch is 0.0496406.
After 1508 training step(s), loss on training batch is 0.0150503.
After 1509 training step(s), loss on training batch is 0.0439257.
After 1510 training step(s), loss on training batch is 0.0376886.
After 1511 training step(s), loss on training batch is 0.026701.
After 1512 training step(s), loss on training batch is 0.0215784.
After 1513 training step(s), loss on training batch is 0.0288534.
After 1514 training step(s), loss on training batch is 0.0369619.
After 1515 training step(s), loss on training batch is 0.0294342.
After 1516 training step(s), loss on training batch is 0.15585.
After 1517 training step(s), loss on training batch is 0.0800658.
After 1518 training step(s), loss on training batch is 0.0167097.
After 1519 training step(s), loss on training batch is 0.0560097.
After 1520 training step(s), loss on training batch is 0.0328621.
After 1521 training step(s), loss on training batch is 0.0278411.
After 1522 training step(s), loss on training batch is 0.0420811.
After 1523 training step(s), loss on training batch is 0.0342753.
After 1524 training step(s), loss on training batch is 0.0159897.
After 1525 training step(s), loss on training batch is 0.0372598.
After 1526 training step(s), loss on training batch is 0.0207768.
After 1527 training step(s), loss on training batch is 0.0257793.
After 1528 training step(s), loss on training batch is 0.020016.
After 1529 training step(s), loss on training batch is 0.0199028.
After 1530 training step(s), loss on training batch is 0.0332496.
After 1531 training step(s), loss on training batch is 0.0341936.
After 1532 training step(s), loss on training batch is 0.0866355.
After 1533 training step(s), loss on training batch is 0.0383506.
After 1534 training step(s), loss on training batch is 0.0279245.
After 1535 training step(s), loss on training batch is 0.0185355.
After 1536 training step(s), loss on training batch is 0.0360818.
After 1537 training step(s), loss on training batch is 0.0229715.
After 1538 training step(s), loss on training batch is 0.099997.
After 1539 training step(s), loss on training batch is 0.0312009.
After 1540 training step(s), loss on training batch is 0.0379949.
After 1541 training step(s), loss on training batch is 0.0416514.
After 1542 training step(s), loss on training batch is 0.0331651.
After 1543 training step(s), loss on training batch is 0.0163169.
After 1544 training step(s), loss on training batch is 0.0345317.
After 1545 training step(s), loss on training batch is 0.0211669.
After 1546 training step(s), loss on training batch is 0.0287252.
After 1547 training step(s), loss on training batch is 0.0533933.
After 1548 training step(s), loss on training batch is 0.0587647.
After 1549 training step(s), loss on training batch is 0.027696.
After 1550 training step(s), loss on training batch is 0.0176072.
After 1551 training step(s), loss on training batch is 0.024667.
After 1552 training step(s), loss on training batch is 0.0434436.
After 1553 training step(s), loss on training batch is 0.0156503.
After 1554 training step(s), loss on training batch is 0.0199612.
After 1555 training step(s), loss on training batch is 0.0284859.
After 1556 training step(s), loss on training batch is 0.0232019.
After 1557 training step(s), loss on training batch is 0.034636.
After 1558 training step(s), loss on training batch is 0.0212711.
After 1559 training step(s), loss on training batch is 0.0254214.
After 1560 training step(s), loss on training batch is 0.0681605.
After 1561 training step(s), loss on training batch is 0.0186035.
After 1562 training step(s), loss on training batch is 0.0473696.
After 1563 training step(s), loss on training batch is 0.0195578.
After 1564 training step(s), loss on training batch is 0.0464189.
After 1565 training step(s), loss on training batch is 0.0293377.
After 1566 training step(s), loss on training batch is 0.0379793.
After 1567 training step(s), loss on training batch is 0.0438358.
After 1568 training step(s), loss on training batch is 0.0242651.
After 1569 training step(s), loss on training batch is 0.0416313.
After 1570 training step(s), loss on training batch is 0.0453775.
After 1571 training step(s), loss on training batch is 0.0249881.
After 1572 training step(s), loss on training batch is 0.026279.
After 1573 training step(s), loss on training batch is 0.0239189.
After 1574 training step(s), loss on training batch is 0.0260036.
After 1575 training step(s), loss on training batch is 0.0531958.
After 1576 training step(s), loss on training batch is 0.0943426.
After 1577 training step(s), loss on training batch is 0.040116.
After 1578 training step(s), loss on training batch is 0.0150381.
After 1579 training step(s), loss on training batch is 0.0276451.
After 1580 training step(s), loss on training batch is 0.0218277.
After 1581 training step(s), loss on training batch is 0.0202696.
After 1582 training step(s), loss on training batch is 0.0180064.
After 1583 training step(s), loss on training batch is 0.0220916.
After 1584 training step(s), loss on training batch is 0.0178799.
After 1585 training step(s), loss on training batch is 0.0207042.
After 1586 training step(s), loss on training batch is 0.0232756.
After 1587 training step(s), loss on training batch is 0.0342079.
After 1588 training step(s), loss on training batch is 0.0289186.
After 1589 training step(s), loss on training batch is 0.017427.
After 1590 training step(s), loss on training batch is 0.0372513.
After 1591 training step(s), loss on training batch is 0.0264058.
After 1592 training step(s), loss on training batch is 0.0187155.
After 1593 training step(s), loss on training batch is 0.0263016.
After 1594 training step(s), loss on training batch is 0.0269391.
After 1595 training step(s), loss on training batch is 0.0710639.
After 1596 training step(s), loss on training batch is 0.0335767.
After 1597 training step(s), loss on training batch is 0.026491.
After 1598 training step(s), loss on training batch is 0.031702.
After 1599 training step(s), loss on training batch is 0.0309337.
After 1600 training step(s), loss on training batch is 0.0189804.
After 1601 training step(s), loss on training batch is 0.0187307.
After 1602 training step(s), loss on training batch is 0.0178965.
After 1603 training step(s), loss on training batch is 0.0201634.
After 1604 training step(s), loss on training batch is 0.0306253.
After 1605 training step(s), loss on training batch is 0.0214027.
After 1606 training step(s), loss on training batch is 0.0174325.
After 1607 training step(s), loss on training batch is 0.0237296.
After 1608 training step(s), loss on training batch is 0.0394502.
After 1609 training step(s), loss on training batch is 0.0173778.
After 1610 training step(s), loss on training batch is 0.0196034.
After 1611 training step(s), loss on training batch is 0.0319046.
After 1612 training step(s), loss on training batch is 0.0380762.
After 1613 training step(s), loss on training batch is 0.0319989.
After 1614 training step(s), loss on training batch is 0.0178075.
After 1615 training step(s), loss on training batch is 0.0568128.
After 1616 training step(s), loss on training batch is 0.0285484.
After 1617 training step(s), loss on training batch is 0.0197312.
After 1618 training step(s), loss on training batch is 0.0312184.
After 1619 training step(s), loss on training batch is 0.0359196.
After 1620 training step(s), loss on training batch is 0.0275566.
After 1621 training step(s), loss on training batch is 0.0469055.
After 1622 training step(s), loss on training batch is 0.025599.
After 1623 training step(s), loss on training batch is 0.0272605.
After 1624 training step(s), loss on training batch is 0.0455305.
After 1625 training step(s), loss on training batch is 0.0400747.
After 1626 training step(s), loss on training batch is 0.0281716.
After 1627 training step(s), loss on training batch is 0.028123.
After 1628 training step(s), loss on training batch is 0.0210463.
After 1629 training step(s), loss on training batch is 0.0375795.
After 1630 training step(s), loss on training batch is 0.020385.
After 1631 training step(s), loss on training batch is 0.0313307.
After 1632 training step(s), loss on training batch is 0.0154624.
After 1633 training step(s), loss on training batch is 0.0164232.
After 1634 training step(s), loss on training batch is 0.01994.
After 1635 training step(s), loss on training batch is 0.0211831.
After 1636 training step(s), loss on training batch is 0.0198963.
After 1637 training step(s), loss on training batch is 0.039905.
After 1638 training step(s), loss on training batch is 0.0292962.
After 1639 training step(s), loss on training batch is 0.0192239.
After 1640 training step(s), loss on training batch is 0.0154974.
After 1641 training step(s), loss on training batch is 0.0146103.
After 1642 training step(s), loss on training batch is 0.0165841.
After 1643 training step(s), loss on training batch is 0.0312633.
After 1644 training step(s), loss on training batch is 0.0383033.
After 1645 training step(s), loss on training batch is 0.021518.
After 1646 training step(s), loss on training batch is 0.0614475.
After 1647 training step(s), loss on training batch is 0.0239844.
After 1648 training step(s), loss on training batch is 0.0200132.
After 1649 training step(s), loss on training batch is 0.0259546.
After 1650 training step(s), loss on training batch is 0.019038.
After 1651 training step(s), loss on training batch is 0.0442797.
After 1652 training step(s), loss on training batch is 0.0364976.
After 1653 training step(s), loss on training batch is 0.0155721.
After 1654 training step(s), loss on training batch is 0.0143605.
After 1655 training step(s), loss on training batch is 0.0328985.
After 1656 training step(s), loss on training batch is 0.0260424.
After 1657 training step(s), loss on training batch is 0.0394788.
After 1658 training step(s), loss on training batch is 0.0258943.
After 1659 training step(s), loss on training batch is 0.0291251.
After 1660 training step(s), loss on training batch is 0.0179943.
After 1661 training step(s), loss on training batch is 0.0189379.
After 1662 training step(s), loss on training batch is 0.0295722.
After 1663 training step(s), loss on training batch is 0.0202385.
After 1664 training step(s), loss on training batch is 0.0222173.
After 1665 training step(s), loss on training batch is 0.0239449.
After 1666 training step(s), loss on training batch is 0.0140842.
After 1667 training step(s), loss on training batch is 0.0192775.
After 1668 training step(s), loss on training batch is 0.030259.
After 1669 training step(s), loss on training batch is 0.0508248.
After 1670 training step(s), loss on training batch is 0.0231545.
After 1671 training step(s), loss on training batch is 0.0249066.
After 1672 training step(s), loss on training batch is 0.0148789.
After 1673 training step(s), loss on training batch is 0.0149528.
After 1674 training step(s), loss on training batch is 0.0146156.
After 1675 training step(s), loss on training batch is 0.0180041.
After 1676 training step(s), loss on training batch is 0.0259921.
After 1677 training step(s), loss on training batch is 0.0256777.
After 1678 training step(s), loss on training batch is 0.0289524.
After 1679 training step(s), loss on training batch is 0.0192282.
After 1680 training step(s), loss on training batch is 0.0196811.
After 1681 training step(s), loss on training batch is 0.0191147.
After 1682 training step(s), loss on training batch is 0.027998.
After 1683 training step(s), loss on training batch is 0.0162574.
After 1684 training step(s), loss on training batch is 0.0267977.
After 1685 training step(s), loss on training batch is 0.0855697.
After 1686 training step(s), loss on training batch is 0.0172711.
After 1687 training step(s), loss on training batch is 0.0152429.
After 1688 training step(s), loss on training batch is 0.045337.
After 1689 training step(s), loss on training batch is 0.0505463.
After 1690 training step(s), loss on training batch is 0.0337958.
After 1691 training step(s), loss on training batch is 0.0198225.
After 1692 training step(s), loss on training batch is 0.0222314.
After 1693 training step(s), loss on training batch is 0.0229858.
After 1694 training step(s), loss on training batch is 0.0189789.
After 1695 training step(s), loss on training batch is 0.0159137.
After 1696 training step(s), loss on training batch is 0.0133806.
After 1697 training step(s), loss on training batch is 0.0153915.
After 1698 training step(s), loss on training batch is 0.0197162.
After 1699 training step(s), loss on training batch is 0.0144372.
After 1700 training step(s), loss on training batch is 0.0192823.
After 1701 training step(s), loss on training batch is 0.0198363.
After 1702 training step(s), loss on training batch is 0.0165079.
After 1703 training step(s), loss on training batch is 0.0170897.
After 1704 training step(s), loss on training batch is 0.0164796.
After 1705 training step(s), loss on training batch is 0.0344709.
After 1706 training step(s), loss on training batch is 0.0280432.
After 1707 training step(s), loss on training batch is 0.0176207.
After 1708 training step(s), loss on training batch is 0.0186101.
After 1709 training step(s), loss on training batch is 0.0213986.
After 1710 training step(s), loss on training batch is 0.0280981.
After 1711 training step(s), loss on training batch is 0.0268065.
After 1712 training step(s), loss on training batch is 0.02188.
After 1713 training step(s), loss on training batch is 0.0246107.
After 1714 training step(s), loss on training batch is 0.0236189.
After 1715 training step(s), loss on training batch is 0.0213583.
After 1716 training step(s), loss on training batch is 0.0339798.
After 1717 training step(s), loss on training batch is 0.0150628.
After 1718 training step(s), loss on training batch is 0.0368992.
After 1719 training step(s), loss on training batch is 0.0214092.
After 1720 training step(s), loss on training batch is 0.0229065.
After 1721 training step(s), loss on training batch is 0.0289824.
After 1722 training step(s), loss on training batch is 0.0168179.
After 1723 training step(s), loss on training batch is 0.0357192.
After 1724 training step(s), loss on training batch is 0.0156326.
After 1725 training step(s), loss on training batch is 0.0231257.
After 1726 training step(s), loss on training batch is 0.0404677.
After 1727 training step(s), loss on training batch is 0.0151357.
After 1728 training step(s), loss on training batch is 0.0218478.
After 1729 training step(s), loss on training batch is 0.0242905.
After 1730 training step(s), loss on training batch is 0.0179139.
After 1731 training step(s), loss on training batch is 0.0167448.
After 1732 training step(s), loss on training batch is 0.0162393.
After 1733 training step(s), loss on training batch is 0.0145208.
After 1734 training step(s), loss on training batch is 0.0526577.
After 1735 training step(s), loss on training batch is 0.0712855.
After 1736 training step(s), loss on training batch is 0.0407723.
After 1737 training step(s), loss on training batch is 0.0317678.
After 1738 training step(s), loss on training batch is 0.0401174.
After 1739 training step(s), loss on training batch is 0.0256883.
After 1740 training step(s), loss on training batch is 0.0260214.
After 1741 training step(s), loss on training batch is 0.0504304.
After 1742 training step(s), loss on training batch is 0.0203503.
After 1743 training step(s), loss on training batch is 0.0617632.
After 1744 training step(s), loss on training batch is 0.0582588.
After 1745 training step(s), loss on training batch is 0.0565664.
After 1746 training step(s), loss on training batch is 0.0178613.
After 1747 training step(s), loss on training batch is 0.026929.
After 1748 training step(s), loss on training batch is 0.0349931.
After 1749 training step(s), loss on training batch is 0.0202348.
After 1750 training step(s), loss on training batch is 0.0277349.
After 1751 training step(s), loss on training batch is 0.0363943.
After 1752 training step(s), loss on training batch is 0.0286359.
After 1753 training step(s), loss on training batch is 0.0516262.
After 1754 training step(s), loss on training batch is 0.0483408.
After 1755 training step(s), loss on training batch is 0.0309845.
After 1756 training step(s), loss on training batch is 0.0254578.
After 1757 training step(s), loss on training batch is 0.024003.
After 1758 training step(s), loss on training batch is 0.0195773.
After 1759 training step(s), loss on training batch is 0.0268394.
After 1760 training step(s), loss on training batch is 0.0213571.
After 1761 training step(s), loss on training batch is 0.0462076.
After 1762 training step(s), loss on training batch is 0.0146718.
After 1763 training step(s), loss on training batch is 0.0190336.
After 1764 training step(s), loss on training batch is 0.0144704.
After 1765 training step(s), loss on training batch is 0.0193398.
After 1766 training step(s), loss on training batch is 0.0239688.
After 1767 training step(s), loss on training batch is 0.0242749.
After 1768 training step(s), loss on training batch is 0.0171418.
After 1769 training step(s), loss on training batch is 0.0223233.
After 1770 training step(s), loss on training batch is 0.019376.
After 1771 training step(s), loss on training batch is 0.020222.
After 1772 training step(s), loss on training batch is 0.013584.
After 1773 training step(s), loss on training batch is 0.017929.
After 1774 training step(s), loss on training batch is 0.0209474.
After 1775 training step(s), loss on training batch is 0.0198426.
After 1776 training step(s), loss on training batch is 0.0407586.
After 1777 training step(s), loss on training batch is 0.0150226.
After 1778 training step(s), loss on training batch is 0.0306692.
After 1779 training step(s), loss on training batch is 0.0172347.
After 1780 training step(s), loss on training batch is 0.0139875.
After 1781 training step(s), loss on training batch is 0.0189323.
After 1782 training step(s), loss on training batch is 0.0246133.
After 1783 training step(s), loss on training batch is 0.0227361.
After 1784 training step(s), loss on training batch is 0.0231023.
After 1785 training step(s), loss on training batch is 0.0169853.
After 1786 training step(s), loss on training batch is 0.0480609.
After 1787 training step(s), loss on training batch is 0.0214856.
After 1788 training step(s), loss on training batch is 0.0228898.
After 1789 training step(s), loss on training batch is 0.0199031.
After 1790 training step(s), loss on training batch is 0.0190139.
After 1791 training step(s), loss on training batch is 0.0198049.
After 1792 training step(s), loss on training batch is 0.0148886.
After 1793 training step(s), loss on training batch is 0.0461786.
After 1794 training step(s), loss on training batch is 0.0170948.
After 1795 training step(s), loss on training batch is 0.0361104.
After 1796 training step(s), loss on training batch is 0.0175713.
After 1797 training step(s), loss on training batch is 0.0140656.
After 1798 training step(s), loss on training batch is 0.0169308.
After 1799 training step(s), loss on training batch is 0.0173617.
After 1800 training step(s), loss on training batch is 0.0148824.
After 1801 training step(s), loss on training batch is 0.012926.
After 1802 training step(s), loss on training batch is 0.0310791.
After 1803 training step(s), loss on training batch is 0.0575929.
After 1804 training step(s), loss on training batch is 0.0173182.
After 1805 training step(s), loss on training batch is 0.0324309.
After 1806 training step(s), loss on training batch is 0.0461133.
After 1807 training step(s), loss on training batch is 0.033548.
After 1808 training step(s), loss on training batch is 0.030411.
After 1809 training step(s), loss on training batch is 0.018842.
After 1810 training step(s), loss on training batch is 0.0369038.
After 1811 training step(s), loss on training batch is 0.0212503.
After 1812 training step(s), loss on training batch is 0.0447065.
After 1813 training step(s), loss on training batch is 0.0219679.
After 1814 training step(s), loss on training batch is 0.0500509.
After 1815 training step(s), loss on training batch is 0.0458407.
After 1816 training step(s), loss on training batch is 0.0487407.
After 1817 training step(s), loss on training batch is 0.0499141.
After 1818 training step(s), loss on training batch is 0.020259.
After 1819 training step(s), loss on training batch is 0.0245839.
After 1820 training step(s), loss on training batch is 0.0245562.
After 1821 training step(s), loss on training batch is 0.0336919.
After 1822 training step(s), loss on training batch is 0.0220716.
After 1823 training step(s), loss on training batch is 0.016727.
After 1824 training step(s), loss on training batch is 0.0182754.
After 1825 training step(s), loss on training batch is 0.019948.
After 1826 training step(s), loss on training batch is 0.0486037.
After 1827 training step(s), loss on training batch is 0.017532.
After 1828 training step(s), loss on training batch is 0.0176346.
After 1829 training step(s), loss on training batch is 0.0324641.
After 1830 training step(s), loss on training batch is 0.0191854.
After 1831 training step(s), loss on training batch is 0.0255102.
After 1832 training step(s), loss on training batch is 0.03054.
After 1833 training step(s), loss on training batch is 0.0299099.
After 1834 training step(s), loss on training batch is 0.0216535.
After 1835 training step(s), loss on training batch is 0.0256769.
After 1836 training step(s), loss on training batch is 0.0317807.
After 1837 training step(s), loss on training batch is 0.0422539.
After 1838 training step(s), loss on training batch is 0.0219447.
After 1839 training step(s), loss on training batch is 0.0188506.
After 1840 training step(s), loss on training batch is 0.0186887.
After 1841 training step(s), loss on training batch is 0.0141983.
After 1842 training step(s), loss on training batch is 0.0168001.
After 1843 training step(s), loss on training batch is 0.0313757.
After 1844 training step(s), loss on training batch is 0.0145834.
After 1845 training step(s), loss on training batch is 0.0229121.
After 1846 training step(s), loss on training batch is 0.0203808.
After 1847 training step(s), loss on training batch is 0.0232738.
After 1848 training step(s), loss on training batch is 0.0348654.
After 1849 training step(s), loss on training batch is 0.0171384.
After 1850 training step(s), loss on training batch is 0.020937.
After 1851 training step(s), loss on training batch is 0.0480422.
After 1852 training step(s), loss on training batch is 0.0284417.
After 1853 training step(s), loss on training batch is 0.0191025.
After 1854 training step(s), loss on training batch is 0.0488255.
After 1855 training step(s), loss on training batch is 0.0196854.
After 1856 training step(s), loss on training batch is 0.0420481.
After 1857 training step(s), loss on training batch is 0.0194161.
After 1858 training step(s), loss on training batch is 0.0310291.
After 1859 training step(s), loss on training batch is 0.0313585.
After 1860 training step(s), loss on training batch is 0.0172409.
After 1861 training step(s), loss on training batch is 0.0257835.
After 1862 training step(s), loss on training batch is 0.0187287.
After 1863 training step(s), loss on training batch is 0.0141814.
After 1864 training step(s), loss on training batch is 0.0322358.
After 1865 training step(s), loss on training batch is 0.0226771.
After 1866 training step(s), loss on training batch is 0.0248053.
After 1867 training step(s), loss on training batch is 0.0210015.
After 1868 training step(s), loss on training batch is 0.023398.
After 1869 training step(s), loss on training batch is 0.0189984.
After 1870 training step(s), loss on training batch is 0.0194213.
After 1871 training step(s), loss on training batch is 0.0190212.
After 1872 training step(s), loss on training batch is 0.018513.
After 1873 training step(s), loss on training batch is 0.035764.
After 1874 training step(s), loss on training batch is 0.0275605.
After 1875 training step(s), loss on training batch is 0.0290737.
After 1876 training step(s), loss on training batch is 0.0404687.
After 1877 training step(s), loss on training batch is 0.0447035.
After 1878 training step(s), loss on training batch is 0.0335475.
After 1879 training step(s), loss on training batch is 0.0188709.
After 1880 training step(s), loss on training batch is 0.0617008.
After 1881 training step(s), loss on training batch is 0.0858711.
After 1882 training step(s), loss on training batch is 0.0709575.
After 1883 training step(s), loss on training batch is 0.018089.
After 1884 training step(s), loss on training batch is 0.0177672.
After 1885 training step(s), loss on training batch is 0.0168334.
After 1886 training step(s), loss on training batch is 0.0262.
After 1887 training step(s), loss on training batch is 0.0231727.
After 1888 training step(s), loss on training batch is 0.0164017.
After 1889 training step(s), loss on training batch is 0.0574048.
After 1890 training step(s), loss on training batch is 0.0227054.
After 1891 training step(s), loss on training batch is 0.0136809.
After 1892 training step(s), loss on training batch is 0.0476812.
After 1893 training step(s), loss on training batch is 0.0403484.
After 1894 training step(s), loss on training batch is 0.0407106.
After 1895 training step(s), loss on training batch is 0.0609418.
After 1896 training step(s), loss on training batch is 0.01743.
After 1897 training step(s), loss on training batch is 0.0504721.
After 1898 training step(s), loss on training batch is 0.019133.
After 1899 training step(s), loss on training batch is 0.0220566.
After 1900 training step(s), loss on training batch is 0.0280477.
After 1901 training step(s), loss on training batch is 0.0267214.
After 1902 training step(s), loss on training batch is 0.0411109.
After 1903 training step(s), loss on training batch is 0.0153449.
After 1904 training step(s), loss on training batch is 0.0454941.
After 1905 training step(s), loss on training batch is 0.0222488.
After 1906 training step(s), loss on training batch is 0.0185553.
After 1907 training step(s), loss on training batch is 0.016753.
After 1908 training step(s), loss on training batch is 0.0355011.
After 1909 training step(s), loss on training batch is 0.0499642.
After 1910 training step(s), loss on training batch is 0.0384553.
After 1911 training step(s), loss on training batch is 0.0339365.
After 1912 training step(s), loss on training batch is 0.0146339.
After 1913 training step(s), loss on training batch is 0.0259524.
After 1914 training step(s), loss on training batch is 0.0266609.
After 1915 training step(s), loss on training batch is 0.0306236.
After 1916 training step(s), loss on training batch is 0.0185972.
After 1917 training step(s), loss on training batch is 0.0208207.
After 1918 training step(s), loss on training batch is 0.0291349.
After 1919 training step(s), loss on training batch is 0.0336316.
After 1920 training step(s), loss on training batch is 0.0160354.
After 1921 training step(s), loss on training batch is 0.0163518.
After 1922 training step(s), loss on training batch is 0.0143267.
After 1923 training step(s), loss on training batch is 0.0369948.
After 1924 training step(s), loss on training batch is 0.0339903.
After 1925 training step(s), loss on training batch is 0.0167027.
After 1926 training step(s), loss on training batch is 0.0141697.
After 1927 training step(s), loss on training batch is 0.0165683.
After 1928 training step(s), loss on training batch is 0.0273049.
After 1929 training step(s), loss on training batch is 0.027157.
After 1930 training step(s), loss on training batch is 0.0161835.
After 1931 training step(s), loss on training batch is 0.0249938.
After 1932 training step(s), loss on training batch is 0.0171857.
After 1933 training step(s), loss on training batch is 0.03917.
After 1934 training step(s), loss on training batch is 0.0187774.
After 1935 training step(s), loss on training batch is 0.0135881.
After 1936 training step(s), loss on training batch is 0.016985.
After 1937 training step(s), loss on training batch is 0.0169157.
After 1938 training step(s), loss on training batch is 0.0182471.
After 1939 training step(s), loss on training batch is 0.0346097.
After 1940 training step(s), loss on training batch is 0.0234913.
After 1941 training step(s), loss on training batch is 0.0312382.
After 1942 training step(s), loss on training batch is 0.0176484.
After 1943 training step(s), loss on training batch is 0.0196864.
After 1944 training step(s), loss on training batch is 0.0136292.
After 1945 training step(s), loss on training batch is 0.0140524.
After 1946 training step(s), loss on training batch is 0.0135475.
After 1947 training step(s), loss on training batch is 0.0162837.
After 1948 training step(s), loss on training batch is 0.0160822.
After 1949 training step(s), loss on training batch is 0.0649588.
After 1950 training step(s), loss on training batch is 0.0167223.
After 1951 training step(s), loss on training batch is 0.0368188.
After 1952 training step(s), loss on training batch is 0.0291541.
After 1953 training step(s), loss on training batch is 0.015105.
After 1954 training step(s), loss on training batch is 0.0295567.
After 1955 training step(s), loss on training batch is 0.02155.
After 1956 training step(s), loss on training batch is 0.0155973.
After 1957 training step(s), loss on training batch is 0.0217129.
After 1958 training step(s), loss on training batch is 0.0311316.
After 1959 training step(s), loss on training batch is 0.0187643.
After 1960 training step(s), loss on training batch is 0.0273185.
After 1961 training step(s), loss on training batch is 0.019551.
After 1962 training step(s), loss on training batch is 0.0137798.
After 1963 training step(s), loss on training batch is 0.0126617.
After 1964 training step(s), loss on training batch is 0.0210077.
After 1965 training step(s), loss on training batch is 0.0175134.
After 1966 training step(s), loss on training batch is 0.0332953.
After 1967 training step(s), loss on training batch is 0.0211665.
After 1968 training step(s), loss on training batch is 0.0135252.
After 1969 training step(s), loss on training batch is 0.0160667.
After 1970 training step(s), loss on training batch is 0.020499.
After 1971 training step(s), loss on training batch is 0.0183563.
After 1972 training step(s), loss on training batch is 0.0158226.
After 1973 training step(s), loss on training batch is 0.0178413.
After 1974 training step(s), loss on training batch is 0.0170761.
After 1975 training step(s), loss on training batch is 0.0439082.
After 1976 training step(s), loss on training batch is 0.0196664.
After 1977 training step(s), loss on training batch is 0.019521.
After 1978 training step(s), loss on training batch is 0.013341.
After 1979 training step(s), loss on training batch is 0.0152393.
After 1980 training step(s), loss on training batch is 0.0296685.
After 1981 training step(s), loss on training batch is 0.0195773.
After 1982 training step(s), loss on training batch is 0.0212947.
After 1983 training step(s), loss on training batch is 0.0166531.
After 1984 training step(s), loss on training batch is 0.0158605.
After 1985 training step(s), loss on training batch is 0.0237635.
After 1986 training step(s), loss on training batch is 0.0219207.
After 1987 training step(s), loss on training batch is 0.0229596.
After 1988 training step(s), loss on training batch is 0.0227229.
After 1989 training step(s), loss on training batch is 0.0144062.
After 1990 training step(s), loss on training batch is 0.0140178.
After 1991 training step(s), loss on training batch is 0.0389045.
After 1992 training step(s), loss on training batch is 0.0161092.
After 1993 training step(s), loss on training batch is 0.0148282.
After 1994 training step(s), loss on training batch is 0.0176886.
After 1995 training step(s), loss on training batch is 0.0319322.
After 1996 training step(s), loss on training batch is 0.0151853.
After 1997 training step(s), loss on training batch is 0.0222778.
After 1998 training step(s), loss on training batch is 0.0121652.
After 1999 training step(s), loss on training batch is 0.014305.
After 2000 training step(s), loss on training batch is 0.0180763.
After 2001 training step(s), loss on training batch is 0.0168408.
After 2002 training step(s), loss on training batch is 0.0189324.
After 2003 training step(s), loss on training batch is 0.0219785.
After 2004 training step(s), loss on training batch is 0.0132417.
After 2005 training step(s), loss on training batch is 0.01714.
After 2006 training step(s), loss on training batch is 0.0147459.
After 2007 training step(s), loss on training batch is 0.0439861.
After 2008 training step(s), loss on training batch is 0.0151557.
After 2009 training step(s), loss on training batch is 0.0376523.
After 2010 training step(s), loss on training batch is 0.0143507.
After 2011 training step(s), loss on training batch is 0.0207267.
After 2012 training step(s), loss on training batch is 0.0272937.
After 2013 training step(s), loss on training batch is 0.018439.
After 2014 training step(s), loss on training batch is 0.0159324.
After 2015 training step(s), loss on training batch is 0.0180789.
After 2016 training step(s), loss on training batch is 0.0279424.
After 2017 training step(s), loss on training batch is 0.0183148.
After 2018 training step(s), loss on training batch is 0.0197128.
After 2019 training step(s), loss on training batch is 0.0133087.
After 2020 training step(s), loss on training batch is 0.0176897.
After 2021 training step(s), loss on training batch is 0.0180975.
After 2022 training step(s), loss on training batch is 0.0188488.
After 2023 training step(s), loss on training batch is 0.0213008.
After 2024 training step(s), loss on training batch is 0.0287323.
After 2025 training step(s), loss on training batch is 0.0181107.
After 2026 training step(s), loss on training batch is 0.0174327.
After 2027 training step(s), loss on training batch is 0.0151939.
After 2028 training step(s), loss on training batch is 0.0142668.
After 2029 training step(s), loss on training batch is 0.0136418.
After 2030 training step(s), loss on training batch is 0.0161015.
After 2031 training step(s), loss on training batch is 0.0204106.
After 2032 training step(s), loss on training batch is 0.0275765.
After 2033 training step(s), loss on training batch is 0.0164418.
After 2034 training step(s), loss on training batch is 0.0285999.
After 2035 training step(s), loss on training batch is 0.019772.
After 2036 training step(s), loss on training batch is 0.022043.
After 2037 training step(s), loss on training batch is 0.016564.
After 2038 training step(s), loss on training batch is 0.0254751.
After 2039 training step(s), loss on training batch is 0.017562.
After 2040 training step(s), loss on training batch is 0.0250785.
After 2041 training step(s), loss on training batch is 0.0145128.
After 2042 training step(s), loss on training batch is 0.0296188.
After 2043 training step(s), loss on training batch is 0.0178541.
After 2044 training step(s), loss on training batch is 0.0306702.
After 2045 training step(s), loss on training batch is 0.0169761.
After 2046 training step(s), loss on training batch is 0.0248206.
After 2047 training step(s), loss on training batch is 0.0141943.
After 2048 training step(s), loss on training batch is 0.0391288.
After 2049 training step(s), loss on training batch is 0.0145504.
After 2050 training step(s), loss on training batch is 0.0156312.
After 2051 training step(s), loss on training batch is 0.013371.
After 2052 training step(s), loss on training batch is 0.014168.
After 2053 training step(s), loss on training batch is 0.0146284.
After 2054 training step(s), loss on training batch is 0.0237091.
After 2055 training step(s), loss on training batch is 0.0184942.
After 2056 training step(s), loss on training batch is 0.0228941.
After 2057 training step(s), loss on training batch is 0.0270539.
After 2058 training step(s), loss on training batch is 0.0239045.
After 2059 training step(s), loss on training batch is 0.0146111.
After 2060 training step(s), loss on training batch is 0.0245321.
After 2061 training step(s), loss on training batch is 0.0263248.
After 2062 training step(s), loss on training batch is 0.0124847.
After 2063 training step(s), loss on training batch is 0.0169035.
After 2064 training step(s), loss on training batch is 0.0342237.
After 2065 training step(s), loss on training batch is 0.0215972.
After 2066 training step(s), loss on training batch is 0.0240056.
After 2067 training step(s), loss on training batch is 0.02079.
After 2068 training step(s), loss on training batch is 0.0414322.
After 2069 training step(s), loss on training batch is 0.0167523.
After 2070 training step(s), loss on training batch is 0.0172549.
After 2071 training step(s), loss on training batch is 0.0381679.
After 2072 training step(s), loss on training batch is 0.0379041.
After 2073 training step(s), loss on training batch is 0.0347466.
After 2074 training step(s), loss on training batch is 0.0226167.
After 2075 training step(s), loss on training batch is 0.0162119.
After 2076 training step(s), loss on training batch is 0.0265816.
After 2077 training step(s), loss on training batch is 0.0141789.
After 2078 training step(s), loss on training batch is 0.0210315.
After 2079 training step(s), loss on training batch is 0.0549323.
After 2080 training step(s), loss on training batch is 0.0230614.
After 2081 training step(s), loss on training batch is 0.0172014.
After 2082 training step(s), loss on training batch is 0.0248421.
After 2083 training step(s), loss on training batch is 0.0171331.
After 2084 training step(s), loss on training batch is 0.0215361.
After 2085 training step(s), loss on training batch is 0.0236235.
After 2086 training step(s), loss on training batch is 0.022656.
After 2087 training step(s), loss on training batch is 0.0163763.
After 2088 training step(s), loss on training batch is 0.0243525.
After 2089 training step(s), loss on training batch is 0.0518362.
After 2090 training step(s), loss on training batch is 0.0381703.
After 2091 training step(s), loss on training batch is 0.020932.
After 2092 training step(s), loss on training batch is 0.0172394.
After 2093 training step(s), loss on training batch is 0.0353823.
After 2094 training step(s), loss on training batch is 0.0148538.
After 2095 training step(s), loss on training batch is 0.0143332.
After 2096 training step(s), loss on training batch is 0.038079.
After 2097 training step(s), loss on training batch is 0.0308204.
After 2098 training step(s), loss on training batch is 0.0198559.
After 2099 training step(s), loss on training batch is 0.0213269.
After 2100 training step(s), loss on training batch is 0.0146245.
After 2101 training step(s), loss on training batch is 0.0309908.
After 2102 training step(s), loss on training batch is 0.0285717.
After 2103 training step(s), loss on training batch is 0.0338903.
After 2104 training step(s), loss on training batch is 0.0278061.
After 2105 training step(s), loss on training batch is 0.0240611.
After 2106 training step(s), loss on training batch is 0.016506.
After 2107 training step(s), loss on training batch is 0.0149694.
After 2108 training step(s), loss on training batch is 0.0129256.
After 2109 training step(s), loss on training batch is 0.0247483.
After 2110 training step(s), loss on training batch is 0.0330237.
After 2111 training step(s), loss on training batch is 0.0138311.
After 2112 training step(s), loss on training batch is 0.0179708.
After 2113 training step(s), loss on training batch is 0.0196137.
After 2114 training step(s), loss on training batch is 0.0129662.
After 2115 training step(s), loss on training batch is 0.0136221.
After 2116 training step(s), loss on training batch is 0.0953523.
After 2117 training step(s), loss on training batch is 0.0383999.
After 2118 training step(s), loss on training batch is 0.0122238.
After 2119 training step(s), loss on training batch is 0.014159.
After 2120 training step(s), loss on training batch is 0.0146855.
After 2121 training step(s), loss on training batch is 0.0134601.
After 2122 training step(s), loss on training batch is 0.0188038.
After 2123 training step(s), loss on training batch is 0.0177892.
After 2124 training step(s), loss on training batch is 0.0149012.
After 2125 training step(s), loss on training batch is 0.0210329.
After 2126 training step(s), loss on training batch is 0.0160133.
After 2127 training step(s), loss on training batch is 0.0173304.
After 2128 training step(s), loss on training batch is 0.0133311.
After 2129 training step(s), loss on training batch is 0.0150485.
After 2130 training step(s), loss on training batch is 0.0263951.
After 2131 training step(s), loss on training batch is 0.0203487.
After 2132 training step(s), loss on training batch is 0.0271503.
After 2133 training step(s), loss on training batch is 0.0204287.
After 2134 training step(s), loss on training batch is 0.0208491.
After 2135 training step(s), loss on training batch is 0.0162677.
After 2136 training step(s), loss on training batch is 0.0338721.
After 2137 training step(s), loss on training batch is 0.0417091.
After 2138 training step(s), loss on training batch is 0.0169574.
After 2139 training step(s), loss on training batch is 0.0171397.
After 2140 training step(s), loss on training batch is 0.0209066.
After 2141 training step(s), loss on training batch is 0.0182574.
After 2142 training step(s), loss on training batch is 0.0140396.
After 2143 training step(s), loss on training batch is 0.0124151.
After 2144 training step(s), loss on training batch is 0.0200983.
After 2145 training step(s), loss on training batch is 0.0456481.
After 2146 training step(s), loss on training batch is 0.0182617.
After 2147 training step(s), loss on training batch is 0.0311379.
After 2148 training step(s), loss on training batch is 0.0123976.
After 2149 training step(s), loss on training batch is 0.0151539.
After 2150 training step(s), loss on training batch is 0.0514627.
After 2151 training step(s), loss on training batch is 0.041685.
After 2152 training step(s), loss on training batch is 0.0191964.
After 2153 training step(s), loss on training batch is 0.0198454.
After 2154 training step(s), loss on training batch is 0.027986.
After 2155 training step(s), loss on training batch is 0.0373697.
After 2156 training step(s), loss on training batch is 0.0212351.
After 2157 training step(s), loss on training batch is 0.0125355.
After 2158 training step(s), loss on training batch is 0.0286513.
After 2159 training step(s), loss on training batch is 0.0248704.
After 2160 training step(s), loss on training batch is 0.0199553.
After 2161 training step(s), loss on training batch is 0.0246701.
After 2162 training step(s), loss on training batch is 0.018704.
After 2163 training step(s), loss on training batch is 0.0202388.
After 2164 training step(s), loss on training batch is 0.0484991.
After 2165 training step(s), loss on training batch is 0.0146432.
After 2166 training step(s), loss on training batch is 0.0163294.
After 2167 training step(s), loss on training batch is 0.0144906.
After 2168 training step(s), loss on training batch is 0.0193897.
After 2169 training step(s), loss on training batch is 0.0200892.
After 2170 training step(s), loss on training batch is 0.0177702.
After 2171 training step(s), loss on training batch is 0.0220797.
After 2172 training step(s), loss on training batch is 0.0156582.
After 2173 training step(s), loss on training batch is 0.0149976.
After 2174 training step(s), loss on training batch is 0.0154957.
After 2175 training step(s), loss on training batch is 0.0302539.
After 2176 training step(s), loss on training batch is 0.0207523.
After 2177 training step(s), loss on training batch is 0.0299274.
After 2178 training step(s), loss on training batch is 0.0485691.
After 2179 training step(s), loss on training batch is 0.0228015.
After 2180 training step(s), loss on training batch is 0.0183648.
After 2181 training step(s), loss on training batch is 0.0135324.
After 2182 training step(s), loss on training batch is 0.0454293.
After 2183 training step(s), loss on training batch is 0.025667.
After 2184 training step(s), loss on training batch is 0.0133024.
After 2185 training step(s), loss on training batch is 0.0152627.
After 2186 training step(s), loss on training batch is 0.012739.
After 2187 training step(s), loss on training batch is 0.013845.
After 2188 training step(s), loss on training batch is 0.0272692.
After 2189 training step(s), loss on training batch is 0.0141806.
After 2190 training step(s), loss on training batch is 0.0195257.
After 2191 training step(s), loss on training batch is 0.0134537.
After 2192 training step(s), loss on training batch is 0.0335363.
After 2193 training step(s), loss on training batch is 0.0519558.
After 2194 training step(s), loss on training batch is 0.0251123.
After 2195 training step(s), loss on training batch is 0.034941.
After 2196 training step(s), loss on training batch is 0.0220577.
After 2197 training step(s), loss on training batch is 0.0260014.
After 2198 training step(s), loss on training batch is 0.0207257.
After 2199 training step(s), loss on training batch is 0.0273124.
After 2200 training step(s), loss on training batch is 0.040637.
After 2201 training step(s), loss on training batch is 0.0390482.
After 2202 training step(s), loss on training batch is 0.0178872.
After 2203 training step(s), loss on training batch is 0.014405.
After 2204 training step(s), loss on training batch is 0.0300347.
After 2205 training step(s), loss on training batch is 0.0153943.
After 2206 training step(s), loss on training batch is 0.0173292.
After 2207 training step(s), loss on training batch is 0.0150973.
After 2208 training step(s), loss on training batch is 0.0187171.
After 2209 training step(s), loss on training batch is 0.0163636.
After 2210 training step(s), loss on training batch is 0.0122667.
After 2211 training step(s), loss on training batch is 0.0269263.
After 2212 training step(s), loss on training batch is 0.0239676.
After 2213 training step(s), loss on training batch is 0.0270055.
After 2214 training step(s), loss on training batch is 0.0169422.
After 2215 training step(s), loss on training batch is 0.0154922.
After 2216 training step(s), loss on training batch is 0.0146903.
After 2217 training step(s), loss on training batch is 0.0260927.
After 2218 training step(s), loss on training batch is 0.0279385.
After 2219 training step(s), loss on training batch is 0.0126158.
After 2220 training step(s), loss on training batch is 0.0338935.
After 2221 training step(s), loss on training batch is 0.0181554.
After 2222 training step(s), loss on training batch is 0.0128614.
After 2223 training step(s), loss on training batch is 0.0188064.
After 2224 training step(s), loss on training batch is 0.0221109.
After 2225 training step(s), loss on training batch is 0.0209427.
After 2226 training step(s), loss on training batch is 0.0157361.
After 2227 training step(s), loss on training batch is 0.0184924.
After 2228 training step(s), loss on training batch is 0.0147581.
After 2229 training step(s), loss on training batch is 0.0155075.
After 2230 training step(s), loss on training batch is 0.0500361.
After 2231 training step(s), loss on training batch is 0.0127864.
After 2232 training step(s), loss on training batch is 0.0381705.
After 2233 training step(s), loss on training batch is 0.0156784.
After 2234 training step(s), loss on training batch is 0.0130012.
After 2235 training step(s), loss on training batch is 0.0147717.
After 2236 training step(s), loss on training batch is 0.026774.
After 2237 training step(s), loss on training batch is 0.0141245.
After 2238 training step(s), loss on training batch is 0.0145012.
After 2239 training step(s), loss on training batch is 0.0154602.
After 2240 training step(s), loss on training batch is 0.0127953.
After 2241 training step(s), loss on training batch is 0.0208345.
After 2242 training step(s), loss on training batch is 0.0169055.
After 2243 training step(s), loss on training batch is 0.0218995.
After 2244 training step(s), loss on training batch is 0.0200532.
After 2245 training step(s), loss on training batch is 0.0136523.
After 2246 training step(s), loss on training batch is 0.0243541.
After 2247 training step(s), loss on training batch is 0.0143497.
After 2248 training step(s), loss on training batch is 0.0148122.
After 2249 training step(s), loss on training batch is 0.0236185.
After 2250 training step(s), loss on training batch is 0.0173757.
After 2251 training step(s), loss on training batch is 0.0150104.
After 2252 training step(s), loss on training batch is 0.0130013.
After 2253 training step(s), loss on training batch is 0.0127676.
After 2254 training step(s), loss on training batch is 0.0127661.
After 2255 training step(s), loss on training batch is 0.0135563.
After 2256 training step(s), loss on training batch is 0.0279539.
After 2257 training step(s), loss on training batch is 0.0166662.
After 2258 training step(s), loss on training batch is 0.0140793.
After 2259 training step(s), loss on training batch is 0.0277171.
After 2260 training step(s), loss on training batch is 0.0124992.
After 2261 training step(s), loss on training batch is 0.0165005.
After 2262 training step(s), loss on training batch is 0.0224212.
After 2263 training step(s), loss on training batch is 0.0180431.
After 2264 training step(s), loss on training batch is 0.0290879.
After 2265 training step(s), loss on training batch is 0.0164975.
After 2266 training step(s), loss on training batch is 0.0146011.
After 2267 training step(s), loss on training batch is 0.0139908.
After 2268 training step(s), loss on training batch is 0.0145959.
After 2269 training step(s), loss on training batch is 0.0141663.
After 2270 training step(s), loss on training batch is 0.0133702.
After 2271 training step(s), loss on training batch is 0.0178287.
After 2272 training step(s), loss on training batch is 0.0120244.
After 2273 training step(s), loss on training batch is 0.0330909.
After 2274 training step(s), loss on training batch is 0.024754.
After 2275 training step(s), loss on training batch is 0.0162477.
After 2276 training step(s), loss on training batch is 0.021594.
After 2277 training step(s), loss on training batch is 0.01655.
After 2278 training step(s), loss on training batch is 0.0183839.
After 2279 training step(s), loss on training batch is 0.0144031.
After 2280 training step(s), loss on training batch is 0.0241557.
After 2281 training step(s), loss on training batch is 0.0308988.
After 2282 training step(s), loss on training batch is 0.0137149.
After 2283 training step(s), loss on training batch is 0.0239916.
After 2284 training step(s), loss on training batch is 0.0123073.
After 2285 training step(s), loss on training batch is 0.014761.
After 2286 training step(s), loss on training batch is 0.0154334.
After 2287 training step(s), loss on training batch is 0.0122901.
After 2288 training step(s), loss on training batch is 0.0144112.
After 2289 training step(s), loss on training batch is 0.0206695.
After 2290 training step(s), loss on training batch is 0.0424918.
After 2291 training step(s), loss on training batch is 0.0265157.
After 2292 training step(s), loss on training batch is 0.0198847.
After 2293 training step(s), loss on training batch is 0.0304481.
After 2294 training step(s), loss on training batch is 0.0143789.
After 2295 training step(s), loss on training batch is 0.0308952.
After 2296 training step(s), loss on training batch is 0.0305513.
After 2297 training step(s), loss on training batch is 0.024016.
After 2298 training step(s), loss on training batch is 0.0260938.
After 2299 training step(s), loss on training batch is 0.0238411.
After 2300 training step(s), loss on training batch is 0.0264044.
After 2301 training step(s), loss on training batch is 0.0440814.
After 2302 training step(s), loss on training batch is 0.0253533.
After 2303 training step(s), loss on training batch is 0.0121276.
After 2304 training step(s), loss on training batch is 0.0133627.
After 2305 training step(s), loss on training batch is 0.0153664.
After 2306 training step(s), loss on training batch is 0.0135226.
After 2307 training step(s), loss on training batch is 0.0159056.
After 2308 training step(s), loss on training batch is 0.0231303.
After 2309 training step(s), loss on training batch is 0.0161522.
After 2310 training step(s), loss on training batch is 0.0178365.
After 2311 training step(s), loss on training batch is 0.0232382.
After 2312 training step(s), loss on training batch is 0.0131081.
After 2313 training step(s), loss on training batch is 0.022449.
After 2314 training step(s), loss on training batch is 0.0357646.
After 2315 training step(s), loss on training batch is 0.0333705.
After 2316 training step(s), loss on training batch is 0.0133039.
After 2317 training step(s), loss on training batch is 0.0216052.
After 2318 training step(s), loss on training batch is 0.0207414.
After 2319 training step(s), loss on training batch is 0.0146416.
After 2320 training step(s), loss on training batch is 0.0142802.
After 2321 training step(s), loss on training batch is 0.0427406.
After 2322 training step(s), loss on training batch is 0.0151891.
After 2323 training step(s), loss on training batch is 0.0206286.
After 2324 training step(s), loss on training batch is 0.0177056.
After 2325 training step(s), loss on training batch is 0.0166528.
After 2326 training step(s), loss on training batch is 0.0134504.
After 2327 training step(s), loss on training batch is 0.020823.
After 2328 training step(s), loss on training batch is 0.0376571.
After 2329 training step(s), loss on training batch is 0.0175681.
After 2330 training step(s), loss on training batch is 0.0128888.
After 2331 training step(s), loss on training batch is 0.0237261.
After 2332 training step(s), loss on training batch is 0.0127422.
After 2333 training step(s), loss on training batch is 0.0132723.
After 2334 training step(s), loss on training batch is 0.0130636.
After 2335 training step(s), loss on training batch is 0.0140231.
After 2336 training step(s), loss on training batch is 0.0158149.
After 2337 training step(s), loss on training batch is 0.0131041.
After 2338 training step(s), loss on training batch is 0.0124938.
After 2339 training step(s), loss on training batch is 0.0170087.
After 2340 training step(s), loss on training batch is 0.013636.
After 2341 training step(s), loss on training batch is 0.0162383.
After 2342 training step(s), loss on training batch is 0.0142597.
After 2343 training step(s), loss on training batch is 0.0128023.
After 2344 training step(s), loss on training batch is 0.0190492.
After 2345 training step(s), loss on training batch is 0.0192581.
After 2346 training step(s), loss on training batch is 0.0345974.
After 2347 training step(s), loss on training batch is 0.027724.
After 2348 training step(s), loss on training batch is 0.0154866.
After 2349 training step(s), loss on training batch is 0.0369832.
After 2350 training step(s), loss on training batch is 0.0153269.
After 2351 training step(s), loss on training batch is 0.0123065.
After 2352 training step(s), loss on training batch is 0.0178002.
After 2353 training step(s), loss on training batch is 0.0138274.
After 2354 training step(s), loss on training batch is 0.0274282.
After 2355 training step(s), loss on training batch is 0.0146329.
After 2356 training step(s), loss on training batch is 0.0180878.
After 2357 training step(s), loss on training batch is 0.021552.
After 2358 training step(s), loss on training batch is 0.0143947.
After 2359 training step(s), loss on training batch is 0.0181557.
After 2360 training step(s), loss on training batch is 0.0125026.
After 2361 training step(s), loss on training batch is 0.0155215.
After 2362 training step(s), loss on training batch is 0.0217924.
After 2363 training step(s), loss on training batch is 0.0319446.
After 2364 training step(s), loss on training batch is 0.0153047.
After 2365 training step(s), loss on training batch is 0.0144048.
After 2366 training step(s), loss on training batch is 0.0178759.
After 2367 training step(s), loss on training batch is 0.0121408.
After 2368 training step(s), loss on training batch is 0.0174795.
After 2369 training step(s), loss on training batch is 0.0127597.
After 2370 training step(s), loss on training batch is 0.0150678.
After 2371 training step(s), loss on training batch is 0.0251355.
After 2372 training step(s), loss on training batch is 0.0585796.
After 2373 training step(s), loss on training batch is 0.0208255.
After 2374 training step(s), loss on training batch is 0.015225.
After 2375 training step(s), loss on training batch is 0.0304711.
After 2376 training step(s), loss on training batch is 0.0736572.
After 2377 training step(s), loss on training batch is 0.016017.
After 2378 training step(s), loss on training batch is 0.0206953.
After 2379 training step(s), loss on training batch is 0.0189843.
After 2380 training step(s), loss on training batch is 0.0219471.
After 2381 training step(s), loss on training batch is 0.0243.
After 2382 training step(s), loss on training batch is 0.025353.
After 2383 training step(s), loss on training batch is 0.0282363.
After 2384 training step(s), loss on training batch is 0.0172072.
After 2385 training step(s), loss on training batch is 0.0171053.
After 2386 training step(s), loss on training batch is 0.0135786.
After 2387 training step(s), loss on training batch is 0.0302738.
After 2388 training step(s), loss on training batch is 0.0133761.
After 2389 training step(s), loss on training batch is 0.0207848.
After 2390 training step(s), loss on training batch is 0.0259901.
After 2391 training step(s), loss on training batch is 0.0153075.
After 2392 training step(s), loss on training batch is 0.0249548.
After 2393 training step(s), loss on training batch is 0.0160422.
After 2394 training step(s), loss on training batch is 0.0517372.
After 2395 training step(s), loss on training batch is 0.025816.
After 2396 training step(s), loss on training batch is 0.0181418.
After 2397 training step(s), loss on training batch is 0.0301584.
After 2398 training step(s), loss on training batch is 0.0427791.
After 2399 training step(s), loss on training batch is 0.024014.
After 2400 training step(s), loss on training batch is 0.0133109.
After 2401 training step(s), loss on training batch is 0.0145298.
After 2402 training step(s), loss on training batch is 0.0118984.
After 2403 training step(s), loss on training batch is 0.0229147.
After 2404 training step(s), loss on training batch is 0.0331274.
After 2405 training step(s), loss on training batch is 0.0269029.
After 2406 training step(s), loss on training batch is 0.0350669.
After 2407 training step(s), loss on training batch is 0.0256189.
After 2408 training step(s), loss on training batch is 0.0188318.
After 2409 training step(s), loss on training batch is 0.0281211.
After 2410 training step(s), loss on training batch is 0.0221009.
After 2411 training step(s), loss on training batch is 0.0435364.
After 2412 training step(s), loss on training batch is 0.0393871.
After 2413 training step(s), loss on training batch is 0.0254579.
After 2414 training step(s), loss on training batch is 0.0132623.
After 2415 training step(s), loss on training batch is 0.0200342.
After 2416 training step(s), loss on training batch is 0.0143081.
After 2417 training step(s), loss on training batch is 0.0144999.
After 2418 training step(s), loss on training batch is 0.0263219.
After 2419 training step(s), loss on training batch is 0.0343853.
After 2420 training step(s), loss on training batch is 0.0181689.
After 2421 training step(s), loss on training batch is 0.013375.
After 2422 training step(s), loss on training batch is 0.0210909.
After 2423 training step(s), loss on training batch is 0.0147231.
After 2424 training step(s), loss on training batch is 0.0193306.
After 2425 training step(s), loss on training batch is 0.0182838.
After 2426 training step(s), loss on training batch is 0.0126181.
After 2427 training step(s), loss on training batch is 0.0140892.
After 2428 training step(s), loss on training batch is 0.0157289.
After 2429 training step(s), loss on training batch is 0.0172422.
After 2430 training step(s), loss on training batch is 0.0149164.
After 2431 training step(s), loss on training batch is 0.0207321.
After 2432 training step(s), loss on training batch is 0.0255209.
After 2433 training step(s), loss on training batch is 0.0133484.
After 2434 training step(s), loss on training batch is 0.0129587.
After 2435 training step(s), loss on training batch is 0.0148063.
After 2436 training step(s), loss on training batch is 0.0117501.
After 2437 training step(s), loss on training batch is 0.0326411.
After 2438 training step(s), loss on training batch is 0.023179.
After 2439 training step(s), loss on training batch is 0.0198273.
After 2440 training step(s), loss on training batch is 0.0219475.
After 2441 training step(s), loss on training batch is 0.0238709.
After 2442 training step(s), loss on training batch is 0.0160963.
After 2443 training step(s), loss on training batch is 0.0147753.
After 2444 training step(s), loss on training batch is 0.0155354.
After 2445 training step(s), loss on training batch is 0.0150568.
After 2446 training step(s), loss on training batch is 0.0124579.
After 2447 training step(s), loss on training batch is 0.0124137.
After 2448 training step(s), loss on training batch is 0.0127203.
After 2449 training step(s), loss on training batch is 0.0274796.
After 2450 training step(s), loss on training batch is 0.0119203.
After 2451 training step(s), loss on training batch is 0.0228929.
After 2452 training step(s), loss on training batch is 0.0294785.
After 2453 training step(s), loss on training batch is 0.0193931.
After 2454 training step(s), loss on training batch is 0.0151449.
After 2455 training step(s), loss on training batch is 0.0242022.
After 2456 training step(s), loss on training batch is 0.0134752.
After 2457 training step(s), loss on training batch is 0.0132938.
After 2458 training step(s), loss on training batch is 0.0137623.
After 2459 training step(s), loss on training batch is 0.0160036.
After 2460 training step(s), loss on training batch is 0.0306398.
After 2461 training step(s), loss on training batch is 0.0189436.
After 2462 training step(s), loss on training batch is 0.0151477.
After 2463 training step(s), loss on training batch is 0.0138074.
After 2464 training step(s), loss on training batch is 0.0125031.
After 2465 training step(s), loss on training batch is 0.0152111.
After 2466 training step(s), loss on training batch is 0.0167445.
After 2467 training step(s), loss on training batch is 0.0506616.
After 2468 training step(s), loss on training batch is 0.0676603.
After 2469 training step(s), loss on training batch is 0.0210601.
After 2470 training step(s), loss on training batch is 0.0167425.
After 2471 training step(s), loss on training batch is 0.0147937.
After 2472 training step(s), loss on training batch is 0.0133787.
After 2473 training step(s), loss on training batch is 0.0230174.
After 2474 training step(s), loss on training batch is 0.0180018.
After 2475 training step(s), loss on training batch is 0.0147975.
After 2476 training step(s), loss on training batch is 0.0189433.
After 2477 training step(s), loss on training batch is 0.0144871.
After 2478 training step(s), loss on training batch is 0.0161404.
After 2479 training step(s), loss on training batch is 0.0189364.
After 2480 training step(s), loss on training batch is 0.0163614.
After 2481 training step(s), loss on training batch is 0.0117706.
After 2482 training step(s), loss on training batch is 0.012736.
After 2483 training step(s), loss on training batch is 0.0256027.
After 2484 training step(s), loss on training batch is 0.010923.
After 2485 training step(s), loss on training batch is 0.0135195.
After 2486 training step(s), loss on training batch is 0.0196035.
After 2487 training step(s), loss on training batch is 0.03173.
After 2488 training step(s), loss on training batch is 0.0187223.
After 2489 training step(s), loss on training batch is 0.0125559.
After 2490 training step(s), loss on training batch is 0.0132567.
After 2491 training step(s), loss on training batch is 0.0140202.
After 2492 training step(s), loss on training batch is 0.0133619.
After 2493 training step(s), loss on training batch is 0.0206949.
After 2494 training step(s), loss on training batch is 0.0144532.
After 2495 training step(s), loss on training batch is 0.0115889.
After 2496 training step(s), loss on training batch is 0.0123523.
After 2497 training step(s), loss on training batch is 0.013723.
After 2498 training step(s), loss on training batch is 0.0142359.
After 2499 training step(s), loss on training batch is 0.0152505.
After 2500 training step(s), loss on training batch is 0.0122337.
After 2501 training step(s), loss on training batch is 0.015645.
After 2502 training step(s), loss on training batch is 0.0204948.
After 2503 training step(s), loss on training batch is 0.0139455.
After 2504 training step(s), loss on training batch is 0.0172166.
After 2505 training step(s), loss on training batch is 0.013839.
After 2506 training step(s), loss on training batch is 0.0139988.
After 2507 training step(s), loss on training batch is 0.013586.
After 2508 training step(s), loss on training batch is 0.0115942.
After 2509 training step(s), loss on training batch is 0.0116745.
After 2510 training step(s), loss on training batch is 0.0125734.
After 2511 training step(s), loss on training batch is 0.0156591.
After 2512 training step(s), loss on training batch is 0.0151195.
After 2513 training step(s), loss on training batch is 0.0185491.
After 2514 training step(s), loss on training batch is 0.0156465.
After 2515 training step(s), loss on training batch is 0.0169487.
After 2516 training step(s), loss on training batch is 0.0122028.
After 2517 training step(s), loss on training batch is 0.0196956.
After 2518 training step(s), loss on training batch is 0.0147609.
After 2519 training step(s), loss on training batch is 0.0208677.
After 2520 training step(s), loss on training batch is 0.0392903.
After 2521 training step(s), loss on training batch is 0.0142466.
After 2522 training step(s), loss on training batch is 0.0273324.
After 2523 training step(s), loss on training batch is 0.0306004.
After 2524 training step(s), loss on training batch is 0.015383.
After 2525 training step(s), loss on training batch is 0.0118671.
After 2526 training step(s), loss on training batch is 0.0141405.
After 2527 training step(s), loss on training batch is 0.0188719.
After 2528 training step(s), loss on training batch is 0.0145048.
After 2529 training step(s), loss on training batch is 0.0124576.
After 2530 training step(s), loss on training batch is 0.0269102.
After 2531 training step(s), loss on training batch is 0.0113787.
After 2532 training step(s), loss on training batch is 0.0148857.
After 2533 training step(s), loss on training batch is 0.0119055.
After 2534 training step(s), loss on training batch is 0.0158236.
After 2535 training step(s), loss on training batch is 0.0240315.
After 2536 training step(s), loss on training batch is 0.0139587.
After 2537 training step(s), loss on training batch is 0.0137497.
After 2538 training step(s), loss on training batch is 0.012679.
After 2539 training step(s), loss on training batch is 0.0138321.
After 2540 training step(s), loss on training batch is 0.0501549.
After 2541 training step(s), loss on training batch is 0.0118725.
After 2542 training step(s), loss on training batch is 0.0292743.
After 2543 training step(s), loss on training batch is 0.0136097.
After 2544 training step(s), loss on training batch is 0.0148123.
After 2545 training step(s), loss on training batch is 0.0134633.
After 2546 training step(s), loss on training batch is 0.0115821.
After 2547 training step(s), loss on training batch is 0.0145691.
After 2548 training step(s), loss on training batch is 0.0194902.
After 2549 training step(s), loss on training batch is 0.0126717.
After 2550 training step(s), loss on training batch is 0.0122651.
After 2551 training step(s), loss on training batch is 0.0223043.
After 2552 training step(s), loss on training batch is 0.0163432.
After 2553 training step(s), loss on training batch is 0.0145483.
After 2554 training step(s), loss on training batch is 0.0124565.
After 2555 training step(s), loss on training batch is 0.011638.
After 2556 training step(s), loss on training batch is 0.0123152.
After 2557 training step(s), loss on training batch is 0.0162356.
After 2558 training step(s), loss on training batch is 0.0117131.
After 2559 training step(s), loss on training batch is 0.0150866.
After 2560 training step(s), loss on training batch is 0.0126484.
After 2561 training step(s), loss on training batch is 0.0182233.
After 2562 training step(s), loss on training batch is 0.0114808.
After 2563 training step(s), loss on training batch is 0.0178242.
After 2564 training step(s), loss on training batch is 0.0184717.
After 2565 training step(s), loss on training batch is 0.0164948.
After 2566 training step(s), loss on training batch is 0.0133088.
After 2567 training step(s), loss on training batch is 0.0215983.
After 2568 training step(s), loss on training batch is 0.0111092.
After 2569 training step(s), loss on training batch is 0.0124219.
After 2570 training step(s), loss on training batch is 0.0198077.
After 2571 training step(s), loss on training batch is 0.0119117.
After 2572 training step(s), loss on training batch is 0.0148337.
After 2573 training step(s), loss on training batch is 0.0136453.
After 2574 training step(s), loss on training batch is 0.0114285.
After 2575 training step(s), loss on training batch is 0.0115444.
After 2576 training step(s), loss on training batch is 0.0214225.
After 2577 training step(s), loss on training batch is 0.0227348.
After 2578 training step(s), loss on training batch is 0.0487604.
After 2579 training step(s), loss on training batch is 0.0134799.
After 2580 training step(s), loss on training batch is 0.0168366.
After 2581 training step(s), loss on training batch is 0.0114046.
After 2582 training step(s), loss on training batch is 0.0124775.
After 2583 training step(s), loss on training batch is 0.0121969.
After 2584 training step(s), loss on training batch is 0.0133523.
After 2585 training step(s), loss on training batch is 0.0332199.
After 2586 training step(s), loss on training batch is 0.0276772.
After 2587 training step(s), loss on training batch is 0.0436132.
After 2588 training step(s), loss on training batch is 0.0146504.
After 2589 training step(s), loss on training batch is 0.02202.
After 2590 training step(s), loss on training batch is 0.0110808.
After 2591 training step(s), loss on training batch is 0.0114199.
After 2592 training step(s), loss on training batch is 0.0167228.
After 2593 training step(s), loss on training batch is 0.012597.
After 2594 training step(s), loss on training batch is 0.0126591.
After 2595 training step(s), loss on training batch is 0.0108992.
After 2596 training step(s), loss on training batch is 0.012067.
After 2597 training step(s), loss on training batch is 0.0135942.
After 2598 training step(s), loss on training batch is 0.0134941.
After 2599 training step(s), loss on training batch is 0.0194956.
After 2600 training step(s), loss on training batch is 0.0198079.
After 2601 training step(s), loss on training batch is 0.0131525.
After 2602 training step(s), loss on training batch is 0.0144176.
After 2603 training step(s), loss on training batch is 0.0182667.
After 2604 training step(s), loss on training batch is 0.0397921.
After 2605 training step(s), loss on training batch is 0.0185757.
After 2606 training step(s), loss on training batch is 0.0215773.
After 2607 training step(s), loss on training batch is 0.0120474.
After 2608 training step(s), loss on training batch is 0.0162927.
After 2609 training step(s), loss on training batch is 0.0132563.
After 2610 training step(s), loss on training batch is 0.0152778.
After 2611 training step(s), loss on training batch is 0.0136124.
After 2612 training step(s), loss on training batch is 0.0366917.
After 2613 training step(s), loss on training batch is 0.0115533.
After 2614 training step(s), loss on training batch is 0.0120159.
After 2615 training step(s), loss on training batch is 0.0169179.
After 2616 training step(s), loss on training batch is 0.0135722.
After 2617 training step(s), loss on training batch is 0.0135206.
After 2618 training step(s), loss on training batch is 0.0273984.
After 2619 training step(s), loss on training batch is 0.0128616.
After 2620 training step(s), loss on training batch is 0.0165579.
After 2621 training step(s), loss on training batch is 0.0164488.
After 2622 training step(s), loss on training batch is 0.0208196.
After 2623 training step(s), loss on training batch is 0.0554052.
After 2624 training step(s), loss on training batch is 0.0307831.
After 2625 training step(s), loss on training batch is 0.0130627.
After 2626 training step(s), loss on training batch is 0.0311639.
After 2627 training step(s), loss on training batch is 0.0147665.
After 2628 training step(s), loss on training batch is 0.0155645.
After 2629 training step(s), loss on training batch is 0.013887.
After 2630 training step(s), loss on training batch is 0.0130872.
After 2631 training step(s), loss on training batch is 0.0161389.
After 2632 training step(s), loss on training batch is 0.0173735.
After 2633 training step(s), loss on training batch is 0.0146374.
After 2634 training step(s), loss on training batch is 0.0222699.
After 2635 training step(s), loss on training batch is 0.0184819.
After 2636 training step(s), loss on training batch is 0.0191975.
After 2637 training step(s), loss on training batch is 0.0136489.
After 2638 training step(s), loss on training batch is 0.0147244.
After 2639 training step(s), loss on training batch is 0.0143552.
After 2640 training step(s), loss on training batch is 0.0132323.
After 2641 training step(s), loss on training batch is 0.0187642.
After 2642 training step(s), loss on training batch is 0.0160515.
After 2643 training step(s), loss on training batch is 0.0269725.
After 2644 training step(s), loss on training batch is 0.0141095.
After 2645 training step(s), loss on training batch is 0.023158.
After 2646 training step(s), loss on training batch is 0.0124472.
After 2647 training step(s), loss on training batch is 0.0747434.
After 2648 training step(s), loss on training batch is 0.0649716.
After 2649 training step(s), loss on training batch is 0.0121223.
After 2650 training step(s), loss on training batch is 0.0132736.
After 2651 training step(s), loss on training batch is 0.0151543.
After 2652 training step(s), loss on training batch is 0.0116555.
After 2653 training step(s), loss on training batch is 0.0110351.
After 2654 training step(s), loss on training batch is 0.0118925.
After 2655 training step(s), loss on training batch is 0.0232191.
After 2656 training step(s), loss on training batch is 0.0184084.
After 2657 training step(s), loss on training batch is 0.0143044.
After 2658 training step(s), loss on training batch is 0.0302503.
After 2659 training step(s), loss on training batch is 0.0126555.
After 2660 training step(s), loss on training batch is 0.0145404.
After 2661 training step(s), loss on training batch is 0.0158795.
After 2662 training step(s), loss on training batch is 0.0131453.
After 2663 training step(s), loss on training batch is 0.0265252.
After 2664 training step(s), loss on training batch is 0.0119679.
After 2665 training step(s), loss on training batch is 0.0215504.
After 2666 training step(s), loss on training batch is 0.0192935.
After 2667 training step(s), loss on training batch is 0.0123598.
After 2668 training step(s), loss on training batch is 0.0268918.
After 2669 training step(s), loss on training batch is 0.0184655.
After 2670 training step(s), loss on training batch is 0.0240925.
After 2671 training step(s), loss on training batch is 0.01859.
After 2672 training step(s), loss on training batch is 0.0171569.
After 2673 training step(s), loss on training batch is 0.0187517.
After 2674 training step(s), loss on training batch is 0.016249.
After 2675 training step(s), loss on training batch is 0.011181.
After 2676 training step(s), loss on training batch is 0.017926.
After 2677 training step(s), loss on training batch is 0.0119185.
After 2678 training step(s), loss on training batch is 0.0291066.
After 2679 training step(s), loss on training batch is 0.0172802.
After 2680 training step(s), loss on training batch is 0.0143835.
After 2681 training step(s), loss on training batch is 0.011993.
After 2682 training step(s), loss on training batch is 0.0213589.
After 2683 training step(s), loss on training batch is 0.013244.
After 2684 training step(s), loss on training batch is 0.0158462.
After 2685 training step(s), loss on training batch is 0.0117368.
After 2686 training step(s), loss on training batch is 0.0132053.
After 2687 training step(s), loss on training batch is 0.0133687.
After 2688 training step(s), loss on training batch is 0.0140164.
After 2689 training step(s), loss on training batch is 0.0255917.
After 2690 training step(s), loss on training batch is 0.0108821.
After 2691 training step(s), loss on training batch is 0.0111183.
After 2692 training step(s), loss on training batch is 0.0152729.
After 2693 training step(s), loss on training batch is 0.0117945.
After 2694 training step(s), loss on training batch is 0.0190992.
After 2695 training step(s), loss on training batch is 0.0148963.
After 2696 training step(s), loss on training batch is 0.0112928.
After 2697 training step(s), loss on training batch is 0.012364.
After 2698 training step(s), loss on training batch is 0.0160202.
After 2699 training step(s), loss on training batch is 0.01927.
After 2700 training step(s), loss on training batch is 0.0134687.
After 2701 training step(s), loss on training batch is 0.0148862.
After 2702 training step(s), loss on training batch is 0.0139003.
After 2703 training step(s), loss on training batch is 0.0185952.
After 2704 training step(s), loss on training batch is 0.0165507.
After 2705 training step(s), loss on training batch is 0.0135719.
After 2706 training step(s), loss on training batch is 0.0179036.
After 2707 training step(s), loss on training batch is 0.0122674.
After 2708 training step(s), loss on training batch is 0.0125242.
After 2709 training step(s), loss on training batch is 0.0331467.
After 2710 training step(s), loss on training batch is 0.0113207.
After 2711 training step(s), loss on training batch is 0.0111016.
After 2712 training step(s), loss on training batch is 0.0205854.
After 2713 training step(s), loss on training batch is 0.0112033.
After 2714 training step(s), loss on training batch is 0.0118217.
After 2715 training step(s), loss on training batch is 0.0119116.
After 2716 training step(s), loss on training batch is 0.0142253.
After 2717 training step(s), loss on training batch is 0.0150303.
After 2718 training step(s), loss on training batch is 0.0220697.
After 2719 training step(s), loss on training batch is 0.0192556.
After 2720 training step(s), loss on training batch is 0.0180251.
After 2721 training step(s), loss on training batch is 0.0170734.
After 2722 training step(s), loss on training batch is 0.0602066.
After 2723 training step(s), loss on training batch is 0.0139101.
After 2724 training step(s), loss on training batch is 0.0305708.
After 2725 training step(s), loss on training batch is 0.0193115.
After 2726 training step(s), loss on training batch is 0.0115212.
After 2727 training step(s), loss on training batch is 0.0162676.
After 2728 training step(s), loss on training batch is 0.0122828.
After 2729 training step(s), loss on training batch is 0.0118833.
After 2730 training step(s), loss on training batch is 0.0171276.
After 2731 training step(s), loss on training batch is 0.013443.
After 2732 training step(s), loss on training batch is 0.0137262.
After 2733 training step(s), loss on training batch is 0.016288.
After 2734 training step(s), loss on training batch is 0.015132.
After 2735 training step(s), loss on training batch is 0.0157582.
After 2736 training step(s), loss on training batch is 0.014985.
After 2737 training step(s), loss on training batch is 0.0169554.
After 2738 training step(s), loss on training batch is 0.0271383.
After 2739 training step(s), loss on training batch is 0.0245614.
After 2740 training step(s), loss on training batch is 0.0138527.
After 2741 training step(s), loss on training batch is 0.0286776.
After 2742 training step(s), loss on training batch is 0.0270077.
After 2743 training step(s), loss on training batch is 0.0175453.
After 2744 training step(s), loss on training batch is 0.015863.
After 2745 training step(s), loss on training batch is 0.0122498.
After 2746 training step(s), loss on training batch is 0.0168013.
After 2747 training step(s), loss on training batch is 0.0195729.
After 2748 training step(s), loss on training batch is 0.0121204.
After 2749 training step(s), loss on training batch is 0.01719.
After 2750 training step(s), loss on training batch is 0.0114306.
After 2751 training step(s), loss on training batch is 0.0130141.
After 2752 training step(s), loss on training batch is 0.0147676.
After 2753 training step(s), loss on training batch is 0.0120621.
After 2754 training step(s), loss on training batch is 0.0148252.
After 2755 training step(s), loss on training batch is 0.0129498.
After 2756 training step(s), loss on training batch is 0.0159842.
After 2757 training step(s), loss on training batch is 0.0139425.
After 2758 training step(s), loss on training batch is 0.0115965.
After 2759 training step(s), loss on training batch is 0.0169273.
After 2760 training step(s), loss on training batch is 0.0145687.
After 2761 training step(s), loss on training batch is 0.011645.
After 2762 training step(s), loss on training batch is 0.0158038.
After 2763 training step(s), loss on training batch is 0.0138735.
After 2764 training step(s), loss on training batch is 0.0129064.
After 2765 training step(s), loss on training batch is 0.013569.
After 2766 training step(s), loss on training batch is 0.0120823.
After 2767 training step(s), loss on training batch is 0.0118294.
After 2768 training step(s), loss on training batch is 0.0133648.
After 2769 training step(s), loss on training batch is 0.0215453.
After 2770 training step(s), loss on training batch is 0.0151104.
After 2771 training step(s), loss on training batch is 0.0143536.
After 2772 training step(s), loss on training batch is 0.0133533.
After 2773 training step(s), loss on training batch is 0.0164538.
After 2774 training step(s), loss on training batch is 0.0214609.
After 2775 training step(s), loss on training batch is 0.0125626.
After 2776 training step(s), loss on training batch is 0.0268856.
After 2777 training step(s), loss on training batch is 0.010741.
After 2778 training step(s), loss on training batch is 0.019989.
After 2779 training step(s), loss on training batch is 0.011888.
After 2780 training step(s), loss on training batch is 0.0130202.
After 2781 training step(s), loss on training batch is 0.013048.
After 2782 training step(s), loss on training batch is 0.0137809.
After 2783 training step(s), loss on training batch is 0.0137102.
After 2784 training step(s), loss on training batch is 0.0136883.
After 2785 training step(s), loss on training batch is 0.0116516.
After 2786 training step(s), loss on training batch is 0.0117818.
After 2787 training step(s), loss on training batch is 0.0117053.
After 2788 training step(s), loss on training batch is 0.0114621.
After 2789 training step(s), loss on training batch is 0.0114236.
After 2790 training step(s), loss on training batch is 0.012916.
After 2791 training step(s), loss on training batch is 0.0132218.
After 2792 training step(s), loss on training batch is 0.0102303.
After 2793 training step(s), loss on training batch is 0.0173067.
After 2794 training step(s), loss on training batch is 0.0108377.
After 2795 training step(s), loss on training batch is 0.0121903.
After 2796 training step(s), loss on training batch is 0.0156266.
After 2797 training step(s), loss on training batch is 0.0145673.
After 2798 training step(s), loss on training batch is 0.0125359.
After 2799 training step(s), loss on training batch is 0.0131765.
After 2800 training step(s), loss on training batch is 0.010606.
After 2801 training step(s), loss on training batch is 0.0118786.
After 2802 training step(s), loss on training batch is 0.0131424.
After 2803 training step(s), loss on training batch is 0.0102749.
After 2804 training step(s), loss on training batch is 0.0177246.
After 2805 training step(s), loss on training batch is 0.0353508.
After 2806 training step(s), loss on training batch is 0.0110911.
After 2807 training step(s), loss on training batch is 0.0211963.
After 2808 training step(s), loss on training batch is 0.0147904.
After 2809 training step(s), loss on training batch is 0.0127141.
After 2810 training step(s), loss on training batch is 0.0128905.
After 2811 training step(s), loss on training batch is 0.0135635.
After 2812 training step(s), loss on training batch is 0.012866.
After 2813 training step(s), loss on training batch is 0.018967.
After 2814 training step(s), loss on training batch is 0.0126288.
After 2815 training step(s), loss on training batch is 0.0118055.
After 2816 training step(s), loss on training batch is 0.0117766.
After 2817 training step(s), loss on training batch is 0.0263922.
After 2818 training step(s), loss on training batch is 0.0163292.
After 2819 training step(s), loss on training batch is 0.0150074.
After 2820 training step(s), loss on training batch is 0.0125467.
After 2821 training step(s), loss on training batch is 0.0134099.
After 2822 training step(s), loss on training batch is 0.0111878.
After 2823 training step(s), loss on training batch is 0.0121164.
After 2824 training step(s), loss on training batch is 0.012216.
After 2825 training step(s), loss on training batch is 0.0150909.
After 2826 training step(s), loss on training batch is 0.0119022.
After 2827 training step(s), loss on training batch is 0.0182528.
After 2828 training step(s), loss on training batch is 0.018445.
After 2829 training step(s), loss on training batch is 0.0241299.
After 2830 training step(s), loss on training batch is 0.0139362.
After 2831 training step(s), loss on training batch is 0.0102169.
After 2832 training step(s), loss on training batch is 0.0180758.
After 2833 training step(s), loss on training batch is 0.0263128.
After 2834 training step(s), loss on training batch is 0.0168348.
After 2835 training step(s), loss on training batch is 0.0146724.
After 2836 training step(s), loss on training batch is 0.0129009.
After 2837 training step(s), loss on training batch is 0.0109599.
After 2838 training step(s), loss on training batch is 0.0158701.
After 2839 training step(s), loss on training batch is 0.013742.
After 2840 training step(s), loss on training batch is 0.013056.
After 2841 training step(s), loss on training batch is 0.0115878.
After 2842 training step(s), loss on training batch is 0.0120154.
After 2843 training step(s), loss on training batch is 0.00994663.
After 2844 training step(s), loss on training batch is 0.0307933.
After 2845 training step(s), loss on training batch is 0.0126764.
After 2846 training step(s), loss on training batch is 0.0131693.
After 2847 training step(s), loss on training batch is 0.0103932.
After 2848 training step(s), loss on training batch is 0.0133571.
After 2849 training step(s), loss on training batch is 0.0136515.
After 2850 training step(s), loss on training batch is 0.0136246.
After 2851 training step(s), loss on training batch is 0.0112266.
After 2852 training step(s), loss on training batch is 0.0155255.
After 2853 training step(s), loss on training batch is 0.0189826.
After 2854 training step(s), loss on training batch is 0.0119239.
After 2855 training step(s), loss on training batch is 0.0109891.
After 2856 training step(s), loss on training batch is 0.0140056.
After 2857 training step(s), loss on training batch is 0.0134821.
After 2858 training step(s), loss on training batch is 0.0119681.
After 2859 training step(s), loss on training batch is 0.0125662.
After 2860 training step(s), loss on training batch is 0.0187928.
After 2861 training step(s), loss on training batch is 0.015702.
After 2862 training step(s), loss on training batch is 0.0105632.
After 2863 training step(s), loss on training batch is 0.023023.
After 2864 training step(s), loss on training batch is 0.0340324.
After 2865 training step(s), loss on training batch is 0.0131176.
After 2866 training step(s), loss on training batch is 0.0251293.
After 2867 training step(s), loss on training batch is 0.0239192.
After 2868 training step(s), loss on training batch is 0.0108946.
After 2869 training step(s), loss on training batch is 0.0110527.
After 2870 training step(s), loss on training batch is 0.0162929.
After 2871 training step(s), loss on training batch is 0.0122124.
After 2872 training step(s), loss on training batch is 0.010867.
After 2873 training step(s), loss on training batch is 0.0106505.
After 2874 training step(s), loss on training batch is 0.0111086.
After 2875 training step(s), loss on training batch is 0.0125416.
After 2876 training step(s), loss on training batch is 0.010924.
After 2877 training step(s), loss on training batch is 0.0120282.
After 2878 training step(s), loss on training batch is 0.0160318.
After 2879 training step(s), loss on training batch is 0.0104967.
After 2880 training step(s), loss on training batch is 0.0133004.
After 2881 training step(s), loss on training batch is 0.0108393.
After 2882 training step(s), loss on training batch is 0.0135339.
After 2883 training step(s), loss on training batch is 0.0199393.
After 2884 training step(s), loss on training batch is 0.0168163.
After 2885 training step(s), loss on training batch is 0.0306595.
After 2886 training step(s), loss on training batch is 0.0194828.
After 2887 training step(s), loss on training batch is 0.0234312.
After 2888 training step(s), loss on training batch is 0.0111761.
After 2889 training step(s), loss on training batch is 0.0112154.
After 2890 training step(s), loss on training batch is 0.0165438.
After 2891 training step(s), loss on training batch is 0.0135073.
After 2892 training step(s), loss on training batch is 0.0124989.
After 2893 training step(s), loss on training batch is 0.0182936.
After 2894 training step(s), loss on training batch is 0.0106091.
After 2895 training step(s), loss on training batch is 0.0121724.
After 2896 training step(s), loss on training batch is 0.0136363.
After 2897 training step(s), loss on training batch is 0.012673.
After 2898 training step(s), loss on training batch is 0.014128.
After 2899 training step(s), loss on training batch is 0.0693858.
After 2900 training step(s), loss on training batch is 0.0210227.
After 2901 training step(s), loss on training batch is 0.0307825.
After 2902 training step(s), loss on training batch is 0.0147633.
After 2903 training step(s), loss on training batch is 0.0192438.
After 2904 training step(s), loss on training batch is 0.0165501.
After 2905 training step(s), loss on training batch is 0.010478.
After 2906 training step(s), loss on training batch is 0.0218859.
After 2907 training step(s), loss on training batch is 0.0136012.
After 2908 training step(s), loss on training batch is 0.0112588.
After 2909 training step(s), loss on training batch is 0.0146085.
After 2910 training step(s), loss on training batch is 0.0133105.
After 2911 training step(s), loss on training batch is 0.028581.
After 2912 training step(s), loss on training batch is 0.013817.
After 2913 training step(s), loss on training batch is 0.0156784.
After 2914 training step(s), loss on training batch is 0.0121068.
After 2915 training step(s), loss on training batch is 0.0114063.
After 2916 training step(s), loss on training batch is 0.0120478.
After 2917 training step(s), loss on training batch is 0.0153568.
After 2918 training step(s), loss on training batch is 0.0112735.
After 2919 training step(s), loss on training batch is 0.0106235.
After 2920 training step(s), loss on training batch is 0.0157288.
After 2921 training step(s), loss on training batch is 0.0116691.
After 2922 training step(s), loss on training batch is 0.0148643.
After 2923 training step(s), loss on training batch is 0.0113866.
After 2924 training step(s), loss on training batch is 0.0163867.
After 2925 training step(s), loss on training batch is 0.0166461.
After 2926 training step(s), loss on training batch is 0.0104035.
After 2927 training step(s), loss on training batch is 0.0119022.
After 2928 training step(s), loss on training batch is 0.0140426.
After 2929 training step(s), loss on training batch is 0.0124653.
After 2930 training step(s), loss on training batch is 0.0114881.
After 2931 training step(s), loss on training batch is 0.0131065.
After 2932 training step(s), loss on training batch is 0.0186043.
After 2933 training step(s), loss on training batch is 0.014933.
After 2934 training step(s), loss on training batch is 0.0136022.
After 2935 training step(s), loss on training batch is 0.0325071.
After 2936 training step(s), loss on training batch is 0.0172931.
After 2937 training step(s), loss on training batch is 0.0148019.
After 2938 training step(s), loss on training batch is 0.0155218.
After 2939 training step(s), loss on training batch is 0.0143834.
After 2940 training step(s), loss on training batch is 0.0124071.
After 2941 training step(s), loss on training batch is 0.0101187.
After 2942 training step(s), loss on training batch is 0.0131274.
After 2943 training step(s), loss on training batch is 0.0234182.
After 2944 training step(s), loss on training batch is 0.0258864.
After 2945 training step(s), loss on training batch is 0.0233558.
After 2946 training step(s), loss on training batch is 0.0116945.
After 2947 training step(s), loss on training batch is 0.0126256.
After 2948 training step(s), loss on training batch is 0.0155253.
After 2949 training step(s), loss on training batch is 0.00993553.
After 2950 training step(s), loss on training batch is 0.0101278.
After 2951 training step(s), loss on training batch is 0.0116963.
After 2952 training step(s), loss on training batch is 0.0108728.
After 2953 training step(s), loss on training batch is 0.0180095.
After 2954 training step(s), loss on training batch is 0.0118123.
After 2955 training step(s), loss on training batch is 0.0211833.
After 2956 training step(s), loss on training batch is 0.0134955.
After 2957 training step(s), loss on training batch is 0.018851.
After 2958 training step(s), loss on training batch is 0.011521.
After 2959 training step(s), loss on training batch is 0.0229708.
After 2960 training step(s), loss on training batch is 0.0115775.
After 2961 training step(s), loss on training batch is 0.013527.
After 2962 training step(s), loss on training batch is 0.0101524.
After 2963 training step(s), loss on training batch is 0.0166964.
After 2964 training step(s), loss on training batch is 0.0109862.
After 2965 training step(s), loss on training batch is 0.0221564.
After 2966 training step(s), loss on training batch is 0.0131452.
After 2967 training step(s), loss on training batch is 0.0102031.
After 2968 training step(s), loss on training batch is 0.0172978.
After 2969 training step(s), loss on training batch is 0.0198659.
After 2970 training step(s), loss on training batch is 0.0148319.
After 2971 training step(s), loss on training batch is 0.0209813.
After 2972 training step(s), loss on training batch is 0.0112916.
After 2973 training step(s), loss on training batch is 0.011474.
After 2974 training step(s), loss on training batch is 0.0443603.
After 2975 training step(s), loss on training batch is 0.0172766.
After 2976 training step(s), loss on training batch is 0.0112126.
After 2977 training step(s), loss on training batch is 0.0142248.
After 2978 training step(s), loss on training batch is 0.0287877.
After 2979 training step(s), loss on training batch is 0.0176555.
After 2980 training step(s), loss on training batch is 0.0137687.
After 2981 training step(s), loss on training batch is 0.0176944.
After 2982 training step(s), loss on training batch is 0.0131599.
After 2983 training step(s), loss on training batch is 0.0153043.
After 2984 training step(s), loss on training batch is 0.0192221.
After 2985 training step(s), loss on training batch is 0.0123721.
After 2986 training step(s), loss on training batch is 0.0139888.
After 2987 training step(s), loss on training batch is 0.016003.
After 2988 training step(s), loss on training batch is 0.0119927.
After 2989 training step(s), loss on training batch is 0.0144876.
After 2990 training step(s), loss on training batch is 0.0159058.
After 2991 training step(s), loss on training batch is 0.0161171.
After 2992 training step(s), loss on training batch is 0.010681.
After 2993 training step(s), loss on training batch is 0.0147346.
After 2994 training step(s), loss on training batch is 0.0326895.
After 2995 training step(s), loss on training batch is 0.01757.
After 2996 training step(s), loss on training batch is 0.0363979.
After 2997 training step(s), loss on training batch is 0.011927.
After 2998 training step(s), loss on training batch is 0.0105511.
After 2999 training step(s), loss on training batch is 0.0127557.
After 3000 training step(s), loss on training batch is 0.0164047.
After 3001 training step(s), loss on training batch is 0.0158152.
After 3002 training step(s), loss on training batch is 0.0309325.
After 3003 training step(s), loss on training batch is 0.0142029.
After 3004 training step(s), loss on training batch is 0.0138007.
After 3005 training step(s), loss on training batch is 0.0147576.
After 3006 training step(s), loss on training batch is 0.0155756.
After 3007 training step(s), loss on training batch is 0.0127273.
After 3008 training step(s), loss on training batch is 0.0140385.
After 3009 training step(s), loss on training batch is 0.0116062.
After 3010 training step(s), loss on training batch is 0.0148927.
After 3011 training step(s), loss on training batch is 0.0105128.
After 3012 training step(s), loss on training batch is 0.014344.
After 3013 training step(s), loss on training batch is 0.0128607.
After 3014 training step(s), loss on training batch is 0.0123453.
After 3015 training step(s), loss on training batch is 0.0107616.
After 3016 training step(s), loss on training batch is 0.0150292.
After 3017 training step(s), loss on training batch is 0.0198261.
After 3018 training step(s), loss on training batch is 0.0106304.
After 3019 training step(s), loss on training batch is 0.0281717.
After 3020 training step(s), loss on training batch is 0.02783.
After 3021 training step(s), loss on training batch is 0.0335127.
After 3022 training step(s), loss on training batch is 0.0138171.
After 3023 training step(s), loss on training batch is 0.0145632.
After 3024 training step(s), loss on training batch is 0.0129384.
After 3025 training step(s), loss on training batch is 0.014183.
After 3026 training step(s), loss on training batch is 0.0112519.
After 3027 training step(s), loss on training batch is 0.0129465.
After 3028 training step(s), loss on training batch is 0.0192559.
After 3029 training step(s), loss on training batch is 0.0141997.
After 3030 training step(s), loss on training batch is 0.0104501.
After 3031 training step(s), loss on training batch is 0.0105727.
After 3032 training step(s), loss on training batch is 0.0113463.
After 3033 training step(s), loss on training batch is 0.0133689.
After 3034 training step(s), loss on training batch is 0.0171073.
After 3035 training step(s), loss on training batch is 0.0136549.
After 3036 training step(s), loss on training batch is 0.011564.
After 3037 training step(s), loss on training batch is 0.0103816.
After 3038 training step(s), loss on training batch is 0.0117333.
After 3039 training step(s), loss on training batch is 0.010584.
After 3040 training step(s), loss on training batch is 0.0195784.
After 3041 training step(s), loss on training batch is 0.0127055.
After 3042 training step(s), loss on training batch is 0.0132412.
After 3043 training step(s), loss on training batch is 0.0101937.
After 3044 training step(s), loss on training batch is 0.01805.
After 3045 training step(s), loss on training batch is 0.0129758.
After 3046 training step(s), loss on training batch is 0.0101608.
After 3047 training step(s), loss on training batch is 0.0104558.
After 3048 training step(s), loss on training batch is 0.0216386.
After 3049 training step(s), loss on training batch is 0.0160758.
After 3050 training step(s), loss on training batch is 0.028219.
After 3051 training step(s), loss on training batch is 0.0105722.
After 3052 training step(s), loss on training batch is 0.0113625.
After 3053 training step(s), loss on training batch is 0.0103102.
After 3054 training step(s), loss on training batch is 0.0140643.
After 3055 training step(s), loss on training batch is 0.0158214.
After 3056 training step(s), loss on training batch is 0.0112577.
After 3057 training step(s), loss on training batch is 0.0141152.
After 3058 training step(s), loss on training batch is 0.0105094.
After 3059 training step(s), loss on training batch is 0.0100616.
After 3060 training step(s), loss on training batch is 0.0130536.
After 3061 training step(s), loss on training batch is 0.0126862.
After 3062 training step(s), loss on training batch is 0.0114786.
After 3063 training step(s), loss on training batch is 0.00985337.
After 3064 training step(s), loss on training batch is 0.011221.
After 3065 training step(s), loss on training batch is 0.0107537.
After 3066 training step(s), loss on training batch is 0.0145665.
After 3067 training step(s), loss on training batch is 0.0110091.
After 3068 training step(s), loss on training batch is 0.0115694.
After 3069 training step(s), loss on training batch is 0.0133058.
After 3070 training step(s), loss on training batch is 0.010178.
After 3071 training step(s), loss on training batch is 0.0122598.
After 3072 training step(s), loss on training batch is 0.0119551.
After 3073 training step(s), loss on training batch is 0.0113138.
After 3074 training step(s), loss on training batch is 0.0112607.
After 3075 training step(s), loss on training batch is 0.0115432.
After 3076 training step(s), loss on training batch is 0.0114927.
After 3077 training step(s), loss on training batch is 0.00992583.
After 3078 training step(s), loss on training batch is 0.0136899.
After 3079 training step(s), loss on training batch is 0.00973091.
After 3080 training step(s), loss on training batch is 0.0111292.
After 3081 training step(s), loss on training batch is 0.0106868.
After 3082 training step(s), loss on training batch is 0.0116832.
After 3083 training step(s), loss on training batch is 0.0116452.
After 3084 training step(s), loss on training batch is 0.0102483.
After 3085 training step(s), loss on training batch is 0.0110608.
After 3086 training step(s), loss on training batch is 0.0125336.
After 3087 training step(s), loss on training batch is 0.00971436.
After 3088 training step(s), loss on training batch is 0.011499.
After 3089 training step(s), loss on training batch is 0.0101344.
After 3090 training step(s), loss on training batch is 0.011033.
After 3091 training step(s), loss on training batch is 0.00972864.
After 3092 training step(s), loss on training batch is 0.0099851.
After 3093 training step(s), loss on training batch is 0.0129569.
After 3094 training step(s), loss on training batch is 0.0102309.
After 3095 training step(s), loss on training batch is 0.0100813.
After 3096 training step(s), loss on training batch is 0.0152329.
After 3097 training step(s), loss on training batch is 0.0120688.
After 3098 training step(s), loss on training batch is 0.0121806.
After 3099 training step(s), loss on training batch is 0.0124542.
After 3100 training step(s), loss on training batch is 0.0120004.
After 3101 training step(s), loss on training batch is 0.0109945.
After 3102 training step(s), loss on training batch is 0.0108574.
After 3103 training step(s), loss on training batch is 0.0102873.
After 3104 training step(s), loss on training batch is 0.0151237.
After 3105 training step(s), loss on training batch is 0.0100763.
After 3106 training step(s), loss on training batch is 0.0129727.
After 3107 training step(s), loss on training batch is 0.0100448.
After 3108 training step(s), loss on training batch is 0.0158163.
After 3109 training step(s), loss on training batch is 0.0102049.
After 3110 training step(s), loss on training batch is 0.0109319.
After 3111 training step(s), loss on training batch is 0.0108016.
After 3112 training step(s), loss on training batch is 0.0104348.
After 3113 training step(s), loss on training batch is 0.0111394.
After 3114 training step(s), loss on training batch is 0.0104902.
After 3115 training step(s), loss on training batch is 0.0100076.
After 3116 training step(s), loss on training batch is 0.0121122.
After 3117 training step(s), loss on training batch is 0.0104056.
After 3118 training step(s), loss on training batch is 0.0106344.
After 3119 training step(s), loss on training batch is 0.010386.
After 3120 training step(s), loss on training batch is 0.0129922.
After 3121 training step(s), loss on training batch is 0.010127.
After 3122 training step(s), loss on training batch is 0.0131583.
After 3123 training step(s), loss on training batch is 0.00997224.
After 3124 training step(s), loss on training batch is 0.0123388.
After 3125 training step(s), loss on training batch is 0.0162732.
After 3126 training step(s), loss on training batch is 0.0108864.
After 3127 training step(s), loss on training batch is 0.0102406.
After 3128 training step(s), loss on training batch is 0.0106626.
After 3129 training step(s), loss on training batch is 0.0104564.
After 3130 training step(s), loss on training batch is 0.0108026.
After 3131 training step(s), loss on training batch is 0.0143728.
After 3132 training step(s), loss on training batch is 0.0138206.
After 3133 training step(s), loss on training batch is 0.0108194.
After 3134 training step(s), loss on training batch is 0.0102252.
After 3135 training step(s), loss on training batch is 0.0132677.
After 3136 training step(s), loss on training batch is 0.0117548.
After 3137 training step(s), loss on training batch is 0.0102585.
After 3138 training step(s), loss on training batch is 0.0103945.
After 3139 training step(s), loss on training batch is 0.0104847.
After 3140 training step(s), loss on training batch is 0.0107922.
After 3141 training step(s), loss on training batch is 0.0103836.
After 3142 training step(s), loss on training batch is 0.0107212.
After 3143 training step(s), loss on training batch is 0.0105309.
After 3144 training step(s), loss on training batch is 0.0124051.
After 3145 training step(s), loss on training batch is 0.0120805.
After 3146 training step(s), loss on training batch is 0.0117972.
After 3147 training step(s), loss on training batch is 0.0164137.
After 3148 training step(s), loss on training batch is 0.0118533.
After 3149 training step(s), loss on training batch is 0.0133941.
After 3150 training step(s), loss on training batch is 0.011129.
After 3151 training step(s), loss on training batch is 0.0125711.
After 3152 training step(s), loss on training batch is 0.0161255.
After 3153 training step(s), loss on training batch is 0.0109503.
After 3154 training step(s), loss on training batch is 0.0100852.
After 3155 training step(s), loss on training batch is 0.0139731.
After 3156 training step(s), loss on training batch is 0.0110793.
After 3157 training step(s), loss on training batch is 0.0123624.
After 3158 training step(s), loss on training batch is 0.011783.
After 3159 training step(s), loss on training batch is 0.034469.
After 3160 training step(s), loss on training batch is 0.0194064.
After 3161 training step(s), loss on training batch is 0.00959661.
After 3162 training step(s), loss on training batch is 0.0142124.
After 3163 training step(s), loss on training batch is 0.011873.
After 3164 training step(s), loss on training batch is 0.0136061.
After 3165 training step(s), loss on training batch is 0.00992802.
After 3166 training step(s), loss on training batch is 0.00965733.
After 3167 training step(s), loss on training batch is 0.0174248.
After 3168 training step(s), loss on training batch is 0.010319.
After 3169 training step(s), loss on training batch is 0.0163148.
After 3170 training step(s), loss on training batch is 0.028368.
After 3171 training step(s), loss on training batch is 0.0171774.
After 3172 training step(s), loss on training batch is 0.013405.
After 3173 training step(s), loss on training batch is 0.0115927.
After 3174 training step(s), loss on training batch is 0.0129961.
After 3175 training step(s), loss on training batch is 0.0116595.
After 3176 training step(s), loss on training batch is 0.0105436.
After 3177 training step(s), loss on training batch is 0.0123244.
After 3178 training step(s), loss on training batch is 0.0221997.
After 3179 training step(s), loss on training batch is 0.0185103.
After 3180 training step(s), loss on training batch is 0.0126754.
After 3181 training step(s), loss on training batch is 0.0251748.
After 3182 training step(s), loss on training batch is 0.0143333.
After 3183 training step(s), loss on training batch is 0.0164723.
After 3184 training step(s), loss on training batch is 0.0102464.
After 3185 training step(s), loss on training batch is 0.0126517.
After 3186 training step(s), loss on training batch is 0.012027.
After 3187 training step(s), loss on training batch is 0.0130974.
After 3188 training step(s), loss on training batch is 0.0133454.
After 3189 training step(s), loss on training batch is 0.010086.
After 3190 training step(s), loss on training batch is 0.0108973.
After 3191 training step(s), loss on training batch is 0.0118857.
After 3192 training step(s), loss on training batch is 0.0129466.
After 3193 training step(s), loss on training batch is 0.0118267.
After 3194 training step(s), loss on training batch is 0.0165554.
After 3195 training step(s), loss on training batch is 0.0106546.
After 3196 training step(s), loss on training batch is 0.0136744.
After 3197 training step(s), loss on training batch is 0.01414.
After 3198 training step(s), loss on training batch is 0.0188034.
After 3199 training step(s), loss on training batch is 0.0101294.
After 3200 training step(s), loss on training batch is 0.0114825.
After 3201 training step(s), loss on training batch is 0.0101162.
After 3202 training step(s), loss on training batch is 0.0108298.
After 3203 training step(s), loss on training batch is 0.0106886.
After 3204 training step(s), loss on training batch is 0.0240831.
After 3205 training step(s), loss on training batch is 0.021689.
After 3206 training step(s), loss on training batch is 0.0144502.
After 3207 training step(s), loss on training batch is 0.0136783.
After 3208 training step(s), loss on training batch is 0.010494.
After 3209 training step(s), loss on training batch is 0.0120655.
After 3210 training step(s), loss on training batch is 0.0248512.
After 3211 training step(s), loss on training batch is 0.0184895.
After 3212 training step(s), loss on training batch is 0.0107173.
After 3213 training step(s), loss on training batch is 0.0116829.
After 3214 training step(s), loss on training batch is 0.0168624.
After 3215 training step(s), loss on training batch is 0.0152917.
After 3216 training step(s), loss on training batch is 0.0175044.
After 3217 training step(s), loss on training batch is 0.0132438.
After 3218 training step(s), loss on training batch is 0.0165042.
After 3219 training step(s), loss on training batch is 0.0104166.
After 3220 training step(s), loss on training batch is 0.0113627.
After 3221 training step(s), loss on training batch is 0.0155551.
After 3222 training step(s), loss on training batch is 0.0168185.
After 3223 training step(s), loss on training batch is 0.0139117.
After 3224 training step(s), loss on training batch is 0.0102355.
After 3225 training step(s), loss on training batch is 0.0111898.
After 3226 training step(s), loss on training batch is 0.0151571.
After 3227 training step(s), loss on training batch is 0.0129913.
After 3228 training step(s), loss on training batch is 0.0241897.
After 3229 training step(s), loss on training batch is 0.0444252.
After 3230 training step(s), loss on training batch is 0.0145037.
After 3231 training step(s), loss on training batch is 0.0181578.
After 3232 training step(s), loss on training batch is 0.0175188.
After 3233 training step(s), loss on training batch is 0.0158087.
After 3234 training step(s), loss on training batch is 0.0233141.
After 3235 training step(s), loss on training batch is 0.0107454.
After 3236 training step(s), loss on training batch is 0.0121642.
After 3237 training step(s), loss on training batch is 0.013061.
After 3238 training step(s), loss on training batch is 0.010777.
After 3239 training step(s), loss on training batch is 0.0106235.
After 3240 training step(s), loss on training batch is 0.0125063.
After 3241 training step(s), loss on training batch is 0.0142243.
After 3242 training step(s), loss on training batch is 0.0121584.
After 3243 training step(s), loss on training batch is 0.0117683.
After 3244 training step(s), loss on training batch is 0.0153503.
After 3245 training step(s), loss on training batch is 0.0104352.
After 3246 training step(s), loss on training batch is 0.0155282.
After 3247 training step(s), loss on training batch is 0.011256.
After 3248 training step(s), loss on training batch is 0.0134107.
After 3249 training step(s), loss on training batch is 0.010717.
After 3250 training step(s), loss on training batch is 0.0293469.
After 3251 training step(s), loss on training batch is 0.0193195.
After 3252 training step(s), loss on training batch is 0.0195463.
After 3253 training step(s), loss on training batch is 0.0114032.
After 3254 training step(s), loss on training batch is 0.0116153.
After 3255 training step(s), loss on training batch is 0.00992654.
After 3256 training step(s), loss on training batch is 0.0121854.
After 3257 training step(s), loss on training batch is 0.0113591.
After 3258 training step(s), loss on training batch is 0.0150535.
After 3259 training step(s), loss on training batch is 0.0115462.
After 3260 training step(s), loss on training batch is 0.0109211.
After 3261 training step(s), loss on training batch is 0.0153811.
After 3262 training step(s), loss on training batch is 0.0143554.
After 3263 training step(s), loss on training batch is 0.0117195.
After 3264 training step(s), loss on training batch is 0.011595.
After 3265 training step(s), loss on training batch is 0.0122722.
After 3266 training step(s), loss on training batch is 0.0207986.
After 3267 training step(s), loss on training batch is 0.012443.
After 3268 training step(s), loss on training batch is 0.010889.
After 3269 training step(s), loss on training batch is 0.0145776.
After 3270 training step(s), loss on training batch is 0.0114015.
After 3271 training step(s), loss on training batch is 0.0106919.
After 3272 training step(s), loss on training batch is 0.0186718.
After 3273 training step(s), loss on training batch is 0.0151536.
After 3274 training step(s), loss on training batch is 0.0151748.
After 3275 training step(s), loss on training batch is 0.0110815.
After 3276 training step(s), loss on training batch is 0.0243858.
After 3277 training step(s), loss on training batch is 0.0368924.
After 3278 training step(s), loss on training batch is 0.0102528.
After 3279 training step(s), loss on training batch is 0.0488889.
After 3280 training step(s), loss on training batch is 0.0147189.
After 3281 training step(s), loss on training batch is 0.0173123.
After 3282 training step(s), loss on training batch is 0.00984445.
After 3283 training step(s), loss on training batch is 0.0104341.
After 3284 training step(s), loss on training batch is 0.0104748.
After 3285 training step(s), loss on training batch is 0.0130562.
After 3286 training step(s), loss on training batch is 0.0575759.
After 3287 training step(s), loss on training batch is 0.0141496.
After 3288 training step(s), loss on training batch is 0.0123092.
After 3289 training step(s), loss on training batch is 0.0115214.
After 3290 training step(s), loss on training batch is 0.0172684.
After 3291 training step(s), loss on training batch is 0.0213215.
After 3292 training step(s), loss on training batch is 0.00970503.
After 3293 training step(s), loss on training batch is 0.0104654.
After 3294 training step(s), loss on training batch is 0.00950942.
After 3295 training step(s), loss on training batch is 0.0134611.
After 3296 training step(s), loss on training batch is 0.01076.
After 3297 training step(s), loss on training batch is 0.0101743.
After 3298 training step(s), loss on training batch is 0.0115702.
After 3299 training step(s), loss on training batch is 0.0118913.
After 3300 training step(s), loss on training batch is 0.00959476.
After 3301 training step(s), loss on training batch is 0.0103693.
After 3302 training step(s), loss on training batch is 0.00967954.
After 3303 training step(s), loss on training batch is 0.0105477.
After 3304 training step(s), loss on training batch is 0.0100301.
After 3305 training step(s), loss on training batch is 0.0101007.
After 3306 training step(s), loss on training batch is 0.00981747.
After 3307 training step(s), loss on training batch is 0.0105654.
After 3308 training step(s), loss on training batch is 0.0112967.
After 3309 training step(s), loss on training batch is 0.011864.
After 3310 training step(s), loss on training batch is 0.00928355.
After 3311 training step(s), loss on training batch is 0.0122725.
After 3312 training step(s), loss on training batch is 0.0116398.
After 3313 training step(s), loss on training batch is 0.0149609.
After 3314 training step(s), loss on training batch is 0.0113519.
After 3315 training step(s), loss on training batch is 0.0147819.
After 3316 training step(s), loss on training batch is 0.0104654.
After 3317 training step(s), loss on training batch is 0.0139337.
After 3318 training step(s), loss on training batch is 0.0105759.
After 3319 training step(s), loss on training batch is 0.0129076.
After 3320 training step(s), loss on training batch is 0.0111963.
After 3321 training step(s), loss on training batch is 0.0129193.
After 3322 training step(s), loss on training batch is 0.0114728.
After 3323 training step(s), loss on training batch is 0.0116972.
After 3324 training step(s), loss on training batch is 0.0103546.
After 3325 training step(s), loss on training batch is 0.0111916.
After 3326 training step(s), loss on training batch is 0.0106981.
After 3327 training step(s), loss on training batch is 0.0103855.
After 3328 training step(s), loss on training batch is 0.0105418.
After 3329 training step(s), loss on training batch is 0.0108602.
After 3330 training step(s), loss on training batch is 0.00980853.
After 3331 training step(s), loss on training batch is 0.0121328.
After 3332 training step(s), loss on training batch is 0.0112233.
After 3333 training step(s), loss on training batch is 0.0130288.
After 3334 training step(s), loss on training batch is 0.00982726.
After 3335 training step(s), loss on training batch is 0.0113933.
After 3336 training step(s), loss on training batch is 0.0147558.
After 3337 training step(s), loss on training batch is 0.0108006.
After 3338 training step(s), loss on training batch is 0.01131.
After 3339 training step(s), loss on training batch is 0.0098956.
After 3340 training step(s), loss on training batch is 0.0133763.
After 3341 training step(s), loss on training batch is 0.00960294.
After 3342 training step(s), loss on training batch is 0.0102768.
After 3343 training step(s), loss on training batch is 0.0208835.
After 3344 training step(s), loss on training batch is 0.00989025.
After 3345 training step(s), loss on training batch is 0.0117211.
After 3346 training step(s), loss on training batch is 0.00994112.
After 3347 training step(s), loss on training batch is 0.0133693.
After 3348 training step(s), loss on training batch is 0.0110947.
After 3349 training step(s), loss on training batch is 0.00948891.
After 3350 training step(s), loss on training batch is 0.0115418.
After 3351 training step(s), loss on training batch is 0.0154936.
After 3352 training step(s), loss on training batch is 0.0111662.
After 3353 training step(s), loss on training batch is 0.0101003.
After 3354 training step(s), loss on training batch is 0.0109476.
After 3355 training step(s), loss on training batch is 0.0155214.
After 3356 training step(s), loss on training batch is 0.0103336.
After 3357 training step(s), loss on training batch is 0.00902926.
After 3358 training step(s), loss on training batch is 0.00956866.
After 3359 training step(s), loss on training batch is 0.0122769.
After 3360 training step(s), loss on training batch is 0.0100818.
After 3361 training step(s), loss on training batch is 0.0113809.
After 3362 training step(s), loss on training batch is 0.0127582.
After 3363 training step(s), loss on training batch is 0.0112846.
After 3364 training step(s), loss on training batch is 0.036249.
After 3365 training step(s), loss on training batch is 0.0114543.
After 3366 training step(s), loss on training batch is 0.026106.
After 3367 training step(s), loss on training batch is 0.0231987.
After 3368 training step(s), loss on training batch is 0.0262008.
After 3369 training step(s), loss on training batch is 0.0111456.
After 3370 training step(s), loss on training batch is 0.00964129.
After 3371 training step(s), loss on training batch is 0.013934.
After 3372 training step(s), loss on training batch is 0.0118383.
After 3373 training step(s), loss on training batch is 0.0109475.
After 3374 training step(s), loss on training batch is 0.0155.
After 3375 training step(s), loss on training batch is 0.0111503.
After 3376 training step(s), loss on training batch is 0.0100327.
After 3377 training step(s), loss on training batch is 0.0100852.
After 3378 training step(s), loss on training batch is 0.0122876.
After 3379 training step(s), loss on training batch is 0.0125713.
After 3380 training step(s), loss on training batch is 0.0106833.
After 3381 training step(s), loss on training batch is 0.00972613.
After 3382 training step(s), loss on training batch is 0.0106405.
After 3383 training step(s), loss on training batch is 0.0118067.
After 3384 training step(s), loss on training batch is 0.0113701.
After 3385 training step(s), loss on training batch is 0.0130685.
After 3386 training step(s), loss on training batch is 0.00941708.
After 3387 training step(s), loss on training batch is 0.0145647.
After 3388 training step(s), loss on training batch is 0.0122199.
After 3389 training step(s), loss on training batch is 0.0146712.
After 3390 training step(s), loss on training batch is 0.0101802.
After 3391 training step(s), loss on training batch is 0.00985859.
After 3392 training step(s), loss on training batch is 0.012257.
After 3393 training step(s), loss on training batch is 0.0127757.
After 3394 training step(s), loss on training batch is 0.0116966.
After 3395 training step(s), loss on training batch is 0.0117844.
After 3396 training step(s), loss on training batch is 0.0110396.
After 3397 training step(s), loss on training batch is 0.011003.
After 3398 training step(s), loss on training batch is 0.0107681.
After 3399 training step(s), loss on training batch is 0.00938765.
After 3400 training step(s), loss on training batch is 0.00962582.
After 3401 training step(s), loss on training batch is 0.0136672.
After 3402 training step(s), loss on training batch is 0.0116586.
After 3403 training step(s), loss on training batch is 0.0125458.
After 3404 training step(s), loss on training batch is 0.0162794.
After 3405 training step(s), loss on training batch is 0.00975252.
After 3406 training step(s), loss on training batch is 0.011273.
After 3407 training step(s), loss on training batch is 0.0122077.
After 3408 training step(s), loss on training batch is 0.00931826.
After 3409 training step(s), loss on training batch is 0.00959698.
After 3410 training step(s), loss on training batch is 0.00971318.
After 3411 training step(s), loss on training batch is 0.0137618.
After 3412 training step(s), loss on training batch is 0.0174813.
After 3413 training step(s), loss on training batch is 0.0149646.
After 3414 training step(s), loss on training batch is 0.0102124.
After 3415 training step(s), loss on training batch is 0.0165068.
After 3416 training step(s), loss on training batch is 0.00990572.
After 3417 training step(s), loss on training batch is 0.0107197.
After 3418 training step(s), loss on training batch is 0.0125377.
After 3419 training step(s), loss on training batch is 0.0113386.
After 3420 training step(s), loss on training batch is 0.0105397.
After 3421 training step(s), loss on training batch is 0.0102661.
After 3422 training step(s), loss on training batch is 0.010499.
After 3423 training step(s), loss on training batch is 0.00929826.
After 3424 training step(s), loss on training batch is 0.0107091.
After 3425 training step(s), loss on training batch is 0.0355456.
After 3426 training step(s), loss on training batch is 0.0120255.
After 3427 training step(s), loss on training batch is 0.00972688.
After 3428 training step(s), loss on training batch is 0.01097.
After 3429 training step(s), loss on training batch is 0.012345.
After 3430 training step(s), loss on training batch is 0.0189374.
After 3431 training step(s), loss on training batch is 0.012651.
After 3432 training step(s), loss on training batch is 0.0181196.
After 3433 training step(s), loss on training batch is 0.0105249.
After 3434 training step(s), loss on training batch is 0.00980024.
After 3435 training step(s), loss on training batch is 0.0101193.
After 3436 training step(s), loss on training batch is 0.0137358.
After 3437 training step(s), loss on training batch is 0.00952686.
After 3438 training step(s), loss on training batch is 0.00973263.
After 3439 training step(s), loss on training batch is 0.00925627.
After 3440 training step(s), loss on training batch is 0.0142898.
After 3441 training step(s), loss on training batch is 0.00988057.
After 3442 training step(s), loss on training batch is 0.016384.
After 3443 training step(s), loss on training batch is 0.0149351.
After 3444 training step(s), loss on training batch is 0.0126799.
After 3445 training step(s), loss on training batch is 0.016064.
After 3446 training step(s), loss on training batch is 0.0107205.
After 3447 training step(s), loss on training batch is 0.0102042.
After 3448 training step(s), loss on training batch is 0.0106661.
After 3449 training step(s), loss on training batch is 0.00937253.
After 3450 training step(s), loss on training batch is 0.0107403.
After 3451 training step(s), loss on training batch is 0.00937959.
After 3452 training step(s), loss on training batch is 0.0173427.
After 3453 training step(s), loss on training batch is 0.0153393.
After 3454 training step(s), loss on training batch is 0.0101393.
After 3455 training step(s), loss on training batch is 0.00955488.
After 3456 training step(s), loss on training batch is 0.0102773.
After 3457 training step(s), loss on training batch is 0.0098882.
After 3458 training step(s), loss on training batch is 0.00939366.
After 3459 training step(s), loss on training batch is 0.0132815.
After 3460 training step(s), loss on training batch is 0.0127704.
After 3461 training step(s), loss on training batch is 0.0196171.
After 3462 training step(s), loss on training batch is 0.00894257.
After 3463 training step(s), loss on training batch is 0.0139934.
After 3464 training step(s), loss on training batch is 0.00942443.
After 3465 training step(s), loss on training batch is 0.00984739.
After 3466 training step(s), loss on training batch is 0.0108542.
After 3467 training step(s), loss on training batch is 0.00975849.
After 3468 training step(s), loss on training batch is 0.00905398.
After 3469 training step(s), loss on training batch is 0.0179065.
After 3470 training step(s), loss on training batch is 0.00984372.
After 3471 training step(s), loss on training batch is 0.0172592.
After 3472 training step(s), loss on training batch is 0.0149377.
After 3473 training step(s), loss on training batch is 0.010945.
After 3474 training step(s), loss on training batch is 0.0132496.
After 3475 training step(s), loss on training batch is 0.0140027.
After 3476 training step(s), loss on training batch is 0.0102307.
After 3477 training step(s), loss on training batch is 0.0147749.
After 3478 training step(s), loss on training batch is 0.0112383.
After 3479 training step(s), loss on training batch is 0.0101529.
After 3480 training step(s), loss on training batch is 0.00940127.
After 3481 training step(s), loss on training batch is 0.00980498.
After 3482 training step(s), loss on training batch is 0.015915.
After 3483 training step(s), loss on training batch is 0.00913606.
After 3484 training step(s), loss on training batch is 0.011501.
After 3485 training step(s), loss on training batch is 0.0108229.
After 3486 training step(s), loss on training batch is 0.0122589.
After 3487 training step(s), loss on training batch is 0.0117122.
After 3488 training step(s), loss on training batch is 0.0100512.
After 3489 training step(s), loss on training batch is 0.00901505.
After 3490 training step(s), loss on training batch is 0.00944236.
After 3491 training step(s), loss on training batch is 0.00982255.
After 3492 training step(s), loss on training batch is 0.0115575.
After 3493 training step(s), loss on training batch is 0.00941473.
After 3494 training step(s), loss on training batch is 0.00975276.
After 3495 training step(s), loss on training batch is 0.0164912.
After 3496 training step(s), loss on training batch is 0.0101674.
After 3497 training step(s), loss on training batch is 0.0146615.
After 3498 training step(s), loss on training batch is 0.0155004.
After 3499 training step(s), loss on training batch is 0.0150315.
After 3500 training step(s), loss on training batch is 0.013174.
After 3501 training step(s), loss on training batch is 0.0099084.
After 3502 training step(s), loss on training batch is 0.0113868.
After 3503 training step(s), loss on training batch is 0.0123059.
After 3504 training step(s), loss on training batch is 0.010224.
After 3505 training step(s), loss on training batch is 0.0114081.
After 3506 training step(s), loss on training batch is 0.0125016.
After 3507 training step(s), loss on training batch is 0.0110446.
After 3508 training step(s), loss on training batch is 0.0108886.
After 3509 training step(s), loss on training batch is 0.0101469.
After 3510 training step(s), loss on training batch is 0.01271.
After 3511 training step(s), loss on training batch is 0.0115309.
After 3512 training step(s), loss on training batch is 0.0106208.
After 3513 training step(s), loss on training batch is 0.0101499.
After 3514 training step(s), loss on training batch is 0.0154107.
After 3515 training step(s), loss on training batch is 0.0121648.
After 3516 training step(s), loss on training batch is 0.0189255.
After 3517 training step(s), loss on training batch is 0.0247195.
After 3518 training step(s), loss on training batch is 0.0130592.
After 3519 training step(s), loss on training batch is 0.0116772.
After 3520 training step(s), loss on training batch is 0.0106452.
After 3521 training step(s), loss on training batch is 0.0123046.
After 3522 training step(s), loss on training batch is 0.0102616.
After 3523 training step(s), loss on training batch is 0.0123869.
After 3524 training step(s), loss on training batch is 0.0112632.
After 3525 training step(s), loss on training batch is 0.0192744.
After 3526 training step(s), loss on training batch is 0.0128537.
After 3527 training step(s), loss on training batch is 0.0153176.
After 3528 training step(s), loss on training batch is 0.00951407.
After 3529 training step(s), loss on training batch is 0.00996761.
After 3530 training step(s), loss on training batch is 0.00959904.
After 3531 training step(s), loss on training batch is 0.01087.
After 3532 training step(s), loss on training batch is 0.0122685.
After 3533 training step(s), loss on training batch is 0.0157962.
After 3534 training step(s), loss on training batch is 0.0146266.
After 3535 training step(s), loss on training batch is 0.01744.
After 3536 training step(s), loss on training batch is 0.01036.
After 3537 training step(s), loss on training batch is 0.0116537.
After 3538 training step(s), loss on training batch is 0.00895293.
After 3539 training step(s), loss on training batch is 0.0092073.
After 3540 training step(s), loss on training batch is 0.0144743.
After 3541 training step(s), loss on training batch is 0.00897571.
After 3542 training step(s), loss on training batch is 0.0102783.
After 3543 training step(s), loss on training batch is 0.0115568.
After 3544 training step(s), loss on training batch is 0.0118801.
After 3545 training step(s), loss on training batch is 0.00929964.
After 3546 training step(s), loss on training batch is 0.0103163.
After 3547 training step(s), loss on training batch is 0.00992855.
After 3548 training step(s), loss on training batch is 0.0100795.
After 3549 training step(s), loss on training batch is 0.0138056.
After 3550 training step(s), loss on training batch is 0.010409.
After 3551 training step(s), loss on training batch is 0.0140192.
After 3552 training step(s), loss on training batch is 0.0138029.
After 3553 training step(s), loss on training batch is 0.0100614.
After 3554 training step(s), loss on training batch is 0.0133612.
After 3555 training step(s), loss on training batch is 0.00947836.
After 3556 training step(s), loss on training batch is 0.00912907.
After 3557 training step(s), loss on training batch is 0.0120184.
After 3558 training step(s), loss on training batch is 0.00882874.
After 3559 training step(s), loss on training batch is 0.0092861.
After 3560 training step(s), loss on training batch is 0.010215.
After 3561 training step(s), loss on training batch is 0.0113578.
After 3562 training step(s), loss on training batch is 0.0115965.
After 3563 training step(s), loss on training batch is 0.0149677.
After 3564 training step(s), loss on training batch is 0.0112236.
After 3565 training step(s), loss on training batch is 0.0283141.
After 3566 training step(s), loss on training batch is 0.0385666.
After 3567 training step(s), loss on training batch is 0.0855918.
After 3568 training step(s), loss on training batch is 0.0128081.
After 3569 training step(s), loss on training batch is 0.0112.
After 3570 training step(s), loss on training batch is 0.0121624.
After 3571 training step(s), loss on training batch is 0.0127372.
After 3572 training step(s), loss on training batch is 0.0145229.
After 3573 training step(s), loss on training batch is 0.0119586.
After 3574 training step(s), loss on training batch is 0.0109591.
After 3575 training step(s), loss on training batch is 0.0165852.
After 3576 training step(s), loss on training batch is 0.00945446.
After 3577 training step(s), loss on training batch is 0.0264928.
After 3578 training step(s), loss on training batch is 0.0167107.
After 3579 training step(s), loss on training batch is 0.00943756.
After 3580 training step(s), loss on training batch is 0.0120714.
After 3581 training step(s), loss on training batch is 0.0104531.
After 3582 training step(s), loss on training batch is 0.0113112.
After 3583 training step(s), loss on training batch is 0.0091622.
After 3584 training step(s), loss on training batch is 0.00940031.
After 3585 training step(s), loss on training batch is 0.00931553.
After 3586 training step(s), loss on training batch is 0.0104448.
After 3587 training step(s), loss on training batch is 0.00924243.
After 3588 training step(s), loss on training batch is 0.00969935.
After 3589 training step(s), loss on training batch is 0.0329758.
After 3590 training step(s), loss on training batch is 0.0352837.
After 3591 training step(s), loss on training batch is 0.0328315.
After 3592 training step(s), loss on training batch is 0.0158315.
After 3593 training step(s), loss on training batch is 0.0121015.
After 3594 training step(s), loss on training batch is 0.0138441.
After 3595 training step(s), loss on training batch is 0.00956305.
After 3596 training step(s), loss on training batch is 0.010878.
After 3597 training step(s), loss on training batch is 0.0118163.
After 3598 training step(s), loss on training batch is 0.00884289.
After 3599 training step(s), loss on training batch is 0.00934178.
After 3600 training step(s), loss on training batch is 0.00895239.
After 3601 training step(s), loss on training batch is 0.0101136.
After 3602 training step(s), loss on training batch is 0.0128071.
After 3603 training step(s), loss on training batch is 0.00910523.
After 3604 training step(s), loss on training batch is 0.0114215.
After 3605 training step(s), loss on training batch is 0.0148436.
After 3606 training step(s), loss on training batch is 0.0131306.
After 3607 training step(s), loss on training batch is 0.0118234.
After 3608 training step(s), loss on training batch is 0.0105184.
After 3609 training step(s), loss on training batch is 0.0101305.
After 3610 training step(s), loss on training batch is 0.0133705.
After 3611 training step(s), loss on training batch is 0.00922854.
After 3612 training step(s), loss on training batch is 0.025643.
After 3613 training step(s), loss on training batch is 0.0222584.
After 3614 training step(s), loss on training batch is 0.0101644.
After 3615 training step(s), loss on training batch is 0.0123762.
After 3616 training step(s), loss on training batch is 0.00971529.
After 3617 training step(s), loss on training batch is 0.00997537.
After 3618 training step(s), loss on training batch is 0.00963603.
After 3619 training step(s), loss on training batch is 0.00953864.
After 3620 training step(s), loss on training batch is 0.0114371.
After 3621 training step(s), loss on training batch is 0.0109096.
After 3622 training step(s), loss on training batch is 0.015244.
After 3623 training step(s), loss on training batch is 0.0122569.
After 3624 training step(s), loss on training batch is 0.0104298.
After 3625 training step(s), loss on training batch is 0.0107664.
After 3626 training step(s), loss on training batch is 0.010856.
After 3627 training step(s), loss on training batch is 0.00893479.
After 3628 training step(s), loss on training batch is 0.0101746.
After 3629 training step(s), loss on training batch is 0.0102876.
After 3630 training step(s), loss on training batch is 0.0100146.
After 3631 training step(s), loss on training batch is 0.00918704.
After 3632 training step(s), loss on training batch is 0.00929903.
After 3633 training step(s), loss on training batch is 0.00884116.
After 3634 training step(s), loss on training batch is 0.00951249.
After 3635 training step(s), loss on training batch is 0.00881589.
After 3636 training step(s), loss on training batch is 0.00988018.
After 3637 training step(s), loss on training batch is 0.00972283.
After 3638 training step(s), loss on training batch is 0.0096021.
After 3639 training step(s), loss on training batch is 0.0092497.
After 3640 training step(s), loss on training batch is 0.00933678.
After 3641 training step(s), loss on training batch is 0.0119705.
After 3642 training step(s), loss on training batch is 0.008863.
After 3643 training step(s), loss on training batch is 0.0174648.
After 3644 training step(s), loss on training batch is 0.0217271.
After 3645 training step(s), loss on training batch is 0.0114257.
After 3646 training step(s), loss on training batch is 0.0113787.
After 3647 training step(s), loss on training batch is 0.0224151.
After 3648 training step(s), loss on training batch is 0.0122419.
After 3649 training step(s), loss on training batch is 0.0107657.
After 3650 training step(s), loss on training batch is 0.0125019.
After 3651 training step(s), loss on training batch is 0.00909456.
After 3652 training step(s), loss on training batch is 0.0109519.
After 3653 training step(s), loss on training batch is 0.0171232.
After 3654 training step(s), loss on training batch is 0.0194227.
After 3655 training step(s), loss on training batch is 0.0115326.
After 3656 training step(s), loss on training batch is 0.0101149.
After 3657 training step(s), loss on training batch is 0.0169243.
After 3658 training step(s), loss on training batch is 0.0276431.
After 3659 training step(s), loss on training batch is 0.0103668.
After 3660 training step(s), loss on training batch is 0.0103512.
After 3661 training step(s), loss on training batch is 0.00993492.
After 3662 training step(s), loss on training batch is 0.00954594.
After 3663 training step(s), loss on training batch is 0.00912743.
After 3664 training step(s), loss on training batch is 0.0132948.
After 3665 training step(s), loss on training batch is 0.00951054.
After 3666 training step(s), loss on training batch is 0.0220291.
After 3667 training step(s), loss on training batch is 0.0251979.
After 3668 training step(s), loss on training batch is 0.0129302.
After 3669 training step(s), loss on training batch is 0.0108653.
After 3670 training step(s), loss on training batch is 0.0236565.
After 3671 training step(s), loss on training batch is 0.0100365.
After 3672 training step(s), loss on training batch is 0.0128961.
After 3673 training step(s), loss on training batch is 0.0128737.
After 3674 training step(s), loss on training batch is 0.00962224.
After 3675 training step(s), loss on training batch is 0.00909745.
After 3676 training step(s), loss on training batch is 0.0122954.
After 3677 training step(s), loss on training batch is 0.00914921.
After 3678 training step(s), loss on training batch is 0.0138665.
After 3679 training step(s), loss on training batch is 0.00955049.
After 3680 training step(s), loss on training batch is 0.00908338.
After 3681 training step(s), loss on training batch is 0.0239119.
After 3682 training step(s), loss on training batch is 0.0136206.
After 3683 training step(s), loss on training batch is 0.0138488.
After 3684 training step(s), loss on training batch is 0.00941173.
After 3685 training step(s), loss on training batch is 0.01311.
After 3686 training step(s), loss on training batch is 0.00984552.
After 3687 training step(s), loss on training batch is 0.00871447.
After 3688 training step(s), loss on training batch is 0.0110557.
After 3689 training step(s), loss on training batch is 0.00894142.
After 3690 training step(s), loss on training batch is 0.0138092.
After 3691 training step(s), loss on training batch is 0.00984427.
After 3692 training step(s), loss on training batch is 0.00893486.
After 3693 training step(s), loss on training batch is 0.0178745.
After 3694 training step(s), loss on training batch is 0.00978687.
After 3695 training step(s), loss on training batch is 0.0110743.
After 3696 training step(s), loss on training batch is 0.0100858.
After 3697 training step(s), loss on training batch is 0.0137482.
After 3698 training step(s), loss on training batch is 0.00973492.
After 3699 training step(s), loss on training batch is 0.0197135.
After 3700 training step(s), loss on training batch is 0.0476215.
After 3701 training step(s), loss on training batch is 0.00972578.
After 3702 training step(s), loss on training batch is 0.0156438.
After 3703 training step(s), loss on training batch is 0.0156253.
After 3704 training step(s), loss on training batch is 0.0132597.
After 3705 training step(s), loss on training batch is 0.015736.
After 3706 training step(s), loss on training batch is 0.0109421.
After 3707 training step(s), loss on training batch is 0.0135808.
After 3708 training step(s), loss on training batch is 0.00867653.
After 3709 training step(s), loss on training batch is 0.0090273.
After 3710 training step(s), loss on training batch is 0.00946932.
After 3711 training step(s), loss on training batch is 0.0148589.
After 3712 training step(s), loss on training batch is 0.013164.
After 3713 training step(s), loss on training batch is 0.00871121.
After 3714 training step(s), loss on training batch is 0.00964189.
After 3715 training step(s), loss on training batch is 0.00980032.
After 3716 training step(s), loss on training batch is 0.0115525.
After 3717 training step(s), loss on training batch is 0.0127803.
After 3718 training step(s), loss on training batch is 0.0122304.
After 3719 training step(s), loss on training batch is 0.00965779.
After 3720 training step(s), loss on training batch is 0.0110057.
After 3721 training step(s), loss on training batch is 0.00927221.
After 3722 training step(s), loss on training batch is 0.00942532.
After 3723 training step(s), loss on training batch is 0.0102766.
After 3724 training step(s), loss on training batch is 0.0115455.
After 3725 training step(s), loss on training batch is 0.0119161.
After 3726 training step(s), loss on training batch is 0.0107978.
After 3727 training step(s), loss on training batch is 0.0086834.
After 3728 training step(s), loss on training batch is 0.0203197.
After 3729 training step(s), loss on training batch is 0.0123932.
After 3730 training step(s), loss on training batch is 0.012446.
After 3731 training step(s), loss on training batch is 0.00983167.
After 3732 training step(s), loss on training batch is 0.00861571.
After 3733 training step(s), loss on training batch is 0.00907886.
After 3734 training step(s), loss on training batch is 0.00901829.
After 3735 training step(s), loss on training batch is 0.0198889.
After 3736 training step(s), loss on training batch is 0.00937274.
After 3737 training step(s), loss on training batch is 0.00898848.
After 3738 training step(s), loss on training batch is 0.0349373.
After 3739 training step(s), loss on training batch is 0.017662.
After 3740 training step(s), loss on training batch is 0.0168429.
After 3741 training step(s), loss on training batch is 0.0102602.
After 3742 training step(s), loss on training batch is 0.0118803.
After 3743 training step(s), loss on training batch is 0.00911806.
After 3744 training step(s), loss on training batch is 0.00906479.
After 3745 training step(s), loss on training batch is 0.00928646.
After 3746 training step(s), loss on training batch is 0.00878701.
After 3747 training step(s), loss on training batch is 0.0100692.
After 3748 training step(s), loss on training batch is 0.00920497.
After 3749 training step(s), loss on training batch is 0.00992551.
After 3750 training step(s), loss on training batch is 0.0133147.
After 3751 training step(s), loss on training batch is 0.00994521.
After 3752 training step(s), loss on training batch is 0.00902805.
After 3753 training step(s), loss on training batch is 0.0137388.
After 3754 training step(s), loss on training batch is 0.014.
After 3755 training step(s), loss on training batch is 0.013229.
After 3756 training step(s), loss on training batch is 0.0110867.
After 3757 training step(s), loss on training batch is 0.0108001.
After 3758 training step(s), loss on training batch is 0.0128637.
After 3759 training step(s), loss on training batch is 0.0112223.
After 3760 training step(s), loss on training batch is 0.00883578.
After 3761 training step(s), loss on training batch is 0.0105898.
After 3762 training step(s), loss on training batch is 0.0264457.
After 3763 training step(s), loss on training batch is 0.016184.
After 3764 training step(s), loss on training batch is 0.0149329.
After 3765 training step(s), loss on training batch is 0.0132556.
After 3766 training step(s), loss on training batch is 0.00906196.
After 3767 training step(s), loss on training batch is 0.0102986.
After 3768 training step(s), loss on training batch is 0.0117666.
After 3769 training step(s), loss on training batch is 0.0100668.
After 3770 training step(s), loss on training batch is 0.0109944.
After 3771 training step(s), loss on training batch is 0.00910139.
After 3772 training step(s), loss on training batch is 0.0155588.
After 3773 training step(s), loss on training batch is 0.00944446.
After 3774 training step(s), loss on training batch is 0.00886805.
After 3775 training step(s), loss on training batch is 0.0122736.
After 3776 training step(s), loss on training batch is 0.00928892.
After 3777 training step(s), loss on training batch is 0.010327.
After 3778 training step(s), loss on training batch is 0.0109666.
After 3779 training step(s), loss on training batch is 0.00955449.
After 3780 training step(s), loss on training batch is 0.0112062.
After 3781 training step(s), loss on training batch is 0.0125073.
After 3782 training step(s), loss on training batch is 0.00862171.
After 3783 training step(s), loss on training batch is 0.0127049.
After 3784 training step(s), loss on training batch is 0.00977852.
After 3785 training step(s), loss on training batch is 0.00989842.
After 3786 training step(s), loss on training batch is 0.0110781.
After 3787 training step(s), loss on training batch is 0.00925282.
After 3788 training step(s), loss on training batch is 0.011433.
After 3789 training step(s), loss on training batch is 0.0197854.
After 3790 training step(s), loss on training batch is 0.0114273.
After 3791 training step(s), loss on training batch is 0.00964791.
After 3792 training step(s), loss on training batch is 0.00857197.
After 3793 training step(s), loss on training batch is 0.00986492.
After 3794 training step(s), loss on training batch is 0.0135796.
After 3795 training step(s), loss on training batch is 0.00921313.
After 3796 training step(s), loss on training batch is 0.0104275.
After 3797 training step(s), loss on training batch is 0.00957061.
After 3798 training step(s), loss on training batch is 0.0103738.
After 3799 training step(s), loss on training batch is 0.0174265.
After 3800 training step(s), loss on training batch is 0.011487.
After 3801 training step(s), loss on training batch is 0.0110033.
After 3802 training step(s), loss on training batch is 0.0114728.
After 3803 training step(s), loss on training batch is 0.0103672.
After 3804 training step(s), loss on training batch is 0.0251673.
After 3805 training step(s), loss on training batch is 0.0165643.
After 3806 training step(s), loss on training batch is 0.0101078.
After 3807 training step(s), loss on training batch is 0.0107011.
After 3808 training step(s), loss on training batch is 0.0098601.
After 3809 training step(s), loss on training batch is 0.00972138.
After 3810 training step(s), loss on training batch is 0.0095846.
After 3811 training step(s), loss on training batch is 0.00924768.
After 3812 training step(s), loss on training batch is 0.0101314.
After 3813 training step(s), loss on training batch is 0.0129517.
After 3814 training step(s), loss on training batch is 0.0118379.
After 3815 training step(s), loss on training batch is 0.0117373.
After 3816 training step(s), loss on training batch is 0.0099194.
After 3817 training step(s), loss on training batch is 0.0128642.
After 3818 training step(s), loss on training batch is 0.00913871.
After 3819 training step(s), loss on training batch is 0.00959852.
After 3820 training step(s), loss on training batch is 0.0117225.
After 3821 training step(s), loss on training batch is 0.0125732.
After 3822 training step(s), loss on training batch is 0.0110582.
After 3823 training step(s), loss on training batch is 0.012024.
After 3824 training step(s), loss on training batch is 0.0106257.
After 3825 training step(s), loss on training batch is 0.0137876.
After 3826 training step(s), loss on training batch is 0.00955091.
After 3827 training step(s), loss on training batch is 0.01128.
After 3828 training step(s), loss on training batch is 0.0105017.
After 3829 training step(s), loss on training batch is 0.00880116.
After 3830 training step(s), loss on training batch is 0.00866922.
After 3831 training step(s), loss on training batch is 0.0149348.
After 3832 training step(s), loss on training batch is 0.0162245.
After 3833 training step(s), loss on training batch is 0.0105724.
After 3834 training step(s), loss on training batch is 0.0116115.
After 3835 training step(s), loss on training batch is 0.0101239.
After 3836 training step(s), loss on training batch is 0.0086988.
After 3837 training step(s), loss on training batch is 0.0189637.
After 3838 training step(s), loss on training batch is 0.0100227.
After 3839 training step(s), loss on training batch is 0.0130987.
After 3840 training step(s), loss on training batch is 0.0105194.
After 3841 training step(s), loss on training batch is 0.0232815.
After 3842 training step(s), loss on training batch is 0.00852454.
After 3843 training step(s), loss on training batch is 0.00908143.
After 3844 training step(s), loss on training batch is 0.013042.
After 3845 training step(s), loss on training batch is 0.0109943.
After 3846 training step(s), loss on training batch is 0.00904786.
After 3847 training step(s), loss on training batch is 0.00988781.
After 3848 training step(s), loss on training batch is 0.0108911.
After 3849 training step(s), loss on training batch is 0.0102736.
After 3850 training step(s), loss on training batch is 0.0173976.
After 3851 training step(s), loss on training batch is 0.0119597.
After 3852 training step(s), loss on training batch is 0.0101419.
After 3853 training step(s), loss on training batch is 0.00890669.
After 3854 training step(s), loss on training batch is 0.00958566.
After 3855 training step(s), loss on training batch is 0.0100769.
After 3856 training step(s), loss on training batch is 0.00930336.
After 3857 training step(s), loss on training batch is 0.00922294.
After 3858 training step(s), loss on training batch is 0.00877457.
After 3859 training step(s), loss on training batch is 0.0095752.
After 3860 training step(s), loss on training batch is 0.0140005.
After 3861 training step(s), loss on training batch is 0.0122293.
After 3862 training step(s), loss on training batch is 0.00905703.
After 3863 training step(s), loss on training batch is 0.00967151.
After 3864 training step(s), loss on training batch is 0.0100845.
After 3865 training step(s), loss on training batch is 0.0101131.
After 3866 training step(s), loss on training batch is 0.0101693.
After 3867 training step(s), loss on training batch is 0.00989162.
After 3868 training step(s), loss on training batch is 0.0119309.
After 3869 training step(s), loss on training batch is 0.00867396.
After 3870 training step(s), loss on training batch is 0.00958443.
After 3871 training step(s), loss on training batch is 0.0102829.
After 3872 training step(s), loss on training batch is 0.0134983.
After 3873 training step(s), loss on training batch is 0.00869241.
After 3874 training step(s), loss on training batch is 0.00953961.
After 3875 training step(s), loss on training batch is 0.00873135.
After 3876 training step(s), loss on training batch is 0.00879501.
After 3877 training step(s), loss on training batch is 0.00989273.
After 3878 training step(s), loss on training batch is 0.0104002.
After 3879 training step(s), loss on training batch is 0.010084.
After 3880 training step(s), loss on training batch is 0.00864652.
After 3881 training step(s), loss on training batch is 0.0103606.
After 3882 training step(s), loss on training batch is 0.00820805.
After 3883 training step(s), loss on training batch is 0.0102554.
After 3884 training step(s), loss on training batch is 0.00876004.
After 3885 training step(s), loss on training batch is 0.0101164.
After 3886 training step(s), loss on training batch is 0.00847052.
After 3887 training step(s), loss on training batch is 0.00840851.
After 3888 training step(s), loss on training batch is 0.00990321.
After 3889 training step(s), loss on training batch is 0.00871538.
After 3890 training step(s), loss on training batch is 0.00890931.
After 3891 training step(s), loss on training batch is 0.00927436.
After 3892 training step(s), loss on training batch is 0.00834899.
After 3893 training step(s), loss on training batch is 0.00934506.
After 3894 training step(s), loss on training batch is 0.00956484.
After 3895 training step(s), loss on training batch is 0.00953692.
After 3896 training step(s), loss on training batch is 0.0113102.
After 3897 training step(s), loss on training batch is 0.0134066.
After 3898 training step(s), loss on training batch is 0.00941367.
After 3899 training step(s), loss on training batch is 0.00949007.
After 3900 training step(s), loss on training batch is 0.00844465.
After 3901 training step(s), loss on training batch is 0.00875077.
After 3902 training step(s), loss on training batch is 0.00975927.
After 3903 training step(s), loss on training batch is 0.00906004.
After 3904 training step(s), loss on training batch is 0.00850748.
After 3905 training step(s), loss on training batch is 0.00815267.
After 3906 training step(s), loss on training batch is 0.0106285.
After 3907 training step(s), loss on training batch is 0.00930003.
After 3908 training step(s), loss on training batch is 0.00896527.
After 3909 training step(s), loss on training batch is 0.00891384.
After 3910 training step(s), loss on training batch is 0.00899261.
After 3911 training step(s), loss on training batch is 0.0165891.
After 3912 training step(s), loss on training batch is 0.027789.
After 3913 training step(s), loss on training batch is 0.0175813.
After 3914 training step(s), loss on training batch is 0.0103524.
After 3915 training step(s), loss on training batch is 0.0091658.
After 3916 training step(s), loss on training batch is 0.0103664.
After 3917 training step(s), loss on training batch is 0.00852655.
After 3918 training step(s), loss on training batch is 0.00835319.
After 3919 training step(s), loss on training batch is 0.00918786.
After 3920 training step(s), loss on training batch is 0.00905364.
After 3921 training step(s), loss on training batch is 0.0127699.
After 3922 training step(s), loss on training batch is 0.0125748.
After 3923 training step(s), loss on training batch is 0.0116956.
After 3924 training step(s), loss on training batch is 0.00885933.
After 3925 training step(s), loss on training batch is 0.010287.
After 3926 training step(s), loss on training batch is 0.0107744.
After 3927 training step(s), loss on training batch is 0.00880075.
After 3928 training step(s), loss on training batch is 0.0124087.
After 3929 training step(s), loss on training batch is 0.0102542.
After 3930 training step(s), loss on training batch is 0.0114124.
After 3931 training step(s), loss on training batch is 0.0089901.
After 3932 training step(s), loss on training batch is 0.0093194.
After 3933 training step(s), loss on training batch is 0.010029.
After 3934 training step(s), loss on training batch is 0.00838453.
After 3935 training step(s), loss on training batch is 0.00961709.
After 3936 training step(s), loss on training batch is 0.00941918.
After 3937 training step(s), loss on training batch is 0.0099502.
After 3938 training step(s), loss on training batch is 0.00928062.
After 3939 training step(s), loss on training batch is 0.00987529.
After 3940 training step(s), loss on training batch is 0.00907978.
After 3941 training step(s), loss on training batch is 0.00891007.
After 3942 training step(s), loss on training batch is 0.0124285.
After 3943 training step(s), loss on training batch is 0.00953746.
After 3944 training step(s), loss on training batch is 0.0270792.
After 3945 training step(s), loss on training batch is 0.00881069.
After 3946 training step(s), loss on training batch is 0.0162834.
After 3947 training step(s), loss on training batch is 0.00878052.
After 3948 training step(s), loss on training batch is 0.0138003.
After 3949 training step(s), loss on training batch is 0.00955938.
After 3950 training step(s), loss on training batch is 0.00883969.
After 3951 training step(s), loss on training batch is 0.01065.
After 3952 training step(s), loss on training batch is 0.0109429.
After 3953 training step(s), loss on training batch is 0.00851667.
After 3954 training step(s), loss on training batch is 0.00905379.
After 3955 training step(s), loss on training batch is 0.00929079.
After 3956 training step(s), loss on training batch is 0.00832234.
After 3957 training step(s), loss on training batch is 0.00856504.
After 3958 training step(s), loss on training batch is 0.00929461.
After 3959 training step(s), loss on training batch is 0.011092.
After 3960 training step(s), loss on training batch is 0.0104558.
After 3961 training step(s), loss on training batch is 0.0107682.
After 3962 training step(s), loss on training batch is 0.0104744.
After 3963 training step(s), loss on training batch is 0.00937962.
After 3964 training step(s), loss on training batch is 0.00892854.
After 3965 training step(s), loss on training batch is 0.00891034.
After 3966 training step(s), loss on training batch is 0.00841296.
After 3967 training step(s), loss on training batch is 0.0120196.
After 3968 training step(s), loss on training batch is 0.0093076.
After 3969 training step(s), loss on training batch is 0.00885388.
After 3970 training step(s), loss on training batch is 0.00906045.
After 3971 training step(s), loss on training batch is 0.00981973.
After 3972 training step(s), loss on training batch is 0.0144733.
After 3973 training step(s), loss on training batch is 0.010423.
After 3974 training step(s), loss on training batch is 0.0102705.
After 3975 training step(s), loss on training batch is 0.00857602.
After 3976 training step(s), loss on training batch is 0.00933472.
After 3977 training step(s), loss on training batch is 0.00868867.
After 3978 training step(s), loss on training batch is 0.00907947.
After 3979 training step(s), loss on training batch is 0.010451.
After 3980 training step(s), loss on training batch is 0.00883389.
After 3981 training step(s), loss on training batch is 0.00939356.
After 3982 training step(s), loss on training batch is 0.00965219.
After 3983 training step(s), loss on training batch is 0.00833602.
After 3984 training step(s), loss on training batch is 0.010399.
After 3985 training step(s), loss on training batch is 0.00888295.
After 3986 training step(s), loss on training batch is 0.00832803.
After 3987 training step(s), loss on training batch is 0.00979815.
After 3988 training step(s), loss on training batch is 0.00867988.
After 3989 training step(s), loss on training batch is 0.0344997.
After 3990 training step(s), loss on training batch is 0.00842943.
After 3991 training step(s), loss on training batch is 0.0101271.
After 3992 training step(s), loss on training batch is 0.0123695.
After 3993 training step(s), loss on training batch is 0.00944141.
After 3994 training step(s), loss on training batch is 0.00815454.
After 3995 training step(s), loss on training batch is 0.00828347.
After 3996 training step(s), loss on training batch is 0.00967837.
After 3997 training step(s), loss on training batch is 0.00853732.
After 3998 training step(s), loss on training batch is 0.0103978.
After 3999 training step(s), loss on training batch is 0.00904295.
After 4000 training step(s), loss on training batch is 0.00919692.
After 4001 training step(s), loss on training batch is 0.0126399.
After 4002 training step(s), loss on training batch is 0.00880682.
After 4003 training step(s), loss on training batch is 0.00906031.
After 4004 training step(s), loss on training batch is 0.00862104.
After 4005 training step(s), loss on training batch is 0.00918969.
After 4006 training step(s), loss on training batch is 0.00909343.
After 4007 training step(s), loss on training batch is 0.00931906.
After 4008 training step(s), loss on training batch is 0.00831019.
After 4009 training step(s), loss on training batch is 0.00985385.
After 4010 training step(s), loss on training batch is 0.00922136.
After 4011 training step(s), loss on training batch is 0.00842242.
After 4012 training step(s), loss on training batch is 0.0152003.
After 4013 training step(s), loss on training batch is 0.00913904.
After 4014 training step(s), loss on training batch is 0.00852894.
After 4015 training step(s), loss on training batch is 0.00922168.
After 4016 training step(s), loss on training batch is 0.00868824.
After 4017 training step(s), loss on training batch is 0.00849805.
After 4018 training step(s), loss on training batch is 0.0103889.
After 4019 training step(s), loss on training batch is 0.00870391.
After 4020 training step(s), loss on training batch is 0.00956212.
After 4021 training step(s), loss on training batch is 0.00912393.
After 4022 training step(s), loss on training batch is 0.00990752.
After 4023 training step(s), loss on training batch is 0.00845579.
After 4024 training step(s), loss on training batch is 0.00856252.
After 4025 training step(s), loss on training batch is 0.00950295.
After 4026 training step(s), loss on training batch is 0.0153344.
After 4027 training step(s), loss on training batch is 0.00944654.
After 4028 training step(s), loss on training batch is 0.00994611.
After 4029 training step(s), loss on training batch is 0.0165418.
After 4030 training step(s), loss on training batch is 0.00957264.
After 4031 training step(s), loss on training batch is 0.0103959.
After 4032 training step(s), loss on training batch is 0.00888731.
After 4033 training step(s), loss on training batch is 0.00913494.
After 4034 training step(s), loss on training batch is 0.00851525.
After 4035 training step(s), loss on training batch is 0.00835416.
After 4036 training step(s), loss on training batch is 0.00924353.
After 4037 training step(s), loss on training batch is 0.00958773.
After 4038 training step(s), loss on training batch is 0.00819954.
After 4039 training step(s), loss on training batch is 0.0141505.
After 4040 training step(s), loss on training batch is 0.0117368.
After 4041 training step(s), loss on training batch is 0.00899057.
After 4042 training step(s), loss on training batch is 0.00864911.
After 4043 training step(s), loss on training batch is 0.0176321.
After 4044 training step(s), loss on training batch is 0.0439817.
After 4045 training step(s), loss on training batch is 0.0091716.
After 4046 training step(s), loss on training batch is 0.0105203.
After 4047 training step(s), loss on training batch is 0.00828468.
After 4048 training step(s), loss on training batch is 0.0136568.
After 4049 training step(s), loss on training batch is 0.0106313.
After 4050 training step(s), loss on training batch is 0.00923442.
After 4051 training step(s), loss on training batch is 0.00959409.
After 4052 training step(s), loss on training batch is 0.0100057.
After 4053 training step(s), loss on training batch is 0.00870638.
After 4054 training step(s), loss on training batch is 0.0109478.
After 4055 training step(s), loss on training batch is 0.00835028.
After 4056 training step(s), loss on training batch is 0.0108936.
After 4057 training step(s), loss on training batch is 0.0118108.
After 4058 training step(s), loss on training batch is 0.0104319.
After 4059 training step(s), loss on training batch is 0.0102585.
After 4060 training step(s), loss on training batch is 0.00919976.
After 4061 training step(s), loss on training batch is 0.00888919.
After 4062 training step(s), loss on training batch is 0.0100573.
After 4063 training step(s), loss on training batch is 0.00824698.
After 4064 training step(s), loss on training batch is 0.0132929.
After 4065 training step(s), loss on training batch is 0.00880075.
After 4066 training step(s), loss on training batch is 0.0095969.
After 4067 training step(s), loss on training batch is 0.00880527.
After 4068 training step(s), loss on training batch is 0.00943798.
After 4069 training step(s), loss on training batch is 0.00909222.
After 4070 training step(s), loss on training batch is 0.00805923.
After 4071 training step(s), loss on training batch is 0.00942909.
After 4072 training step(s), loss on training batch is 0.00799657.
After 4073 training step(s), loss on training batch is 0.00829453.
After 4074 training step(s), loss on training batch is 0.00906413.
After 4075 training step(s), loss on training batch is 0.00814925.
After 4076 training step(s), loss on training batch is 0.0136583.
After 4077 training step(s), loss on training batch is 0.0193846.
After 4078 training step(s), loss on training batch is 0.00838937.
After 4079 training step(s), loss on training batch is 0.00834277.
After 4080 training step(s), loss on training batch is 0.0106641.
After 4081 training step(s), loss on training batch is 0.00976352.
After 4082 training step(s), loss on training batch is 0.0119557.
After 4083 training step(s), loss on training batch is 0.0104925.
After 4084 training step(s), loss on training batch is 0.010376.
After 4085 training step(s), loss on training batch is 0.00833768.
After 4086 training step(s), loss on training batch is 0.0110954.
After 4087 training step(s), loss on training batch is 0.0110914.
After 4088 training step(s), loss on training batch is 0.00839165.
After 4089 training step(s), loss on training batch is 0.00803895.
After 4090 training step(s), loss on training batch is 0.010363.
After 4091 training step(s), loss on training batch is 0.00894077.
After 4092 training step(s), loss on training batch is 0.00804264.
After 4093 training step(s), loss on training batch is 0.00937343.
After 4094 training step(s), loss on training batch is 0.00883176.
After 4095 training step(s), loss on training batch is 0.0111874.
After 4096 training step(s), loss on training batch is 0.00827732.
After 4097 training step(s), loss on training batch is 0.0087858.
After 4098 training step(s), loss on training batch is 0.00845748.
After 4099 training step(s), loss on training batch is 0.0107679.
After 4100 training step(s), loss on training batch is 0.00890296.
After 4101 training step(s), loss on training batch is 0.00961486.
After 4102 training step(s), loss on training batch is 0.0108561.
After 4103 training step(s), loss on training batch is 0.00890567.
After 4104 training step(s), loss on training batch is 0.00967434.
After 4105 training step(s), loss on training batch is 0.00853297.
After 4106 training step(s), loss on training batch is 0.0101719.
After 4107 training step(s), loss on training batch is 0.00819264.
After 4108 training step(s), loss on training batch is 0.0129669.
After 4109 training step(s), loss on training batch is 0.00981705.
After 4110 training step(s), loss on training batch is 0.00928133.
After 4111 training step(s), loss on training batch is 0.00890555.
After 4112 training step(s), loss on training batch is 0.0161943.
After 4113 training step(s), loss on training batch is 0.00925141.
After 4114 training step(s), loss on training batch is 0.00900549.
After 4115 training step(s), loss on training batch is 0.00810495.
After 4116 training step(s), loss on training batch is 0.0087803.
After 4117 training step(s), loss on training batch is 0.00917285.
After 4118 training step(s), loss on training batch is 0.017293.
After 4119 training step(s), loss on training batch is 0.00853943.
After 4120 training step(s), loss on training batch is 0.00838954.
After 4121 training step(s), loss on training batch is 0.0121457.
After 4122 training step(s), loss on training batch is 0.0107129.
After 4123 training step(s), loss on training batch is 0.0123985.
After 4124 training step(s), loss on training batch is 0.00909974.
After 4125 training step(s), loss on training batch is 0.0204861.
After 4126 training step(s), loss on training batch is 0.00977653.
After 4127 training step(s), loss on training batch is 0.00978409.
After 4128 training step(s), loss on training batch is 0.00818059.
After 4129 training step(s), loss on training batch is 0.0080402.
After 4130 training step(s), loss on training batch is 0.00837061.
After 4131 training step(s), loss on training batch is 0.0079791.
After 4132 training step(s), loss on training batch is 0.00880921.
After 4133 training step(s), loss on training batch is 0.0101744.
After 4134 training step(s), loss on training batch is 0.00876923.
After 4135 training step(s), loss on training batch is 0.00930635.
After 4136 training step(s), loss on training batch is 0.0097316.
After 4137 training step(s), loss on training batch is 0.00921874.
After 4138 training step(s), loss on training batch is 0.00879808.
After 4139 training step(s), loss on training batch is 0.0081526.
After 4140 training step(s), loss on training batch is 0.00868311.
After 4141 training step(s), loss on training batch is 0.00799.
After 4142 training step(s), loss on training batch is 0.00913012.
After 4143 training step(s), loss on training batch is 0.00915402.
After 4144 training step(s), loss on training batch is 0.0100161.
After 4145 training step(s), loss on training batch is 0.0106608.
After 4146 training step(s), loss on training batch is 0.00857069.
After 4147 training step(s), loss on training batch is 0.00826381.
After 4148 training step(s), loss on training batch is 0.0106519.
After 4149 training step(s), loss on training batch is 0.0113419.
After 4150 training step(s), loss on training batch is 0.00859707.
After 4151 training step(s), loss on training batch is 0.00950609.
After 4152 training step(s), loss on training batch is 0.0121039.
After 4153 training step(s), loss on training batch is 0.00812676.
After 4154 training step(s), loss on training batch is 0.00960347.
After 4155 training step(s), loss on training batch is 0.00930616.
After 4156 training step(s), loss on training batch is 0.0085207.
After 4157 training step(s), loss on training batch is 0.00808277.
After 4158 training step(s), loss on training batch is 0.00990185.
After 4159 training step(s), loss on training batch is 0.00804541.
After 4160 training step(s), loss on training batch is 0.00900337.
After 4161 training step(s), loss on training batch is 0.00800588.
After 4162 training step(s), loss on training batch is 0.00908493.
After 4163 training step(s), loss on training batch is 0.00929459.
After 4164 training step(s), loss on training batch is 0.00830108.
After 4165 training step(s), loss on training batch is 0.00830472.
After 4166 training step(s), loss on training batch is 0.00989088.
After 4167 training step(s), loss on training batch is 0.00822509.
After 4168 training step(s), loss on training batch is 0.00887668.
After 4169 training step(s), loss on training batch is 0.0112395.
After 4170 training step(s), loss on training batch is 0.00850332.
After 4171 training step(s), loss on training batch is 0.0106378.
After 4172 training step(s), loss on training batch is 0.0081701.
After 4173 training step(s), loss on training batch is 0.00811131.
After 4174 training step(s), loss on training batch is 0.00854087.
After 4175 training step(s), loss on training batch is 0.00820669.
After 4176 training step(s), loss on training batch is 0.00825993.
After 4177 training step(s), loss on training batch is 0.0086798.
After 4178 training step(s), loss on training batch is 0.00839736.
After 4179 training step(s), loss on training batch is 0.00832251.
After 4180 training step(s), loss on training batch is 0.00946621.
After 4181 training step(s), loss on training batch is 0.00910666.
After 4182 training step(s), loss on training batch is 0.00898214.
After 4183 training step(s), loss on training batch is 0.00863852.
After 4184 training step(s), loss on training batch is 0.00799533.
After 4185 training step(s), loss on training batch is 0.00834784.
After 4186 training step(s), loss on training batch is 0.00808195.
After 4187 training step(s), loss on training batch is 0.0101714.
After 4188 training step(s), loss on training batch is 0.00816386.
After 4189 training step(s), loss on training batch is 0.00953071.
After 4190 training step(s), loss on training batch is 0.00867628.
After 4191 training step(s), loss on training batch is 0.00839688.
After 4192 training step(s), loss on training batch is 0.00807898.
After 4193 training step(s), loss on training batch is 0.00789165.
After 4194 training step(s), loss on training batch is 0.0100841.
After 4195 training step(s), loss on training batch is 0.011218.
After 4196 training step(s), loss on training batch is 0.00816935.
After 4197 training step(s), loss on training batch is 0.00901175.
After 4198 training step(s), loss on training batch is 0.00803945.
After 4199 training step(s), loss on training batch is 0.0105918.
After 4200 training step(s), loss on training batch is 0.00984753.
After 4201 training step(s), loss on training batch is 0.00880662.
After 4202 training step(s), loss on training batch is 0.00815176.
After 4203 training step(s), loss on training batch is 0.00881836.
After 4204 training step(s), loss on training batch is 0.0104436.
After 4205 training step(s), loss on training batch is 0.0107502.
After 4206 training step(s), loss on training batch is 0.0089319.
After 4207 training step(s), loss on training batch is 0.00927997.
After 4208 training step(s), loss on training batch is 0.00816654.
After 4209 training step(s), loss on training batch is 0.0108034.
After 4210 training step(s), loss on training batch is 0.00869903.
After 4211 training step(s), loss on training batch is 0.00863875.
After 4212 training step(s), loss on training batch is 0.00952572.
After 4213 training step(s), loss on training batch is 0.00792536.
After 4214 training step(s), loss on training batch is 0.0160382.
After 4215 training step(s), loss on training batch is 0.0111961.
After 4216 training step(s), loss on training batch is 0.00984216.
After 4217 training step(s), loss on training batch is 0.00866355.
After 4218 training step(s), loss on training batch is 0.00950137.
After 4219 training step(s), loss on training batch is 0.00768737.
After 4220 training step(s), loss on training batch is 0.00885185.
After 4221 training step(s), loss on training batch is 0.0128869.
After 4222 training step(s), loss on training batch is 0.00853045.
After 4223 training step(s), loss on training batch is 0.00828578.
After 4224 training step(s), loss on training batch is 0.0104222.
After 4225 training step(s), loss on training batch is 0.0106664.
After 4226 training step(s), loss on training batch is 0.00952106.
After 4227 training step(s), loss on training batch is 0.00830183.
After 4228 training step(s), loss on training batch is 0.0105594.
After 4229 training step(s), loss on training batch is 0.00967801.
After 4230 training step(s), loss on training batch is 0.0146283.
After 4231 training step(s), loss on training batch is 0.0185757.
After 4232 training step(s), loss on training batch is 0.00883323.
After 4233 training step(s), loss on training batch is 0.00826922.
After 4234 training step(s), loss on training batch is 0.00826158.
After 4235 training step(s), loss on training batch is 0.0112425.
After 4236 training step(s), loss on training batch is 0.00898072.
After 4237 training step(s), loss on training batch is 0.00944475.
After 4238 training step(s), loss on training batch is 0.00842496.
After 4239 training step(s), loss on training batch is 0.00874411.
After 4240 training step(s), loss on training batch is 0.00928111.
After 4241 training step(s), loss on training batch is 0.00899175.
After 4242 training step(s), loss on training batch is 0.0107.
After 4243 training step(s), loss on training batch is 0.00826584.
After 4244 training step(s), loss on training batch is 0.00854525.
After 4245 training step(s), loss on training batch is 0.0101785.
After 4246 training step(s), loss on training batch is 0.00823383.
After 4247 training step(s), loss on training batch is 0.00875278.
After 4248 training step(s), loss on training batch is 0.00764282.
After 4249 training step(s), loss on training batch is 0.00996284.
After 4250 training step(s), loss on training batch is 0.00849615.
After 4251 training step(s), loss on training batch is 0.00791615.
After 4252 training step(s), loss on training batch is 0.00920529.
After 4253 training step(s), loss on training batch is 0.00851976.
After 4254 training step(s), loss on training batch is 0.00860876.
After 4255 training step(s), loss on training batch is 0.010028.
After 4256 training step(s), loss on training batch is 0.007724.
After 4257 training step(s), loss on training batch is 0.00908322.
After 4258 training step(s), loss on training batch is 0.00810508.
After 4259 training step(s), loss on training batch is 0.0123637.
After 4260 training step(s), loss on training batch is 0.00757186.
After 4261 training step(s), loss on training batch is 0.00787941.
After 4262 training step(s), loss on training batch is 0.00965036.
After 4263 training step(s), loss on training batch is 0.00823272.
After 4264 training step(s), loss on training batch is 0.00972666.
After 4265 training step(s), loss on training batch is 0.00909574.
After 4266 training step(s), loss on training batch is 0.00846001.
After 4267 training step(s), loss on training batch is 0.00808932.
After 4268 training step(s), loss on training batch is 0.00853309.
After 4269 training step(s), loss on training batch is 0.0102681.
After 4270 training step(s), loss on training batch is 0.00821667.
After 4271 training step(s), loss on training batch is 0.00925974.
After 4272 training step(s), loss on training batch is 0.0110264.
After 4273 training step(s), loss on training batch is 0.00989307.
After 4274 training step(s), loss on training batch is 0.00868492.
After 4275 training step(s), loss on training batch is 0.00804854.
After 4276 training step(s), loss on training batch is 0.00831606.
After 4277 training step(s), loss on training batch is 0.00775768.
After 4278 training step(s), loss on training batch is 0.00872171.
After 4279 training step(s), loss on training batch is 0.00800924.
After 4280 training step(s), loss on training batch is 0.00952149.
After 4281 training step(s), loss on training batch is 0.00906801.
After 4282 training step(s), loss on training batch is 0.0084192.
After 4283 training step(s), loss on training batch is 0.00758312.
After 4284 training step(s), loss on training batch is 0.00825244.
After 4285 training step(s), loss on training batch is 0.00795129.
After 4286 training step(s), loss on training batch is 0.00760404.
After 4287 training step(s), loss on training batch is 0.0121362.
After 4288 training step(s), loss on training batch is 0.00888129.
After 4289 training step(s), loss on training batch is 0.00912766.
After 4290 training step(s), loss on training batch is 0.0304116.
After 4291 training step(s), loss on training batch is 0.0204297.
After 4292 training step(s), loss on training batch is 0.0279811.
After 4293 training step(s), loss on training batch is 0.012127.
After 4294 training step(s), loss on training batch is 0.0158859.
After 4295 training step(s), loss on training batch is 0.0115907.
After 4296 training step(s), loss on training batch is 0.0080661.
After 4297 training step(s), loss on training batch is 0.0108194.
After 4298 training step(s), loss on training batch is 0.0102743.
After 4299 training step(s), loss on training batch is 0.0082437.
After 4300 training step(s), loss on training batch is 0.00867232.
After 4301 training step(s), loss on training batch is 0.0100021.
After 4302 training step(s), loss on training batch is 0.00816384.
After 4303 training step(s), loss on training batch is 0.0156085.
After 4304 training step(s), loss on training batch is 0.00775245.
After 4305 training step(s), loss on training batch is 0.00807252.
After 4306 training step(s), loss on training batch is 0.0105453.
After 4307 training step(s), loss on training batch is 0.0086252.
After 4308 training step(s), loss on training batch is 0.00871165.
After 4309 training step(s), loss on training batch is 0.00948351.
After 4310 training step(s), loss on training batch is 0.00904733.
After 4311 training step(s), loss on training batch is 0.007969.
After 4312 training step(s), loss on training batch is 0.00999269.
After 4313 training step(s), loss on training batch is 0.00844852.
After 4314 training step(s), loss on training batch is 0.00839639.
After 4315 training step(s), loss on training batch is 0.00790957.
After 4316 training step(s), loss on training batch is 0.00831327.
After 4317 training step(s), loss on training batch is 0.00854241.
After 4318 training step(s), loss on training batch is 0.00972561.
After 4319 training step(s), loss on training batch is 0.00966633.
After 4320 training step(s), loss on training batch is 0.00806254.
After 4321 training step(s), loss on training batch is 0.00815351.
After 4322 training step(s), loss on training batch is 0.00828476.
After 4323 training step(s), loss on training batch is 0.0090519.
After 4324 training step(s), loss on training batch is 0.00811974.
After 4325 training step(s), loss on training batch is 0.0115705.
After 4326 training step(s), loss on training batch is 0.0078825.
After 4327 training step(s), loss on training batch is 0.0082419.
After 4328 training step(s), loss on training batch is 0.0144813.
After 4329 training step(s), loss on training batch is 0.0087725.
After 4330 training step(s), loss on training batch is 0.0111529.
After 4331 training step(s), loss on training batch is 0.00906348.
After 4332 training step(s), loss on training batch is 0.00891877.
After 4333 training step(s), loss on training batch is 0.00813062.
After 4334 training step(s), loss on training batch is 0.00914426.
After 4335 training step(s), loss on training batch is 0.00799918.
After 4336 training step(s), loss on training batch is 0.00863735.
After 4337 training step(s), loss on training batch is 0.0077842.
After 4338 training step(s), loss on training batch is 0.0131132.
After 4339 training step(s), loss on training batch is 0.0107098.
After 4340 training step(s), loss on training batch is 0.00916944.
After 4341 training step(s), loss on training batch is 0.00777141.
After 4342 training step(s), loss on training batch is 0.00885812.
After 4343 training step(s), loss on training batch is 0.0085212.
After 4344 training step(s), loss on training batch is 0.00763342.
After 4345 training step(s), loss on training batch is 0.00931156.
After 4346 training step(s), loss on training batch is 0.00799008.
After 4347 training step(s), loss on training batch is 0.00819987.
After 4348 training step(s), loss on training batch is 0.0087017.
After 4349 training step(s), loss on training batch is 0.0119061.
After 4350 training step(s), loss on training batch is 0.0128221.
After 4351 training step(s), loss on training batch is 0.00944106.
After 4352 training step(s), loss on training batch is 0.00798467.
After 4353 training step(s), loss on training batch is 0.00811232.
After 4354 training step(s), loss on training batch is 0.00981998.
After 4355 training step(s), loss on training batch is 0.0130747.
After 4356 training step(s), loss on training batch is 0.0103771.
After 4357 training step(s), loss on training batch is 0.00814475.
After 4358 training step(s), loss on training batch is 0.010477.
After 4359 training step(s), loss on training batch is 0.00817119.
After 4360 training step(s), loss on training batch is 0.00999928.
After 4361 training step(s), loss on training batch is 0.0120998.
After 4362 training step(s), loss on training batch is 0.00807965.
After 4363 training step(s), loss on training batch is 0.00751194.
After 4364 training step(s), loss on training batch is 0.00806384.
After 4365 training step(s), loss on training batch is 0.0102223.
After 4366 training step(s), loss on training batch is 0.00739522.
After 4367 training step(s), loss on training batch is 0.0081922.
After 4368 training step(s), loss on training batch is 0.0107734.
After 4369 training step(s), loss on training batch is 0.00827809.
After 4370 training step(s), loss on training batch is 0.00793722.
After 4371 training step(s), loss on training batch is 0.00862895.
After 4372 training step(s), loss on training batch is 0.0076828.
After 4373 training step(s), loss on training batch is 0.00770747.
After 4374 training step(s), loss on training batch is 0.0101599.
After 4375 training step(s), loss on training batch is 0.00800401.
After 4376 training step(s), loss on training batch is 0.00847379.
After 4377 training step(s), loss on training batch is 0.0109237.
After 4378 training step(s), loss on training batch is 0.00814772.
After 4379 training step(s), loss on training batch is 0.00927083.
After 4380 training step(s), loss on training batch is 0.00771167.
After 4381 training step(s), loss on training batch is 0.00826929.
After 4382 training step(s), loss on training batch is 0.00817397.
After 4383 training step(s), loss on training batch is 0.00996056.
After 4384 training step(s), loss on training batch is 0.00816746.
After 4385 training step(s), loss on training batch is 0.00762685.
After 4386 training step(s), loss on training batch is 0.0105074.
After 4387 training step(s), loss on training batch is 0.00868121.
After 4388 training step(s), loss on training batch is 0.00951563.
After 4389 training step(s), loss on training batch is 0.00857414.
After 4390 training step(s), loss on training batch is 0.00855795.
After 4391 training step(s), loss on training batch is 0.0115227.
After 4392 training step(s), loss on training batch is 0.0109386.
After 4393 training step(s), loss on training batch is 0.00772444.
After 4394 training step(s), loss on training batch is 0.00934809.
After 4395 training step(s), loss on training batch is 0.00797836.
After 4396 training step(s), loss on training batch is 0.0080518.
After 4397 training step(s), loss on training batch is 0.0122684.
After 4398 training step(s), loss on training batch is 0.0116913.
After 4399 training step(s), loss on training batch is 0.0101018.
After 4400 training step(s), loss on training batch is 0.0124167.
After 4401 training step(s), loss on training batch is 0.00777356.
After 4402 training step(s), loss on training batch is 0.0103618.
After 4403 training step(s), loss on training batch is 0.00962776.
After 4404 training step(s), loss on training batch is 0.00822403.
After 4405 training step(s), loss on training batch is 0.00913274.
After 4406 training step(s), loss on training batch is 0.00789083.
After 4407 training step(s), loss on training batch is 0.00758036.
After 4408 training step(s), loss on training batch is 0.00762193.
After 4409 training step(s), loss on training batch is 0.00821884.
After 4410 training step(s), loss on training batch is 0.00837497.
After 4411 training step(s), loss on training batch is 0.0100122.
After 4412 training step(s), loss on training batch is 0.00805146.
After 4413 training step(s), loss on training batch is 0.00868835.
After 4414 training step(s), loss on training batch is 0.00877488.
After 4415 training step(s), loss on training batch is 0.00893959.
After 4416 training step(s), loss on training batch is 0.0121302.
After 4417 training step(s), loss on training batch is 0.00861412.
After 4418 training step(s), loss on training batch is 0.0077824.
After 4419 training step(s), loss on training batch is 0.00810953.
After 4420 training step(s), loss on training batch is 0.0113673.
After 4421 training step(s), loss on training batch is 0.00954841.
After 4422 training step(s), loss on training batch is 0.0114256.
After 4423 training step(s), loss on training batch is 0.00867857.
After 4424 training step(s), loss on training batch is 0.0107086.
After 4425 training step(s), loss on training batch is 0.00751749.
After 4426 training step(s), loss on training batch is 0.00814454.
After 4427 training step(s), loss on training batch is 0.00924136.
After 4428 training step(s), loss on training batch is 0.00853569.
After 4429 training step(s), loss on training batch is 0.00886273.
After 4430 training step(s), loss on training batch is 0.012353.
After 4431 training step(s), loss on training batch is 0.00748105.
After 4432 training step(s), loss on training batch is 0.0204422.
After 4433 training step(s), loss on training batch is 0.00866643.
After 4434 training step(s), loss on training batch is 0.0092898.
After 4435 training step(s), loss on training batch is 0.00789877.
After 4436 training step(s), loss on training batch is 0.00778905.
After 4437 training step(s), loss on training batch is 0.00788762.
After 4438 training step(s), loss on training batch is 0.00962675.
After 4439 training step(s), loss on training batch is 0.00770887.
After 4440 training step(s), loss on training batch is 0.0226332.
After 4441 training step(s), loss on training batch is 0.0116448.
After 4442 training step(s), loss on training batch is 0.00961221.
After 4443 training step(s), loss on training batch is 0.00878536.
After 4444 training step(s), loss on training batch is 0.0205529.
After 4445 training step(s), loss on training batch is 0.0140109.
After 4446 training step(s), loss on training batch is 0.0143377.
After 4447 training step(s), loss on training batch is 0.0254468.
After 4448 training step(s), loss on training batch is 0.0126388.
After 4449 training step(s), loss on training batch is 0.00833625.
After 4450 training step(s), loss on training batch is 0.0131699.
After 4451 training step(s), loss on training batch is 0.00775053.
After 4452 training step(s), loss on training batch is 0.0139621.
After 4453 training step(s), loss on training batch is 0.00934419.
After 4454 training step(s), loss on training batch is 0.00736495.
After 4455 training step(s), loss on training batch is 0.0121812.
After 4456 training step(s), loss on training batch is 0.0077176.
After 4457 training step(s), loss on training batch is 0.0101998.
After 4458 training step(s), loss on training batch is 0.00744015.
After 4459 training step(s), loss on training batch is 0.00831683.
After 4460 training step(s), loss on training batch is 0.00872767.
After 4461 training step(s), loss on training batch is 0.00794087.
After 4462 training step(s), loss on training batch is 0.00786336.
After 4463 training step(s), loss on training batch is 0.0101056.
After 4464 training step(s), loss on training batch is 0.00800402.
After 4465 training step(s), loss on training batch is 0.00918553.
After 4466 training step(s), loss on training batch is 0.00770234.
After 4467 training step(s), loss on training batch is 0.00974411.
After 4468 training step(s), loss on training batch is 0.00923466.
After 4469 training step(s), loss on training batch is 0.00928072.
After 4470 training step(s), loss on training batch is 0.011452.
After 4471 training step(s), loss on training batch is 0.00947949.
After 4472 training step(s), loss on training batch is 0.00859737.
After 4473 training step(s), loss on training batch is 0.00782154.
After 4474 training step(s), loss on training batch is 0.00930436.
After 4475 training step(s), loss on training batch is 0.00762194.
After 4476 training step(s), loss on training batch is 0.00861808.
After 4477 training step(s), loss on training batch is 0.00802328.
After 4478 training step(s), loss on training batch is 0.00888902.
After 4479 training step(s), loss on training batch is 0.00758078.
After 4480 training step(s), loss on training batch is 0.00807599.
After 4481 training step(s), loss on training batch is 0.00874483.
After 4482 training step(s), loss on training batch is 0.00963664.
After 4483 training step(s), loss on training batch is 0.0126375.
After 4484 training step(s), loss on training batch is 0.00835865.
After 4485 training step(s), loss on training batch is 0.00738214.
After 4486 training step(s), loss on training batch is 0.0099488.
After 4487 training step(s), loss on training batch is 0.00829941.
After 4488 training step(s), loss on training batch is 0.00849924.
After 4489 training step(s), loss on training batch is 0.0101009.
After 4490 training step(s), loss on training batch is 0.00834237.
After 4491 training step(s), loss on training batch is 0.0083538.
After 4492 training step(s), loss on training batch is 0.00835126.
After 4493 training step(s), loss on training batch is 0.00786075.
After 4494 training step(s), loss on training batch is 0.00808662.
After 4495 training step(s), loss on training batch is 0.00837513.
After 4496 training step(s), loss on training batch is 0.00775652.
After 4497 training step(s), loss on training batch is 0.00775664.
After 4498 training step(s), loss on training batch is 0.00841456.
After 4499 training step(s), loss on training batch is 0.00830435.
After 4500 training step(s), loss on training batch is 0.00748651.
After 4501 training step(s), loss on training batch is 0.0128925.
After 4502 training step(s), loss on training batch is 0.0123055.
After 4503 training step(s), loss on training batch is 0.00876884.
After 4504 training step(s), loss on training batch is 0.00907977.
After 4505 training step(s), loss on training batch is 0.00837523.
After 4506 training step(s), loss on training batch is 0.00796069.
After 4507 training step(s), loss on training batch is 0.00948725.
After 4508 training step(s), loss on training batch is 0.0133029.
After 4509 training step(s), loss on training batch is 0.00922326.
After 4510 training step(s), loss on training batch is 0.00909651.
After 4511 training step(s), loss on training batch is 0.00958646.
After 4512 training step(s), loss on training batch is 0.011162.
After 4513 training step(s), loss on training batch is 0.00971542.
After 4514 training step(s), loss on training batch is 0.00845245.
After 4515 training step(s), loss on training batch is 0.00872724.
After 4516 training step(s), loss on training batch is 0.00794616.
After 4517 training step(s), loss on training batch is 0.00743213.
After 4518 training step(s), loss on training batch is 0.00771739.
After 4519 training step(s), loss on training batch is 0.00769649.
After 4520 training step(s), loss on training batch is 0.00849444.
After 4521 training step(s), loss on training batch is 0.00751486.
After 4522 training step(s), loss on training batch is 0.0099627.
After 4523 training step(s), loss on training batch is 0.00744335.
After 4524 training step(s), loss on training batch is 0.00774267.
After 4525 training step(s), loss on training batch is 0.00771339.
After 4526 training step(s), loss on training batch is 0.00855665.
After 4527 training step(s), loss on training batch is 0.00777142.
After 4528 training step(s), loss on training batch is 0.0084001.
After 4529 training step(s), loss on training batch is 0.00799859.
After 4530 training step(s), loss on training batch is 0.00737638.
After 4531 training step(s), loss on training batch is 0.00776321.
After 4532 training step(s), loss on training batch is 0.00798628.
After 4533 training step(s), loss on training batch is 0.00927371.
After 4534 training step(s), loss on training batch is 0.00934385.
After 4535 training step(s), loss on training batch is 0.00775065.
After 4536 training step(s), loss on training batch is 0.00863907.
After 4537 training step(s), loss on training batch is 0.00884102.
After 4538 training step(s), loss on training batch is 0.00951492.
After 4539 training step(s), loss on training batch is 0.00893836.
After 4540 training step(s), loss on training batch is 0.00794258.
After 4541 training step(s), loss on training batch is 0.00759614.
After 4542 training step(s), loss on training batch is 0.00806114.
After 4543 training step(s), loss on training batch is 0.00736468.
After 4544 training step(s), loss on training batch is 0.00788489.
After 4545 training step(s), loss on training batch is 0.00789602.
After 4546 training step(s), loss on training batch is 0.00742545.
After 4547 training step(s), loss on training batch is 0.00858141.
After 4548 training step(s), loss on training batch is 0.0101375.
After 4549 training step(s), loss on training batch is 0.00848994.
After 4550 training step(s), loss on training batch is 0.00784606.
After 4551 training step(s), loss on training batch is 0.00845981.
After 4552 training step(s), loss on training batch is 0.00797794.
After 4553 training step(s), loss on training batch is 0.00817589.
After 4554 training step(s), loss on training batch is 0.00799575.
After 4555 training step(s), loss on training batch is 0.00765566.
After 4556 training step(s), loss on training batch is 0.00773769.
After 4557 training step(s), loss on training batch is 0.00753692.
After 4558 training step(s), loss on training batch is 0.00754978.
After 4559 training step(s), loss on training batch is 0.00872837.
After 4560 training step(s), loss on training batch is 0.00797134.
After 4561 training step(s), loss on training batch is 0.00918514.
After 4562 training step(s), loss on training batch is 0.00837428.
After 4563 training step(s), loss on training batch is 0.00805436.
After 4564 training step(s), loss on training batch is 0.0122151.
After 4565 training step(s), loss on training batch is 0.00882114.
After 4566 training step(s), loss on training batch is 0.00792089.
After 4567 training step(s), loss on training batch is 0.00786032.
After 4568 training step(s), loss on training batch is 0.00813023.
After 4569 training step(s), loss on training batch is 0.00831283.
After 4570 training step(s), loss on training batch is 0.00784764.
After 4571 training step(s), loss on training batch is 0.00749221.
After 4572 training step(s), loss on training batch is 0.00813903.
After 4573 training step(s), loss on training batch is 0.0131294.
After 4574 training step(s), loss on training batch is 0.00892922.
After 4575 training step(s), loss on training batch is 0.00817955.
After 4576 training step(s), loss on training batch is 0.00776829.
After 4577 training step(s), loss on training batch is 0.0121831.
After 4578 training step(s), loss on training batch is 0.00891857.
After 4579 training step(s), loss on training batch is 0.00820917.
After 4580 training step(s), loss on training batch is 0.00795083.
After 4581 training step(s), loss on training batch is 0.0107669.
After 4582 training step(s), loss on training batch is 0.00843969.
After 4583 training step(s), loss on training batch is 0.00966841.
After 4584 training step(s), loss on training batch is 0.00757064.
After 4585 training step(s), loss on training batch is 0.00867898.
After 4586 training step(s), loss on training batch is 0.00764897.
After 4587 training step(s), loss on training batch is 0.00816655.
After 4588 training step(s), loss on training batch is 0.00875139.
After 4589 training step(s), loss on training batch is 0.0084796.
After 4590 training step(s), loss on training batch is 0.00924997.
After 4591 training step(s), loss on training batch is 0.0085461.
After 4592 training step(s), loss on training batch is 0.0123456.
After 4593 training step(s), loss on training batch is 0.0116551.
After 4594 training step(s), loss on training batch is 0.00807419.
After 4595 training step(s), loss on training batch is 0.0132596.
After 4596 training step(s), loss on training batch is 0.00825567.
After 4597 training step(s), loss on training batch is 0.00916821.
After 4598 training step(s), loss on training batch is 0.00792608.
After 4599 training step(s), loss on training batch is 0.0083044.
After 4600 training step(s), loss on training batch is 0.0126178.
After 4601 training step(s), loss on training batch is 0.0079152.
After 4602 training step(s), loss on training batch is 0.00904958.
After 4603 training step(s), loss on training batch is 0.00772056.
After 4604 training step(s), loss on training batch is 0.00785469.
After 4605 training step(s), loss on training batch is 0.00747501.
After 4606 training step(s), loss on training batch is 0.0079299.
After 4607 training step(s), loss on training batch is 0.00926976.
After 4608 training step(s), loss on training batch is 0.00913125.
After 4609 training step(s), loss on training batch is 0.00891033.
After 4610 training step(s), loss on training batch is 0.0108954.
After 4611 training step(s), loss on training batch is 0.00859815.
After 4612 training step(s), loss on training batch is 0.0084544.
After 4613 training step(s), loss on training batch is 0.00740877.
After 4614 training step(s), loss on training batch is 0.00962497.
After 4615 training step(s), loss on training batch is 0.0080204.
After 4616 training step(s), loss on training batch is 0.00785542.
After 4617 training step(s), loss on training batch is 0.00848134.
After 4618 training step(s), loss on training batch is 0.00842566.
After 4619 training step(s), loss on training batch is 0.00919673.
After 4620 training step(s), loss on training batch is 0.00749803.
After 4621 training step(s), loss on training batch is 0.00804953.
After 4622 training step(s), loss on training batch is 0.00975487.
After 4623 training step(s), loss on training batch is 0.00769209.
After 4624 training step(s), loss on training batch is 0.00849287.
After 4625 training step(s), loss on training batch is 0.0104896.
After 4626 training step(s), loss on training batch is 0.00789268.
After 4627 training step(s), loss on training batch is 0.00874234.
After 4628 training step(s), loss on training batch is 0.00748384.
After 4629 training step(s), loss on training batch is 0.00745719.
After 4630 training step(s), loss on training batch is 0.00904789.
After 4631 training step(s), loss on training batch is 0.00728349.
After 4632 training step(s), loss on training batch is 0.0073262.
After 4633 training step(s), loss on training batch is 0.00764532.
After 4634 training step(s), loss on training batch is 0.00762174.
After 4635 training step(s), loss on training batch is 0.00854451.
After 4636 training step(s), loss on training batch is 0.00867848.
After 4637 training step(s), loss on training batch is 0.00798201.
After 4638 training step(s), loss on training batch is 0.00755077.
After 4639 training step(s), loss on training batch is 0.0081823.
After 4640 training step(s), loss on training batch is 0.00912498.
After 4641 training step(s), loss on training batch is 0.00713366.
After 4642 training step(s), loss on training batch is 0.00970809.
After 4643 training step(s), loss on training batch is 0.0079301.
After 4644 training step(s), loss on training batch is 0.00787445.
After 4645 training step(s), loss on training batch is 0.00807237.
After 4646 training step(s), loss on training batch is 0.00816663.
After 4647 training step(s), loss on training batch is 0.0138558.
After 4648 training step(s), loss on training batch is 0.0238772.
After 4649 training step(s), loss on training batch is 0.03292.
After 4650 training step(s), loss on training batch is 0.00954981.
After 4651 training step(s), loss on training batch is 0.00998918.
After 4652 training step(s), loss on training batch is 0.0142855.
After 4653 training step(s), loss on training batch is 0.00740999.
After 4654 training step(s), loss on training batch is 0.00992076.
After 4655 training step(s), loss on training batch is 0.00982037.
After 4656 training step(s), loss on training batch is 0.00918992.
After 4657 training step(s), loss on training batch is 0.00997675.
After 4658 training step(s), loss on training batch is 0.00733491.
After 4659 training step(s), loss on training batch is 0.00819477.
After 4660 training step(s), loss on training batch is 0.0187697.
After 4661 training step(s), loss on training batch is 0.00740822.
After 4662 training step(s), loss on training batch is 0.0077651.
After 4663 training step(s), loss on training batch is 0.0194847.
After 4664 training step(s), loss on training batch is 0.0105164.
After 4665 training step(s), loss on training batch is 0.00849134.
After 4666 training step(s), loss on training batch is 0.0133974.
After 4667 training step(s), loss on training batch is 0.00951939.
After 4668 training step(s), loss on training batch is 0.00988616.
After 4669 training step(s), loss on training batch is 0.00753113.
After 4670 training step(s), loss on training batch is 0.00971199.
After 4671 training step(s), loss on training batch is 0.00767031.
After 4672 training step(s), loss on training batch is 0.00820669.
After 4673 training step(s), loss on training batch is 0.0122332.
After 4674 training step(s), loss on training batch is 0.00744987.
After 4675 training step(s), loss on training batch is 0.00789036.
After 4676 training step(s), loss on training batch is 0.00890127.
After 4677 training step(s), loss on training batch is 0.00798106.
After 4678 training step(s), loss on training batch is 0.00745012.
After 4679 training step(s), loss on training batch is 0.00779863.
After 4680 training step(s), loss on training batch is 0.00784369.
After 4681 training step(s), loss on training batch is 0.010059.
After 4682 training step(s), loss on training batch is 0.00788587.
After 4683 training step(s), loss on training batch is 0.00757573.
After 4684 training step(s), loss on training batch is 0.00906271.
After 4685 training step(s), loss on training batch is 0.00909318.
After 4686 training step(s), loss on training batch is 0.0139858.
After 4687 training step(s), loss on training batch is 0.0111185.
After 4688 training step(s), loss on training batch is 0.00795655.
After 4689 training step(s), loss on training batch is 0.00785928.
After 4690 training step(s), loss on training batch is 0.00731253.
After 4691 training step(s), loss on training batch is 0.0106444.
After 4692 training step(s), loss on training batch is 0.00745887.
After 4693 training step(s), loss on training batch is 0.00867386.
After 4694 training step(s), loss on training batch is 0.00875512.
After 4695 training step(s), loss on training batch is 0.0108921.
After 4696 training step(s), loss on training batch is 0.00750329.
After 4697 training step(s), loss on training batch is 0.00775191.
After 4698 training step(s), loss on training batch is 0.00719736.
After 4699 training step(s), loss on training batch is 0.00901191.
After 4700 training step(s), loss on training batch is 0.00794171.
After 4701 training step(s), loss on training batch is 0.00784843.
After 4702 training step(s), loss on training batch is 0.00767875.
After 4703 training step(s), loss on training batch is 0.00774496.
After 4704 training step(s), loss on training batch is 0.00790504.
After 4705 training step(s), loss on training batch is 0.00730161.
After 4706 training step(s), loss on training batch is 0.00913356.
After 4707 training step(s), loss on training batch is 0.00711947.
After 4708 training step(s), loss on training batch is 0.00810393.
After 4709 training step(s), loss on training batch is 0.00805209.
After 4710 training step(s), loss on training batch is 0.00788153.
After 4711 training step(s), loss on training batch is 0.0122919.
After 4712 training step(s), loss on training batch is 0.00873867.
After 4713 training step(s), loss on training batch is 0.0082074.
After 4714 training step(s), loss on training batch is 0.00944477.
After 4715 training step(s), loss on training batch is 0.00937547.
After 4716 training step(s), loss on training batch is 0.00787275.
After 4717 training step(s), loss on training batch is 0.00825586.
After 4718 training step(s), loss on training batch is 0.00742276.
After 4719 training step(s), loss on training batch is 0.0102001.
After 4720 training step(s), loss on training batch is 0.0125279.
After 4721 training step(s), loss on training batch is 0.0100977.
After 4722 training step(s), loss on training batch is 0.00740319.
After 4723 training step(s), loss on training batch is 0.00981377.
After 4724 training step(s), loss on training batch is 0.00714942.
After 4725 training step(s), loss on training batch is 0.00720627.
After 4726 training step(s), loss on training batch is 0.0071754.
After 4727 training step(s), loss on training batch is 0.00801431.
After 4728 training step(s), loss on training batch is 0.00841524.
After 4729 training step(s), loss on training batch is 0.00753372.
After 4730 training step(s), loss on training batch is 0.00958254.
After 4731 training step(s), loss on training batch is 0.00760448.
After 4732 training step(s), loss on training batch is 0.00747845.
After 4733 training step(s), loss on training batch is 0.00776737.
After 4734 training step(s), loss on training batch is 0.00709181.
After 4735 training step(s), loss on training batch is 0.00735013.
After 4736 training step(s), loss on training batch is 0.00718452.
After 4737 training step(s), loss on training batch is 0.00925768.
After 4738 training step(s), loss on training batch is 0.00705183.
After 4739 training step(s), loss on training batch is 0.00799983.
After 4740 training step(s), loss on training batch is 0.00892846.
After 4741 training step(s), loss on training batch is 0.0087548.
After 4742 training step(s), loss on training batch is 0.00770702.
After 4743 training step(s), loss on training batch is 0.00743121.
After 4744 training step(s), loss on training batch is 0.00806018.
After 4745 training step(s), loss on training batch is 0.00945131.
After 4746 training step(s), loss on training batch is 0.00759037.
After 4747 training step(s), loss on training batch is 0.00891889.
After 4748 training step(s), loss on training batch is 0.00774234.
After 4749 training step(s), loss on training batch is 0.0076361.
After 4750 training step(s), loss on training batch is 0.00758591.
After 4751 training step(s), loss on training batch is 0.00731556.
After 4752 training step(s), loss on training batch is 0.00754042.
After 4753 training step(s), loss on training batch is 0.00906513.
After 4754 training step(s), loss on training batch is 0.00894529.
After 4755 training step(s), loss on training batch is 0.00884534.
After 4756 training step(s), loss on training batch is 0.00731041.
After 4757 training step(s), loss on training batch is 0.00735445.
After 4758 training step(s), loss on training batch is 0.00800853.
After 4759 training step(s), loss on training batch is 0.00727873.
After 4760 training step(s), loss on training batch is 0.00726591.
After 4761 training step(s), loss on training batch is 0.00720246.
After 4762 training step(s), loss on training batch is 0.0075735.
After 4763 training step(s), loss on training batch is 0.00760108.
After 4764 training step(s), loss on training batch is 0.00938792.
After 4765 training step(s), loss on training batch is 0.00801195.
After 4766 training step(s), loss on training batch is 0.00731532.
After 4767 training step(s), loss on training batch is 0.00767518.
After 4768 training step(s), loss on training batch is 0.00742469.
After 4769 training step(s), loss on training batch is 0.00895063.
After 4770 training step(s), loss on training batch is 0.00714677.
After 4771 training step(s), loss on training batch is 0.00850699.
After 4772 training step(s), loss on training batch is 0.00772819.
After 4773 training step(s), loss on training batch is 0.00964514.
After 4774 training step(s), loss on training batch is 0.0082617.
After 4775 training step(s), loss on training batch is 0.00780327.
After 4776 training step(s), loss on training batch is 0.00765715.
After 4777 training step(s), loss on training batch is 0.00753478.
After 4778 training step(s), loss on training batch is 0.00814665.
After 4779 training step(s), loss on training batch is 0.00748747.
After 4780 training step(s), loss on training batch is 0.00720993.
After 4781 training step(s), loss on training batch is 0.00797386.
After 4782 training step(s), loss on training batch is 0.00759253.
After 4783 training step(s), loss on training batch is 0.00908297.
After 4784 training step(s), loss on training batch is 0.00742495.
After 4785 training step(s), loss on training batch is 0.0107241.
After 4786 training step(s), loss on training batch is 0.00744919.
After 4787 training step(s), loss on training batch is 0.00823214.
After 4788 training step(s), loss on training batch is 0.00881123.
After 4789 training step(s), loss on training batch is 0.00942791.
After 4790 training step(s), loss on training batch is 0.00743392.
After 4791 training step(s), loss on training batch is 0.00695319.
After 4792 training step(s), loss on training batch is 0.007402.
After 4793 training step(s), loss on training batch is 0.00817079.
After 4794 training step(s), loss on training batch is 0.00812627.
After 4795 training step(s), loss on training batch is 0.00865213.
After 4796 training step(s), loss on training batch is 0.0078111.
After 4797 training step(s), loss on training batch is 0.00740282.
After 4798 training step(s), loss on training batch is 0.007098.
After 4799 training step(s), loss on training batch is 0.00897759.
After 4800 training step(s), loss on training batch is 0.00686097.
After 4801 training step(s), loss on training batch is 0.00736301.
After 4802 training step(s), loss on training batch is 0.00772022.
After 4803 training step(s), loss on training batch is 0.00828739.
After 4804 training step(s), loss on training batch is 0.00729985.
After 4805 training step(s), loss on training batch is 0.00893289.
After 4806 training step(s), loss on training batch is 0.00799843.
After 4807 training step(s), loss on training batch is 0.00798091.
After 4808 training step(s), loss on training batch is 0.00754954.
After 4809 training step(s), loss on training batch is 0.00888253.
After 4810 training step(s), loss on training batch is 0.00795854.
After 4811 training step(s), loss on training batch is 0.00856038.
After 4812 training step(s), loss on training batch is 0.00736895.
After 4813 training step(s), loss on training batch is 0.00854291.
After 4814 training step(s), loss on training batch is 0.0100414.
After 4815 training step(s), loss on training batch is 0.00841627.
After 4816 training step(s), loss on training batch is 0.00733417.
After 4817 training step(s), loss on training batch is 0.00767843.
After 4818 training step(s), loss on training batch is 0.00746489.
After 4819 training step(s), loss on training batch is 0.00758201.
After 4820 training step(s), loss on training batch is 0.00733748.
After 4821 training step(s), loss on training batch is 0.00778159.
After 4822 training step(s), loss on training batch is 0.00743249.
After 4823 training step(s), loss on training batch is 0.00775133.
After 4824 training step(s), loss on training batch is 0.00713254.
After 4825 training step(s), loss on training batch is 0.00764436.
After 4826 training step(s), loss on training batch is 0.00858769.
After 4827 training step(s), loss on training batch is 0.00797464.
After 4828 training step(s), loss on training batch is 0.00887689.
After 4829 training step(s), loss on training batch is 0.00786586.
After 4830 training step(s), loss on training batch is 0.00913863.
After 4831 training step(s), loss on training batch is 0.00769217.
After 4832 training step(s), loss on training batch is 0.00782491.
After 4833 training step(s), loss on training batch is 0.00741852.
After 4834 training step(s), loss on training batch is 0.0081027.
After 4835 training step(s), loss on training batch is 0.00788231.
After 4836 training step(s), loss on training batch is 0.00807977.
After 4837 training step(s), loss on training batch is 0.00743502.
After 4838 training step(s), loss on training batch is 0.00700632.
After 4839 training step(s), loss on training batch is 0.00787902.
After 4840 training step(s), loss on training batch is 0.00741203.
After 4841 training step(s), loss on training batch is 0.00796415.
After 4842 training step(s), loss on training batch is 0.00740857.
After 4843 training step(s), loss on training batch is 0.00761465.
After 4844 training step(s), loss on training batch is 0.00716117.
After 4845 training step(s), loss on training batch is 0.00726835.
After 4846 training step(s), loss on training batch is 0.0150712.
After 4847 training step(s), loss on training batch is 0.00714143.
After 4848 training step(s), loss on training batch is 0.00743178.
After 4849 training step(s), loss on training batch is 0.00810887.
After 4850 training step(s), loss on training batch is 0.0074136.
After 4851 training step(s), loss on training batch is 0.00750945.
After 4852 training step(s), loss on training batch is 0.00780242.
After 4853 training step(s), loss on training batch is 0.0101233.
After 4854 training step(s), loss on training batch is 0.00757539.
After 4855 training step(s), loss on training batch is 0.0070766.
After 4856 training step(s), loss on training batch is 0.006988.
After 4857 training step(s), loss on training batch is 0.00838014.
After 4858 training step(s), loss on training batch is 0.00864065.
After 4859 training step(s), loss on training batch is 0.00806484.
After 4860 training step(s), loss on training batch is 0.00901255.
After 4861 training step(s), loss on training batch is 0.00831068.
After 4862 training step(s), loss on training batch is 0.00722839.
After 4863 training step(s), loss on training batch is 0.00916244.
After 4864 training step(s), loss on training batch is 0.00817402.
After 4865 training step(s), loss on training batch is 0.00799259.
After 4866 training step(s), loss on training batch is 0.00815154.
After 4867 training step(s), loss on training batch is 0.0072457.
After 4868 training step(s), loss on training batch is 0.00766812.
After 4869 training step(s), loss on training batch is 0.00709889.
After 4870 training step(s), loss on training batch is 0.0121256.
After 4871 training step(s), loss on training batch is 0.00697215.
After 4872 training step(s), loss on training batch is 0.0133883.
After 4873 training step(s), loss on training batch is 0.00905718.
After 4874 training step(s), loss on training batch is 0.0157585.
After 4875 training step(s), loss on training batch is 0.00754828.
After 4876 training step(s), loss on training batch is 0.00797231.
After 4877 training step(s), loss on training batch is 0.0207812.
After 4878 training step(s), loss on training batch is 0.00743303.
After 4879 training step(s), loss on training batch is 0.00900651.
After 4880 training step(s), loss on training batch is 0.00732655.
After 4881 training step(s), loss on training batch is 0.00750693.
After 4882 training step(s), loss on training batch is 0.00757321.
After 4883 training step(s), loss on training batch is 0.00715091.
After 4884 training step(s), loss on training batch is 0.00829693.
After 4885 training step(s), loss on training batch is 0.00702754.
After 4886 training step(s), loss on training batch is 0.00711794.
After 4887 training step(s), loss on training batch is 0.00695524.
After 4888 training step(s), loss on training batch is 0.00775242.
After 4889 training step(s), loss on training batch is 0.00949586.
After 4890 training step(s), loss on training batch is 0.00772032.
After 4891 training step(s), loss on training batch is 0.00768991.
After 4892 training step(s), loss on training batch is 0.00764852.
After 4893 training step(s), loss on training batch is 0.00774433.
After 4894 training step(s), loss on training batch is 0.00715424.
After 4895 training step(s), loss on training batch is 0.00744432.
After 4896 training step(s), loss on training batch is 0.00903672.
After 4897 training step(s), loss on training batch is 0.0088289.
After 4898 training step(s), loss on training batch is 0.00743057.
After 4899 training step(s), loss on training batch is 0.00739444.
After 4900 training step(s), loss on training batch is 0.00687083.
After 4901 training step(s), loss on training batch is 0.00715793.
After 4902 training step(s), loss on training batch is 0.00764478.
After 4903 training step(s), loss on training batch is 0.0077318.
After 4904 training step(s), loss on training batch is 0.00731756.
After 4905 training step(s), loss on training batch is 0.00768059.
After 4906 training step(s), loss on training batch is 0.00783648.
After 4907 training step(s), loss on training batch is 0.00777956.
After 4908 training step(s), loss on training batch is 0.00731163.
After 4909 training step(s), loss on training batch is 0.00794286.
After 4910 training step(s), loss on training batch is 0.00733143.
After 4911 training step(s), loss on training batch is 0.00701353.
After 4912 training step(s), loss on training batch is 0.00737134.
After 4913 training step(s), loss on training batch is 0.00725018.
After 4914 training step(s), loss on training batch is 0.00844245.
After 4915 training step(s), loss on training batch is 0.00767125.
After 4916 training step(s), loss on training batch is 0.00740435.
After 4917 training step(s), loss on training batch is 0.00743685.
After 4918 training step(s), loss on training batch is 0.00690075.
After 4919 training step(s), loss on training batch is 0.0079953.
After 4920 training step(s), loss on training batch is 0.00738855.
After 4921 training step(s), loss on training batch is 0.00905224.
After 4922 training step(s), loss on training batch is 0.00984416.
After 4923 training step(s), loss on training batch is 0.00725466.
After 4924 training step(s), loss on training batch is 0.00757481.
After 4925 training step(s), loss on training batch is 0.00905549.
After 4926 training step(s), loss on training batch is 0.00701294.
After 4927 training step(s), loss on training batch is 0.00762289.
After 4928 training step(s), loss on training batch is 0.00887442.
After 4929 training step(s), loss on training batch is 0.0123062.
After 4930 training step(s), loss on training batch is 0.010104.
After 4931 training step(s), loss on training batch is 0.00729397.
After 4932 training step(s), loss on training batch is 0.00814273.
After 4933 training step(s), loss on training batch is 0.0091963.
After 4934 training step(s), loss on training batch is 0.0142237.
After 4935 training step(s), loss on training batch is 0.0078597.
After 4936 training step(s), loss on training batch is 0.00866917.
After 4937 training step(s), loss on training batch is 0.00681679.
After 4938 training step(s), loss on training batch is 0.00782784.
After 4939 training step(s), loss on training batch is 0.0102844.
After 4940 training step(s), loss on training batch is 0.00775659.
After 4941 training step(s), loss on training batch is 0.00715793.
After 4942 training step(s), loss on training batch is 0.00702129.
After 4943 training step(s), loss on training batch is 0.00856005.
After 4944 training step(s), loss on training batch is 0.00692519.
After 4945 training step(s), loss on training batch is 0.00757423.
After 4946 training step(s), loss on training batch is 0.00739039.
After 4947 training step(s), loss on training batch is 0.00848776.
After 4948 training step(s), loss on training batch is 0.00706553.
After 4949 training step(s), loss on training batch is 0.0071987.
After 4950 training step(s), loss on training batch is 0.00873753.
After 4951 training step(s), loss on training batch is 0.00708372.
After 4952 training step(s), loss on training batch is 0.00749083.
After 4953 training step(s), loss on training batch is 0.00743376.
After 4954 training step(s), loss on training batch is 0.00779588.
After 4955 training step(s), loss on training batch is 0.00919049.
After 4956 training step(s), loss on training batch is 0.0076389.
After 4957 training step(s), loss on training batch is 0.00733466.
After 4958 training step(s), loss on training batch is 0.00735623.
After 4959 training step(s), loss on training batch is 0.00802639.
After 4960 training step(s), loss on training batch is 0.00685868.
After 4961 training step(s), loss on training batch is 0.00796433.
After 4962 training step(s), loss on training batch is 0.00715435.
After 4963 training step(s), loss on training batch is 0.00692077.
After 4964 training step(s), loss on training batch is 0.00726062.
After 4965 training step(s), loss on training batch is 0.00702714.
After 4966 training step(s), loss on training batch is 0.00694004.
After 4967 training step(s), loss on training batch is 0.0081797.
After 4968 training step(s), loss on training batch is 0.0072998.
After 4969 training step(s), loss on training batch is 0.00673498.
After 4970 training step(s), loss on training batch is 0.00678418.
After 4971 training step(s), loss on training batch is 0.0073138.
After 4972 training step(s), loss on training batch is 0.00699469.
After 4973 training step(s), loss on training batch is 0.00720236.
After 4974 training step(s), loss on training batch is 0.00722246.
After 4975 training step(s), loss on training batch is 0.00687039.
After 4976 training step(s), loss on training batch is 0.0070381.
After 4977 training step(s), loss on training batch is 0.00703068.
After 4978 training step(s), loss on training batch is 0.00714673.
After 4979 training step(s), loss on training batch is 0.00850753.
After 4980 training step(s), loss on training batch is 0.00744804.
After 4981 training step(s), loss on training batch is 0.00727919.
After 4982 training step(s), loss on training batch is 0.0075405.
After 4983 training step(s), loss on training batch is 0.00773252.
After 4984 training step(s), loss on training batch is 0.00764299.
After 4985 training step(s), loss on training batch is 0.00706884.
After 4986 training step(s), loss on training batch is 0.00715286.
After 4987 training step(s), loss on training batch is 0.00971347.
After 4988 training step(s), loss on training batch is 0.0104227.
After 4989 training step(s), loss on training batch is 0.00745773.
After 4990 training step(s), loss on training batch is 0.00824193.
After 4991 training step(s), loss on training batch is 0.00759301.
After 4992 training step(s), loss on training batch is 0.00859976.
After 4993 training step(s), loss on training batch is 0.00735701.
After 4994 training step(s), loss on training batch is 0.0077421.
After 4995 training step(s), loss on training batch is 0.00825054.
After 4996 training step(s), loss on training batch is 0.00683666.
After 4997 training step(s), loss on training batch is 0.00793109.
After 4998 training step(s), loss on training batch is 0.00711231.
After 4999 training step(s), loss on training batch is 0.00874892.
After 5000 training step(s), loss on training batch is 0.00695927.
After 5001 training step(s), loss on training batch is 0.00701904.
After 5002 training step(s), loss on training batch is 0.00720193.
After 5003 training step(s), loss on training batch is 0.00695753.
After 5004 training step(s), loss on training batch is 0.00713471.
After 5005 training step(s), loss on training batch is 0.00786551.
After 5006 training step(s), loss on training batch is 0.00709104.
After 5007 training step(s), loss on training batch is 0.00800188.
After 5008 training step(s), loss on training batch is 0.00729335.
After 5009 training step(s), loss on training batch is 0.00763567.
After 5010 training step(s), loss on training batch is 0.00792755.
After 5011 training step(s), loss on training batch is 0.00918977.
After 5012 training step(s), loss on training batch is 0.00673757.
After 5013 training step(s), loss on training batch is 0.00673878.
After 5014 training step(s), loss on training batch is 0.0080944.
After 5015 training step(s), loss on training batch is 0.00727909.
After 5016 training step(s), loss on training batch is 0.00814517.
After 5017 training step(s), loss on training batch is 0.00722298.
After 5018 training step(s), loss on training batch is 0.00807392.
After 5019 training step(s), loss on training batch is 0.00727308.
After 5020 training step(s), loss on training batch is 0.00886815.
After 5021 training step(s), loss on training batch is 0.00902356.
After 5022 training step(s), loss on training batch is 0.00771149.
After 5023 training step(s), loss on training batch is 0.00775022.
After 5024 training step(s), loss on training batch is 0.00769759.
After 5025 training step(s), loss on training batch is 0.00692877.
After 5026 training step(s), loss on training batch is 0.00677945.
After 5027 training step(s), loss on training batch is 0.00750398.
After 5028 training step(s), loss on training batch is 0.00733674.
After 5029 training step(s), loss on training batch is 0.00733669.
After 5030 training step(s), loss on training batch is 0.00768743.
After 5031 training step(s), loss on training batch is 0.00848597.
After 5032 training step(s), loss on training batch is 0.00783238.
After 5033 training step(s), loss on training batch is 0.00768942.
After 5034 training step(s), loss on training batch is 0.00760681.
After 5035 training step(s), loss on training batch is 0.00725955.
After 5036 training step(s), loss on training batch is 0.00722989.
After 5037 training step(s), loss on training batch is 0.00683671.
After 5038 training step(s), loss on training batch is 0.00817323.
After 5039 training step(s), loss on training batch is 0.00717849.
After 5040 training step(s), loss on training batch is 0.00679144.
After 5041 training step(s), loss on training batch is 0.00741861.
After 5042 training step(s), loss on training batch is 0.00692965.
After 5043 training step(s), loss on training batch is 0.00867509.
After 5044 training step(s), loss on training batch is 0.00870257.
After 5045 training step(s), loss on training batch is 0.00749412.
After 5046 training step(s), loss on training batch is 0.00809598.
After 5047 training step(s), loss on training batch is 0.00749747.
After 5048 training step(s), loss on training batch is 0.00759544.
After 5049 training step(s), loss on training batch is 0.00752489.
After 5050 training step(s), loss on training batch is 0.00682329.
After 5051 training step(s), loss on training batch is 0.00787043.
After 5052 training step(s), loss on training batch is 0.0110574.
After 5053 training step(s), loss on training batch is 0.00802346.
After 5054 training step(s), loss on training batch is 0.00881496.
After 5055 training step(s), loss on training batch is 0.00674938.
After 5056 training step(s), loss on training batch is 0.0102643.
After 5057 training step(s), loss on training batch is 0.0078731.
After 5058 training step(s), loss on training batch is 0.0107756.
After 5059 training step(s), loss on training batch is 0.00676807.
After 5060 training step(s), loss on training batch is 0.0089657.
After 5061 training step(s), loss on training batch is 0.00700447.
After 5062 training step(s), loss on training batch is 0.0074738.
After 5063 training step(s), loss on training batch is 0.00751482.
After 5064 training step(s), loss on training batch is 0.0086816.
After 5065 training step(s), loss on training batch is 0.00730862.
After 5066 training step(s), loss on training batch is 0.00687583.
After 5067 training step(s), loss on training batch is 0.00685745.
After 5068 training step(s), loss on training batch is 0.00665151.
After 5069 training step(s), loss on training batch is 0.00670394.
After 5070 training step(s), loss on training batch is 0.00841138.
After 5071 training step(s), loss on training batch is 0.00772059.
After 5072 training step(s), loss on training batch is 0.00849138.
After 5073 training step(s), loss on training batch is 0.00712644.
After 5074 training step(s), loss on training batch is 0.00827611.
After 5075 training step(s), loss on training batch is 0.00913537.
After 5076 training step(s), loss on training batch is 0.00760842.
After 5077 training step(s), loss on training batch is 0.010583.
After 5078 training step(s), loss on training batch is 0.00720008.
After 5079 training step(s), loss on training batch is 0.00931345.
After 5080 training step(s), loss on training batch is 0.00741938.
After 5081 training step(s), loss on training batch is 0.00934107.
After 5082 training step(s), loss on training batch is 0.00729049.
After 5083 training step(s), loss on training batch is 0.0093399.
After 5084 training step(s), loss on training batch is 0.00743887.
After 5085 training step(s), loss on training batch is 0.0074877.
After 5086 training step(s), loss on training batch is 0.00671119.
After 5087 training step(s), loss on training batch is 0.00717411.
After 5088 training step(s), loss on training batch is 0.00660387.
After 5089 training step(s), loss on training batch is 0.0076342.
After 5090 training step(s), loss on training batch is 0.00764912.
After 5091 training step(s), loss on training batch is 0.0112707.
After 5092 training step(s), loss on training batch is 0.00655417.
After 5093 training step(s), loss on training batch is 0.00765178.
After 5094 training step(s), loss on training batch is 0.00719523.
After 5095 training step(s), loss on training batch is 0.00705533.
After 5096 training step(s), loss on training batch is 0.00690165.
After 5097 training step(s), loss on training batch is 0.00681651.
After 5098 training step(s), loss on training batch is 0.00714537.
After 5099 training step(s), loss on training batch is 0.0075393.
After 5100 training step(s), loss on training batch is 0.00805399.
After 5101 training step(s), loss on training batch is 0.00723605.
After 5102 training step(s), loss on training batch is 0.00696686.
After 5103 training step(s), loss on training batch is 0.00730538.
After 5104 training step(s), loss on training batch is 0.00659567.
After 5105 training step(s), loss on training batch is 0.00668715.
After 5106 training step(s), loss on training batch is 0.00703579.
After 5107 training step(s), loss on training batch is 0.00784385.
After 5108 training step(s), loss on training batch is 0.0077328.
After 5109 training step(s), loss on training batch is 0.00727915.
After 5110 training step(s), loss on training batch is 0.00712226.
After 5111 training step(s), loss on training batch is 0.00763932.
After 5112 training step(s), loss on training batch is 0.00749298.
After 5113 training step(s), loss on training batch is 0.0143007.
After 5114 training step(s), loss on training batch is 0.00696325.
After 5115 training step(s), loss on training batch is 0.00701156.
After 5116 training step(s), loss on training batch is 0.00699468.
After 5117 training step(s), loss on training batch is 0.0100136.
After 5118 training step(s), loss on training batch is 0.0082884.
After 5119 training step(s), loss on training batch is 0.00670685.
After 5120 training step(s), loss on training batch is 0.00705754.
After 5121 training step(s), loss on training batch is 0.00713428.
After 5122 training step(s), loss on training batch is 0.00689666.
After 5123 training step(s), loss on training batch is 0.00688961.
After 5124 training step(s), loss on training batch is 0.00732399.
After 5125 training step(s), loss on training batch is 0.00748647.
After 5126 training step(s), loss on training batch is 0.00732888.
After 5127 training step(s), loss on training batch is 0.00710125.
After 5128 training step(s), loss on training batch is 0.0073113.
After 5129 training step(s), loss on training batch is 0.00671662.
After 5130 training step(s), loss on training batch is 0.00687694.
After 5131 training step(s), loss on training batch is 0.0093079.
After 5132 training step(s), loss on training batch is 0.00781009.
After 5133 training step(s), loss on training batch is 0.00672513.
After 5134 training step(s), loss on training batch is 0.0117242.
After 5135 training step(s), loss on training batch is 0.00820901.
After 5136 training step(s), loss on training batch is 0.0083878.
After 5137 training step(s), loss on training batch is 0.00700099.
After 5138 training step(s), loss on training batch is 0.00760492.
After 5139 training step(s), loss on training batch is 0.00760365.
After 5140 training step(s), loss on training batch is 0.0101681.
After 5141 training step(s), loss on training batch is 0.00756297.
After 5142 training step(s), loss on training batch is 0.0254945.
After 5143 training step(s), loss on training batch is 0.0534047.
After 5144 training step(s), loss on training batch is 0.287822.
After 5145 training step(s), loss on training batch is 1.76551.
After 5146 training step(s), loss on training batch is 10.1059.
After 5147 training step(s), loss on training batch is 16.5779.
After 5148 training step(s), loss on training batch is 4.79302.
After 5149 training step(s), loss on training batch is 2.4797.
After 5150 training step(s), loss on training batch is 2.38161.
After 5151 training step(s), loss on training batch is 2.24524.
After 5152 training step(s), loss on training batch is 2.13181.
After 5153 training step(s), loss on training batch is 1.99263.
After 5154 training step(s), loss on training batch is 1.90988.
After 5155 training step(s), loss on training batch is 1.64356.
After 5156 training step(s), loss on training batch is 1.55099.
After 5157 training step(s), loss on training batch is 2.02622.
After 5158 training step(s), loss on training batch is 2.12118.
After 5159 training step(s), loss on training batch is 2.50437.
After 5160 training step(s), loss on training batch is 2.13959.
After 5161 training step(s), loss on training batch is 1.86298.
After 5162 training step(s), loss on training batch is 1.58918.
After 5163 training step(s), loss on training batch is 1.28904.
After 5164 training step(s), loss on training batch is 1.12454.
After 5165 training step(s), loss on training batch is 0.904682.
After 5166 training step(s), loss on training batch is 1.15715.
After 5167 training step(s), loss on training batch is 1.41966.
After 5168 training step(s), loss on training batch is 1.43322.
After 5169 training step(s), loss on training batch is 1.03017.
After 5170 training step(s), loss on training batch is 0.79976.
After 5171 training step(s), loss on training batch is 0.547341.
After 5172 training step(s), loss on training batch is 0.453423.
After 5173 training step(s), loss on training batch is 0.459838.
After 5174 training step(s), loss on training batch is 0.418746.
After 5175 training step(s), loss on training batch is 0.680113.
After 5176 training step(s), loss on training batch is 0.760019.
After 5177 training step(s), loss on training batch is 0.513234.
After 5178 training step(s), loss on training batch is 0.391215.
After 5179 training step(s), loss on training batch is 0.313352.
After 5180 training step(s), loss on training batch is 0.251204.
After 5181 training step(s), loss on training batch is 0.245716.
After 5182 training step(s), loss on training batch is 0.249996.
After 5183 training step(s), loss on training batch is 0.246289.
After 5184 training step(s), loss on training batch is 0.205323.
After 5185 training step(s), loss on training batch is 0.175213.
After 5186 training step(s), loss on training batch is 0.253813.
After 5187 training step(s), loss on training batch is 0.229697.
After 5188 training step(s), loss on training batch is 0.318469.
After 5189 training step(s), loss on training batch is 0.277902.
After 5190 training step(s), loss on training batch is 0.255033.
After 5191 training step(s), loss on training batch is 0.21876.
After 5192 training step(s), loss on training batch is 0.295687.
After 5193 training step(s), loss on training batch is 0.213741.
After 5194 training step(s), loss on training batch is 0.167804.
After 5195 training step(s), loss on training batch is 0.150599.
After 5196 training step(s), loss on training batch is 0.220069.
After 5197 training step(s), loss on training batch is 0.195313.
After 5198 training step(s), loss on training batch is 0.185223.
After 5199 training step(s), loss on training batch is 0.228845.
After 5200 training step(s), loss on training batch is 0.225291.
After 5201 training step(s), loss on training batch is 0.149107.
After 5202 training step(s), loss on training batch is 0.174743.
After 5203 training step(s), loss on training batch is 0.214919.
After 5204 training step(s), loss on training batch is 0.248246.
After 5205 training step(s), loss on training batch is 0.319404.
After 5206 training step(s), loss on training batch is 0.570919.
After 5207 training step(s), loss on training batch is 0.628738.
After 5208 training step(s), loss on training batch is 1.04513.
After 5209 training step(s), loss on training batch is 1.00393.
After 5210 training step(s), loss on training batch is 0.250225.
After 5211 training step(s), loss on training batch is 0.385274.
After 5212 training step(s), loss on training batch is 0.231881.
After 5213 training step(s), loss on training batch is 0.195104.
After 5214 training step(s), loss on training batch is 0.16344.
After 5215 training step(s), loss on training batch is 0.235187.
After 5216 training step(s), loss on training batch is 0.244365.
After 5217 training step(s), loss on training batch is 0.244993.
After 5218 training step(s), loss on training batch is 0.185883.
After 5219 training step(s), loss on training batch is 0.170968.
After 5220 training step(s), loss on training batch is 0.18642.
After 5221 training step(s), loss on training batch is 0.165483.
After 5222 training step(s), loss on training batch is 0.235795.
After 5223 training step(s), loss on training batch is 0.134288.
After 5224 training step(s), loss on training batch is 0.128328.
After 5225 training step(s), loss on training batch is 0.236589.
After 5226 training step(s), loss on training batch is 0.117404.
After 5227 training step(s), loss on training batch is 0.0969232.
After 5228 training step(s), loss on training batch is 0.130984.
After 5229 training step(s), loss on training batch is 0.161488.
After 5230 training step(s), loss on training batch is 0.128188.
After 5231 training step(s), loss on training batch is 0.14861.
After 5232 training step(s), loss on training batch is 0.122006.
After 5233 training step(s), loss on training batch is 0.128419.
After 5234 training step(s), loss on training batch is 0.125878.
After 5235 training step(s), loss on training batch is 0.0640582.
After 5236 training step(s), loss on training batch is 0.169091.
After 5237 training step(s), loss on training batch is 0.160433.
After 5238 training step(s), loss on training batch is 0.141116.
After 5239 training step(s), loss on training batch is 0.189372.
After 5240 training step(s), loss on training batch is 0.133703.
After 5241 training step(s), loss on training batch is 0.104732.
After 5242 training step(s), loss on training batch is 0.0874136.
After 5243 training step(s), loss on training batch is 0.108609.
After 5244 training step(s), loss on training batch is 0.111622.
After 5245 training step(s), loss on training batch is 0.131215.
After 5246 training step(s), loss on training batch is 0.106306.
After 5247 training step(s), loss on training batch is 0.1205.
After 5248 training step(s), loss on training batch is 0.1962.
After 5249 training step(s), loss on training batch is 0.0756983.
After 5250 training step(s), loss on training batch is 0.0639575.
After 5251 training step(s), loss on training batch is 0.18542.
After 5252 training step(s), loss on training batch is 0.125537.
After 5253 training step(s), loss on training batch is 0.163324.
After 5254 training step(s), loss on training batch is 0.148.
After 5255 training step(s), loss on training batch is 0.0990441.
After 5256 training step(s), loss on training batch is 0.107544.
After 5257 training step(s), loss on training batch is 0.0941382.
After 5258 training step(s), loss on training batch is 0.146013.
After 5259 training step(s), loss on training batch is 0.147381.
After 5260 training step(s), loss on training batch is 0.115734.
After 5261 training step(s), loss on training batch is 0.108497.
After 5262 training step(s), loss on training batch is 0.0963012.
After 5263 training step(s), loss on training batch is 0.08738.
After 5264 training step(s), loss on training batch is 0.101857.
After 5265 training step(s), loss on training batch is 0.144831.
After 5266 training step(s), loss on training batch is 0.0694828.
After 5267 training step(s), loss on training batch is 0.158439.
After 5268 training step(s), loss on training batch is 0.101974.
After 5269 training step(s), loss on training batch is 0.18637.
After 5270 training step(s), loss on training batch is 0.190125.
After 5271 training step(s), loss on training batch is 0.0891735.
After 5272 training step(s), loss on training batch is 0.062885.
After 5273 training step(s), loss on training batch is 0.130687.
After 5274 training step(s), loss on training batch is 0.0663771.
After 5275 training step(s), loss on training batch is 0.0895406.
After 5276 training step(s), loss on training batch is 0.100635.
After 5277 training step(s), loss on training batch is 0.0943749.
After 5278 training step(s), loss on training batch is 0.0815446.
After 5279 training step(s), loss on training batch is 0.148221.
After 5280 training step(s), loss on training batch is 0.103877.
After 5281 training step(s), loss on training batch is 0.120965.
After 5282 training step(s), loss on training batch is 0.0680017.
After 5283 training step(s), loss on training batch is 0.0599051.
After 5284 training step(s), loss on training batch is 0.102886.
After 5285 training step(s), loss on training batch is 0.118574.
After 5286 training step(s), loss on training batch is 0.158306.
After 5287 training step(s), loss on training batch is 0.097203.
After 5288 training step(s), loss on training batch is 0.11281.
After 5289 training step(s), loss on training batch is 0.053545.
After 5290 training step(s), loss on training batch is 0.0844296.
After 5291 training step(s), loss on training batch is 0.0775933.
After 5292 training step(s), loss on training batch is 0.132142.
After 5293 training step(s), loss on training batch is 0.115872.
After 5294 training step(s), loss on training batch is 0.106225.
After 5295 training step(s), loss on training batch is 0.127083.
After 5296 training step(s), loss on training batch is 0.0480445.
After 5297 training step(s), loss on training batch is 0.11599.
After 5298 training step(s), loss on training batch is 0.0818962.
After 5299 training step(s), loss on training batch is 0.152166.
After 5300 training step(s), loss on training batch is 0.094697.
After 5301 training step(s), loss on training batch is 0.0773493.
After 5302 training step(s), loss on training batch is 0.0627073.
After 5303 training step(s), loss on training batch is 0.0500826.
After 5304 training step(s), loss on training batch is 0.1021.
After 5305 training step(s), loss on training batch is 0.0827355.
After 5306 training step(s), loss on training batch is 0.135209.
After 5307 training step(s), loss on training batch is 0.155074.
After 5308 training step(s), loss on training batch is 0.112197.
After 5309 training step(s), loss on training batch is 0.117395.
After 5310 training step(s), loss on training batch is 0.0767498.
After 5311 training step(s), loss on training batch is 0.0892264.
After 5312 training step(s), loss on training batch is 0.0931307.
After 5313 training step(s), loss on training batch is 0.119461.
After 5314 training step(s), loss on training batch is 0.0932995.
After 5315 training step(s), loss on training batch is 0.109881.
After 5316 training step(s), loss on training batch is 0.138493.
After 5317 training step(s), loss on training batch is 0.0856096.
After 5318 training step(s), loss on training batch is 0.092632.
After 5319 training step(s), loss on training batch is 0.115784.
After 5320 training step(s), loss on training batch is 0.0628366.
After 5321 training step(s), loss on training batch is 0.104746.
After 5322 training step(s), loss on training batch is 0.106278.
After 5323 training step(s), loss on training batch is 0.0582297.
After 5324 training step(s), loss on training batch is 0.0731299.
After 5325 training step(s), loss on training batch is 0.163727.
After 5326 training step(s), loss on training batch is 0.115556.
After 5327 training step(s), loss on training batch is 0.0601176.
After 5328 training step(s), loss on training batch is 0.163717.
After 5329 training step(s), loss on training batch is 0.0746795.
After 5330 training step(s), loss on training batch is 0.100403.
After 5331 training step(s), loss on training batch is 0.154393.
After 5332 training step(s), loss on training batch is 0.0897303.
After 5333 training step(s), loss on training batch is 0.0831207.
After 5334 training step(s), loss on training batch is 0.111758.
After 5335 training step(s), loss on training batch is 0.131383.
After 5336 training step(s), loss on training batch is 0.0738792.
After 5337 training step(s), loss on training batch is 0.0734405.
After 5338 training step(s), loss on training batch is 0.0523202.
After 5339 training step(s), loss on training batch is 0.0740003.
After 5340 training step(s), loss on training batch is 0.0795097.
After 5341 training step(s), loss on training batch is 0.0684599.
After 5342 training step(s), loss on training batch is 0.120041.
After 5343 training step(s), loss on training batch is 0.106847.
After 5344 training step(s), loss on training batch is 0.135401.
After 5345 training step(s), loss on training batch is 0.0682994.
After 5346 training step(s), loss on training batch is 0.0610418.
After 5347 training step(s), loss on training batch is 0.126319.
After 5348 training step(s), loss on training batch is 0.0465165.
After 5349 training step(s), loss on training batch is 0.0731931.
After 5350 training step(s), loss on training batch is 0.111129.
After 5351 training step(s), loss on training batch is 0.182929.
After 5352 training step(s), loss on training batch is 0.0784956.
After 5353 training step(s), loss on training batch is 0.067969.
After 5354 training step(s), loss on training batch is 0.0865186.
After 5355 training step(s), loss on training batch is 0.131465.
After 5356 training step(s), loss on training batch is 0.104417.
After 5357 training step(s), loss on training batch is 0.0863626.
After 5358 training step(s), loss on training batch is 0.108634.
After 5359 training step(s), loss on training batch is 0.0891001.
After 5360 training step(s), loss on training batch is 0.0772109.
After 5361 training step(s), loss on training batch is 0.0714516.
After 5362 training step(s), loss on training batch is 0.121313.
After 5363 training step(s), loss on training batch is 0.0884487.
After 5364 training step(s), loss on training batch is 0.0909022.
After 5365 training step(s), loss on training batch is 0.077257.
After 5366 training step(s), loss on training batch is 0.0846626.
After 5367 training step(s), loss on training batch is 0.0929548.
After 5368 training step(s), loss on training batch is 0.0729605.
After 5369 training step(s), loss on training batch is 0.0759198.
After 5370 training step(s), loss on training batch is 0.113766.
After 5371 training step(s), loss on training batch is 0.0349827.
After 5372 training step(s), loss on training batch is 0.0750368.
After 5373 training step(s), loss on training batch is 0.0819299.
After 5374 training step(s), loss on training batch is 0.085175.
After 5375 training step(s), loss on training batch is 0.0630294.
After 5376 training step(s), loss on training batch is 0.0794369.
After 5377 training step(s), loss on training batch is 0.0895962.
After 5378 training step(s), loss on training batch is 0.107151.
After 5379 training step(s), loss on training batch is 0.106739.
After 5380 training step(s), loss on training batch is 0.100898.
After 5381 training step(s), loss on training batch is 0.0346813.
After 5382 training step(s), loss on training batch is 0.063997.
After 5383 training step(s), loss on training batch is 0.133246.
After 5384 training step(s), loss on training batch is 0.146638.
After 5385 training step(s), loss on training batch is 0.0441817.
After 5386 training step(s), loss on training batch is 0.121258.
After 5387 training step(s), loss on training batch is 0.132479.
After 5388 training step(s), loss on training batch is 0.132502.
After 5389 training step(s), loss on training batch is 0.101549.
After 5390 training step(s), loss on training batch is 0.0589995.
After 5391 training step(s), loss on training batch is 0.0575015.
After 5392 training step(s), loss on training batch is 0.0829876.
After 5393 training step(s), loss on training batch is 0.0786316.
After 5394 training step(s), loss on training batch is 0.117739.
After 5395 training step(s), loss on training batch is 0.0851455.
After 5396 training step(s), loss on training batch is 0.0943872.
After 5397 training step(s), loss on training batch is 0.106531.
After 5398 training step(s), loss on training batch is 0.118581.
After 5399 training step(s), loss on training batch is 0.119593.
After 5400 training step(s), loss on training batch is 0.105722.
After 5401 training step(s), loss on training batch is 0.0697159.
After 5402 training step(s), loss on training batch is 0.0810414.
After 5403 training step(s), loss on training batch is 0.0694622.
After 5404 training step(s), loss on training batch is 0.121881.
After 5405 training step(s), loss on training batch is 0.105597.
After 5406 training step(s), loss on training batch is 0.0600402.
After 5407 training step(s), loss on training batch is 0.129568.
After 5408 training step(s), loss on training batch is 0.0638642.
After 5409 training step(s), loss on training batch is 0.0548277.
After 5410 training step(s), loss on training batch is 0.151118.
After 5411 training step(s), loss on training batch is 0.101384.
After 5412 training step(s), loss on training batch is 0.078607.
After 5413 training step(s), loss on training batch is 0.141647.
After 5414 training step(s), loss on training batch is 0.127251.
After 5415 training step(s), loss on training batch is 0.0725694.
After 5416 training step(s), loss on training batch is 0.0639633.
After 5417 training step(s), loss on training batch is 0.0869715.
After 5418 training step(s), loss on training batch is 0.104407.
After 5419 training step(s), loss on training batch is 0.0805588.
After 5420 training step(s), loss on training batch is 0.0864664.
After 5421 training step(s), loss on training batch is 0.0887644.
After 5422 training step(s), loss on training batch is 0.107064.
After 5423 training step(s), loss on training batch is 0.137576.
After 5424 training step(s), loss on training batch is 0.0663542.
After 5425 training step(s), loss on training batch is 0.055491.
After 5426 training step(s), loss on training batch is 0.0972025.
After 5427 training step(s), loss on training batch is 0.0462645.
After 5428 training step(s), loss on training batch is 0.0695106.
After 5429 training step(s), loss on training batch is 0.0682092.
After 5430 training step(s), loss on training batch is 0.17913.
After 5431 training step(s), loss on training batch is 0.0781354.
After 5432 training step(s), loss on training batch is 0.0458919.
After 5433 training step(s), loss on training batch is 0.0540306.
After 5434 training step(s), loss on training batch is 0.0760868.
After 5435 training step(s), loss on training batch is 0.0987957.
After 5436 training step(s), loss on training batch is 0.071468.
After 5437 training step(s), loss on training batch is 0.0724821.
After 5438 training step(s), loss on training batch is 0.0410646.
After 5439 training step(s), loss on training batch is 0.126936.
After 5440 training step(s), loss on training batch is 0.11648.
After 5441 training step(s), loss on training batch is 0.0747563.
After 5442 training step(s), loss on training batch is 0.0959961.
After 5443 training step(s), loss on training batch is 0.0787041.
After 5444 training step(s), loss on training batch is 0.0648377.
After 5445 training step(s), loss on training batch is 0.10035.
After 5446 training step(s), loss on training batch is 0.102235.
After 5447 training step(s), loss on training batch is 0.106563.
After 5448 training step(s), loss on training batch is 0.0397694.
After 5449 training step(s), loss on training batch is 0.107182.
After 5450 training step(s), loss on training batch is 0.0833066.
After 5451 training step(s), loss on training batch is 0.0539531.
After 5452 training step(s), loss on training batch is 0.0553733.
After 5453 training step(s), loss on training batch is 0.0698405.
After 5454 training step(s), loss on training batch is 0.0759086.
After 5455 training step(s), loss on training batch is 0.0694548.
After 5456 training step(s), loss on training batch is 0.0584518.
After 5457 training step(s), loss on training batch is 0.0942251.
After 5458 training step(s), loss on training batch is 0.0886913.
After 5459 training step(s), loss on training batch is 0.0468334.
After 5460 training step(s), loss on training batch is 0.0529815.
After 5461 training step(s), loss on training batch is 0.0807544.
After 5462 training step(s), loss on training batch is 0.127076.
After 5463 training step(s), loss on training batch is 0.060215.
After 5464 training step(s), loss on training batch is 0.0400049.
After 5465 training step(s), loss on training batch is 0.0663305.
After 5466 training step(s), loss on training batch is 0.119447.
After 5467 training step(s), loss on training batch is 0.0975722.
After 5468 training step(s), loss on training batch is 0.0682367.
After 5469 training step(s), loss on training batch is 0.0885363.
After 5470 training step(s), loss on training batch is 0.10264.
After 5471 training step(s), loss on training batch is 0.0758478.
After 5472 training step(s), loss on training batch is 0.0770072.
After 5473 training step(s), loss on training batch is 0.127637.
After 5474 training step(s), loss on training batch is 0.131297.
After 5475 training step(s), loss on training batch is 0.0382855.
After 5476 training step(s), loss on training batch is 0.0531227.
After 5477 training step(s), loss on training batch is 0.0917708.
After 5478 training step(s), loss on training batch is 0.100402.
After 5479 training step(s), loss on training batch is 0.0549042.
After 5480 training step(s), loss on training batch is 0.0789593.
After 5481 training step(s), loss on training batch is 0.070248.
After 5482 training step(s), loss on training batch is 0.0305884.
After 5483 training step(s), loss on training batch is 0.0687201.
After 5484 training step(s), loss on training batch is 0.0626987.
After 5485 training step(s), loss on training batch is 0.061069.
After 5486 training step(s), loss on training batch is 0.094629.
After 5487 training step(s), loss on training batch is 0.0903979.
After 5488 training step(s), loss on training batch is 0.0843232.
After 5489 training step(s), loss on training batch is 0.0755439.
After 5490 training step(s), loss on training batch is 0.0545818.
After 5491 training step(s), loss on training batch is 0.0355885.
After 5492 training step(s), loss on training batch is 0.0871919.
After 5493 training step(s), loss on training batch is 0.0743666.
After 5494 training step(s), loss on training batch is 0.102105.
After 5495 training step(s), loss on training batch is 0.0556298.
After 5496 training step(s), loss on training batch is 0.0791275.
After 5497 training step(s), loss on training batch is 0.0994857.
After 5498 training step(s), loss on training batch is 0.0786132.
After 5499 training step(s), loss on training batch is 0.103712.
After 5500 training step(s), loss on training batch is 0.0300638.
After 5501 training step(s), loss on training batch is 0.0304398.
After 5502 training step(s), loss on training batch is 0.0367099.
After 5503 training step(s), loss on training batch is 0.0452151.
After 5504 training step(s), loss on training batch is 0.058583.
After 5505 training step(s), loss on training batch is 0.0316933.
After 5506 training step(s), loss on training batch is 0.0974498.
After 5507 training step(s), loss on training batch is 0.061605.
After 5508 training step(s), loss on training batch is 0.0551463.
After 5509 training step(s), loss on training batch is 0.0386486.
After 5510 training step(s), loss on training batch is 0.0676719.
After 5511 training step(s), loss on training batch is 0.0515738.
After 5512 training step(s), loss on training batch is 0.0784399.
After 5513 training step(s), loss on training batch is 0.0413055.
After 5514 training step(s), loss on training batch is 0.0420722.
After 5515 training step(s), loss on training batch is 0.0584639.
After 5516 training step(s), loss on training batch is 0.0649142.
After 5517 training step(s), loss on training batch is 0.0742903.
After 5518 training step(s), loss on training batch is 0.13856.
After 5519 training step(s), loss on training batch is 0.0775067.
After 5520 training step(s), loss on training batch is 0.0602267.
After 5521 training step(s), loss on training batch is 0.0978258.
After 5522 training step(s), loss on training batch is 0.0634093.
After 5523 training step(s), loss on training batch is 0.0593121.
After 5524 training step(s), loss on training batch is 0.069735.
After 5525 training step(s), loss on training batch is 0.0561514.
After 5526 training step(s), loss on training batch is 0.0616409.
After 5527 training step(s), loss on training batch is 0.0962531.
After 5528 training step(s), loss on training batch is 0.0722803.
After 5529 training step(s), loss on training batch is 0.0768135.
After 5530 training step(s), loss on training batch is 0.0921582.
After 5531 training step(s), loss on training batch is 0.0984495.
After 5532 training step(s), loss on training batch is 0.0821344.
After 5533 training step(s), loss on training batch is 0.0648031.
After 5534 training step(s), loss on training batch is 0.0711378.
After 5535 training step(s), loss on training batch is 0.089986.
After 5536 training step(s), loss on training batch is 0.0762844.
After 5537 training step(s), loss on training batch is 0.0441072.
After 5538 training step(s), loss on training batch is 0.136048.
After 5539 training step(s), loss on training batch is 0.117082.
After 5540 training step(s), loss on training batch is 0.0807131.
After 5541 training step(s), loss on training batch is 0.0544251.
After 5542 training step(s), loss on training batch is 0.0622074.
After 5543 training step(s), loss on training batch is 0.0478684.
After 5544 training step(s), loss on training batch is 0.0614804.
After 5545 training step(s), loss on training batch is 0.0559555.
After 5546 training step(s), loss on training batch is 0.0608215.
After 5547 training step(s), loss on training batch is 0.0537328.
After 5548 training step(s), loss on training batch is 0.040096.
After 5549 training step(s), loss on training batch is 0.0685361.
After 5550 training step(s), loss on training batch is 0.0723215.
After 5551 training step(s), loss on training batch is 0.0490091.
After 5552 training step(s), loss on training batch is 0.0412377.
After 5553 training step(s), loss on training batch is 0.0535849.
After 5554 training step(s), loss on training batch is 0.0766892.
After 5555 training step(s), loss on training batch is 0.0337937.
After 5556 training step(s), loss on training batch is 0.030819.
After 5557 training step(s), loss on training batch is 0.040431.
After 5558 training step(s), loss on training batch is 0.0453542.
After 5559 training step(s), loss on training batch is 0.0441562.
After 5560 training step(s), loss on training batch is 0.0805999.
After 5561 training step(s), loss on training batch is 0.0928702.
After 5562 training step(s), loss on training batch is 0.0322236.
After 5563 training step(s), loss on training batch is 0.0611175.
After 5564 training step(s), loss on training batch is 0.0813725.
After 5565 training step(s), loss on training batch is 0.0601014.
After 5566 training step(s), loss on training batch is 0.101469.
After 5567 training step(s), loss on training batch is 0.0377975.
After 5568 training step(s), loss on training batch is 0.138658.
After 5569 training step(s), loss on training batch is 0.127382.
After 5570 training step(s), loss on training batch is 0.0988749.
After 5571 training step(s), loss on training batch is 0.0620421.
After 5572 training step(s), loss on training batch is 0.0835653.
After 5573 training step(s), loss on training batch is 0.0441839.
After 5574 training step(s), loss on training batch is 0.027343.
After 5575 training step(s), loss on training batch is 0.0493506.
After 5576 training step(s), loss on training batch is 0.096358.
After 5577 training step(s), loss on training batch is 0.0371965.
After 5578 training step(s), loss on training batch is 0.0504633.
After 5579 training step(s), loss on training batch is 0.0386007.
After 5580 training step(s), loss on training batch is 0.0346414.
After 5581 training step(s), loss on training batch is 0.0592481.
After 5582 training step(s), loss on training batch is 0.0711561.
After 5583 training step(s), loss on training batch is 0.0406321.
After 5584 training step(s), loss on training batch is 0.0253579.
After 5585 training step(s), loss on training batch is 0.103065.
After 5586 training step(s), loss on training batch is 0.055102.
After 5587 training step(s), loss on training batch is 0.0974875.
After 5588 training step(s), loss on training batch is 0.0463966.
After 5589 training step(s), loss on training batch is 0.0552311.
After 5590 training step(s), loss on training batch is 0.0604353.
After 5591 training step(s), loss on training batch is 0.0556146.
After 5592 training step(s), loss on training batch is 0.0989621.
After 5593 training step(s), loss on training batch is 0.0507883.
After 5594 training step(s), loss on training batch is 0.0465424.
After 5595 training step(s), loss on training batch is 0.0514083.
After 5596 training step(s), loss on training batch is 0.0586854.
After 5597 training step(s), loss on training batch is 0.0397151.
After 5598 training step(s), loss on training batch is 0.0620702.
After 5599 training step(s), loss on training batch is 0.0615408.
After 5600 training step(s), loss on training batch is 0.034451.
After 5601 training step(s), loss on training batch is 0.0465275.
After 5602 training step(s), loss on training batch is 0.0464105.
After 5603 training step(s), loss on training batch is 0.0558271.
After 5604 training step(s), loss on training batch is 0.0524425.
After 5605 training step(s), loss on training batch is 0.0856366.
After 5606 training step(s), loss on training batch is 0.0586483.
After 5607 training step(s), loss on training batch is 0.0512833.
After 5608 training step(s), loss on training batch is 0.065336.
After 5609 training step(s), loss on training batch is 0.0502957.
After 5610 training step(s), loss on training batch is 0.0984211.
After 5611 training step(s), loss on training batch is 0.168757.
After 5612 training step(s), loss on training batch is 0.0865782.
After 5613 training step(s), loss on training batch is 0.0738032.
After 5614 training step(s), loss on training batch is 0.0428449.
After 5615 training step(s), loss on training batch is 0.0832194.
After 5616 training step(s), loss on training batch is 0.0921657.
After 5617 training step(s), loss on training batch is 0.109483.
After 5618 training step(s), loss on training batch is 0.0634813.
After 5619 training step(s), loss on training batch is 0.050447.
After 5620 training step(s), loss on training batch is 0.0928873.
After 5621 training step(s), loss on training batch is 0.0627145.
After 5622 training step(s), loss on training batch is 0.0656097.
After 5623 training step(s), loss on training batch is 0.0627693.
After 5624 training step(s), loss on training batch is 0.0444669.
After 5625 training step(s), loss on training batch is 0.0301296.
After 5626 training step(s), loss on training batch is 0.0821256.
After 5627 training step(s), loss on training batch is 0.0483061.
After 5628 training step(s), loss on training batch is 0.100982.
After 5629 training step(s), loss on training batch is 0.0561762.
After 5630 training step(s), loss on training batch is 0.0710724.
After 5631 training step(s), loss on training batch is 0.0602953.
After 5632 training step(s), loss on training batch is 0.0403463.
After 5633 training step(s), loss on training batch is 0.0372023.
After 5634 training step(s), loss on training batch is 0.0249702.
After 5635 training step(s), loss on training batch is 0.0479668.
After 5636 training step(s), loss on training batch is 0.0308751.
After 5637 training step(s), loss on training batch is 0.048195.
After 5638 training step(s), loss on training batch is 0.0535411.
After 5639 training step(s), loss on training batch is 0.0390159.
After 5640 training step(s), loss on training batch is 0.0904987.
After 5641 training step(s), loss on training batch is 0.0790824.
After 5642 training step(s), loss on training batch is 0.0384412.
After 5643 training step(s), loss on training batch is 0.0944295.
After 5644 training step(s), loss on training batch is 0.0361828.
After 5645 training step(s), loss on training batch is 0.0475208.
After 5646 training step(s), loss on training batch is 0.105839.
After 5647 training step(s), loss on training batch is 0.0421445.
After 5648 training step(s), loss on training batch is 0.0520382.
After 5649 training step(s), loss on training batch is 0.0696815.
After 5650 training step(s), loss on training batch is 0.0813231.
After 5651 training step(s), loss on training batch is 0.0486741.
After 5652 training step(s), loss on training batch is 0.0333675.
After 5653 training step(s), loss on training batch is 0.0835021.
After 5654 training step(s), loss on training batch is 0.0281313.
After 5655 training step(s), loss on training batch is 0.0906002.
After 5656 training step(s), loss on training batch is 0.0640587.
After 5657 training step(s), loss on training batch is 0.0578683.
After 5658 training step(s), loss on training batch is 0.0499902.
After 5659 training step(s), loss on training batch is 0.0444738.
After 5660 training step(s), loss on training batch is 0.0609687.
After 5661 training step(s), loss on training batch is 0.0957283.
After 5662 training step(s), loss on training batch is 0.0517077.
After 5663 training step(s), loss on training batch is 0.0933881.
After 5664 training step(s), loss on training batch is 0.0433198.
After 5665 training step(s), loss on training batch is 0.0312898.
After 5666 training step(s), loss on training batch is 0.0600646.
After 5667 training step(s), loss on training batch is 0.0699967.
After 5668 training step(s), loss on training batch is 0.0679388.
After 5669 training step(s), loss on training batch is 0.0855233.
After 5670 training step(s), loss on training batch is 0.10344.
After 5671 training step(s), loss on training batch is 0.0241186.
After 5672 training step(s), loss on training batch is 0.0461843.
After 5673 training step(s), loss on training batch is 0.0578396.
After 5674 training step(s), loss on training batch is 0.0474454.
After 5675 training step(s), loss on training batch is 0.0453727.
After 5676 training step(s), loss on training batch is 0.0401537.
After 5677 training step(s), loss on training batch is 0.0337515.
After 5678 training step(s), loss on training batch is 0.0465892.
After 5679 training step(s), loss on training batch is 0.0892532.
After 5680 training step(s), loss on training batch is 0.0340815.
After 5681 training step(s), loss on training batch is 0.0455949.
After 5682 training step(s), loss on training batch is 0.0406536.
After 5683 training step(s), loss on training batch is 0.0527643.
After 5684 training step(s), loss on training batch is 0.060679.
After 5685 training step(s), loss on training batch is 0.0498402.
After 5686 training step(s), loss on training batch is 0.0366276.
After 5687 training step(s), loss on training batch is 0.0721656.
After 5688 training step(s), loss on training batch is 0.0474737.
After 5689 training step(s), loss on training batch is 0.0623995.
After 5690 training step(s), loss on training batch is 0.119627.
After 5691 training step(s), loss on training batch is 0.0612526.
After 5692 training step(s), loss on training batch is 0.0266672.
After 5693 training step(s), loss on training batch is 0.0763563.
After 5694 training step(s), loss on training batch is 0.0376027.
After 5695 training step(s), loss on training batch is 0.049658.
After 5696 training step(s), loss on training batch is 0.0335511.
After 5697 training step(s), loss on training batch is 0.0519782.
After 5698 training step(s), loss on training batch is 0.034769.
After 5699 training step(s), loss on training batch is 0.0490363.
After 5700 training step(s), loss on training batch is 0.0536959.
After 5701 training step(s), loss on training batch is 0.0687064.
After 5702 training step(s), loss on training batch is 0.109527.
After 5703 training step(s), loss on training batch is 0.0358142.
After 5704 training step(s), loss on training batch is 0.0435896.
After 5705 training step(s), loss on training batch is 0.0501751.
After 5706 training step(s), loss on training batch is 0.0950084.
After 5707 training step(s), loss on training batch is 0.0665937.
After 5708 training step(s), loss on training batch is 0.0606248.
After 5709 training step(s), loss on training batch is 0.0802459.
After 5710 training step(s), loss on training batch is 0.119274.
After 5711 training step(s), loss on training batch is 0.0553702.
After 5712 training step(s), loss on training batch is 0.0346249.
After 5713 training step(s), loss on training batch is 0.0497553.
After 5714 training step(s), loss on training batch is 0.0535401.
After 5715 training step(s), loss on training batch is 0.079784.
After 5716 training step(s), loss on training batch is 0.0479825.
After 5717 training step(s), loss on training batch is 0.0514141.
After 5718 training step(s), loss on training batch is 0.0277672.
After 5719 training step(s), loss on training batch is 0.0502512.
After 5720 training step(s), loss on training batch is 0.0629706.
After 5721 training step(s), loss on training batch is 0.0680911.
After 5722 training step(s), loss on training batch is 0.0705989.
After 5723 training step(s), loss on training batch is 0.0468282.
After 5724 training step(s), loss on training batch is 0.0463888.
After 5725 training step(s), loss on training batch is 0.0540499.
After 5726 training step(s), loss on training batch is 0.0339467.
After 5727 training step(s), loss on training batch is 0.0617653.
After 5728 training step(s), loss on training batch is 0.0484999.
After 5729 training step(s), loss on training batch is 0.0418744.
After 5730 training step(s), loss on training batch is 0.0411851.
After 5731 training step(s), loss on training batch is 0.0496373.
After 5732 training step(s), loss on training batch is 0.0797907.
After 5733 training step(s), loss on training batch is 0.0535238.
After 5734 training step(s), loss on training batch is 0.0956028.
After 5735 training step(s), loss on training batch is 0.0617564.
After 5736 training step(s), loss on training batch is 0.0520906.
After 5737 training step(s), loss on training batch is 0.0344187.
After 5738 training step(s), loss on training batch is 0.0416235.
After 5739 training step(s), loss on training batch is 0.0644027.
After 5740 training step(s), loss on training batch is 0.0543545.
After 5741 training step(s), loss on training batch is 0.126654.
After 5742 training step(s), loss on training batch is 0.0986442.
After 5743 training step(s), loss on training batch is 0.0376179.
After 5744 training step(s), loss on training batch is 0.0667193.
After 5745 training step(s), loss on training batch is 0.047189.
After 5746 training step(s), loss on training batch is 0.0421315.
After 5747 training step(s), loss on training batch is 0.0354953.
After 5748 training step(s), loss on training batch is 0.0428476.
After 5749 training step(s), loss on training batch is 0.0512445.
After 5750 training step(s), loss on training batch is 0.0810191.
After 5751 training step(s), loss on training batch is 0.0386653.
After 5752 training step(s), loss on training batch is 0.0567707.
After 5753 training step(s), loss on training batch is 0.0473526.
After 5754 training step(s), loss on training batch is 0.0442439.
After 5755 training step(s), loss on training batch is 0.0360578.
After 5756 training step(s), loss on training batch is 0.0525152.
After 5757 training step(s), loss on training batch is 0.037919.
After 5758 training step(s), loss on training batch is 0.0829898.
After 5759 training step(s), loss on training batch is 0.0941268.
After 5760 training step(s), loss on training batch is 0.0791572.
After 5761 training step(s), loss on training batch is 0.0404373.
After 5762 training step(s), loss on training batch is 0.0652816.
After 5763 training step(s), loss on training batch is 0.0520006.
After 5764 training step(s), loss on training batch is 0.0941329.
After 5765 training step(s), loss on training batch is 0.0600907.
After 5766 training step(s), loss on training batch is 0.0411237.
After 5767 training step(s), loss on training batch is 0.0281114.
After 5768 training step(s), loss on training batch is 0.0360865.
After 5769 training step(s), loss on training batch is 0.0487727.
After 5770 training step(s), loss on training batch is 0.081305.
After 5771 training step(s), loss on training batch is 0.0664434.
After 5772 training step(s), loss on training batch is 0.0914248.
After 5773 training step(s), loss on training batch is 0.0410318.
After 5774 training step(s), loss on training batch is 0.0437044.
After 5775 training step(s), loss on training batch is 0.0442916.
After 5776 training step(s), loss on training batch is 0.0684594.
After 5777 training step(s), loss on training batch is 0.0737113.
After 5778 training step(s), loss on training batch is 0.0691123.
After 5779 training step(s), loss on training batch is 0.026864.
After 5780 training step(s), loss on training batch is 0.0393247.
After 5781 training step(s), loss on training batch is 0.0540995.
After 5782 training step(s), loss on training batch is 0.0586756.
After 5783 training step(s), loss on training batch is 0.0508618.
After 5784 training step(s), loss on training batch is 0.0606764.
After 5785 training step(s), loss on training batch is 0.0327251.
After 5786 training step(s), loss on training batch is 0.0327508.
After 5787 training step(s), loss on training batch is 0.0485362.
After 5788 training step(s), loss on training batch is 0.0364671.
After 5789 training step(s), loss on training batch is 0.0767027.
After 5790 training step(s), loss on training batch is 0.0650768.
After 5791 training step(s), loss on training batch is 0.0557976.
After 5792 training step(s), loss on training batch is 0.0260418.
After 5793 training step(s), loss on training batch is 0.0368996.
After 5794 training step(s), loss on training batch is 0.0220073.
After 5795 training step(s), loss on training batch is 0.0468125.
After 5796 training step(s), loss on training batch is 0.036363.
After 5797 training step(s), loss on training batch is 0.0370569.
After 5798 training step(s), loss on training batch is 0.0609952.
After 5799 training step(s), loss on training batch is 0.0402827.
After 5800 training step(s), loss on training batch is 0.0908539.
After 5801 training step(s), loss on training batch is 0.0317626.
After 5802 training step(s), loss on training batch is 0.0482669.
After 5803 training step(s), loss on training batch is 0.0423815.
After 5804 training step(s), loss on training batch is 0.0311469.
After 5805 training step(s), loss on training batch is 0.048144.
After 5806 training step(s), loss on training batch is 0.0749967.
After 5807 training step(s), loss on training batch is 0.0352961.
After 5808 training step(s), loss on training batch is 0.0662439.
After 5809 training step(s), loss on training batch is 0.0344125.
After 5810 training step(s), loss on training batch is 0.0355182.
After 5811 training step(s), loss on training batch is 0.047768.
After 5812 training step(s), loss on training batch is 0.0487398.
After 5813 training step(s), loss on training batch is 0.0437058.
After 5814 training step(s), loss on training batch is 0.0712292.
After 5815 training step(s), loss on training batch is 0.0559126.
After 5816 training step(s), loss on training batch is 0.0259864.
After 5817 training step(s), loss on training batch is 0.0523417.
After 5818 training step(s), loss on training batch is 0.0336901.
After 5819 training step(s), loss on training batch is 0.0393597.
After 5820 training step(s), loss on training batch is 0.0333194.
After 5821 training step(s), loss on training batch is 0.0601941.
After 5822 training step(s), loss on training batch is 0.0537018.
After 5823 training step(s), loss on training batch is 0.0278633.
After 5824 training step(s), loss on training batch is 0.0348783.
After 5825 training step(s), loss on training batch is 0.0294645.
After 5826 training step(s), loss on training batch is 0.0309929.
After 5827 training step(s), loss on training batch is 0.0318674.
After 5828 training step(s), loss on training batch is 0.0412475.
After 5829 training step(s), loss on training batch is 0.0206561.
After 5830 training step(s), loss on training batch is 0.0263164.
After 5831 training step(s), loss on training batch is 0.0391604.
After 5832 training step(s), loss on training batch is 0.0361765.
After 5833 training step(s), loss on training batch is 0.046036.
After 5834 training step(s), loss on training batch is 0.0415236.
After 5835 training step(s), loss on training batch is 0.036247.
After 5836 training step(s), loss on training batch is 0.0295168.
After 5837 training step(s), loss on training batch is 0.0430426.
After 5838 training step(s), loss on training batch is 0.0753681.
After 5839 training step(s), loss on training batch is 0.0385593.
After 5840 training step(s), loss on training batch is 0.0681978.
After 5841 training step(s), loss on training batch is 0.0375841.
After 5842 training step(s), loss on training batch is 0.0655479.
After 5843 training step(s), loss on training batch is 0.0703787.
After 5844 training step(s), loss on training batch is 0.0390571.
After 5845 training step(s), loss on training batch is 0.0448146.
After 5846 training step(s), loss on training batch is 0.0424921.
After 5847 training step(s), loss on training batch is 0.0266051.
After 5848 training step(s), loss on training batch is 0.0755385.
After 5849 training step(s), loss on training batch is 0.0504538.
After 5850 training step(s), loss on training batch is 0.0259437.
After 5851 training step(s), loss on training batch is 0.0875271.
After 5852 training step(s), loss on training batch is 0.0214913.
After 5853 training step(s), loss on training batch is 0.0444185.
After 5854 training step(s), loss on training batch is 0.0315164.
After 5855 training step(s), loss on training batch is 0.0220287.
After 5856 training step(s), loss on training batch is 0.0426519.
After 5857 training step(s), loss on training batch is 0.0387401.
After 5858 training step(s), loss on training batch is 0.0543619.
After 5859 training step(s), loss on training batch is 0.053895.
After 5860 training step(s), loss on training batch is 0.0300642.
After 5861 training step(s), loss on training batch is 0.0478943.
After 5862 training step(s), loss on training batch is 0.0265963.
After 5863 training step(s), loss on training batch is 0.0184293.
After 5864 training step(s), loss on training batch is 0.029253.
After 5865 training step(s), loss on training batch is 0.0215174.
After 5866 training step(s), loss on training batch is 0.0445765.
After 5867 training step(s), loss on training batch is 0.0591443.
After 5868 training step(s), loss on training batch is 0.0403497.
After 5869 training step(s), loss on training batch is 0.0322162.
After 5870 training step(s), loss on training batch is 0.0846335.
After 5871 training step(s), loss on training batch is 0.0571466.
After 5872 training step(s), loss on training batch is 0.0404577.
After 5873 training step(s), loss on training batch is 0.0553483.
After 5874 training step(s), loss on training batch is 0.0361953.
After 5875 training step(s), loss on training batch is 0.0681242.
After 5876 training step(s), loss on training batch is 0.0316966.
After 5877 training step(s), loss on training batch is 0.0423362.
After 5878 training step(s), loss on training batch is 0.062703.
After 5879 training step(s), loss on training batch is 0.0725186.
After 5880 training step(s), loss on training batch is 0.0729881.
After 5881 training step(s), loss on training batch is 0.0633375.
After 5882 training step(s), loss on training batch is 0.0511025.
After 5883 training step(s), loss on training batch is 0.0254614.
After 5884 training step(s), loss on training batch is 0.047557.
After 5885 training step(s), loss on training batch is 0.0237235.
After 5886 training step(s), loss on training batch is 0.029778.
After 5887 training step(s), loss on training batch is 0.0731265.
After 5888 training step(s), loss on training batch is 0.0519211.
After 5889 training step(s), loss on training batch is 0.0671847.
After 5890 training step(s), loss on training batch is 0.0592773.
After 5891 training step(s), loss on training batch is 0.0594831.
After 5892 training step(s), loss on training batch is 0.0356313.
After 5893 training step(s), loss on training batch is 0.0707329.
After 5894 training step(s), loss on training batch is 0.0407812.
After 5895 training step(s), loss on training batch is 0.0435413.
After 5896 training step(s), loss on training batch is 0.0272029.
After 5897 training step(s), loss on training batch is 0.0471509.
After 5898 training step(s), loss on training batch is 0.0228695.
After 5899 training step(s), loss on training batch is 0.0636487.
After 5900 training step(s), loss on training batch is 0.0634279.
After 5901 training step(s), loss on training batch is 0.0588836.
After 5902 training step(s), loss on training batch is 0.0234863.
After 5903 training step(s), loss on training batch is 0.0361916.
After 5904 training step(s), loss on training batch is 0.0564974.
After 5905 training step(s), loss on training batch is 0.0395558.
After 5906 training step(s), loss on training batch is 0.0556172.
After 5907 training step(s), loss on training batch is 0.0795969.
After 5908 training step(s), loss on training batch is 0.075894.
After 5909 training step(s), loss on training batch is 0.03579.
After 5910 training step(s), loss on training batch is 0.0704582.
After 5911 training step(s), loss on training batch is 0.0533788.
After 5912 training step(s), loss on training batch is 0.0389733.
After 5913 training step(s), loss on training batch is 0.0621326.
After 5914 training step(s), loss on training batch is 0.0439492.
After 5915 training step(s), loss on training batch is 0.0403597.
After 5916 training step(s), loss on training batch is 0.0645568.
After 5917 training step(s), loss on training batch is 0.0234647.
After 5918 training step(s), loss on training batch is 0.0573065.
After 5919 training step(s), loss on training batch is 0.0371809.
After 5920 training step(s), loss on training batch is 0.0460454.
After 5921 training step(s), loss on training batch is 0.0300519.
After 5922 training step(s), loss on training batch is 0.0515178.
After 5923 training step(s), loss on training batch is 0.0648668.
After 5924 training step(s), loss on training batch is 0.037698.
After 5925 training step(s), loss on training batch is 0.0300721.
After 5926 training step(s), loss on training batch is 0.0506378.
After 5927 training step(s), loss on training batch is 0.050017.
After 5928 training step(s), loss on training batch is 0.067614.
After 5929 training step(s), loss on training batch is 0.0704678.
After 5930 training step(s), loss on training batch is 0.0295669.
After 5931 training step(s), loss on training batch is 0.0333544.
After 5932 training step(s), loss on training batch is 0.0289757.
After 5933 training step(s), loss on training batch is 0.0333777.
After 5934 training step(s), loss on training batch is 0.0810952.
After 5935 training step(s), loss on training batch is 0.0688193.
After 5936 training step(s), loss on training batch is 0.0569717.
After 5937 training step(s), loss on training batch is 0.0286555.
After 5938 training step(s), loss on training batch is 0.0278639.
After 5939 training step(s), loss on training batch is 0.0455028.
After 5940 training step(s), loss on training batch is 0.0339607.
After 5941 training step(s), loss on training batch is 0.0598216.
After 5942 training step(s), loss on training batch is 0.0262504.
After 5943 training step(s), loss on training batch is 0.0507316.
After 5944 training step(s), loss on training batch is 0.0239029.
After 5945 training step(s), loss on training batch is 0.0382536.
After 5946 training step(s), loss on training batch is 0.0231576.
After 5947 training step(s), loss on training batch is 0.0438347.
After 5948 training step(s), loss on training batch is 0.0331686.
After 5949 training step(s), loss on training batch is 0.0264512.
After 5950 training step(s), loss on training batch is 0.0570622.
After 5951 training step(s), loss on training batch is 0.0214025.
After 5952 training step(s), loss on training batch is 0.034995.
After 5953 training step(s), loss on training batch is 0.030265.
After 5954 training step(s), loss on training batch is 0.043654.
After 5955 training step(s), loss on training batch is 0.0438221.
After 5956 training step(s), loss on training batch is 0.033682.
After 5957 training step(s), loss on training batch is 0.0810263.
After 5958 training step(s), loss on training batch is 0.0294688.
After 5959 training step(s), loss on training batch is 0.04439.
After 5960 training step(s), loss on training batch is 0.0579805.
After 5961 training step(s), loss on training batch is 0.0578148.
After 5962 training step(s), loss on training batch is 0.0641748.
After 5963 training step(s), loss on training batch is 0.034274.
After 5964 training step(s), loss on training batch is 0.0402303.
After 5965 training step(s), loss on training batch is 0.0433777.
After 5966 training step(s), loss on training batch is 0.0363467.
After 5967 training step(s), loss on training batch is 0.0349797.
After 5968 training step(s), loss on training batch is 0.0257968.
After 5969 training step(s), loss on training batch is 0.0323986.
After 5970 training step(s), loss on training batch is 0.0297812.
After 5971 training step(s), loss on training batch is 0.0463264.
After 5972 training step(s), loss on training batch is 0.0338806.
After 5973 training step(s), loss on training batch is 0.0635063.
After 5974 training step(s), loss on training batch is 0.0598526.
After 5975 training step(s), loss on training batch is 0.0381563.
After 5976 training step(s), loss on training batch is 0.0343893.
After 5977 training step(s), loss on training batch is 0.0305022.
After 5978 training step(s), loss on training batch is 0.085655.
After 5979 training step(s), loss on training batch is 0.0552229.
After 5980 training step(s), loss on training batch is 0.0294802.
After 5981 training step(s), loss on training batch is 0.0357904.
After 5982 training step(s), loss on training batch is 0.0235237.
After 5983 training step(s), loss on training batch is 0.0595803.
After 5984 training step(s), loss on training batch is 0.0260487.
After 5985 training step(s), loss on training batch is 0.0498856.
After 5986 training step(s), loss on training batch is 0.0316987.
After 5987 training step(s), loss on training batch is 0.057977.
After 5988 training step(s), loss on training batch is 0.0270925.
After 5989 training step(s), loss on training batch is 0.0647389.
After 5990 training step(s), loss on training batch is 0.0958121.
After 5991 training step(s), loss on training batch is 0.0389453.
After 5992 training step(s), loss on training batch is 0.0808567.
After 5993 training step(s), loss on training batch is 0.0944877.
After 5994 training step(s), loss on training batch is 0.072145.
After 5995 training step(s), loss on training batch is 0.0634637.
After 5996 training step(s), loss on training batch is 0.066349.
After 5997 training step(s), loss on training batch is 0.0393089.
After 5998 training step(s), loss on training batch is 0.082874.
After 5999 training step(s), loss on training batch is 0.0679243.
After 6000 training step(s), loss on training batch is 0.0308598.
After 6001 training step(s), loss on training batch is 0.0494617.
After 6002 training step(s), loss on training batch is 0.0511582.
After 6003 training step(s), loss on training batch is 0.0284471.
After 6004 training step(s), loss on training batch is 0.122772.
After 6005 training step(s), loss on training batch is 0.0305535.
After 6006 training step(s), loss on training batch is 0.0428845.
After 6007 training step(s), loss on training batch is 0.0225826.
After 6008 training step(s), loss on training batch is 0.0566299.
After 6009 training step(s), loss on training batch is 0.0930535.
After 6010 training step(s), loss on training batch is 0.042117.
After 6011 training step(s), loss on training batch is 0.0319742.
After 6012 training step(s), loss on training batch is 0.0545815.
After 6013 training step(s), loss on training batch is 0.0971591.
After 6014 training step(s), loss on training batch is 0.0500016.
After 6015 training step(s), loss on training batch is 0.034454.
After 6016 training step(s), loss on training batch is 0.0863569.
After 6017 training step(s), loss on training batch is 0.0618468.
After 6018 training step(s), loss on training batch is 0.0478491.
After 6019 training step(s), loss on training batch is 0.0507038.
After 6020 training step(s), loss on training batch is 0.0752354.
After 6021 training step(s), loss on training batch is 0.0388054.
After 6022 training step(s), loss on training batch is 0.0266323.
After 6023 training step(s), loss on training batch is 0.0527864.
After 6024 training step(s), loss on training batch is 0.0393854.
After 6025 training step(s), loss on training batch is 0.0205496.
After 6026 training step(s), loss on training batch is 0.0549358.
After 6027 training step(s), loss on training batch is 0.047845.
After 6028 training step(s), loss on training batch is 0.0604666.
After 6029 training step(s), loss on training batch is 0.0295669.
After 6030 training step(s), loss on training batch is 0.0833655.
After 6031 training step(s), loss on training batch is 0.0502556.
After 6032 training step(s), loss on training batch is 0.0484234.
After 6033 training step(s), loss on training batch is 0.0726138.
After 6034 training step(s), loss on training batch is 0.0357623.
After 6035 training step(s), loss on training batch is 0.0388108.
After 6036 training step(s), loss on training batch is 0.0416215.
After 6037 training step(s), loss on training batch is 0.042445.
After 6038 training step(s), loss on training batch is 0.0340565.
After 6039 training step(s), loss on training batch is 0.0417052.
After 6040 training step(s), loss on training batch is 0.0393269.
After 6041 training step(s), loss on training batch is 0.0437189.
After 6042 training step(s), loss on training batch is 0.0329852.
After 6043 training step(s), loss on training batch is 0.0737365.
After 6044 training step(s), loss on training batch is 0.0221247.
After 6045 training step(s), loss on training batch is 0.0594689.
After 6046 training step(s), loss on training batch is 0.0475497.
After 6047 training step(s), loss on training batch is 0.0378018.
After 6048 training step(s), loss on training batch is 0.0618688.
After 6049 training step(s), loss on training batch is 0.0703903.
After 6050 training step(s), loss on training batch is 0.0414909.
After 6051 training step(s), loss on training batch is 0.0434755.
After 6052 training step(s), loss on training batch is 0.0527547.
After 6053 training step(s), loss on training batch is 0.0240613.
After 6054 training step(s), loss on training batch is 0.0319244.
After 6055 training step(s), loss on training batch is 0.0428322.
After 6056 training step(s), loss on training batch is 0.0301953.
After 6057 training step(s), loss on training batch is 0.0280996.
After 6058 training step(s), loss on training batch is 0.0319406.
After 6059 training step(s), loss on training batch is 0.0487423.
After 6060 training step(s), loss on training batch is 0.0530227.
After 6061 training step(s), loss on training batch is 0.0412156.
After 6062 training step(s), loss on training batch is 0.0372455.
After 6063 training step(s), loss on training batch is 0.047499.
After 6064 training step(s), loss on training batch is 0.0512095.
After 6065 training step(s), loss on training batch is 0.0439119.
After 6066 training step(s), loss on training batch is 0.0295001.
After 6067 training step(s), loss on training batch is 0.050392.
After 6068 training step(s), loss on training batch is 0.0377087.
After 6069 training step(s), loss on training batch is 0.0540116.
After 6070 training step(s), loss on training batch is 0.0311817.
After 6071 training step(s), loss on training batch is 0.0448616.
After 6072 training step(s), loss on training batch is 0.0413711.
After 6073 training step(s), loss on training batch is 0.041673.
After 6074 training step(s), loss on training batch is 0.0399042.
After 6075 training step(s), loss on training batch is 0.029332.
After 6076 training step(s), loss on training batch is 0.0290236.
After 6077 training step(s), loss on training batch is 0.0293934.
After 6078 training step(s), loss on training batch is 0.0268861.
After 6079 training step(s), loss on training batch is 0.0650933.
After 6080 training step(s), loss on training batch is 0.0550669.
After 6081 training step(s), loss on training batch is 0.063004.
After 6082 training step(s), loss on training batch is 0.0341754.
After 6083 training step(s), loss on training batch is 0.0267734.
After 6084 training step(s), loss on training batch is 0.0270781.
After 6085 training step(s), loss on training batch is 0.0292535.
After 6086 training step(s), loss on training batch is 0.026802.
After 6087 training step(s), loss on training batch is 0.0294144.
After 6088 training step(s), loss on training batch is 0.0170276.
After 6089 training step(s), loss on training batch is 0.025101.
After 6090 training step(s), loss on training batch is 0.0479687.
After 6091 training step(s), loss on training batch is 0.0276447.
After 6092 training step(s), loss on training batch is 0.0271058.
After 6093 training step(s), loss on training batch is 0.0264215.
After 6094 training step(s), loss on training batch is 0.0290325.
After 6095 training step(s), loss on training batch is 0.0187545.
After 6096 training step(s), loss on training batch is 0.0325501.
After 6097 training step(s), loss on training batch is 0.0308909.
After 6098 training step(s), loss on training batch is 0.0301708.
After 6099 training step(s), loss on training batch is 0.0238973.
After 6100 training step(s), loss on training batch is 0.020361.
After 6101 training step(s), loss on training batch is 0.0205739.
After 6102 training step(s), loss on training batch is 0.045537.
After 6103 training step(s), loss on training batch is 0.0277777.
After 6104 training step(s), loss on training batch is 0.0211879.
After 6105 training step(s), loss on training batch is 0.0182161.
After 6106 training step(s), loss on training batch is 0.0228938.
After 6107 training step(s), loss on training batch is 0.0506549.
After 6108 training step(s), loss on training batch is 0.02907.
After 6109 training step(s), loss on training batch is 0.0404494.
After 6110 training step(s), loss on training batch is 0.0265766.
After 6111 training step(s), loss on training batch is 0.0302811.
After 6112 training step(s), loss on training batch is 0.0928274.
After 6113 training step(s), loss on training batch is 0.0302139.
After 6114 training step(s), loss on training batch is 0.0682841.
After 6115 training step(s), loss on training batch is 0.0440248.
After 6116 training step(s), loss on training batch is 0.0281204.
After 6117 training step(s), loss on training batch is 0.0574042.
After 6118 training step(s), loss on training batch is 0.0444661.
After 6119 training step(s), loss on training batch is 0.0577108.
After 6120 training step(s), loss on training batch is 0.027.
After 6121 training step(s), loss on training batch is 0.036467.
After 6122 training step(s), loss on training batch is 0.0430076.
After 6123 training step(s), loss on training batch is 0.0374498.
After 6124 training step(s), loss on training batch is 0.0508428.
After 6125 training step(s), loss on training batch is 0.0439039.
After 6126 training step(s), loss on training batch is 0.0213134.
After 6127 training step(s), loss on training batch is 0.0388882.
After 6128 training step(s), loss on training batch is 0.0619191.
After 6129 training step(s), loss on training batch is 0.0371346.
After 6130 training step(s), loss on training batch is 0.0537712.
After 6131 training step(s), loss on training batch is 0.0597201.
After 6132 training step(s), loss on training batch is 0.0401009.
After 6133 training step(s), loss on training batch is 0.0372667.
After 6134 training step(s), loss on training batch is 0.0259587.
After 6135 training step(s), loss on training batch is 0.0368725.
After 6136 training step(s), loss on training batch is 0.0532712.
After 6137 training step(s), loss on training batch is 0.0469559.
After 6138 training step(s), loss on training batch is 0.0310809.
After 6139 training step(s), loss on training batch is 0.0282342.
After 6140 training step(s), loss on training batch is 0.057174.
After 6141 training step(s), loss on training batch is 0.0405906.
After 6142 training step(s), loss on training batch is 0.0533169.
After 6143 training step(s), loss on training batch is 0.0422493.
After 6144 training step(s), loss on training batch is 0.0558435.
After 6145 training step(s), loss on training batch is 0.0896505.
After 6146 training step(s), loss on training batch is 0.0247952.
After 6147 training step(s), loss on training batch is 0.0756778.
After 6148 training step(s), loss on training batch is 0.0463.
After 6149 training step(s), loss on training batch is 0.0478727.
After 6150 training step(s), loss on training batch is 0.0309794.
After 6151 training step(s), loss on training batch is 0.0248395.
After 6152 training step(s), loss on training batch is 0.0301802.
After 6153 training step(s), loss on training batch is 0.0651301.
After 6154 training step(s), loss on training batch is 0.0424568.
After 6155 training step(s), loss on training batch is 0.0251024.
After 6156 training step(s), loss on training batch is 0.0294913.
After 6157 training step(s), loss on training batch is 0.038641.
After 6158 training step(s), loss on training batch is 0.0288729.
After 6159 training step(s), loss on training batch is 0.0235398.
After 6160 training step(s), loss on training batch is 0.0189641.
After 6161 training step(s), loss on training batch is 0.0276355.
After 6162 training step(s), loss on training batch is 0.0251334.
After 6163 training step(s), loss on training batch is 0.0297135.
After 6164 training step(s), loss on training batch is 0.0493357.
After 6165 training step(s), loss on training batch is 0.0401354.
After 6166 training step(s), loss on training batch is 0.0307278.
After 6167 training step(s), loss on training batch is 0.0646902.
After 6168 training step(s), loss on training batch is 0.067663.
After 6169 training step(s), loss on training batch is 0.0260397.
After 6170 training step(s), loss on training batch is 0.0615597.
After 6171 training step(s), loss on training batch is 0.040454.
After 6172 training step(s), loss on training batch is 0.0160551.
After 6173 training step(s), loss on training batch is 0.0561665.
After 6174 training step(s), loss on training batch is 0.0323749.
After 6175 training step(s), loss on training batch is 0.0486773.
After 6176 training step(s), loss on training batch is 0.0385764.
After 6177 training step(s), loss on training batch is 0.037802.
After 6178 training step(s), loss on training batch is 0.0564572.
After 6179 training step(s), loss on training batch is 0.0262035.
After 6180 training step(s), loss on training batch is 0.0471735.
After 6181 training step(s), loss on training batch is 0.0284388.
After 6182 training step(s), loss on training batch is 0.0278805.
After 6183 training step(s), loss on training batch is 0.0417711.
After 6184 training step(s), loss on training batch is 0.037157.
After 6185 training step(s), loss on training batch is 0.0233783.
After 6186 training step(s), loss on training batch is 0.0223164.
After 6187 training step(s), loss on training batch is 0.0195549.
After 6188 training step(s), loss on training batch is 0.0449585.
After 6189 training step(s), loss on training batch is 0.0220306.
After 6190 training step(s), loss on training batch is 0.029095.
After 6191 training step(s), loss on training batch is 0.0672346.
After 6192 training step(s), loss on training batch is 0.0220483.
After 6193 training step(s), loss on training batch is 0.0394494.
After 6194 training step(s), loss on training batch is 0.032882.
After 6195 training step(s), loss on training batch is 0.0262947.
After 6196 training step(s), loss on training batch is 0.0451438.
After 6197 training step(s), loss on training batch is 0.0370706.
After 6198 training step(s), loss on training batch is 0.0376951.
After 6199 training step(s), loss on training batch is 0.0478784.
After 6200 training step(s), loss on training batch is 0.0284064.
After 6201 training step(s), loss on training batch is 0.0628674.
After 6202 training step(s), loss on training batch is 0.0688119.
After 6203 training step(s), loss on training batch is 0.0351529.
After 6204 training step(s), loss on training batch is 0.0351601.
After 6205 training step(s), loss on training batch is 0.0167046.
After 6206 training step(s), loss on training batch is 0.0288873.
After 6207 training step(s), loss on training batch is 0.0492248.
After 6208 training step(s), loss on training batch is 0.0517864.
After 6209 training step(s), loss on training batch is 0.0224372.
After 6210 training step(s), loss on training batch is 0.0263293.
After 6211 training step(s), loss on training batch is 0.0161539.
After 6212 training step(s), loss on training batch is 0.0173722.
After 6213 training step(s), loss on training batch is 0.0301761.
After 6214 training step(s), loss on training batch is 0.0578593.
After 6215 training step(s), loss on training batch is 0.0547256.
After 6216 training step(s), loss on training batch is 0.0230916.
After 6217 training step(s), loss on training batch is 0.0517724.
After 6218 training step(s), loss on training batch is 0.0583824.
After 6219 training step(s), loss on training batch is 0.0337576.
After 6220 training step(s), loss on training batch is 0.029261.
After 6221 training step(s), loss on training batch is 0.0373635.
After 6222 training step(s), loss on training batch is 0.0337466.
After 6223 training step(s), loss on training batch is 0.0261857.
After 6224 training step(s), loss on training batch is 0.0572186.
After 6225 training step(s), loss on training batch is 0.0512357.
After 6226 training step(s), loss on training batch is 0.0815592.
After 6227 training step(s), loss on training batch is 0.0338104.
After 6228 training step(s), loss on training batch is 0.0967448.
After 6229 training step(s), loss on training batch is 0.0238902.
After 6230 training step(s), loss on training batch is 0.0401434.
After 6231 training step(s), loss on training batch is 0.0339069.
After 6232 training step(s), loss on training batch is 0.0194742.
After 6233 training step(s), loss on training batch is 0.0409153.
After 6234 training step(s), loss on training batch is 0.0397916.
After 6235 training step(s), loss on training batch is 0.028497.
After 6236 training step(s), loss on training batch is 0.0206088.
After 6237 training step(s), loss on training batch is 0.0434282.
After 6238 training step(s), loss on training batch is 0.0162549.
After 6239 training step(s), loss on training batch is 0.0316216.
After 6240 training step(s), loss on training batch is 0.0723258.
After 6241 training step(s), loss on training batch is 0.0309443.
After 6242 training step(s), loss on training batch is 0.038036.
After 6243 training step(s), loss on training batch is 0.0186643.
After 6244 training step(s), loss on training batch is 0.0531386.
After 6245 training step(s), loss on training batch is 0.061488.
After 6246 training step(s), loss on training batch is 0.033731.
After 6247 training step(s), loss on training batch is 0.0321162.
After 6248 training step(s), loss on training batch is 0.0709351.
After 6249 training step(s), loss on training batch is 0.0256774.
After 6250 training step(s), loss on training batch is 0.0440937.
After 6251 training step(s), loss on training batch is 0.0550147.
After 6252 training step(s), loss on training batch is 0.031196.
After 6253 training step(s), loss on training batch is 0.0722183.
After 6254 training step(s), loss on training batch is 0.054075.
After 6255 training step(s), loss on training batch is 0.0304545.
After 6256 training step(s), loss on training batch is 0.0202304.
After 6257 training step(s), loss on training batch is 0.0264104.
After 6258 training step(s), loss on training batch is 0.0323742.
After 6259 training step(s), loss on training batch is 0.103229.
After 6260 training step(s), loss on training batch is 0.0442541.
After 6261 training step(s), loss on training batch is 0.0797538.
After 6262 training step(s), loss on training batch is 0.0196896.
After 6263 training step(s), loss on training batch is 0.0291585.
After 6264 training step(s), loss on training batch is 0.0289819.
After 6265 training step(s), loss on training batch is 0.0417696.
After 6266 training step(s), loss on training batch is 0.0304262.
After 6267 training step(s), loss on training batch is 0.025602.
After 6268 training step(s), loss on training batch is 0.0708874.
After 6269 training step(s), loss on training batch is 0.046053.
After 6270 training step(s), loss on training batch is 0.0331213.
After 6271 training step(s), loss on training batch is 0.0362757.
After 6272 training step(s), loss on training batch is 0.0241015.
After 6273 training step(s), loss on training batch is 0.047722.
After 6274 training step(s), loss on training batch is 0.0196964.
After 6275 training step(s), loss on training batch is 0.0233299.
After 6276 training step(s), loss on training batch is 0.031041.
After 6277 training step(s), loss on training batch is 0.0293542.
After 6278 training step(s), loss on training batch is 0.0280289.
After 6279 training step(s), loss on training batch is 0.0330407.
After 6280 training step(s), loss on training batch is 0.0209041.
After 6281 training step(s), loss on training batch is 0.0420636.
After 6282 training step(s), loss on training batch is 0.0607727.
After 6283 training step(s), loss on training batch is 0.0224315.
After 6284 training step(s), loss on training batch is 0.0508984.
After 6285 training step(s), loss on training batch is 0.021819.
After 6286 training step(s), loss on training batch is 0.0333352.
After 6287 training step(s), loss on training batch is 0.0172838.
After 6288 training step(s), loss on training batch is 0.0321176.
After 6289 training step(s), loss on training batch is 0.0445409.
After 6290 training step(s), loss on training batch is 0.0723081.
After 6291 training step(s), loss on training batch is 0.0640708.
After 6292 training step(s), loss on training batch is 0.0901584.
After 6293 training step(s), loss on training batch is 0.0572593.
After 6294 training step(s), loss on training batch is 0.0347106.
After 6295 training step(s), loss on training batch is 0.0258658.
After 6296 training step(s), loss on training batch is 0.0415776.
After 6297 training step(s), loss on training batch is 0.0692796.
After 6298 training step(s), loss on training batch is 0.0308873.
After 6299 training step(s), loss on training batch is 0.0320807.
After 6300 training step(s), loss on training batch is 0.028079.
After 6301 training step(s), loss on training batch is 0.0292902.
After 6302 training step(s), loss on training batch is 0.0287302.
After 6303 training step(s), loss on training batch is 0.0192071.
After 6304 training step(s), loss on training batch is 0.0485685.
After 6305 training step(s), loss on training batch is 0.0512334.
After 6306 training step(s), loss on training batch is 0.0667057.
After 6307 training step(s), loss on training batch is 0.0357173.
After 6308 training step(s), loss on training batch is 0.0212386.
After 6309 training step(s), loss on training batch is 0.0323142.
After 6310 training step(s), loss on training batch is 0.0392056.
After 6311 training step(s), loss on training batch is 0.036263.
After 6312 training step(s), loss on training batch is 0.0777076.
After 6313 training step(s), loss on training batch is 0.12836.
After 6314 training step(s), loss on training batch is 0.0408988.
After 6315 training step(s), loss on training batch is 0.0372041.
After 6316 training step(s), loss on training batch is 0.0233991.
After 6317 training step(s), loss on training batch is 0.0179166.
After 6318 training step(s), loss on training batch is 0.0453229.
After 6319 training step(s), loss on training batch is 0.0251186.
After 6320 training step(s), loss on training batch is 0.0625243.
After 6321 training step(s), loss on training batch is 0.0744024.
After 6322 training step(s), loss on training batch is 0.0736963.
After 6323 training step(s), loss on training batch is 0.0317194.
After 6324 training step(s), loss on training batch is 0.0233477.
After 6325 training step(s), loss on training batch is 0.104443.
After 6326 training step(s), loss on training batch is 0.0520729.
After 6327 training step(s), loss on training batch is 0.0390039.
After 6328 training step(s), loss on training batch is 0.0332745.
After 6329 training step(s), loss on training batch is 0.0330296.
After 6330 training step(s), loss on training batch is 0.0424207.
After 6331 training step(s), loss on training batch is 0.0333185.
After 6332 training step(s), loss on training batch is 0.033401.
After 6333 training step(s), loss on training batch is 0.0272201.
After 6334 training step(s), loss on training batch is 0.0325153.
After 6335 training step(s), loss on training batch is 0.0282574.
After 6336 training step(s), loss on training batch is 0.0504543.
After 6337 training step(s), loss on training batch is 0.0261847.
After 6338 training step(s), loss on training batch is 0.024333.
After 6339 training step(s), loss on training batch is 0.0306739.
After 6340 training step(s), loss on training batch is 0.0197169.
After 6341 training step(s), loss on training batch is 0.0288715.
After 6342 training step(s), loss on training batch is 0.0372253.
After 6343 training step(s), loss on training batch is 0.0229766.
After 6344 training step(s), loss on training batch is 0.0273691.
After 6345 training step(s), loss on training batch is 0.0218144.
After 6346 training step(s), loss on training batch is 0.0190208.
After 6347 training step(s), loss on training batch is 0.0346001.
After 6348 training step(s), loss on training batch is 0.0232381.
After 6349 training step(s), loss on training batch is 0.03028.
After 6350 training step(s), loss on training batch is 0.0501481.
After 6351 training step(s), loss on training batch is 0.0496401.
After 6352 training step(s), loss on training batch is 0.0200412.
After 6353 training step(s), loss on training batch is 0.0277324.
After 6354 training step(s), loss on training batch is 0.0247755.
After 6355 training step(s), loss on training batch is 0.023454.
After 6356 training step(s), loss on training batch is 0.0287114.
After 6357 training step(s), loss on training batch is 0.0457816.
After 6358 training step(s), loss on training batch is 0.0211945.
After 6359 training step(s), loss on training batch is 0.0243625.
After 6360 training step(s), loss on training batch is 0.0535099.
After 6361 training step(s), loss on training batch is 0.0287974.
After 6362 training step(s), loss on training batch is 0.0182526.
After 6363 training step(s), loss on training batch is 0.0181792.
After 6364 training step(s), loss on training batch is 0.0308096.
After 6365 training step(s), loss on training batch is 0.0593532.
After 6366 training step(s), loss on training batch is 0.0663707.
After 6367 training step(s), loss on training batch is 0.0884915.
After 6368 training step(s), loss on training batch is 0.0287579.
After 6369 training step(s), loss on training batch is 0.0401618.
After 6370 training step(s), loss on training batch is 0.0312248.
After 6371 training step(s), loss on training batch is 0.103659.
After 6372 training step(s), loss on training batch is 0.0202973.
After 6373 training step(s), loss on training batch is 0.0289334.
After 6374 training step(s), loss on training batch is 0.0345144.
After 6375 training step(s), loss on training batch is 0.0189492.
After 6376 training step(s), loss on training batch is 0.0312494.
After 6377 training step(s), loss on training batch is 0.0182505.
After 6378 training step(s), loss on training batch is 0.017995.
After 6379 training step(s), loss on training batch is 0.0509088.
After 6380 training step(s), loss on training batch is 0.0378634.
After 6381 training step(s), loss on training batch is 0.0222572.
After 6382 training step(s), loss on training batch is 0.0285001.
After 6383 training step(s), loss on training batch is 0.0293136.
After 6384 training step(s), loss on training batch is 0.0201922.
After 6385 training step(s), loss on training batch is 0.0226212.
After 6386 training step(s), loss on training batch is 0.0228718.
After 6387 training step(s), loss on training batch is 0.05543.
After 6388 training step(s), loss on training batch is 0.0491923.
After 6389 training step(s), loss on training batch is 0.0550125.
After 6390 training step(s), loss on training batch is 0.0283856.
After 6391 training step(s), loss on training batch is 0.0274661.
After 6392 training step(s), loss on training batch is 0.0414846.
After 6393 training step(s), loss on training batch is 0.0296734.
After 6394 training step(s), loss on training batch is 0.0228543.
After 6395 training step(s), loss on training batch is 0.0203817.
After 6396 training step(s), loss on training batch is 0.0403446.
After 6397 training step(s), loss on training batch is 0.0340812.
After 6398 training step(s), loss on training batch is 0.0208173.
After 6399 training step(s), loss on training batch is 0.0165659.
After 6400 training step(s), loss on training batch is 0.0485477.
After 6401 training step(s), loss on training batch is 0.037101.
After 6402 training step(s), loss on training batch is 0.0289038.
After 6403 training step(s), loss on training batch is 0.0254613.
After 6404 training step(s), loss on training batch is 0.0259521.
After 6405 training step(s), loss on training batch is 0.0374461.
After 6406 training step(s), loss on training batch is 0.0209023.
After 6407 training step(s), loss on training batch is 0.021861.
After 6408 training step(s), loss on training batch is 0.0187462.
After 6409 training step(s), loss on training batch is 0.043782.
After 6410 training step(s), loss on training batch is 0.0343226.
After 6411 training step(s), loss on training batch is 0.039034.
After 6412 training step(s), loss on training batch is 0.0384816.
After 6413 training step(s), loss on training batch is 0.0309264.
After 6414 training step(s), loss on training batch is 0.0766363.
After 6415 training step(s), loss on training batch is 0.0584744.
After 6416 training step(s), loss on training batch is 0.036376.
After 6417 training step(s), loss on training batch is 0.044066.
After 6418 training step(s), loss on training batch is 0.0296892.
After 6419 training step(s), loss on training batch is 0.0320117.
After 6420 training step(s), loss on training batch is 0.0164342.
After 6421 training step(s), loss on training batch is 0.0387209.
After 6422 training step(s), loss on training batch is 0.0254353.
After 6423 training step(s), loss on training batch is 0.0470808.
After 6424 training step(s), loss on training batch is 0.0446802.
After 6425 training step(s), loss on training batch is 0.0387777.
After 6426 training step(s), loss on training batch is 0.0520498.
After 6427 training step(s), loss on training batch is 0.0550931.
After 6428 training step(s), loss on training batch is 0.0283799.
After 6429 training step(s), loss on training batch is 0.0270857.
After 6430 training step(s), loss on training batch is 0.0353427.
After 6431 training step(s), loss on training batch is 0.0300138.
After 6432 training step(s), loss on training batch is 0.0249491.
After 6433 training step(s), loss on training batch is 0.0328599.
After 6434 training step(s), loss on training batch is 0.0748987.
After 6435 training step(s), loss on training batch is 0.0395206.
After 6436 training step(s), loss on training batch is 0.0374661.
After 6437 training step(s), loss on training batch is 0.0287228.
After 6438 training step(s), loss on training batch is 0.042513.
After 6439 training step(s), loss on training batch is 0.0534129.
After 6440 training step(s), loss on training batch is 0.057541.
After 6441 training step(s), loss on training batch is 0.0368321.
After 6442 training step(s), loss on training batch is 0.0341853.
After 6443 training step(s), loss on training batch is 0.0302623.
After 6444 training step(s), loss on training batch is 0.0361653.
After 6445 training step(s), loss on training batch is 0.0174404.
After 6446 training step(s), loss on training batch is 0.0301014.
After 6447 training step(s), loss on training batch is 0.0370225.
After 6448 training step(s), loss on training batch is 0.0379107.
After 6449 training step(s), loss on training batch is 0.0191908.
After 6450 training step(s), loss on training batch is 0.0380475.
After 6451 training step(s), loss on training batch is 0.0303152.
After 6452 training step(s), loss on training batch is 0.0193608.
After 6453 training step(s), loss on training batch is 0.0170479.
After 6454 training step(s), loss on training batch is 0.0485283.
After 6455 training step(s), loss on training batch is 0.0252509.
After 6456 training step(s), loss on training batch is 0.0250849.
After 6457 training step(s), loss on training batch is 0.0261414.
After 6458 training step(s), loss on training batch is 0.0190169.
After 6459 training step(s), loss on training batch is 0.0282744.
After 6460 training step(s), loss on training batch is 0.0279613.
After 6461 training step(s), loss on training batch is 0.0646986.
After 6462 training step(s), loss on training batch is 0.0176638.
After 6463 training step(s), loss on training batch is 0.0183408.
After 6464 training step(s), loss on training batch is 0.0257966.
After 6465 training step(s), loss on training batch is 0.0537486.
After 6466 training step(s), loss on training batch is 0.0930109.
After 6467 training step(s), loss on training batch is 0.0558524.
After 6468 training step(s), loss on training batch is 0.0297217.
After 6469 training step(s), loss on training batch is 0.022945.
After 6470 training step(s), loss on training batch is 0.0206108.
After 6471 training step(s), loss on training batch is 0.0252576.
After 6472 training step(s), loss on training batch is 0.0300043.
After 6473 training step(s), loss on training batch is 0.0380228.
After 6474 training step(s), loss on training batch is 0.0233803.
After 6475 training step(s), loss on training batch is 0.0651443.
After 6476 training step(s), loss on training batch is 0.0178103.
After 6477 training step(s), loss on training batch is 0.0315276.
After 6478 training step(s), loss on training batch is 0.0302244.
After 6479 training step(s), loss on training batch is 0.0167881.
After 6480 training step(s), loss on training batch is 0.0288519.
After 6481 training step(s), loss on training batch is 0.0315631.
After 6482 training step(s), loss on training batch is 0.0495926.
After 6483 training step(s), loss on training batch is 0.0245182.
After 6484 training step(s), loss on training batch is 0.0259686.
After 6485 training step(s), loss on training batch is 0.0417747.
After 6486 training step(s), loss on training batch is 0.0471194.
After 6487 training step(s), loss on training batch is 0.0773121.
After 6488 training step(s), loss on training batch is 0.0218909.
After 6489 training step(s), loss on training batch is 0.0309549.
After 6490 training step(s), loss on training batch is 0.0289918.
After 6491 training step(s), loss on training batch is 0.0137234.
After 6492 training step(s), loss on training batch is 0.0166343.
After 6493 training step(s), loss on training batch is 0.0268715.
After 6494 training step(s), loss on training batch is 0.016062.
After 6495 training step(s), loss on training batch is 0.0296288.
After 6496 training step(s), loss on training batch is 0.0337018.
After 6497 training step(s), loss on training batch is 0.0321146.
After 6498 training step(s), loss on training batch is 0.0194749.
After 6499 training step(s), loss on training batch is 0.0302676.
After 6500 training step(s), loss on training batch is 0.0487273.
After 6501 training step(s), loss on training batch is 0.0179369.
After 6502 training step(s), loss on training batch is 0.0332334.
After 6503 training step(s), loss on training batch is 0.0195416.
After 6504 training step(s), loss on training batch is 0.0851909.
After 6505 training step(s), loss on training batch is 0.0274398.
After 6506 training step(s), loss on training batch is 0.0495637.
After 6507 training step(s), loss on training batch is 0.0160685.
After 6508 training step(s), loss on training batch is 0.0281947.
After 6509 training step(s), loss on training batch is 0.0152247.
After 6510 training step(s), loss on training batch is 0.0345993.
After 6511 training step(s), loss on training batch is 0.0296901.
After 6512 training step(s), loss on training batch is 0.0332523.
After 6513 training step(s), loss on training batch is 0.0335373.
After 6514 training step(s), loss on training batch is 0.0356294.
After 6515 training step(s), loss on training batch is 0.0453092.
After 6516 training step(s), loss on training batch is 0.0333247.
After 6517 training step(s), loss on training batch is 0.0468117.
After 6518 training step(s), loss on training batch is 0.029095.
After 6519 training step(s), loss on training batch is 0.0383312.
After 6520 training step(s), loss on training batch is 0.0417642.
After 6521 training step(s), loss on training batch is 0.0577862.
After 6522 training step(s), loss on training batch is 0.0324149.
After 6523 training step(s), loss on training batch is 0.0173523.
After 6524 training step(s), loss on training batch is 0.0238483.
After 6525 training step(s), loss on training batch is 0.0180295.
After 6526 training step(s), loss on training batch is 0.0798018.
After 6527 training step(s), loss on training batch is 0.0424846.
After 6528 training step(s), loss on training batch is 0.0438657.
After 6529 training step(s), loss on training batch is 0.0168289.
After 6530 training step(s), loss on training batch is 0.0479274.
After 6531 training step(s), loss on training batch is 0.0149655.
After 6532 training step(s), loss on training batch is 0.0276676.
After 6533 training step(s), loss on training batch is 0.0225721.
After 6534 training step(s), loss on training batch is 0.0179609.
After 6535 training step(s), loss on training batch is 0.0225147.
After 6536 training step(s), loss on training batch is 0.0229175.
After 6537 training step(s), loss on training batch is 0.0163631.
After 6538 training step(s), loss on training batch is 0.0417695.
After 6539 training step(s), loss on training batch is 0.0231989.
After 6540 training step(s), loss on training batch is 0.0228437.
After 6541 training step(s), loss on training batch is 0.0363892.
After 6542 training step(s), loss on training batch is 0.020404.
After 6543 training step(s), loss on training batch is 0.0232735.
After 6544 training step(s), loss on training batch is 0.0197805.
After 6545 training step(s), loss on training batch is 0.0199348.
After 6546 training step(s), loss on training batch is 0.0396584.
After 6547 training step(s), loss on training batch is 0.0483625.
After 6548 training step(s), loss on training batch is 0.016149.
After 6549 training step(s), loss on training batch is 0.0202589.
After 6550 training step(s), loss on training batch is 0.0389826.
After 6551 training step(s), loss on training batch is 0.0428572.
After 6552 training step(s), loss on training batch is 0.0271002.
After 6553 training step(s), loss on training batch is 0.0181061.
After 6554 training step(s), loss on training batch is 0.0488936.
After 6555 training step(s), loss on training batch is 0.0192966.
After 6556 training step(s), loss on training batch is 0.0260807.
After 6557 training step(s), loss on training batch is 0.0185392.
After 6558 training step(s), loss on training batch is 0.0230668.
After 6559 training step(s), loss on training batch is 0.0219803.
After 6560 training step(s), loss on training batch is 0.0156628.
After 6561 training step(s), loss on training batch is 0.0165155.
After 6562 training step(s), loss on training batch is 0.0422865.
After 6563 training step(s), loss on training batch is 0.0299647.
After 6564 training step(s), loss on training batch is 0.0175969.
After 6565 training step(s), loss on training batch is 0.0148464.
After 6566 training step(s), loss on training batch is 0.0348383.
After 6567 training step(s), loss on training batch is 0.0264433.
After 6568 training step(s), loss on training batch is 0.0494678.
After 6569 training step(s), loss on training batch is 0.0175485.
After 6570 training step(s), loss on training batch is 0.026633.
After 6571 training step(s), loss on training batch is 0.0557062.
After 6572 training step(s), loss on training batch is 0.0163839.
After 6573 training step(s), loss on training batch is 0.0260634.
After 6574 training step(s), loss on training batch is 0.0205489.
After 6575 training step(s), loss on training batch is 0.0298224.
After 6576 training step(s), loss on training batch is 0.0258264.
After 6577 training step(s), loss on training batch is 0.0512472.
After 6578 training step(s), loss on training batch is 0.0199593.
After 6579 training step(s), loss on training batch is 0.0253132.
After 6580 training step(s), loss on training batch is 0.0423787.
After 6581 training step(s), loss on training batch is 0.016071.
After 6582 training step(s), loss on training batch is 0.0671698.
After 6583 training step(s), loss on training batch is 0.0226434.
After 6584 training step(s), loss on training batch is 0.0164611.
After 6585 training step(s), loss on training batch is 0.0588261.
After 6586 training step(s), loss on training batch is 0.0693679.
After 6587 training step(s), loss on training batch is 0.0409578.
After 6588 training step(s), loss on training batch is 0.0395483.
After 6589 training step(s), loss on training batch is 0.0551238.
After 6590 training step(s), loss on training batch is 0.0229618.
After 6591 training step(s), loss on training batch is 0.0242414.
After 6592 training step(s), loss on training batch is 0.0324249.
After 6593 training step(s), loss on training batch is 0.0316321.
After 6594 training step(s), loss on training batch is 0.0595322.
After 6595 training step(s), loss on training batch is 0.0298679.
After 6596 training step(s), loss on training batch is 0.0343317.
After 6597 training step(s), loss on training batch is 0.0269952.
After 6598 training step(s), loss on training batch is 0.0300671.
After 6599 training step(s), loss on training batch is 0.0240595.
After 6600 training step(s), loss on training batch is 0.0351978.
After 6601 training step(s), loss on training batch is 0.0322438.
After 6602 training step(s), loss on training batch is 0.0305126.
After 6603 training step(s), loss on training batch is 0.0184254.
After 6604 training step(s), loss on training batch is 0.045842.
After 6605 training step(s), loss on training batch is 0.067101.
After 6606 training step(s), loss on training batch is 0.021845.
After 6607 training step(s), loss on training batch is 0.0302044.
After 6608 training step(s), loss on training batch is 0.0305492.
After 6609 training step(s), loss on training batch is 0.0328491.
After 6610 training step(s), loss on training batch is 0.0194264.
After 6611 training step(s), loss on training batch is 0.0205073.
After 6612 training step(s), loss on training batch is 0.0171053.
After 6613 training step(s), loss on training batch is 0.0253456.
After 6614 training step(s), loss on training batch is 0.0207504.
After 6615 training step(s), loss on training batch is 0.0379482.
After 6616 training step(s), loss on training batch is 0.0248784.
After 6617 training step(s), loss on training batch is 0.0333847.
After 6618 training step(s), loss on training batch is 0.0315928.
After 6619 training step(s), loss on training batch is 0.0273585.
After 6620 training step(s), loss on training batch is 0.0256348.
After 6621 training step(s), loss on training batch is 0.0251584.
After 6622 training step(s), loss on training batch is 0.0175511.
After 6623 training step(s), loss on training batch is 0.0287723.
After 6624 training step(s), loss on training batch is 0.020424.
After 6625 training step(s), loss on training batch is 0.021464.
After 6626 training step(s), loss on training batch is 0.045842.
After 6627 training step(s), loss on training batch is 0.0311689.
After 6628 training step(s), loss on training batch is 0.0222475.
After 6629 training step(s), loss on training batch is 0.0173772.
After 6630 training step(s), loss on training batch is 0.0214112.
After 6631 training step(s), loss on training batch is 0.0171287.
After 6632 training step(s), loss on training batch is 0.06678.
After 6633 training step(s), loss on training batch is 0.0269206.
After 6634 training step(s), loss on training batch is 0.0233596.
After 6635 training step(s), loss on training batch is 0.0181481.
After 6636 training step(s), loss on training batch is 0.0455147.
After 6637 training step(s), loss on training batch is 0.0248347.
After 6638 training step(s), loss on training batch is 0.0403307.
After 6639 training step(s), loss on training batch is 0.0192032.
After 6640 training step(s), loss on training batch is 0.0236703.
After 6641 training step(s), loss on training batch is 0.0164251.
After 6642 training step(s), loss on training batch is 0.0183966.
After 6643 training step(s), loss on training batch is 0.0347475.
After 6644 training step(s), loss on training batch is 0.025358.
After 6645 training step(s), loss on training batch is 0.0216901.
After 6646 training step(s), loss on training batch is 0.0309582.
After 6647 training step(s), loss on training batch is 0.0212734.
After 6648 training step(s), loss on training batch is 0.0226128.
After 6649 training step(s), loss on training batch is 0.0264542.
After 6650 training step(s), loss on training batch is 0.0411262.
After 6651 training step(s), loss on training batch is 0.0177815.
After 6652 training step(s), loss on training batch is 0.0172406.
After 6653 training step(s), loss on training batch is 0.0278804.
After 6654 training step(s), loss on training batch is 0.0556686.
After 6655 training step(s), loss on training batch is 0.0279225.
After 6656 training step(s), loss on training batch is 0.0385147.
After 6657 training step(s), loss on training batch is 0.0308856.
After 6658 training step(s), loss on training batch is 0.0294695.
After 6659 training step(s), loss on training batch is 0.0306265.
After 6660 training step(s), loss on training batch is 0.0208668.
After 6661 training step(s), loss on training batch is 0.0317651.
After 6662 training step(s), loss on training batch is 0.0359457.
After 6663 training step(s), loss on training batch is 0.0275894.
After 6664 training step(s), loss on training batch is 0.0212685.
After 6665 training step(s), loss on training batch is 0.0417909.
After 6666 training step(s), loss on training batch is 0.0207661.
After 6667 training step(s), loss on training batch is 0.0245936.
After 6668 training step(s), loss on training batch is 0.0201564.
After 6669 training step(s), loss on training batch is 0.0169303.
After 6670 training step(s), loss on training batch is 0.0224387.
After 6671 training step(s), loss on training batch is 0.0198911.
After 6672 training step(s), loss on training batch is 0.0204286.
After 6673 training step(s), loss on training batch is 0.0254041.
After 6674 training step(s), loss on training batch is 0.0158617.
After 6675 training step(s), loss on training batch is 0.0177203.
After 6676 training step(s), loss on training batch is 0.0175819.
After 6677 training step(s), loss on training batch is 0.0167953.
After 6678 training step(s), loss on training batch is 0.0494266.
After 6679 training step(s), loss on training batch is 0.0285004.
After 6680 training step(s), loss on training batch is 0.0257278.
After 6681 training step(s), loss on training batch is 0.024858.
After 6682 training step(s), loss on training batch is 0.0386495.
After 6683 training step(s), loss on training batch is 0.0313995.
After 6684 training step(s), loss on training batch is 0.0373017.
After 6685 training step(s), loss on training batch is 0.0269371.
After 6686 training step(s), loss on training batch is 0.0155619.
After 6687 training step(s), loss on training batch is 0.039867.
After 6688 training step(s), loss on training batch is 0.0176683.
After 6689 training step(s), loss on training batch is 0.0249945.
After 6690 training step(s), loss on training batch is 0.0139892.
After 6691 training step(s), loss on training batch is 0.0196854.
After 6692 training step(s), loss on training batch is 0.0194241.
After 6693 training step(s), loss on training batch is 0.0261145.
After 6694 training step(s), loss on training batch is 0.0374338.
After 6695 training step(s), loss on training batch is 0.0139543.
After 6696 training step(s), loss on training batch is 0.0210568.
After 6697 training step(s), loss on training batch is 0.0175794.
After 6698 training step(s), loss on training batch is 0.0193291.
After 6699 training step(s), loss on training batch is 0.0497764.
After 6700 training step(s), loss on training batch is 0.0732991.
After 6701 training step(s), loss on training batch is 0.0289753.
After 6702 training step(s), loss on training batch is 0.021888.
After 6703 training step(s), loss on training batch is 0.0187764.
After 6704 training step(s), loss on training batch is 0.0476233.
After 6705 training step(s), loss on training batch is 0.0349751.
After 6706 training step(s), loss on training batch is 0.0552635.
After 6707 training step(s), loss on training batch is 0.0317816.
After 6708 training step(s), loss on training batch is 0.0246355.
After 6709 training step(s), loss on training batch is 0.0201265.
After 6710 training step(s), loss on training batch is 0.0166063.
After 6711 training step(s), loss on training batch is 0.0261911.
After 6712 training step(s), loss on training batch is 0.023462.
After 6713 training step(s), loss on training batch is 0.0149452.
After 6714 training step(s), loss on training batch is 0.0180826.
After 6715 training step(s), loss on training batch is 0.0246423.
After 6716 training step(s), loss on training batch is 0.0154592.
After 6717 training step(s), loss on training batch is 0.0272752.
After 6718 training step(s), loss on training batch is 0.0293287.
After 6719 training step(s), loss on training batch is 0.0421756.
After 6720 training step(s), loss on training batch is 0.0189576.
After 6721 training step(s), loss on training batch is 0.0219853.
After 6722 training step(s), loss on training batch is 0.0222609.
After 6723 training step(s), loss on training batch is 0.0200747.
After 6724 training step(s), loss on training batch is 0.0142011.
After 6725 training step(s), loss on training batch is 0.0173915.
After 6726 training step(s), loss on training batch is 0.0229612.
After 6727 training step(s), loss on training batch is 0.0450186.
After 6728 training step(s), loss on training batch is 0.0208722.
After 6729 training step(s), loss on training batch is 0.0204986.
After 6730 training step(s), loss on training batch is 0.0226667.
After 6731 training step(s), loss on training batch is 0.0229641.
After 6732 training step(s), loss on training batch is 0.0572334.
After 6733 training step(s), loss on training batch is 0.0309206.
After 6734 training step(s), loss on training batch is 0.0524405.
After 6735 training step(s), loss on training batch is 0.030459.
After 6736 training step(s), loss on training batch is 0.0195391.
After 6737 training step(s), loss on training batch is 0.0225789.
After 6738 training step(s), loss on training batch is 0.0354827.
After 6739 training step(s), loss on training batch is 0.0227544.
After 6740 training step(s), loss on training batch is 0.0300256.
After 6741 training step(s), loss on training batch is 0.0360001.
After 6742 training step(s), loss on training batch is 0.0447197.
After 6743 training step(s), loss on training batch is 0.0554125.
After 6744 training step(s), loss on training batch is 0.0191084.
After 6745 training step(s), loss on training batch is 0.0317757.
After 6746 training step(s), loss on training batch is 0.0234599.
After 6747 training step(s), loss on training batch is 0.0182966.
After 6748 training step(s), loss on training batch is 0.0212656.
After 6749 training step(s), loss on training batch is 0.0181788.
After 6750 training step(s), loss on training batch is 0.0161192.
After 6751 training step(s), loss on training batch is 0.0277382.
After 6752 training step(s), loss on training batch is 0.018627.
After 6753 training step(s), loss on training batch is 0.0211029.
After 6754 training step(s), loss on training batch is 0.0232653.
After 6755 training step(s), loss on training batch is 0.042977.
After 6756 training step(s), loss on training batch is 0.0350329.
After 6757 training step(s), loss on training batch is 0.0304284.
After 6758 training step(s), loss on training batch is 0.0324877.
After 6759 training step(s), loss on training batch is 0.0228262.
After 6760 training step(s), loss on training batch is 0.0331272.
After 6761 training step(s), loss on training batch is 0.0281827.
After 6762 training step(s), loss on training batch is 0.0445221.
After 6763 training step(s), loss on training batch is 0.0162447.
After 6764 training step(s), loss on training batch is 0.0311301.
After 6765 training step(s), loss on training batch is 0.0150271.
After 6766 training step(s), loss on training batch is 0.022964.
After 6767 training step(s), loss on training batch is 0.0163098.
After 6768 training step(s), loss on training batch is 0.0245186.
After 6769 training step(s), loss on training batch is 0.0319222.
After 6770 training step(s), loss on training batch is 0.0301423.
After 6771 training step(s), loss on training batch is 0.0277999.
After 6772 training step(s), loss on training batch is 0.0208465.
After 6773 training step(s), loss on training batch is 0.0201228.
After 6774 training step(s), loss on training batch is 0.0168943.
After 6775 training step(s), loss on training batch is 0.0159015.
After 6776 training step(s), loss on training batch is 0.0203211.
After 6777 training step(s), loss on training batch is 0.0437463.
After 6778 training step(s), loss on training batch is 0.0249146.
After 6779 training step(s), loss on training batch is 0.0289031.
After 6780 training step(s), loss on training batch is 0.0479493.
After 6781 training step(s), loss on training batch is 0.0260611.
After 6782 training step(s), loss on training batch is 0.022157.
After 6783 training step(s), loss on training batch is 0.0748449.
After 6784 training step(s), loss on training batch is 0.0386999.
After 6785 training step(s), loss on training batch is 0.0176988.
After 6786 training step(s), loss on training batch is 0.0335486.
After 6787 training step(s), loss on training batch is 0.0394549.
After 6788 training step(s), loss on training batch is 0.0278325.
After 6789 training step(s), loss on training batch is 0.0256737.
After 6790 training step(s), loss on training batch is 0.0469503.
After 6791 training step(s), loss on training batch is 0.0448853.
After 6792 training step(s), loss on training batch is 0.0246086.
After 6793 training step(s), loss on training batch is 0.0252279.
After 6794 training step(s), loss on training batch is 0.0184434.
After 6795 training step(s), loss on training batch is 0.023612.
After 6796 training step(s), loss on training batch is 0.0236736.
After 6797 training step(s), loss on training batch is 0.0348413.
After 6798 training step(s), loss on training batch is 0.0144681.
After 6799 training step(s), loss on training batch is 0.0216031.
After 6800 training step(s), loss on training batch is 0.0156624.
After 6801 training step(s), loss on training batch is 0.0269584.
After 6802 training step(s), loss on training batch is 0.0355048.
After 6803 training step(s), loss on training batch is 0.0286221.
After 6804 training step(s), loss on training batch is 0.0303339.
After 6805 training step(s), loss on training batch is 0.0363776.
After 6806 training step(s), loss on training batch is 0.0200827.
After 6807 training step(s), loss on training batch is 0.0167061.
After 6808 training step(s), loss on training batch is 0.0378097.
After 6809 training step(s), loss on training batch is 0.0338401.
After 6810 training step(s), loss on training batch is 0.0363648.
After 6811 training step(s), loss on training batch is 0.0267788.
After 6812 training step(s), loss on training batch is 0.0164444.
After 6813 training step(s), loss on training batch is 0.02542.
After 6814 training step(s), loss on training batch is 0.0191093.
After 6815 training step(s), loss on training batch is 0.017761.
After 6816 training step(s), loss on training batch is 0.0192821.
After 6817 training step(s), loss on training batch is 0.0192323.
After 6818 training step(s), loss on training batch is 0.0257775.
After 6819 training step(s), loss on training batch is 0.0253661.
After 6820 training step(s), loss on training batch is 0.0173302.
After 6821 training step(s), loss on training batch is 0.0206234.
After 6822 training step(s), loss on training batch is 0.018619.
After 6823 training step(s), loss on training batch is 0.0218191.
After 6824 training step(s), loss on training batch is 0.0408192.
After 6825 training step(s), loss on training batch is 0.0713421.
After 6826 training step(s), loss on training batch is 0.019564.
After 6827 training step(s), loss on training batch is 0.0156969.
After 6828 training step(s), loss on training batch is 0.0280139.
After 6829 training step(s), loss on training batch is 0.0321769.
After 6830 training step(s), loss on training batch is 0.031981.
After 6831 training step(s), loss on training batch is 0.0208812.
After 6832 training step(s), loss on training batch is 0.0279309.
After 6833 training step(s), loss on training batch is 0.0285504.
After 6834 training step(s), loss on training batch is 0.0220445.
After 6835 training step(s), loss on training batch is 0.0181254.
After 6836 training step(s), loss on training batch is 0.0260323.
After 6837 training step(s), loss on training batch is 0.0225255.
After 6838 training step(s), loss on training batch is 0.0185742.
After 6839 training step(s), loss on training batch is 0.0169079.
After 6840 training step(s), loss on training batch is 0.0500504.
After 6841 training step(s), loss on training batch is 0.0324221.
After 6842 training step(s), loss on training batch is 0.0437639.
After 6843 training step(s), loss on training batch is 0.0268257.
After 6844 training step(s), loss on training batch is 0.0380895.
After 6845 training step(s), loss on training batch is 0.0257657.
After 6846 training step(s), loss on training batch is 0.0501455.
After 6847 training step(s), loss on training batch is 0.0152115.
After 6848 training step(s), loss on training batch is 0.0169477.
After 6849 training step(s), loss on training batch is 0.0184561.
After 6850 training step(s), loss on training batch is 0.0389258.
After 6851 training step(s), loss on training batch is 0.0180487.
After 6852 training step(s), loss on training batch is 0.0147949.
After 6853 training step(s), loss on training batch is 0.0308976.
After 6854 training step(s), loss on training batch is 0.0186304.
After 6855 training step(s), loss on training batch is 0.0347968.
After 6856 training step(s), loss on training batch is 0.0203879.
After 6857 training step(s), loss on training batch is 0.0311912.
After 6858 training step(s), loss on training batch is 0.0333443.
After 6859 training step(s), loss on training batch is 0.0261598.
After 6860 training step(s), loss on training batch is 0.0244704.
After 6861 training step(s), loss on training batch is 0.0212401.
After 6862 training step(s), loss on training batch is 0.0194129.
After 6863 training step(s), loss on training batch is 0.0406066.
After 6864 training step(s), loss on training batch is 0.0432187.
After 6865 training step(s), loss on training batch is 0.0507173.
After 6866 training step(s), loss on training batch is 0.0190995.
After 6867 training step(s), loss on training batch is 0.0370894.
After 6868 training step(s), loss on training batch is 0.020152.
After 6869 training step(s), loss on training batch is 0.0206753.
After 6870 training step(s), loss on training batch is 0.0197602.
After 6871 training step(s), loss on training batch is 0.0339293.
After 6872 training step(s), loss on training batch is 0.0509217.
After 6873 training step(s), loss on training batch is 0.0406099.
After 6874 training step(s), loss on training batch is 0.041951.
After 6875 training step(s), loss on training batch is 0.0394477.
After 6876 training step(s), loss on training batch is 0.0173427.
After 6877 training step(s), loss on training batch is 0.0190608.
After 6878 training step(s), loss on training batch is 0.0300464.
After 6879 training step(s), loss on training batch is 0.0164325.
After 6880 training step(s), loss on training batch is 0.0252461.
After 6881 training step(s), loss on training batch is 0.0253088.
After 6882 training step(s), loss on training batch is 0.0154767.
After 6883 training step(s), loss on training batch is 0.0160153.
After 6884 training step(s), loss on training batch is 0.0213843.
After 6885 training step(s), loss on training batch is 0.0262988.
After 6886 training step(s), loss on training batch is 0.0434746.
After 6887 training step(s), loss on training batch is 0.0202758.
After 6888 training step(s), loss on training batch is 0.045944.
After 6889 training step(s), loss on training batch is 0.0176428.
After 6890 training step(s), loss on training batch is 0.0234598.
After 6891 training step(s), loss on training batch is 0.0251277.
After 6892 training step(s), loss on training batch is 0.0368981.
After 6893 training step(s), loss on training batch is 0.0162503.
After 6894 training step(s), loss on training batch is 0.0425772.
After 6895 training step(s), loss on training batch is 0.0405555.
After 6896 training step(s), loss on training batch is 0.0197715.
After 6897 training step(s), loss on training batch is 0.0131491.
After 6898 training step(s), loss on training batch is 0.0197392.
After 6899 training step(s), loss on training batch is 0.0176579.
After 6900 training step(s), loss on training batch is 0.020188.
After 6901 training step(s), loss on training batch is 0.0272372.
After 6902 training step(s), loss on training batch is 0.0150516.
After 6903 training step(s), loss on training batch is 0.0241599.
After 6904 training step(s), loss on training batch is 0.0165177.
After 6905 training step(s), loss on training batch is 0.0333407.
After 6906 training step(s), loss on training batch is 0.0256458.
After 6907 training step(s), loss on training batch is 0.0291176.
After 6908 training step(s), loss on training batch is 0.0212737.
After 6909 training step(s), loss on training batch is 0.0130571.
After 6910 training step(s), loss on training batch is 0.0195112.
After 6911 training step(s), loss on training batch is 0.0198235.
After 6912 training step(s), loss on training batch is 0.0183.
After 6913 training step(s), loss on training batch is 0.0186336.
After 6914 training step(s), loss on training batch is 0.0167749.
After 6915 training step(s), loss on training batch is 0.0202627.
After 6916 training step(s), loss on training batch is 0.0208139.
After 6917 training step(s), loss on training batch is 0.0185429.
After 6918 training step(s), loss on training batch is 0.022127.
After 6919 training step(s), loss on training batch is 0.0181398.
After 6920 training step(s), loss on training batch is 0.014256.
After 6921 training step(s), loss on training batch is 0.0175665.
After 6922 training step(s), loss on training batch is 0.0187651.
After 6923 training step(s), loss on training batch is 0.0181013.
After 6924 training step(s), loss on training batch is 0.0257641.
After 6925 training step(s), loss on training batch is 0.013546.
After 6926 training step(s), loss on training batch is 0.0358599.
After 6927 training step(s), loss on training batch is 0.0510226.
After 6928 training step(s), loss on training batch is 0.0218955.
After 6929 training step(s), loss on training batch is 0.0284257.
After 6930 training step(s), loss on training batch is 0.0363004.
After 6931 training step(s), loss on training batch is 0.033546.
After 6932 training step(s), loss on training batch is 0.016563.
After 6933 training step(s), loss on training batch is 0.0211833.
After 6934 training step(s), loss on training batch is 0.0319982.
After 6935 training step(s), loss on training batch is 0.0180644.
After 6936 training step(s), loss on training batch is 0.0166865.
After 6937 training step(s), loss on training batch is 0.0491268.
After 6938 training step(s), loss on training batch is 0.0223254.
After 6939 training step(s), loss on training batch is 0.0139399.
After 6940 training step(s), loss on training batch is 0.0127638.
After 6941 training step(s), loss on training batch is 0.0132605.
After 6942 training step(s), loss on training batch is 0.0186304.
After 6943 training step(s), loss on training batch is 0.0211612.
After 6944 training step(s), loss on training batch is 0.0260237.
After 6945 training step(s), loss on training batch is 0.027979.
After 6946 training step(s), loss on training batch is 0.0316178.
After 6947 training step(s), loss on training batch is 0.0144242.
After 6948 training step(s), loss on training batch is 0.024805.
After 6949 training step(s), loss on training batch is 0.0170926.
After 6950 training step(s), loss on training batch is 0.0222941.
After 6951 training step(s), loss on training batch is 0.0202531.
After 6952 training step(s), loss on training batch is 0.0190988.
After 6953 training step(s), loss on training batch is 0.0135485.
After 6954 training step(s), loss on training batch is 0.0183011.
After 6955 training step(s), loss on training batch is 0.0164841.
After 6956 training step(s), loss on training batch is 0.0242334.
After 6957 training step(s), loss on training batch is 0.0202832.
After 6958 training step(s), loss on training batch is 0.0208678.
After 6959 training step(s), loss on training batch is 0.0218121.
After 6960 training step(s), loss on training batch is 0.0188728.
After 6961 training step(s), loss on training batch is 0.0151832.
After 6962 training step(s), loss on training batch is 0.0260749.
After 6963 training step(s), loss on training batch is 0.0336722.
After 6964 training step(s), loss on training batch is 0.0392636.
After 6965 training step(s), loss on training batch is 0.0200433.
After 6966 training step(s), loss on training batch is 0.0188419.
After 6967 training step(s), loss on training batch is 0.0180874.
After 6968 training step(s), loss on training batch is 0.0329917.
After 6969 training step(s), loss on training batch is 0.0187854.
After 6970 training step(s), loss on training batch is 0.0195303.
After 6971 training step(s), loss on training batch is 0.0139328.
After 6972 training step(s), loss on training batch is 0.0272072.
After 6973 training step(s), loss on training batch is 0.016012.
After 6974 training step(s), loss on training batch is 0.0231715.
After 6975 training step(s), loss on training batch is 0.020336.
After 6976 training step(s), loss on training batch is 0.0152221.
After 6977 training step(s), loss on training batch is 0.0254398.
After 6978 training step(s), loss on training batch is 0.029503.
After 6979 training step(s), loss on training batch is 0.0449038.
After 6980 training step(s), loss on training batch is 0.0297451.
After 6981 training step(s), loss on training batch is 0.0196613.
After 6982 training step(s), loss on training batch is 0.0232047.
After 6983 training step(s), loss on training batch is 0.0201859.
After 6984 training step(s), loss on training batch is 0.0470581.
After 6985 training step(s), loss on training batch is 0.0381131.
After 6986 training step(s), loss on training batch is 0.0621075.
After 6987 training step(s), loss on training batch is 0.0179367.
After 6988 training step(s), loss on training batch is 0.0201295.
After 6989 training step(s), loss on training batch is 0.014282.
After 6990 training step(s), loss on training batch is 0.0146766.
After 6991 training step(s), loss on training batch is 0.0271459.
After 6992 training step(s), loss on training batch is 0.0171243.
After 6993 training step(s), loss on training batch is 0.0176978.
After 6994 training step(s), loss on training batch is 0.0448909.
After 6995 training step(s), loss on training batch is 0.0262299.
After 6996 training step(s), loss on training batch is 0.0213905.
After 6997 training step(s), loss on training batch is 0.0397776.
After 6998 training step(s), loss on training batch is 0.0456123.
After 6999 training step(s), loss on training batch is 0.0312833.
After 7000 training step(s), loss on training batch is 0.0403747.
After 7001 training step(s), loss on training batch is 0.0347288.
After 7002 training step(s), loss on training batch is 0.0191718.
After 7003 training step(s), loss on training batch is 0.0759081.
After 7004 training step(s), loss on training batch is 0.0195225.
After 7005 training step(s), loss on training batch is 0.0222025.
After 7006 training step(s), loss on training batch is 0.0422482.
After 7007 training step(s), loss on training batch is 0.0287424.
After 7008 training step(s), loss on training batch is 0.0209181.
After 7009 training step(s), loss on training batch is 0.0285342.
After 7010 training step(s), loss on training batch is 0.0284693.
After 7011 training step(s), loss on training batch is 0.0333795.
After 7012 training step(s), loss on training batch is 0.0323461.
After 7013 training step(s), loss on training batch is 0.0188477.
After 7014 training step(s), loss on training batch is 0.0675976.
After 7015 training step(s), loss on training batch is 0.0228236.
After 7016 training step(s), loss on training batch is 0.0187112.
After 7017 training step(s), loss on training batch is 0.0238982.
After 7018 training step(s), loss on training batch is 0.0405278.
After 7019 training step(s), loss on training batch is 0.0221597.
After 7020 training step(s), loss on training batch is 0.013648.
After 7021 training step(s), loss on training batch is 0.0365653.
After 7022 training step(s), loss on training batch is 0.0375383.
After 7023 training step(s), loss on training batch is 0.0450171.
After 7024 training step(s), loss on training batch is 0.0535264.
After 7025 training step(s), loss on training batch is 0.0150094.
After 7026 training step(s), loss on training batch is 0.0206888.
After 7027 training step(s), loss on training batch is 0.0222967.
After 7028 training step(s), loss on training batch is 0.0246211.
After 7029 training step(s), loss on training batch is 0.0199904.
After 7030 training step(s), loss on training batch is 0.0158697.
After 7031 training step(s), loss on training batch is 0.0183435.
After 7032 training step(s), loss on training batch is 0.0168363.
After 7033 training step(s), loss on training batch is 0.0272019.
After 7034 training step(s), loss on training batch is 0.0269391.
After 7035 training step(s), loss on training batch is 0.0149556.
After 7036 training step(s), loss on training batch is 0.0255155.
After 7037 training step(s), loss on training batch is 0.0174822.
After 7038 training step(s), loss on training batch is 0.0221765.
After 7039 training step(s), loss on training batch is 0.0335838.
After 7040 training step(s), loss on training batch is 0.0126711.
After 7041 training step(s), loss on training batch is 0.0181397.
After 7042 training step(s), loss on training batch is 0.0184714.
After 7043 training step(s), loss on training batch is 0.0345985.
After 7044 training step(s), loss on training batch is 0.0282174.
After 7045 training step(s), loss on training batch is 0.0312566.
After 7046 training step(s), loss on training batch is 0.0208531.
After 7047 training step(s), loss on training batch is 0.0127334.
After 7048 training step(s), loss on training batch is 0.0829933.
After 7049 training step(s), loss on training batch is 0.023731.
After 7050 training step(s), loss on training batch is 0.0292168.
After 7051 training step(s), loss on training batch is 0.0217742.
After 7052 training step(s), loss on training batch is 0.0262319.
After 7053 training step(s), loss on training batch is 0.0167771.
After 7054 training step(s), loss on training batch is 0.0218468.
After 7055 training step(s), loss on training batch is 0.0195977.
After 7056 training step(s), loss on training batch is 0.0516549.
After 7057 training step(s), loss on training batch is 0.0550142.
After 7058 training step(s), loss on training batch is 0.0267112.
After 7059 training step(s), loss on training batch is 0.0412916.
After 7060 training step(s), loss on training batch is 0.0176404.
After 7061 training step(s), loss on training batch is 0.013111.
After 7062 training step(s), loss on training batch is 0.0145016.
After 7063 training step(s), loss on training batch is 0.0271196.
After 7064 training step(s), loss on training batch is 0.0224274.
After 7065 training step(s), loss on training batch is 0.0235681.
After 7066 training step(s), loss on training batch is 0.015013.
After 7067 training step(s), loss on training batch is 0.0720973.
After 7068 training step(s), loss on training batch is 0.0563673.
After 7069 training step(s), loss on training batch is 0.0174194.
After 7070 training step(s), loss on training batch is 0.0243943.
After 7071 training step(s), loss on training batch is 0.0270552.
After 7072 training step(s), loss on training batch is 0.0363532.
After 7073 training step(s), loss on training batch is 0.0207835.
After 7074 training step(s), loss on training batch is 0.0232946.
After 7075 training step(s), loss on training batch is 0.0273865.
After 7076 training step(s), loss on training batch is 0.0372673.
After 7077 training step(s), loss on training batch is 0.0269909.
After 7078 training step(s), loss on training batch is 0.0152228.
After 7079 training step(s), loss on training batch is 0.0162162.
After 7080 training step(s), loss on training batch is 0.0178593.
After 7081 training step(s), loss on training batch is 0.0132735.
After 7082 training step(s), loss on training batch is 0.0220118.
After 7083 training step(s), loss on training batch is 0.013213.
After 7084 training step(s), loss on training batch is 0.0339902.
After 7085 training step(s), loss on training batch is 0.0138603.
After 7086 training step(s), loss on training batch is 0.033362.
After 7087 training step(s), loss on training batch is 0.017006.
After 7088 training step(s), loss on training batch is 0.0283191.
After 7089 training step(s), loss on training batch is 0.0258423.
After 7090 training step(s), loss on training batch is 0.019293.
After 7091 training step(s), loss on training batch is 0.0199949.
After 7092 training step(s), loss on training batch is 0.0263407.
After 7093 training step(s), loss on training batch is 0.0174565.
After 7094 training step(s), loss on training batch is 0.0404979.
After 7095 training step(s), loss on training batch is 0.0237946.
After 7096 training step(s), loss on training batch is 0.0228318.
After 7097 training step(s), loss on training batch is 0.0166132.
After 7098 training step(s), loss on training batch is 0.0346053.
After 7099 training step(s), loss on training batch is 0.0197445.
After 7100 training step(s), loss on training batch is 0.025938.
After 7101 training step(s), loss on training batch is 0.0213414.
After 7102 training step(s), loss on training batch is 0.0158383.
After 7103 training step(s), loss on training batch is 0.0199175.
After 7104 training step(s), loss on training batch is 0.0177359.
After 7105 training step(s), loss on training batch is 0.0196424.
After 7106 training step(s), loss on training batch is 0.0174803.
After 7107 training step(s), loss on training batch is 0.0159922.
After 7108 training step(s), loss on training batch is 0.0183644.
After 7109 training step(s), loss on training batch is 0.0260414.
After 7110 training step(s), loss on training batch is 0.0173682.
After 7111 training step(s), loss on training batch is 0.0157932.
After 7112 training step(s), loss on training batch is 0.0316588.
After 7113 training step(s), loss on training batch is 0.0169166.
After 7114 training step(s), loss on training batch is 0.0259098.
After 7115 training step(s), loss on training batch is 0.0125746.
After 7116 training step(s), loss on training batch is 0.0131376.
After 7117 training step(s), loss on training batch is 0.0341047.
After 7118 training step(s), loss on training batch is 0.0270973.
After 7119 training step(s), loss on training batch is 0.0152843.
After 7120 training step(s), loss on training batch is 0.0202412.
After 7121 training step(s), loss on training batch is 0.0183591.
After 7122 training step(s), loss on training batch is 0.0195654.
After 7123 training step(s), loss on training batch is 0.0135484.
After 7124 training step(s), loss on training batch is 0.0189251.
After 7125 training step(s), loss on training batch is 0.0197418.
After 7126 training step(s), loss on training batch is 0.0167581.
After 7127 training step(s), loss on training batch is 0.0191915.
After 7128 training step(s), loss on training batch is 0.0138679.
After 7129 training step(s), loss on training batch is 0.017788.
After 7130 training step(s), loss on training batch is 0.0422769.
After 7131 training step(s), loss on training batch is 0.0137305.
After 7132 training step(s), loss on training batch is 0.0453804.
After 7133 training step(s), loss on training batch is 0.0365685.
After 7134 training step(s), loss on training batch is 0.0190559.
After 7135 training step(s), loss on training batch is 0.0219887.
After 7136 training step(s), loss on training batch is 0.0180211.
After 7137 training step(s), loss on training batch is 0.0233837.
After 7138 training step(s), loss on training batch is 0.0201851.
After 7139 training step(s), loss on training batch is 0.0252228.
After 7140 training step(s), loss on training batch is 0.0160124.
After 7141 training step(s), loss on training batch is 0.035145.
After 7142 training step(s), loss on training batch is 0.0530757.
After 7143 training step(s), loss on training batch is 0.0343644.
After 7144 training step(s), loss on training batch is 0.0297811.
After 7145 training step(s), loss on training batch is 0.026156.
After 7146 training step(s), loss on training batch is 0.0354764.
After 7147 training step(s), loss on training batch is 0.0560418.
After 7148 training step(s), loss on training batch is 0.0375494.
After 7149 training step(s), loss on training batch is 0.0452381.
After 7150 training step(s), loss on training batch is 0.0196961.
After 7151 training step(s), loss on training batch is 0.0233573.
After 7152 training step(s), loss on training batch is 0.0159334.
After 7153 training step(s), loss on training batch is 0.0144429.
After 7154 training step(s), loss on training batch is 0.0705677.
After 7155 training step(s), loss on training batch is 0.0212252.
After 7156 training step(s), loss on training batch is 0.0157005.
After 7157 training step(s), loss on training batch is 0.0263412.
After 7158 training step(s), loss on training batch is 0.0187941.
After 7159 training step(s), loss on training batch is 0.0233085.
After 7160 training step(s), loss on training batch is 0.0229147.
After 7161 training step(s), loss on training batch is 0.0200066.
After 7162 training step(s), loss on training batch is 0.0254194.
After 7163 training step(s), loss on training batch is 0.0386201.
After 7164 training step(s), loss on training batch is 0.015539.
After 7165 training step(s), loss on training batch is 0.0221068.
After 7166 training step(s), loss on training batch is 0.0342425.
After 7167 training step(s), loss on training batch is 0.0189579.
After 7168 training step(s), loss on training batch is 0.0232797.
After 7169 training step(s), loss on training batch is 0.0208866.
After 7170 training step(s), loss on training batch is 0.0178783.
After 7171 training step(s), loss on training batch is 0.0216695.
After 7172 training step(s), loss on training batch is 0.0227972.
After 7173 training step(s), loss on training batch is 0.0184306.
After 7174 training step(s), loss on training batch is 0.0217433.
After 7175 training step(s), loss on training batch is 0.0152784.
After 7176 training step(s), loss on training batch is 0.0151192.
After 7177 training step(s), loss on training batch is 0.0169477.
After 7178 training step(s), loss on training batch is 0.0320289.
After 7179 training step(s), loss on training batch is 0.0158348.
After 7180 training step(s), loss on training batch is 0.0184294.
After 7181 training step(s), loss on training batch is 0.0158521.
After 7182 training step(s), loss on training batch is 0.0149383.
After 7183 training step(s), loss on training batch is 0.0161216.
After 7184 training step(s), loss on training batch is 0.016037.
After 7185 training step(s), loss on training batch is 0.0184079.
After 7186 training step(s), loss on training batch is 0.0178512.
After 7187 training step(s), loss on training batch is 0.0437812.
After 7188 training step(s), loss on training batch is 0.0227866.
After 7189 training step(s), loss on training batch is 0.0157732.
After 7190 training step(s), loss on training batch is 0.0222752.
After 7191 training step(s), loss on training batch is 0.0310201.
After 7192 training step(s), loss on training batch is 0.0189992.
After 7193 training step(s), loss on training batch is 0.0157955.
After 7194 training step(s), loss on training batch is 0.0170432.
After 7195 training step(s), loss on training batch is 0.0224734.
After 7196 training step(s), loss on training batch is 0.0155755.
After 7197 training step(s), loss on training batch is 0.0139398.
After 7198 training step(s), loss on training batch is 0.0155127.
After 7199 training step(s), loss on training batch is 0.0124135.
After 7200 training step(s), loss on training batch is 0.0184339.
After 7201 training step(s), loss on training batch is 0.0140713.
After 7202 training step(s), loss on training batch is 0.0178651.
After 7203 training step(s), loss on training batch is 0.0131736.
After 7204 training step(s), loss on training batch is 0.0205491.
After 7205 training step(s), loss on training batch is 0.019445.
After 7206 training step(s), loss on training batch is 0.0229441.
After 7207 training step(s), loss on training batch is 0.0139079.
After 7208 training step(s), loss on training batch is 0.0156948.
After 7209 training step(s), loss on training batch is 0.0155899.
After 7210 training step(s), loss on training batch is 0.0185189.
After 7211 training step(s), loss on training batch is 0.0170182.
After 7212 training step(s), loss on training batch is 0.0122891.
After 7213 training step(s), loss on training batch is 0.0386967.
After 7214 training step(s), loss on training batch is 0.0243948.
After 7215 training step(s), loss on training batch is 0.0335083.
After 7216 training step(s), loss on training batch is 0.0190339.
After 7217 training step(s), loss on training batch is 0.0238769.
After 7218 training step(s), loss on training batch is 0.0150422.
After 7219 training step(s), loss on training batch is 0.0216818.
After 7220 training step(s), loss on training batch is 0.0185018.
After 7221 training step(s), loss on training batch is 0.0164139.
After 7222 training step(s), loss on training batch is 0.0166502.
After 7223 training step(s), loss on training batch is 0.0228619.
After 7224 training step(s), loss on training batch is 0.020486.
After 7225 training step(s), loss on training batch is 0.0143132.
After 7226 training step(s), loss on training batch is 0.0145011.
After 7227 training step(s), loss on training batch is 0.0164363.
After 7228 training step(s), loss on training batch is 0.0137716.
After 7229 training step(s), loss on training batch is 0.0142129.
After 7230 training step(s), loss on training batch is 0.0147165.
After 7231 training step(s), loss on training batch is 0.0153886.
After 7232 training step(s), loss on training batch is 0.0139871.
After 7233 training step(s), loss on training batch is 0.0122021.
After 7234 training step(s), loss on training batch is 0.0158774.
After 7235 training step(s), loss on training batch is 0.0194437.
After 7236 training step(s), loss on training batch is 0.0144192.
After 7237 training step(s), loss on training batch is 0.0243514.
After 7238 training step(s), loss on training batch is 0.0157662.
After 7239 training step(s), loss on training batch is 0.0169464.
After 7240 training step(s), loss on training batch is 0.0343499.
After 7241 training step(s), loss on training batch is 0.0416118.
After 7242 training step(s), loss on training batch is 0.027591.
After 7243 training step(s), loss on training batch is 0.0167927.
After 7244 training step(s), loss on training batch is 0.0151606.
After 7245 training step(s), loss on training batch is 0.0124087.
After 7246 training step(s), loss on training batch is 0.0232109.
After 7247 training step(s), loss on training batch is 0.0191548.
After 7248 training step(s), loss on training batch is 0.0168396.
After 7249 training step(s), loss on training batch is 0.0310784.
After 7250 training step(s), loss on training batch is 0.0145356.
After 7251 training step(s), loss on training batch is 0.0223707.
After 7252 training step(s), loss on training batch is 0.0207559.
After 7253 training step(s), loss on training batch is 0.019574.
After 7254 training step(s), loss on training batch is 0.0133055.
After 7255 training step(s), loss on training batch is 0.0334591.
After 7256 training step(s), loss on training batch is 0.0277902.
After 7257 training step(s), loss on training batch is 0.023321.
After 7258 training step(s), loss on training batch is 0.0166427.
After 7259 training step(s), loss on training batch is 0.0152217.
After 7260 training step(s), loss on training batch is 0.0170007.
After 7261 training step(s), loss on training batch is 0.0188591.
After 7262 training step(s), loss on training batch is 0.0141245.
After 7263 training step(s), loss on training batch is 0.027474.
After 7264 training step(s), loss on training batch is 0.0151539.
After 7265 training step(s), loss on training batch is 0.0315074.
After 7266 training step(s), loss on training batch is 0.0224836.
After 7267 training step(s), loss on training batch is 0.0362885.
After 7268 training step(s), loss on training batch is 0.0206602.
After 7269 training step(s), loss on training batch is 0.0152535.
After 7270 training step(s), loss on training batch is 0.01281.
After 7271 training step(s), loss on training batch is 0.0177182.
After 7272 training step(s), loss on training batch is 0.0153718.
After 7273 training step(s), loss on training batch is 0.0159105.
After 7274 training step(s), loss on training batch is 0.0142826.
After 7275 training step(s), loss on training batch is 0.0219412.
After 7276 training step(s), loss on training batch is 0.0199249.
After 7277 training step(s), loss on training batch is 0.0271598.
After 7278 training step(s), loss on training batch is 0.0408635.
After 7279 training step(s), loss on training batch is 0.0336481.
After 7280 training step(s), loss on training batch is 0.0164913.
After 7281 training step(s), loss on training batch is 0.0151466.
After 7282 training step(s), loss on training batch is 0.0197162.
After 7283 training step(s), loss on training batch is 0.0448608.
After 7284 training step(s), loss on training batch is 0.0208097.
After 7285 training step(s), loss on training batch is 0.0144312.
After 7286 training step(s), loss on training batch is 0.0229175.
After 7287 training step(s), loss on training batch is 0.0122123.
After 7288 training step(s), loss on training batch is 0.0211883.
After 7289 training step(s), loss on training batch is 0.0307037.
After 7290 training step(s), loss on training batch is 0.018203.
After 7291 training step(s), loss on training batch is 0.0245781.
After 7292 training step(s), loss on training batch is 0.0257732.
After 7293 training step(s), loss on training batch is 0.0150053.
After 7294 training step(s), loss on training batch is 0.0182032.
After 7295 training step(s), loss on training batch is 0.0371523.
After 7296 training step(s), loss on training batch is 0.0299638.
After 7297 training step(s), loss on training batch is 0.0134187.
After 7298 training step(s), loss on training batch is 0.0179512.
After 7299 training step(s), loss on training batch is 0.0187315.
After 7300 training step(s), loss on training batch is 0.0180887.
After 7301 training step(s), loss on training batch is 0.0176888.
After 7302 training step(s), loss on training batch is 0.0153681.
After 7303 training step(s), loss on training batch is 0.0133357.
After 7304 training step(s), loss on training batch is 0.0164574.
After 7305 training step(s), loss on training batch is 0.0173783.
After 7306 training step(s), loss on training batch is 0.0183787.
After 7307 training step(s), loss on training batch is 0.0149341.
After 7308 training step(s), loss on training batch is 0.0498048.
After 7309 training step(s), loss on training batch is 0.0279355.
After 7310 training step(s), loss on training batch is 0.054807.
After 7311 training step(s), loss on training batch is 0.0200501.
After 7312 training step(s), loss on training batch is 0.013987.
After 7313 training step(s), loss on training batch is 0.0168627.
After 7314 training step(s), loss on training batch is 0.0271626.
After 7315 training step(s), loss on training batch is 0.0384979.
After 7316 training step(s), loss on training batch is 0.0719077.
After 7317 training step(s), loss on training batch is 0.0139829.
After 7318 training step(s), loss on training batch is 0.0134665.
After 7319 training step(s), loss on training batch is 0.0433138.
After 7320 training step(s), loss on training batch is 0.0162281.
After 7321 training step(s), loss on training batch is 0.016483.
After 7322 training step(s), loss on training batch is 0.0171559.
After 7323 training step(s), loss on training batch is 0.0184364.
After 7324 training step(s), loss on training batch is 0.0201385.
After 7325 training step(s), loss on training batch is 0.0344958.
After 7326 training step(s), loss on training batch is 0.0185431.
After 7327 training step(s), loss on training batch is 0.0201259.
After 7328 training step(s), loss on training batch is 0.0203129.
After 7329 training step(s), loss on training batch is 0.0155834.
After 7330 training step(s), loss on training batch is 0.014997.
After 7331 training step(s), loss on training batch is 0.0149044.
After 7332 training step(s), loss on training batch is 0.0134451.
After 7333 training step(s), loss on training batch is 0.0178148.
After 7334 training step(s), loss on training batch is 0.0342839.
After 7335 training step(s), loss on training batch is 0.0376006.
After 7336 training step(s), loss on training batch is 0.0145043.
After 7337 training step(s), loss on training batch is 0.0248882.
After 7338 training step(s), loss on training batch is 0.0217755.
After 7339 training step(s), loss on training batch is 0.0256504.
After 7340 training step(s), loss on training batch is 0.0364877.
After 7341 training step(s), loss on training batch is 0.0210134.
After 7342 training step(s), loss on training batch is 0.0222945.
After 7343 training step(s), loss on training batch is 0.0306003.
After 7344 training step(s), loss on training batch is 0.0242659.
After 7345 training step(s), loss on training batch is 0.0155492.
After 7346 training step(s), loss on training batch is 0.0243552.
After 7347 training step(s), loss on training batch is 0.0649657.
After 7348 training step(s), loss on training batch is 0.0286323.
After 7349 training step(s), loss on training batch is 0.023359.
After 7350 training step(s), loss on training batch is 0.018936.
After 7351 training step(s), loss on training batch is 0.0288865.
After 7352 training step(s), loss on training batch is 0.0135381.
After 7353 training step(s), loss on training batch is 0.0477185.
After 7354 training step(s), loss on training batch is 0.0299816.
After 7355 training step(s), loss on training batch is 0.015384.
After 7356 training step(s), loss on training batch is 0.0230998.
After 7357 training step(s), loss on training batch is 0.0194088.
After 7358 training step(s), loss on training batch is 0.0278001.
After 7359 training step(s), loss on training batch is 0.0149328.
After 7360 training step(s), loss on training batch is 0.0181152.
After 7361 training step(s), loss on training batch is 0.0269201.
After 7362 training step(s), loss on training batch is 0.0307298.
After 7363 training step(s), loss on training batch is 0.0146834.
After 7364 training step(s), loss on training batch is 0.0195762.
After 7365 training step(s), loss on training batch is 0.0397914.
After 7366 training step(s), loss on training batch is 0.0237399.
After 7367 training step(s), loss on training batch is 0.0208894.
After 7368 training step(s), loss on training batch is 0.0196794.
After 7369 training step(s), loss on training batch is 0.0217345.
After 7370 training step(s), loss on training batch is 0.016876.
After 7371 training step(s), loss on training batch is 0.0197892.
After 7372 training step(s), loss on training batch is 0.0322136.
After 7373 training step(s), loss on training batch is 0.015212.
After 7374 training step(s), loss on training batch is 0.0223152.
After 7375 training step(s), loss on training batch is 0.0341715.
After 7376 training step(s), loss on training batch is 0.0120708.
After 7377 training step(s), loss on training batch is 0.0180147.
After 7378 training step(s), loss on training batch is 0.0174976.
After 7379 training step(s), loss on training batch is 0.0153064.
After 7380 training step(s), loss on training batch is 0.0182799.
After 7381 training step(s), loss on training batch is 0.0347474.
After 7382 training step(s), loss on training batch is 0.0124549.
After 7383 training step(s), loss on training batch is 0.0278804.
After 7384 training step(s), loss on training batch is 0.0242378.
After 7385 training step(s), loss on training batch is 0.0346702.
After 7386 training step(s), loss on training batch is 0.0245662.
After 7387 training step(s), loss on training batch is 0.0160825.
After 7388 training step(s), loss on training batch is 0.0474832.
After 7389 training step(s), loss on training batch is 0.0187787.
After 7390 training step(s), loss on training batch is 0.0200601.
After 7391 training step(s), loss on training batch is 0.0120699.
After 7392 training step(s), loss on training batch is 0.0219178.
After 7393 training step(s), loss on training batch is 0.0147558.
After 7394 training step(s), loss on training batch is 0.0168102.
After 7395 training step(s), loss on training batch is 0.0185618.
After 7396 training step(s), loss on training batch is 0.0391407.
After 7397 training step(s), loss on training batch is 0.0205175.
After 7398 training step(s), loss on training batch is 0.0126837.
After 7399 training step(s), loss on training batch is 0.0342687.
After 7400 training step(s), loss on training batch is 0.0154538.
After 7401 training step(s), loss on training batch is 0.0128677.
After 7402 training step(s), loss on training batch is 0.0220286.
After 7403 training step(s), loss on training batch is 0.0157959.
After 7404 training step(s), loss on training batch is 0.0180587.
After 7405 training step(s), loss on training batch is 0.027378.
After 7406 training step(s), loss on training batch is 0.0189367.
After 7407 training step(s), loss on training batch is 0.0124111.
After 7408 training step(s), loss on training batch is 0.0166062.
After 7409 training step(s), loss on training batch is 0.015888.
After 7410 training step(s), loss on training batch is 0.017597.
After 7411 training step(s), loss on training batch is 0.0709179.
After 7412 training step(s), loss on training batch is 0.0329873.
After 7413 training step(s), loss on training batch is 0.0331686.
After 7414 training step(s), loss on training batch is 0.028165.
After 7415 training step(s), loss on training batch is 0.0209613.
After 7416 training step(s), loss on training batch is 0.0139524.
After 7417 training step(s), loss on training batch is 0.0186197.
After 7418 training step(s), loss on training batch is 0.014217.
After 7419 training step(s), loss on training batch is 0.0192588.
After 7420 training step(s), loss on training batch is 0.0120259.
After 7421 training step(s), loss on training batch is 0.0176137.
After 7422 training step(s), loss on training batch is 0.024806.
After 7423 training step(s), loss on training batch is 0.0261108.
After 7424 training step(s), loss on training batch is 0.0200458.
After 7425 training step(s), loss on training batch is 0.018218.
After 7426 training step(s), loss on training batch is 0.0236846.
After 7427 training step(s), loss on training batch is 0.0271028.
After 7428 training step(s), loss on training batch is 0.0119215.
After 7429 training step(s), loss on training batch is 0.0128139.
After 7430 training step(s), loss on training batch is 0.0149949.
After 7431 training step(s), loss on training batch is 0.0125961.
After 7432 training step(s), loss on training batch is 0.0141021.
After 7433 training step(s), loss on training batch is 0.018578.
After 7434 training step(s), loss on training batch is 0.0163318.
After 7435 training step(s), loss on training batch is 0.0147427.
After 7436 training step(s), loss on training batch is 0.013678.
After 7437 training step(s), loss on training batch is 0.0173762.
After 7438 training step(s), loss on training batch is 0.0231534.
After 7439 training step(s), loss on training batch is 0.0163877.
After 7440 training step(s), loss on training batch is 0.0278605.
After 7441 training step(s), loss on training batch is 0.0138057.
After 7442 training step(s), loss on training batch is 0.0122241.
After 7443 training step(s), loss on training batch is 0.0352875.
After 7444 training step(s), loss on training batch is 0.0142825.
After 7445 training step(s), loss on training batch is 0.050446.
After 7446 training step(s), loss on training batch is 0.0181634.
After 7447 training step(s), loss on training batch is 0.015739.
After 7448 training step(s), loss on training batch is 0.0140754.
After 7449 training step(s), loss on training batch is 0.0118091.
After 7450 training step(s), loss on training batch is 0.019162.
After 7451 training step(s), loss on training batch is 0.0123109.
After 7452 training step(s), loss on training batch is 0.0169476.
After 7453 training step(s), loss on training batch is 0.0154204.
After 7454 training step(s), loss on training batch is 0.019824.
After 7455 training step(s), loss on training batch is 0.0167118.
After 7456 training step(s), loss on training batch is 0.0158674.
After 7457 training step(s), loss on training batch is 0.0231079.
After 7458 training step(s), loss on training batch is 0.0147593.
After 7459 training step(s), loss on training batch is 0.0126268.
After 7460 training step(s), loss on training batch is 0.0252587.
After 7461 training step(s), loss on training batch is 0.0147905.
After 7462 training step(s), loss on training batch is 0.0154251.
After 7463 training step(s), loss on training batch is 0.0215615.
After 7464 training step(s), loss on training batch is 0.0125819.
After 7465 training step(s), loss on training batch is 0.0349701.
After 7466 training step(s), loss on training batch is 0.0151038.
After 7467 training step(s), loss on training batch is 0.0143422.
After 7468 training step(s), loss on training batch is 0.0155943.
After 7469 training step(s), loss on training batch is 0.0145049.
After 7470 training step(s), loss on training batch is 0.0218751.
After 7471 training step(s), loss on training batch is 0.0124708.
After 7472 training step(s), loss on training batch is 0.0133626.
After 7473 training step(s), loss on training batch is 0.0234195.
After 7474 training step(s), loss on training batch is 0.02405.
After 7475 training step(s), loss on training batch is 0.0274089.
After 7476 training step(s), loss on training batch is 0.0158584.
After 7477 training step(s), loss on training batch is 0.0148035.
After 7478 training step(s), loss on training batch is 0.0226873.
After 7479 training step(s), loss on training batch is 0.0347201.
After 7480 training step(s), loss on training batch is 0.0276261.
After 7481 training step(s), loss on training batch is 0.0202085.
After 7482 training step(s), loss on training batch is 0.0155136.
After 7483 training step(s), loss on training batch is 0.0125427.
After 7484 training step(s), loss on training batch is 0.0156293.
After 7485 training step(s), loss on training batch is 0.0298622.
After 7486 training step(s), loss on training batch is 0.0161685.
After 7487 training step(s), loss on training batch is 0.0120949.
After 7488 training step(s), loss on training batch is 0.0321813.
After 7489 training step(s), loss on training batch is 0.0153506.
After 7490 training step(s), loss on training batch is 0.0256447.
After 7491 training step(s), loss on training batch is 0.0184616.
After 7492 training step(s), loss on training batch is 0.0190183.
After 7493 training step(s), loss on training batch is 0.014011.
After 7494 training step(s), loss on training batch is 0.0133977.
After 7495 training step(s), loss on training batch is 0.0133919.
After 7496 training step(s), loss on training batch is 0.0173598.
After 7497 training step(s), loss on training batch is 0.0404017.
After 7498 training step(s), loss on training batch is 0.0261604.
After 7499 training step(s), loss on training batch is 0.0244104.
After 7500 training step(s), loss on training batch is 0.0259645.
After 7501 training step(s), loss on training batch is 0.0124134.
After 7502 training step(s), loss on training batch is 0.015385.
After 7503 training step(s), loss on training batch is 0.0120434.
After 7504 training step(s), loss on training batch is 0.0131348.
After 7505 training step(s), loss on training batch is 0.0160727.
After 7506 training step(s), loss on training batch is 0.013151.
After 7507 training step(s), loss on training batch is 0.0148179.
After 7508 training step(s), loss on training batch is 0.0160902.
After 7509 training step(s), loss on training batch is 0.0569664.
After 7510 training step(s), loss on training batch is 0.0232169.
After 7511 training step(s), loss on training batch is 0.0202385.
After 7512 training step(s), loss on training batch is 0.0138795.
After 7513 training step(s), loss on training batch is 0.0128961.
After 7514 training step(s), loss on training batch is 0.0199446.
After 7515 training step(s), loss on training batch is 0.0239227.
After 7516 training step(s), loss on training batch is 0.017574.
After 7517 training step(s), loss on training batch is 0.0167299.
After 7518 training step(s), loss on training batch is 0.0186457.
After 7519 training step(s), loss on training batch is 0.0123051.
After 7520 training step(s), loss on training batch is 0.0296145.
After 7521 training step(s), loss on training batch is 0.0198227.
After 7522 training step(s), loss on training batch is 0.027064.
After 7523 training step(s), loss on training batch is 0.0150987.
After 7524 training step(s), loss on training batch is 0.0176517.
After 7525 training step(s), loss on training batch is 0.0128962.
After 7526 training step(s), loss on training batch is 0.0238137.
After 7527 training step(s), loss on training batch is 0.0185425.
After 7528 training step(s), loss on training batch is 0.0137648.
After 7529 training step(s), loss on training batch is 0.0309585.
After 7530 training step(s), loss on training batch is 0.0142019.
After 7531 training step(s), loss on training batch is 0.0501588.
After 7532 training step(s), loss on training batch is 0.0282934.
After 7533 training step(s), loss on training batch is 0.0255787.
After 7534 training step(s), loss on training batch is 0.0132499.
After 7535 training step(s), loss on training batch is 0.0115805.
After 7536 training step(s), loss on training batch is 0.0157048.
After 7537 training step(s), loss on training batch is 0.0170976.
After 7538 training step(s), loss on training batch is 0.0235769.
After 7539 training step(s), loss on training batch is 0.0130304.
After 7540 training step(s), loss on training batch is 0.0114714.
After 7541 training step(s), loss on training batch is 0.0398858.
After 7542 training step(s), loss on training batch is 0.0146101.
After 7543 training step(s), loss on training batch is 0.0391117.
After 7544 training step(s), loss on training batch is 0.0306939.
After 7545 training step(s), loss on training batch is 0.015587.
After 7546 training step(s), loss on training batch is 0.0179776.
After 7547 training step(s), loss on training batch is 0.0250289.
After 7548 training step(s), loss on training batch is 0.0137445.
After 7549 training step(s), loss on training batch is 0.0239546.
After 7550 training step(s), loss on training batch is 0.0131591.
After 7551 training step(s), loss on training batch is 0.0185607.
After 7552 training step(s), loss on training batch is 0.0260819.
After 7553 training step(s), loss on training batch is 0.0226877.
After 7554 training step(s), loss on training batch is 0.019786.
After 7555 training step(s), loss on training batch is 0.0470238.
After 7556 training step(s), loss on training batch is 0.0170119.
After 7557 training step(s), loss on training batch is 0.0148269.
After 7558 training step(s), loss on training batch is 0.0135429.
After 7559 training step(s), loss on training batch is 0.0148064.
After 7560 training step(s), loss on training batch is 0.0181519.
After 7561 training step(s), loss on training batch is 0.0414029.
After 7562 training step(s), loss on training batch is 0.0411563.
After 7563 training step(s), loss on training batch is 0.0164397.
After 7564 training step(s), loss on training batch is 0.0237421.
After 7565 training step(s), loss on training batch is 0.0148749.
After 7566 training step(s), loss on training batch is 0.026622.
After 7567 training step(s), loss on training batch is 0.0155602.
After 7568 training step(s), loss on training batch is 0.0141283.
After 7569 training step(s), loss on training batch is 0.0148179.
After 7570 training step(s), loss on training batch is 0.0166975.
After 7571 training step(s), loss on training batch is 0.0121622.
After 7572 training step(s), loss on training batch is 0.0143348.
After 7573 training step(s), loss on training batch is 0.0140983.
After 7574 training step(s), loss on training batch is 0.0224985.
After 7575 training step(s), loss on training batch is 0.0118929.
After 7576 training step(s), loss on training batch is 0.0163658.
After 7577 training step(s), loss on training batch is 0.0135377.
After 7578 training step(s), loss on training batch is 0.022941.
After 7579 training step(s), loss on training batch is 0.0247751.
After 7580 training step(s), loss on training batch is 0.0219232.
After 7581 training step(s), loss on training batch is 0.0130807.
After 7582 training step(s), loss on training batch is 0.0120472.
After 7583 training step(s), loss on training batch is 0.056928.
After 7584 training step(s), loss on training batch is 0.0327978.
After 7585 training step(s), loss on training batch is 0.0400841.
After 7586 training step(s), loss on training batch is 0.0255664.
After 7587 training step(s), loss on training batch is 0.0129508.
After 7588 training step(s), loss on training batch is 0.0254484.
After 7589 training step(s), loss on training batch is 0.0156993.
After 7590 training step(s), loss on training batch is 0.0240301.
After 7591 training step(s), loss on training batch is 0.017692.
After 7592 training step(s), loss on training batch is 0.0163885.
After 7593 training step(s), loss on training batch is 0.0176569.
After 7594 training step(s), loss on training batch is 0.0148311.
After 7595 training step(s), loss on training batch is 0.0177834.
After 7596 training step(s), loss on training batch is 0.0264918.
After 7597 training step(s), loss on training batch is 0.0128723.
After 7598 training step(s), loss on training batch is 0.0139916.
After 7599 training step(s), loss on training batch is 0.034288.
After 7600 training step(s), loss on training batch is 0.0370851.
After 7601 training step(s), loss on training batch is 0.0156941.
After 7602 training step(s), loss on training batch is 0.0258409.
After 7603 training step(s), loss on training batch is 0.0254297.
After 7604 training step(s), loss on training batch is 0.0185969.
After 7605 training step(s), loss on training batch is 0.0162559.
After 7606 training step(s), loss on training batch is 0.0228501.
After 7607 training step(s), loss on training batch is 0.0127239.
After 7608 training step(s), loss on training batch is 0.0128156.
After 7609 training step(s), loss on training batch is 0.0178684.
After 7610 training step(s), loss on training batch is 0.0177819.
After 7611 training step(s), loss on training batch is 0.0198311.
After 7612 training step(s), loss on training batch is 0.016987.
After 7613 training step(s), loss on training batch is 0.0203843.
After 7614 training step(s), loss on training batch is 0.0231849.
After 7615 training step(s), loss on training batch is 0.0151088.
After 7616 training step(s), loss on training batch is 0.0399575.
After 7617 training step(s), loss on training batch is 0.0191541.
After 7618 training step(s), loss on training batch is 0.0167274.
After 7619 training step(s), loss on training batch is 0.021396.
After 7620 training step(s), loss on training batch is 0.0138075.
After 7621 training step(s), loss on training batch is 0.01759.
After 7622 training step(s), loss on training batch is 0.0138499.
After 7623 training step(s), loss on training batch is 0.0124854.
After 7624 training step(s), loss on training batch is 0.0137983.
After 7625 training step(s), loss on training batch is 0.0150603.
After 7626 training step(s), loss on training batch is 0.0140059.
After 7627 training step(s), loss on training batch is 0.0154603.
After 7628 training step(s), loss on training batch is 0.0111777.
After 7629 training step(s), loss on training batch is 0.0204176.
After 7630 training step(s), loss on training batch is 0.0287786.
After 7631 training step(s), loss on training batch is 0.0304194.
After 7632 training step(s), loss on training batch is 0.0173141.
After 7633 training step(s), loss on training batch is 0.0221885.
After 7634 training step(s), loss on training batch is 0.0192051.
After 7635 training step(s), loss on training batch is 0.013357.
After 7636 training step(s), loss on training batch is 0.0280349.
After 7637 training step(s), loss on training batch is 0.026351.
After 7638 training step(s), loss on training batch is 0.0166061.
After 7639 training step(s), loss on training batch is 0.0176728.
After 7640 training step(s), loss on training batch is 0.015875.
After 7641 training step(s), loss on training batch is 0.0137445.
After 7642 training step(s), loss on training batch is 0.0147807.
After 7643 training step(s), loss on training batch is 0.0236896.
After 7644 training step(s), loss on training batch is 0.0201216.
After 7645 training step(s), loss on training batch is 0.0152607.
After 7646 training step(s), loss on training batch is 0.0354642.
After 7647 training step(s), loss on training batch is 0.035499.
After 7648 training step(s), loss on training batch is 0.012767.
After 7649 training step(s), loss on training batch is 0.0473202.
After 7650 training step(s), loss on training batch is 0.0333843.
After 7651 training step(s), loss on training batch is 0.0154146.
After 7652 training step(s), loss on training batch is 0.0125945.
After 7653 training step(s), loss on training batch is 0.0205815.
After 7654 training step(s), loss on training batch is 0.0190982.
After 7655 training step(s), loss on training batch is 0.0117589.
After 7656 training step(s), loss on training batch is 0.011291.
After 7657 training step(s), loss on training batch is 0.0211802.
After 7658 training step(s), loss on training batch is 0.0136345.
After 7659 training step(s), loss on training batch is 0.0191802.
After 7660 training step(s), loss on training batch is 0.0353618.
After 7661 training step(s), loss on training batch is 0.0191172.
After 7662 training step(s), loss on training batch is 0.0291943.
After 7663 training step(s), loss on training batch is 0.0165169.
After 7664 training step(s), loss on training batch is 0.0163275.
After 7665 training step(s), loss on training batch is 0.0135457.
After 7666 training step(s), loss on training batch is 0.0174342.
After 7667 training step(s), loss on training batch is 0.0226932.
After 7668 training step(s), loss on training batch is 0.0489648.
After 7669 training step(s), loss on training batch is 0.0219438.
After 7670 training step(s), loss on training batch is 0.0178963.
After 7671 training step(s), loss on training batch is 0.012574.
After 7672 training step(s), loss on training batch is 0.0212212.
After 7673 training step(s), loss on training batch is 0.0142236.
After 7674 training step(s), loss on training batch is 0.013116.
After 7675 training step(s), loss on training batch is 0.0190558.
After 7676 training step(s), loss on training batch is 0.0150807.
After 7677 training step(s), loss on training batch is 0.0154635.
After 7678 training step(s), loss on training batch is 0.0201776.
After 7679 training step(s), loss on training batch is 0.0165726.
After 7680 training step(s), loss on training batch is 0.0161564.
After 7681 training step(s), loss on training batch is 0.0152922.
After 7682 training step(s), loss on training batch is 0.0270632.
After 7683 training step(s), loss on training batch is 0.0260793.
After 7684 training step(s), loss on training batch is 0.0127654.
After 7685 training step(s), loss on training batch is 0.0289106.
After 7686 training step(s), loss on training batch is 0.0127966.
After 7687 training step(s), loss on training batch is 0.0132359.
After 7688 training step(s), loss on training batch is 0.0164058.
After 7689 training step(s), loss on training batch is 0.0255842.
After 7690 training step(s), loss on training batch is 0.0115203.
After 7691 training step(s), loss on training batch is 0.0143527.
After 7692 training step(s), loss on training batch is 0.0250285.
After 7693 training step(s), loss on training batch is 0.0179392.
After 7694 training step(s), loss on training batch is 0.0193938.
After 7695 training step(s), loss on training batch is 0.0243287.
After 7696 training step(s), loss on training batch is 0.0331278.
After 7697 training step(s), loss on training batch is 0.0114158.
After 7698 training step(s), loss on training batch is 0.0201015.
After 7699 training step(s), loss on training batch is 0.0139355.
After 7700 training step(s), loss on training batch is 0.0182789.
After 7701 training step(s), loss on training batch is 0.0132491.
After 7702 training step(s), loss on training batch is 0.0134776.
After 7703 training step(s), loss on training batch is 0.0162762.
After 7704 training step(s), loss on training batch is 0.0178664.
After 7705 training step(s), loss on training batch is 0.0279908.
After 7706 training step(s), loss on training batch is 0.0125706.
After 7707 training step(s), loss on training batch is 0.0134475.
After 7708 training step(s), loss on training batch is 0.0138637.
After 7709 training step(s), loss on training batch is 0.0164601.
After 7710 training step(s), loss on training batch is 0.0142971.
After 7711 training step(s), loss on training batch is 0.0157751.
After 7712 training step(s), loss on training batch is 0.0136223.
After 7713 training step(s), loss on training batch is 0.011478.
After 7714 training step(s), loss on training batch is 0.014646.
After 7715 training step(s), loss on training batch is 0.0125639.
After 7716 training step(s), loss on training batch is 0.0130209.
After 7717 training step(s), loss on training batch is 0.0285759.
After 7718 training step(s), loss on training batch is 0.0152824.
After 7719 training step(s), loss on training batch is 0.0305504.
After 7720 training step(s), loss on training batch is 0.0131269.
After 7721 training step(s), loss on training batch is 0.0256779.
After 7722 training step(s), loss on training batch is 0.0277412.
After 7723 training step(s), loss on training batch is 0.0116624.
After 7724 training step(s), loss on training batch is 0.0152992.
After 7725 training step(s), loss on training batch is 0.0132459.
After 7726 training step(s), loss on training batch is 0.0118023.
After 7727 training step(s), loss on training batch is 0.021577.
After 7728 training step(s), loss on training batch is 0.011836.
After 7729 training step(s), loss on training batch is 0.0120447.
After 7730 training step(s), loss on training batch is 0.0139843.
After 7731 training step(s), loss on training batch is 0.0123607.
After 7732 training step(s), loss on training batch is 0.0228133.
After 7733 training step(s), loss on training batch is 0.0189272.
After 7734 training step(s), loss on training batch is 0.0165489.
After 7735 training step(s), loss on training batch is 0.0139754.
After 7736 training step(s), loss on training batch is 0.0123791.
After 7737 training step(s), loss on training batch is 0.0150501.
After 7738 training step(s), loss on training batch is 0.0142699.
After 7739 training step(s), loss on training batch is 0.0131653.
After 7740 training step(s), loss on training batch is 0.0198986.
After 7741 training step(s), loss on training batch is 0.0139609.
After 7742 training step(s), loss on training batch is 0.0121283.
After 7743 training step(s), loss on training batch is 0.0127396.
After 7744 training step(s), loss on training batch is 0.0158582.
After 7745 training step(s), loss on training batch is 0.0165923.
After 7746 training step(s), loss on training batch is 0.0108309.
After 7747 training step(s), loss on training batch is 0.0194739.
After 7748 training step(s), loss on training batch is 0.0160295.
After 7749 training step(s), loss on training batch is 0.0111565.
After 7750 training step(s), loss on training batch is 0.012306.
After 7751 training step(s), loss on training batch is 0.0120523.
After 7752 training step(s), loss on training batch is 0.0192882.
After 7753 training step(s), loss on training batch is 0.0241676.
After 7754 training step(s), loss on training batch is 0.0146474.
After 7755 training step(s), loss on training batch is 0.0132297.
After 7756 training step(s), loss on training batch is 0.0132223.
After 7757 training step(s), loss on training batch is 0.0138626.
After 7758 training step(s), loss on training batch is 0.0296308.
After 7759 training step(s), loss on training batch is 0.0336228.
After 7760 training step(s), loss on training batch is 0.0168283.
After 7761 training step(s), loss on training batch is 0.0146682.
After 7762 training step(s), loss on training batch is 0.0288738.
After 7763 training step(s), loss on training batch is 0.0128148.
After 7764 training step(s), loss on training batch is 0.0127141.
After 7765 training step(s), loss on training batch is 0.0527144.
After 7766 training step(s), loss on training batch is 0.0162281.
After 7767 training step(s), loss on training batch is 0.0131273.
After 7768 training step(s), loss on training batch is 0.0129913.
After 7769 training step(s), loss on training batch is 0.0125163.
After 7770 training step(s), loss on training batch is 0.0139063.
After 7771 training step(s), loss on training batch is 0.0130145.
After 7772 training step(s), loss on training batch is 0.0160087.
After 7773 training step(s), loss on training batch is 0.0152097.
After 7774 training step(s), loss on training batch is 0.0160098.
After 7775 training step(s), loss on training batch is 0.0134514.
After 7776 training step(s), loss on training batch is 0.013337.
After 7777 training step(s), loss on training batch is 0.0176266.
After 7778 training step(s), loss on training batch is 0.0486812.
After 7779 training step(s), loss on training batch is 0.035461.
After 7780 training step(s), loss on training batch is 0.0182233.
After 7781 training step(s), loss on training batch is 0.0116841.
After 7782 training step(s), loss on training batch is 0.0140065.
After 7783 training step(s), loss on training batch is 0.0188073.
After 7784 training step(s), loss on training batch is 0.0154659.
After 7785 training step(s), loss on training batch is 0.0193087.
After 7786 training step(s), loss on training batch is 0.0164016.
After 7787 training step(s), loss on training batch is 0.0146252.
After 7788 training step(s), loss on training batch is 0.0199605.
After 7789 training step(s), loss on training batch is 0.0123599.
After 7790 training step(s), loss on training batch is 0.0119051.
After 7791 training step(s), loss on training batch is 0.0122947.
After 7792 training step(s), loss on training batch is 0.0153954.
After 7793 training step(s), loss on training batch is 0.0145488.
After 7794 training step(s), loss on training batch is 0.014822.
After 7795 training step(s), loss on training batch is 0.018431.
After 7796 training step(s), loss on training batch is 0.0164817.
After 7797 training step(s), loss on training batch is 0.013391.
After 7798 training step(s), loss on training batch is 0.0117104.
After 7799 training step(s), loss on training batch is 0.0142154.
After 7800 training step(s), loss on training batch is 0.012498.
After 7801 training step(s), loss on training batch is 0.0138111.
After 7802 training step(s), loss on training batch is 0.011458.
After 7803 training step(s), loss on training batch is 0.0136933.
After 7804 training step(s), loss on training batch is 0.014923.
After 7805 training step(s), loss on training batch is 0.0108938.
After 7806 training step(s), loss on training batch is 0.0123692.
After 7807 training step(s), loss on training batch is 0.0150835.
After 7808 training step(s), loss on training batch is 0.0160408.
After 7809 training step(s), loss on training batch is 0.0117605.
After 7810 training step(s), loss on training batch is 0.014549.
After 7811 training step(s), loss on training batch is 0.0143798.
After 7812 training step(s), loss on training batch is 0.0287756.
After 7813 training step(s), loss on training batch is 0.011522.
After 7814 training step(s), loss on training batch is 0.0247119.
After 7815 training step(s), loss on training batch is 0.0106124.
After 7816 training step(s), loss on training batch is 0.013904.
After 7817 training step(s), loss on training batch is 0.0137329.
After 7818 training step(s), loss on training batch is 0.0191569.
After 7819 training step(s), loss on training batch is 0.0225058.
After 7820 training step(s), loss on training batch is 0.0138105.
After 7821 training step(s), loss on training batch is 0.0149196.
After 7822 training step(s), loss on training batch is 0.0126249.
After 7823 training step(s), loss on training batch is 0.010616.
After 7824 training step(s), loss on training batch is 0.020951.
After 7825 training step(s), loss on training batch is 0.0115521.
After 7826 training step(s), loss on training batch is 0.0128511.
After 7827 training step(s), loss on training batch is 0.0199168.
After 7828 training step(s), loss on training batch is 0.0233993.
After 7829 training step(s), loss on training batch is 0.0128163.
After 7830 training step(s), loss on training batch is 0.0339566.
After 7831 training step(s), loss on training batch is 0.0288325.
After 7832 training step(s), loss on training batch is 0.0150133.
After 7833 training step(s), loss on training batch is 0.0127244.
After 7834 training step(s), loss on training batch is 0.0147118.
After 7835 training step(s), loss on training batch is 0.070094.
After 7836 training step(s), loss on training batch is 0.0168518.
After 7837 training step(s), loss on training batch is 0.0377567.
After 7838 training step(s), loss on training batch is 0.017161.
After 7839 training step(s), loss on training batch is 0.0178507.
After 7840 training step(s), loss on training batch is 0.0227915.
After 7841 training step(s), loss on training batch is 0.0201737.
After 7842 training step(s), loss on training batch is 0.0207245.
After 7843 training step(s), loss on training batch is 0.0278113.
After 7844 training step(s), loss on training batch is 0.0142723.
After 7845 training step(s), loss on training batch is 0.0165173.
After 7846 training step(s), loss on training batch is 0.0121335.
After 7847 training step(s), loss on training batch is 0.0118089.
After 7848 training step(s), loss on training batch is 0.0144414.
After 7849 training step(s), loss on training batch is 0.0159423.
After 7850 training step(s), loss on training batch is 0.0222962.
After 7851 training step(s), loss on training batch is 0.0145945.
After 7852 training step(s), loss on training batch is 0.0130063.
After 7853 training step(s), loss on training batch is 0.0265771.
After 7854 training step(s), loss on training batch is 0.0130927.
After 7855 training step(s), loss on training batch is 0.0208065.
After 7856 training step(s), loss on training batch is 0.0110284.
After 7857 training step(s), loss on training batch is 0.0121052.
After 7858 training step(s), loss on training batch is 0.0137509.
After 7859 training step(s), loss on training batch is 0.0204477.
After 7860 training step(s), loss on training batch is 0.0282781.
After 7861 training step(s), loss on training batch is 0.013801.
After 7862 training step(s), loss on training batch is 0.0150613.
After 7863 training step(s), loss on training batch is 0.0190477.
After 7864 training step(s), loss on training batch is 0.0107593.
After 7865 training step(s), loss on training batch is 0.01301.
After 7866 training step(s), loss on training batch is 0.0223346.
After 7867 training step(s), loss on training batch is 0.0124757.
After 7868 training step(s), loss on training batch is 0.0214614.
After 7869 training step(s), loss on training batch is 0.0156184.
After 7870 training step(s), loss on training batch is 0.0113234.
After 7871 training step(s), loss on training batch is 0.0116541.
After 7872 training step(s), loss on training batch is 0.0145829.
After 7873 training step(s), loss on training batch is 0.0125163.
After 7874 training step(s), loss on training batch is 0.0129236.
After 7875 training step(s), loss on training batch is 0.0124712.
After 7876 training step(s), loss on training batch is 0.0126305.
After 7877 training step(s), loss on training batch is 0.0122354.
After 7878 training step(s), loss on training batch is 0.0119805.
After 7879 training step(s), loss on training batch is 0.0274582.
After 7880 training step(s), loss on training batch is 0.0212558.
After 7881 training step(s), loss on training batch is 0.0155568.
After 7882 training step(s), loss on training batch is 0.0152428.
After 7883 training step(s), loss on training batch is 0.0145455.
After 7884 training step(s), loss on training batch is 0.0183306.
After 7885 training step(s), loss on training batch is 0.0131839.
After 7886 training step(s), loss on training batch is 0.0444699.
After 7887 training step(s), loss on training batch is 0.0236849.
After 7888 training step(s), loss on training batch is 0.0484666.
After 7889 training step(s), loss on training batch is 0.0165047.
After 7890 training step(s), loss on training batch is 0.0223503.
After 7891 training step(s), loss on training batch is 0.0165516.
After 7892 training step(s), loss on training batch is 0.0141981.
After 7893 training step(s), loss on training batch is 0.0157023.
After 7894 training step(s), loss on training batch is 0.0130465.
After 7895 training step(s), loss on training batch is 0.0142254.
After 7896 training step(s), loss on training batch is 0.0292216.
After 7897 training step(s), loss on training batch is 0.0118677.
After 7898 training step(s), loss on training batch is 0.016796.
After 7899 training step(s), loss on training batch is 0.0166713.
After 7900 training step(s), loss on training batch is 0.0189596.
After 7901 training step(s), loss on training batch is 0.0130201.
After 7902 training step(s), loss on training batch is 0.0350553.
After 7903 training step(s), loss on training batch is 0.0113596.
After 7904 training step(s), loss on training batch is 0.0121151.
After 7905 training step(s), loss on training batch is 0.0173189.
After 7906 training step(s), loss on training batch is 0.0112671.
After 7907 training step(s), loss on training batch is 0.0161367.
After 7908 training step(s), loss on training batch is 0.0272257.
After 7909 training step(s), loss on training batch is 0.0156915.
After 7910 training step(s), loss on training batch is 0.0196594.
After 7911 training step(s), loss on training batch is 0.0191702.
After 7912 training step(s), loss on training batch is 0.0154538.
After 7913 training step(s), loss on training batch is 0.0152093.
After 7914 training step(s), loss on training batch is 0.020537.
After 7915 training step(s), loss on training batch is 0.0242064.
After 7916 training step(s), loss on training batch is 0.018292.
After 7917 training step(s), loss on training batch is 0.0290377.
After 7918 training step(s), loss on training batch is 0.0303472.
After 7919 training step(s), loss on training batch is 0.0160131.
After 7920 training step(s), loss on training batch is 0.0128103.
After 7921 training step(s), loss on training batch is 0.0108305.
After 7922 training step(s), loss on training batch is 0.0111284.
After 7923 training step(s), loss on training batch is 0.0207503.
After 7924 training step(s), loss on training batch is 0.0183312.
After 7925 training step(s), loss on training batch is 0.0234441.
After 7926 training step(s), loss on training batch is 0.0166448.
After 7927 training step(s), loss on training batch is 0.0117887.
After 7928 training step(s), loss on training batch is 0.0112466.
After 7929 training step(s), loss on training batch is 0.0248173.
After 7930 training step(s), loss on training batch is 0.033127.
After 7931 training step(s), loss on training batch is 0.0151242.
After 7932 training step(s), loss on training batch is 0.0306461.
After 7933 training step(s), loss on training batch is 0.01236.
After 7934 training step(s), loss on training batch is 0.0189652.
After 7935 training step(s), loss on training batch is 0.0137343.
After 7936 training step(s), loss on training batch is 0.0162178.
After 7937 training step(s), loss on training batch is 0.0174578.
After 7938 training step(s), loss on training batch is 0.0118268.
After 7939 training step(s), loss on training batch is 0.0133662.
After 7940 training step(s), loss on training batch is 0.018779.
After 7941 training step(s), loss on training batch is 0.0117402.
After 7942 training step(s), loss on training batch is 0.0117528.
After 7943 training step(s), loss on training batch is 0.01284.
After 7944 training step(s), loss on training batch is 0.0142911.
After 7945 training step(s), loss on training batch is 0.0169932.
After 7946 training step(s), loss on training batch is 0.0163582.
After 7947 training step(s), loss on training batch is 0.0127995.
After 7948 training step(s), loss on training batch is 0.0110888.
After 7949 training step(s), loss on training batch is 0.0228065.
After 7950 training step(s), loss on training batch is 0.0279284.
After 7951 training step(s), loss on training batch is 0.0129748.
After 7952 training step(s), loss on training batch is 0.0120585.
After 7953 training step(s), loss on training batch is 0.015164.
After 7954 training step(s), loss on training batch is 0.0114612.
After 7955 training step(s), loss on training batch is 0.0257611.
After 7956 training step(s), loss on training batch is 0.0167558.
After 7957 training step(s), loss on training batch is 0.0161967.
After 7958 training step(s), loss on training batch is 0.0126176.
After 7959 training step(s), loss on training batch is 0.0206047.
After 7960 training step(s), loss on training batch is 0.0171789.
After 7961 training step(s), loss on training batch is 0.0144453.
After 7962 training step(s), loss on training batch is 0.0438786.
After 7963 training step(s), loss on training batch is 0.0205984.
After 7964 training step(s), loss on training batch is 0.0128126.
After 7965 training step(s), loss on training batch is 0.0259659.
After 7966 training step(s), loss on training batch is 0.0203009.
After 7967 training step(s), loss on training batch is 0.0151445.
After 7968 training step(s), loss on training batch is 0.0116673.
After 7969 training step(s), loss on training batch is 0.0124428.
After 7970 training step(s), loss on training batch is 0.0121888.
After 7971 training step(s), loss on training batch is 0.0162325.
After 7972 training step(s), loss on training batch is 0.0123433.
After 7973 training step(s), loss on training batch is 0.0121793.
After 7974 training step(s), loss on training batch is 0.0171308.
After 7975 training step(s), loss on training batch is 0.0153796.
After 7976 training step(s), loss on training batch is 0.0467636.
After 7977 training step(s), loss on training batch is 0.0109733.
After 7978 training step(s), loss on training batch is 0.0139548.
After 7979 training step(s), loss on training batch is 0.0125164.
After 7980 training step(s), loss on training batch is 0.0140518.
After 7981 training step(s), loss on training batch is 0.0170411.
After 7982 training step(s), loss on training batch is 0.0120746.
After 7983 training step(s), loss on training batch is 0.0119353.
After 7984 training step(s), loss on training batch is 0.0136521.
After 7985 training step(s), loss on training batch is 0.015131.
After 7986 training step(s), loss on training batch is 0.0109734.
After 7987 training step(s), loss on training batch is 0.0116494.
After 7988 training step(s), loss on training batch is 0.0106113.
After 7989 training step(s), loss on training batch is 0.0348606.
After 7990 training step(s), loss on training batch is 0.0148633.
After 7991 training step(s), loss on training batch is 0.0134961.
After 7992 training step(s), loss on training batch is 0.0121824.
After 7993 training step(s), loss on training batch is 0.0200176.
After 7994 training step(s), loss on training batch is 0.0129508.
After 7995 training step(s), loss on training batch is 0.0142807.
After 7996 training step(s), loss on training batch is 0.0118251.
After 7997 training step(s), loss on training batch is 0.012841.
After 7998 training step(s), loss on training batch is 0.0138768.
After 7999 training step(s), loss on training batch is 0.011624.
After 8000 training step(s), loss on training batch is 0.0104901.
After 8001 training step(s), loss on training batch is 0.0112134.
After 8002 training step(s), loss on training batch is 0.0146088.
After 8003 training step(s), loss on training batch is 0.0112345.
After 8004 training step(s), loss on training batch is 0.0214204.
After 8005 training step(s), loss on training batch is 0.0129028.
After 8006 training step(s), loss on training batch is 0.0279234.
After 8007 training step(s), loss on training batch is 0.0156222.
After 8008 training step(s), loss on training batch is 0.0356065.
After 8009 training step(s), loss on training batch is 0.036379.
After 8010 training step(s), loss on training batch is 0.0425938.
After 8011 training step(s), loss on training batch is 0.0146602.
After 8012 training step(s), loss on training batch is 0.0137499.
After 8013 training step(s), loss on training batch is 0.0132697.
After 8014 training step(s), loss on training batch is 0.0140357.
After 8015 training step(s), loss on training batch is 0.0214778.
After 8016 training step(s), loss on training batch is 0.0354133.
After 8017 training step(s), loss on training batch is 0.0165366.
After 8018 training step(s), loss on training batch is 0.0113186.
After 8019 training step(s), loss on training batch is 0.0211118.
After 8020 training step(s), loss on training batch is 0.0132863.
After 8021 training step(s), loss on training batch is 0.0109455.
After 8022 training step(s), loss on training batch is 0.0124702.
After 8023 training step(s), loss on training batch is 0.0104801.
After 8024 training step(s), loss on training batch is 0.0170946.
After 8025 training step(s), loss on training batch is 0.014898.
After 8026 training step(s), loss on training batch is 0.0106885.
After 8027 training step(s), loss on training batch is 0.0145066.
After 8028 training step(s), loss on training batch is 0.0136396.
After 8029 training step(s), loss on training batch is 0.0112084.
After 8030 training step(s), loss on training batch is 0.0145249.
After 8031 training step(s), loss on training batch is 0.0226843.
After 8032 training step(s), loss on training batch is 0.0131304.
After 8033 training step(s), loss on training batch is 0.0160237.
After 8034 training step(s), loss on training batch is 0.0157549.
After 8035 training step(s), loss on training batch is 0.0133378.
After 8036 training step(s), loss on training batch is 0.0111454.
After 8037 training step(s), loss on training batch is 0.0136523.
After 8038 training step(s), loss on training batch is 0.0107208.
After 8039 training step(s), loss on training batch is 0.0159869.
After 8040 training step(s), loss on training batch is 0.0132317.
After 8041 training step(s), loss on training batch is 0.0141454.
After 8042 training step(s), loss on training batch is 0.01727.
After 8043 training step(s), loss on training batch is 0.0152017.
After 8044 training step(s), loss on training batch is 0.0128729.
After 8045 training step(s), loss on training batch is 0.0182155.
After 8046 training step(s), loss on training batch is 0.0176042.
After 8047 training step(s), loss on training batch is 0.0194202.
After 8048 training step(s), loss on training batch is 0.0107582.
After 8049 training step(s), loss on training batch is 0.0149227.
After 8050 training step(s), loss on training batch is 0.010897.
After 8051 training step(s), loss on training batch is 0.0129934.
After 8052 training step(s), loss on training batch is 0.0105582.
After 8053 training step(s), loss on training batch is 0.0136924.
After 8054 training step(s), loss on training batch is 0.0113252.
After 8055 training step(s), loss on training batch is 0.0130261.
After 8056 training step(s), loss on training batch is 0.014787.
After 8057 training step(s), loss on training batch is 0.0131276.
After 8058 training step(s), loss on training batch is 0.0127423.
After 8059 training step(s), loss on training batch is 0.0141151.
After 8060 training step(s), loss on training batch is 0.0144113.
After 8061 training step(s), loss on training batch is 0.0116906.
After 8062 training step(s), loss on training batch is 0.0121094.
After 8063 training step(s), loss on training batch is 0.0106907.
After 8064 training step(s), loss on training batch is 0.0161421.
After 8065 training step(s), loss on training batch is 0.0117885.
After 8066 training step(s), loss on training batch is 0.0129283.
After 8067 training step(s), loss on training batch is 0.0163491.
After 8068 training step(s), loss on training batch is 0.0109349.
After 8069 training step(s), loss on training batch is 0.0149538.
After 8070 training step(s), loss on training batch is 0.0110516.
After 8071 training step(s), loss on training batch is 0.0110073.
After 8072 training step(s), loss on training batch is 0.0156145.
After 8073 training step(s), loss on training batch is 0.0260975.
After 8074 training step(s), loss on training batch is 0.0170912.
After 8075 training step(s), loss on training batch is 0.0119043.
After 8076 training step(s), loss on training batch is 0.011831.
After 8077 training step(s), loss on training batch is 0.0114704.
After 8078 training step(s), loss on training batch is 0.0121664.
After 8079 training step(s), loss on training batch is 0.0133108.
After 8080 training step(s), loss on training batch is 0.0138691.
After 8081 training step(s), loss on training batch is 0.0163134.
After 8082 training step(s), loss on training batch is 0.0148425.
After 8083 training step(s), loss on training batch is 0.0128265.
After 8084 training step(s), loss on training batch is 0.0124414.
After 8085 training step(s), loss on training batch is 0.0117349.
After 8086 training step(s), loss on training batch is 0.0109632.
After 8087 training step(s), loss on training batch is 0.0131905.
After 8088 training step(s), loss on training batch is 0.0163483.
After 8089 training step(s), loss on training batch is 0.0123389.
After 8090 training step(s), loss on training batch is 0.0123246.
After 8091 training step(s), loss on training batch is 0.0147297.
After 8092 training step(s), loss on training batch is 0.0118762.
After 8093 training step(s), loss on training batch is 0.0121878.
After 8094 training step(s), loss on training batch is 0.0131695.
After 8095 training step(s), loss on training batch is 0.0206889.
After 8096 training step(s), loss on training batch is 0.012524.
After 8097 training step(s), loss on training batch is 0.0119625.
After 8098 training step(s), loss on training batch is 0.0107995.
After 8099 training step(s), loss on training batch is 0.0134685.
After 8100 training step(s), loss on training batch is 0.0127813.
After 8101 training step(s), loss on training batch is 0.0140454.
After 8102 training step(s), loss on training batch is 0.0216813.
After 8103 training step(s), loss on training batch is 0.0154312.
After 8104 training step(s), loss on training batch is 0.0141561.
After 8105 training step(s), loss on training batch is 0.0123766.
After 8106 training step(s), loss on training batch is 0.0111048.
After 8107 training step(s), loss on training batch is 0.0200372.
After 8108 training step(s), loss on training batch is 0.0106285.
After 8109 training step(s), loss on training batch is 0.0163812.
After 8110 training step(s), loss on training batch is 0.0109705.
After 8111 training step(s), loss on training batch is 0.0186993.
After 8112 training step(s), loss on training batch is 0.0113339.
After 8113 training step(s), loss on training batch is 0.0146107.
After 8114 training step(s), loss on training batch is 0.0187244.
After 8115 training step(s), loss on training batch is 0.0132857.
After 8116 training step(s), loss on training batch is 0.017797.
After 8117 training step(s), loss on training batch is 0.0246155.
After 8118 training step(s), loss on training batch is 0.0391878.
After 8119 training step(s), loss on training batch is 0.0169007.
After 8120 training step(s), loss on training batch is 0.0352881.
After 8121 training step(s), loss on training batch is 0.0271225.
After 8122 training step(s), loss on training batch is 0.0194568.
After 8123 training step(s), loss on training batch is 0.0108313.
After 8124 training step(s), loss on training batch is 0.0152197.
After 8125 training step(s), loss on training batch is 0.0107627.
After 8126 training step(s), loss on training batch is 0.0125558.
After 8127 training step(s), loss on training batch is 0.0113081.
After 8128 training step(s), loss on training batch is 0.0105038.
After 8129 training step(s), loss on training batch is 0.0110235.
After 8130 training step(s), loss on training batch is 0.0107502.
After 8131 training step(s), loss on training batch is 0.0138127.
After 8132 training step(s), loss on training batch is 0.0121425.
After 8133 training step(s), loss on training batch is 0.0136752.
After 8134 training step(s), loss on training batch is 0.011673.
After 8135 training step(s), loss on training batch is 0.0175263.
After 8136 training step(s), loss on training batch is 0.0118281.
After 8137 training step(s), loss on training batch is 0.0166456.
After 8138 training step(s), loss on training batch is 0.0366899.
After 8139 training step(s), loss on training batch is 0.0118401.
After 8140 training step(s), loss on training batch is 0.0178382.
After 8141 training step(s), loss on training batch is 0.0109865.
After 8142 training step(s), loss on training batch is 0.0144193.
After 8143 training step(s), loss on training batch is 0.0107047.
After 8144 training step(s), loss on training batch is 0.0117366.
After 8145 training step(s), loss on training batch is 0.0138176.
After 8146 training step(s), loss on training batch is 0.0153371.
After 8147 training step(s), loss on training batch is 0.0121785.
After 8148 training step(s), loss on training batch is 0.0131661.
After 8149 training step(s), loss on training batch is 0.0209009.
After 8150 training step(s), loss on training batch is 0.0126438.
After 8151 training step(s), loss on training batch is 0.0150143.
After 8152 training step(s), loss on training batch is 0.0237264.
After 8153 training step(s), loss on training batch is 0.0169374.
After 8154 training step(s), loss on training batch is 0.012066.
After 8155 training step(s), loss on training batch is 0.0154401.
After 8156 training step(s), loss on training batch is 0.0286954.
After 8157 training step(s), loss on training batch is 0.0198495.
After 8158 training step(s), loss on training batch is 0.0135417.
After 8159 training step(s), loss on training batch is 0.0135468.
After 8160 training step(s), loss on training batch is 0.0137988.
After 8161 training step(s), loss on training batch is 0.0178228.
After 8162 training step(s), loss on training batch is 0.0279796.
After 8163 training step(s), loss on training batch is 0.011238.
After 8164 training step(s), loss on training batch is 0.012813.
After 8165 training step(s), loss on training batch is 0.0248545.
After 8166 training step(s), loss on training batch is 0.0110324.
After 8167 training step(s), loss on training batch is 0.0212169.
After 8168 training step(s), loss on training batch is 0.0117045.
After 8169 training step(s), loss on training batch is 0.0263744.
After 8170 training step(s), loss on training batch is 0.0119292.
After 8171 training step(s), loss on training batch is 0.0154054.
After 8172 training step(s), loss on training batch is 0.0203.
After 8173 training step(s), loss on training batch is 0.0149044.
After 8174 training step(s), loss on training batch is 0.0152619.
After 8175 training step(s), loss on training batch is 0.0123068.
After 8176 training step(s), loss on training batch is 0.0127827.
After 8177 training step(s), loss on training batch is 0.0124473.
After 8178 training step(s), loss on training batch is 0.0120693.
After 8179 training step(s), loss on training batch is 0.0108223.
After 8180 training step(s), loss on training batch is 0.0130386.
After 8181 training step(s), loss on training batch is 0.0123052.
After 8182 training step(s), loss on training batch is 0.0106455.
After 8183 training step(s), loss on training batch is 0.0173843.
After 8184 training step(s), loss on training batch is 0.0153725.
After 8185 training step(s), loss on training batch is 0.0144083.
After 8186 training step(s), loss on training batch is 0.0222986.
After 8187 training step(s), loss on training batch is 0.0177726.
After 8188 training step(s), loss on training batch is 0.0133028.
After 8189 training step(s), loss on training batch is 0.0123282.
After 8190 training step(s), loss on training batch is 0.0119833.
After 8191 training step(s), loss on training batch is 0.0124217.
After 8192 training step(s), loss on training batch is 0.0291829.
After 8193 training step(s), loss on training batch is 0.0138102.
After 8194 training step(s), loss on training batch is 0.0183024.
After 8195 training step(s), loss on training batch is 0.0154777.
After 8196 training step(s), loss on training batch is 0.0263425.
After 8197 training step(s), loss on training batch is 0.016479.
After 8198 training step(s), loss on training batch is 0.0161442.
After 8199 training step(s), loss on training batch is 0.0109348.
After 8200 training step(s), loss on training batch is 0.0149274.
After 8201 training step(s), loss on training batch is 0.0114651.
After 8202 training step(s), loss on training batch is 0.0144253.
After 8203 training step(s), loss on training batch is 0.0163469.
After 8204 training step(s), loss on training batch is 0.0109605.
After 8205 training step(s), loss on training batch is 0.0126703.
After 8206 training step(s), loss on training batch is 0.0303757.
After 8207 training step(s), loss on training batch is 0.0118559.
After 8208 training step(s), loss on training batch is 0.0126923.
After 8209 training step(s), loss on training batch is 0.0127058.
After 8210 training step(s), loss on training batch is 0.0143659.
After 8211 training step(s), loss on training batch is 0.0101517.
After 8212 training step(s), loss on training batch is 0.0114575.
After 8213 training step(s), loss on training batch is 0.029606.
After 8214 training step(s), loss on training batch is 0.0145882.
After 8215 training step(s), loss on training batch is 0.0186626.
After 8216 training step(s), loss on training batch is 0.0122809.
After 8217 training step(s), loss on training batch is 0.0218301.
After 8218 training step(s), loss on training batch is 0.0111146.
After 8219 training step(s), loss on training batch is 0.0121316.
After 8220 training step(s), loss on training batch is 0.0113161.
After 8221 training step(s), loss on training batch is 0.0194082.
After 8222 training step(s), loss on training batch is 0.017945.
After 8223 training step(s), loss on training batch is 0.0238831.
After 8224 training step(s), loss on training batch is 0.059675.
After 8225 training step(s), loss on training batch is 0.0147681.
After 8226 training step(s), loss on training batch is 0.0167783.
After 8227 training step(s), loss on training batch is 0.019016.
After 8228 training step(s), loss on training batch is 0.0104931.
After 8229 training step(s), loss on training batch is 0.0121063.
After 8230 training step(s), loss on training batch is 0.0271181.
After 8231 training step(s), loss on training batch is 0.0113626.
After 8232 training step(s), loss on training batch is 0.0192405.
After 8233 training step(s), loss on training batch is 0.0135088.
After 8234 training step(s), loss on training batch is 0.0164992.
After 8235 training step(s), loss on training batch is 0.0136359.
After 8236 training step(s), loss on training batch is 0.0139426.
After 8237 training step(s), loss on training batch is 0.0195601.
After 8238 training step(s), loss on training batch is 0.018856.
After 8239 training step(s), loss on training batch is 0.0158777.
After 8240 training step(s), loss on training batch is 0.011099.
After 8241 training step(s), loss on training batch is 0.0134143.
After 8242 training step(s), loss on training batch is 0.0142931.
After 8243 training step(s), loss on training batch is 0.0126632.
After 8244 training step(s), loss on training batch is 0.0247039.
After 8245 training step(s), loss on training batch is 0.0226823.
After 8246 training step(s), loss on training batch is 0.0122162.
After 8247 training step(s), loss on training batch is 0.010936.
After 8248 training step(s), loss on training batch is 0.0178469.
After 8249 training step(s), loss on training batch is 0.0172746.
After 8250 training step(s), loss on training batch is 0.0175945.
After 8251 training step(s), loss on training batch is 0.0104453.
After 8252 training step(s), loss on training batch is 0.0101669.
After 8253 training step(s), loss on training batch is 0.0114979.
After 8254 training step(s), loss on training batch is 0.0113029.
After 8255 training step(s), loss on training batch is 0.0132615.
After 8256 training step(s), loss on training batch is 0.0163074.
After 8257 training step(s), loss on training batch is 0.0145379.
After 8258 training step(s), loss on training batch is 0.0151581.
After 8259 training step(s), loss on training batch is 0.0107315.
After 8260 training step(s), loss on training batch is 0.0106537.
After 8261 training step(s), loss on training batch is 0.0106306.
After 8262 training step(s), loss on training batch is 0.0144802.
After 8263 training step(s), loss on training batch is 0.0116876.
After 8264 training step(s), loss on training batch is 0.0117516.
After 8265 training step(s), loss on training batch is 0.0117185.
After 8266 training step(s), loss on training batch is 0.0118453.
After 8267 training step(s), loss on training batch is 0.0112513.
After 8268 training step(s), loss on training batch is 0.0134099.
After 8269 training step(s), loss on training batch is 0.0107196.
After 8270 training step(s), loss on training batch is 0.0106759.
After 8271 training step(s), loss on training batch is 0.0140044.
After 8272 training step(s), loss on training batch is 0.0117018.
After 8273 training step(s), loss on training batch is 0.0108015.
After 8274 training step(s), loss on training batch is 0.0122442.
After 8275 training step(s), loss on training batch is 0.0237704.
After 8276 training step(s), loss on training batch is 0.0153752.
After 8277 training step(s), loss on training batch is 0.0232242.
After 8278 training step(s), loss on training batch is 0.0126651.
After 8279 training step(s), loss on training batch is 0.023676.
After 8280 training step(s), loss on training batch is 0.0151006.
After 8281 training step(s), loss on training batch is 0.0228773.
After 8282 training step(s), loss on training batch is 0.00993879.
After 8283 training step(s), loss on training batch is 0.012277.
After 8284 training step(s), loss on training batch is 0.0222081.
After 8285 training step(s), loss on training batch is 0.0116858.
After 8286 training step(s), loss on training batch is 0.0160456.
After 8287 training step(s), loss on training batch is 0.0111429.
After 8288 training step(s), loss on training batch is 0.0176934.
After 8289 training step(s), loss on training batch is 0.0219757.
After 8290 training step(s), loss on training batch is 0.014264.
After 8291 training step(s), loss on training batch is 0.0109008.
After 8292 training step(s), loss on training batch is 0.022728.
After 8293 training step(s), loss on training batch is 0.0132908.
After 8294 training step(s), loss on training batch is 0.0245457.
After 8295 training step(s), loss on training batch is 0.016852.
After 8296 training step(s), loss on training batch is 0.013883.
After 8297 training step(s), loss on training batch is 0.0159651.
After 8298 training step(s), loss on training batch is 0.0121866.
After 8299 training step(s), loss on training batch is 0.0126808.
After 8300 training step(s), loss on training batch is 0.0142081.
After 8301 training step(s), loss on training batch is 0.0114026.
After 8302 training step(s), loss on training batch is 0.010595.
After 8303 training step(s), loss on training batch is 0.012749.
After 8304 training step(s), loss on training batch is 0.0119248.
After 8305 training step(s), loss on training batch is 0.0200043.
After 8306 training step(s), loss on training batch is 0.0106019.
After 8307 training step(s), loss on training batch is 0.0269205.
After 8308 training step(s), loss on training batch is 0.0140932.
After 8309 training step(s), loss on training batch is 0.0183106.
After 8310 training step(s), loss on training batch is 0.0115117.
After 8311 training step(s), loss on training batch is 0.0119892.
After 8312 training step(s), loss on training batch is 0.0138303.
After 8313 training step(s), loss on training batch is 0.015743.
After 8314 training step(s), loss on training batch is 0.0122176.
After 8315 training step(s), loss on training batch is 0.0114933.
After 8316 training step(s), loss on training batch is 0.0123493.
After 8317 training step(s), loss on training batch is 0.0120417.
After 8318 training step(s), loss on training batch is 0.0102882.
After 8319 training step(s), loss on training batch is 0.0116206.
After 8320 training step(s), loss on training batch is 0.011216.
After 8321 training step(s), loss on training batch is 0.0114499.
After 8322 training step(s), loss on training batch is 0.010628.
After 8323 training step(s), loss on training batch is 0.0105979.
After 8324 training step(s), loss on training batch is 0.0120236.
After 8325 training step(s), loss on training batch is 0.0142954.
After 8326 training step(s), loss on training batch is 0.0289265.
After 8327 training step(s), loss on training batch is 0.0167421.
After 8328 training step(s), loss on training batch is 0.0104369.
After 8329 training step(s), loss on training batch is 0.0124497.
After 8330 training step(s), loss on training batch is 0.0109939.
After 8331 training step(s), loss on training batch is 0.0136527.
After 8332 training step(s), loss on training batch is 0.0131124.
After 8333 training step(s), loss on training batch is 0.0113762.
After 8334 training step(s), loss on training batch is 0.0102571.
After 8335 training step(s), loss on training batch is 0.0109729.
After 8336 training step(s), loss on training batch is 0.0129177.
After 8337 training step(s), loss on training batch is 0.0113125.
After 8338 training step(s), loss on training batch is 0.0106859.
After 8339 training step(s), loss on training batch is 0.012732.
After 8340 training step(s), loss on training batch is 0.0152984.
After 8341 training step(s), loss on training batch is 0.0147003.
After 8342 training step(s), loss on training batch is 0.0112947.
After 8343 training step(s), loss on training batch is 0.0130352.
After 8344 training step(s), loss on training batch is 0.0131988.
After 8345 training step(s), loss on training batch is 0.0110173.
After 8346 training step(s), loss on training batch is 0.0209357.
After 8347 training step(s), loss on training batch is 0.0172229.
After 8348 training step(s), loss on training batch is 0.011272.
After 8349 training step(s), loss on training batch is 0.0130745.
After 8350 training step(s), loss on training batch is 0.0110214.
After 8351 training step(s), loss on training batch is 0.0185114.
After 8352 training step(s), loss on training batch is 0.0162707.
After 8353 training step(s), loss on training batch is 0.023835.
After 8354 training step(s), loss on training batch is 0.0109255.
After 8355 training step(s), loss on training batch is 0.0124388.
After 8356 training step(s), loss on training batch is 0.0251095.
After 8357 training step(s), loss on training batch is 0.0121745.
After 8358 training step(s), loss on training batch is 0.0112313.
After 8359 training step(s), loss on training batch is 0.0177764.
After 8360 training step(s), loss on training batch is 0.0137759.
After 8361 training step(s), loss on training batch is 0.0143292.
After 8362 training step(s), loss on training batch is 0.0113678.
After 8363 training step(s), loss on training batch is 0.0205707.
After 8364 training step(s), loss on training batch is 0.0245898.
After 8365 training step(s), loss on training batch is 0.0145664.
After 8366 training step(s), loss on training batch is 0.0110631.
After 8367 training step(s), loss on training batch is 0.0125456.
After 8368 training step(s), loss on training batch is 0.0134692.
After 8369 training step(s), loss on training batch is 0.0201593.
After 8370 training step(s), loss on training batch is 0.0130487.
After 8371 training step(s), loss on training batch is 0.0110299.
After 8372 training step(s), loss on training batch is 0.0115796.
After 8373 training step(s), loss on training batch is 0.0113254.
After 8374 training step(s), loss on training batch is 0.0124475.
After 8375 training step(s), loss on training batch is 0.0178415.
After 8376 training step(s), loss on training batch is 0.0131525.
After 8377 training step(s), loss on training batch is 0.0164347.
After 8378 training step(s), loss on training batch is 0.0172802.
After 8379 training step(s), loss on training batch is 0.0110335.
After 8380 training step(s), loss on training batch is 0.00967578.
After 8381 training step(s), loss on training batch is 0.0313844.
After 8382 training step(s), loss on training batch is 0.0156823.
After 8383 training step(s), loss on training batch is 0.0184816.
After 8384 training step(s), loss on training batch is 0.0169213.
After 8385 training step(s), loss on training batch is 0.00943991.
After 8386 training step(s), loss on training batch is 0.0139782.
After 8387 training step(s), loss on training batch is 0.0107883.
After 8388 training step(s), loss on training batch is 0.011857.
After 8389 training step(s), loss on training batch is 0.0140263.
After 8390 training step(s), loss on training batch is 0.0107031.
After 8391 training step(s), loss on training batch is 0.010917.
After 8392 training step(s), loss on training batch is 0.00967058.
After 8393 training step(s), loss on training batch is 0.0114513.
After 8394 training step(s), loss on training batch is 0.0105616.
After 8395 training step(s), loss on training batch is 0.0145507.
After 8396 training step(s), loss on training batch is 0.0135467.
After 8397 training step(s), loss on training batch is 0.0128652.
After 8398 training step(s), loss on training batch is 0.0107431.
After 8399 training step(s), loss on training batch is 0.0111884.
After 8400 training step(s), loss on training batch is 0.0127511.
After 8401 training step(s), loss on training batch is 0.0159401.
After 8402 training step(s), loss on training batch is 0.0111856.
After 8403 training step(s), loss on training batch is 0.010431.
After 8404 training step(s), loss on training batch is 0.0125337.
After 8405 training step(s), loss on training batch is 0.0116636.
After 8406 training step(s), loss on training batch is 0.0102664.
After 8407 training step(s), loss on training batch is 0.0233115.
After 8408 training step(s), loss on training batch is 0.0124947.
After 8409 training step(s), loss on training batch is 0.0183008.
After 8410 training step(s), loss on training batch is 0.0102525.
After 8411 training step(s), loss on training batch is 0.0107575.
After 8412 training step(s), loss on training batch is 0.0110349.
After 8413 training step(s), loss on training batch is 0.0108015.
After 8414 training step(s), loss on training batch is 0.0376545.
After 8415 training step(s), loss on training batch is 0.0135412.
After 8416 training step(s), loss on training batch is 0.00980903.
After 8417 training step(s), loss on training batch is 0.0180403.
After 8418 training step(s), loss on training batch is 0.0118309.
After 8419 training step(s), loss on training batch is 0.0182917.
After 8420 training step(s), loss on training batch is 0.0275016.
After 8421 training step(s), loss on training batch is 0.0147491.
After 8422 training step(s), loss on training batch is 0.0172623.
After 8423 training step(s), loss on training batch is 0.0187337.
After 8424 training step(s), loss on training batch is 0.0125365.
After 8425 training step(s), loss on training batch is 0.0102603.
After 8426 training step(s), loss on training batch is 0.0124264.
After 8427 training step(s), loss on training batch is 0.0175036.
After 8428 training step(s), loss on training batch is 0.021441.
After 8429 training step(s), loss on training batch is 0.0129909.
After 8430 training step(s), loss on training batch is 0.0123818.
After 8431 training step(s), loss on training batch is 0.0109041.
After 8432 training step(s), loss on training batch is 0.0182226.
After 8433 training step(s), loss on training batch is 0.011777.
After 8434 training step(s), loss on training batch is 0.0190229.
After 8435 training step(s), loss on training batch is 0.00982833.
After 8436 training step(s), loss on training batch is 0.0141891.
After 8437 training step(s), loss on training batch is 0.0128317.
After 8438 training step(s), loss on training batch is 0.0109807.
After 8439 training step(s), loss on training batch is 0.0120788.
After 8440 training step(s), loss on training batch is 0.0116729.
After 8441 training step(s), loss on training batch is 0.0113353.
After 8442 training step(s), loss on training batch is 0.0109247.
After 8443 training step(s), loss on training batch is 0.00988616.
After 8444 training step(s), loss on training batch is 0.0137144.
After 8445 training step(s), loss on training batch is 0.0211917.
After 8446 training step(s), loss on training batch is 0.0137444.
After 8447 training step(s), loss on training batch is 0.0204126.
After 8448 training step(s), loss on training batch is 0.0472713.
After 8449 training step(s), loss on training batch is 0.0157646.
After 8450 training step(s), loss on training batch is 0.0221926.
After 8451 training step(s), loss on training batch is 0.0107062.
After 8452 training step(s), loss on training batch is 0.0127094.
After 8453 training step(s), loss on training batch is 0.0393931.
After 8454 training step(s), loss on training batch is 0.0138263.
After 8455 training step(s), loss on training batch is 0.010988.
After 8456 training step(s), loss on training batch is 0.0112694.
After 8457 training step(s), loss on training batch is 0.00968494.
After 8458 training step(s), loss on training batch is 0.0161409.
After 8459 training step(s), loss on training batch is 0.0165397.
After 8460 training step(s), loss on training batch is 0.0152355.
After 8461 training step(s), loss on training batch is 0.0157861.
After 8462 training step(s), loss on training batch is 0.0107186.
After 8463 training step(s), loss on training batch is 0.0161823.
After 8464 training step(s), loss on training batch is 0.0221762.
After 8465 training step(s), loss on training batch is 0.0146862.
After 8466 training step(s), loss on training batch is 0.0126759.
After 8467 training step(s), loss on training batch is 0.0129511.
After 8468 training step(s), loss on training batch is 0.0106745.
After 8469 training step(s), loss on training batch is 0.0113964.
After 8470 training step(s), loss on training batch is 0.0101014.
After 8471 training step(s), loss on training batch is 0.0119187.
After 8472 training step(s), loss on training batch is 0.0171499.
After 8473 training step(s), loss on training batch is 0.0102249.
After 8474 training step(s), loss on training batch is 0.0111561.
After 8475 training step(s), loss on training batch is 0.0209175.
After 8476 training step(s), loss on training batch is 0.014013.
After 8477 training step(s), loss on training batch is 0.0224494.
After 8478 training step(s), loss on training batch is 0.016641.
After 8479 training step(s), loss on training batch is 0.0103364.
After 8480 training step(s), loss on training batch is 0.0135806.
After 8481 training step(s), loss on training batch is 0.0396056.
After 8482 training step(s), loss on training batch is 0.0358431.
After 8483 training step(s), loss on training batch is 0.0148205.
After 8484 training step(s), loss on training batch is 0.0266309.
After 8485 training step(s), loss on training batch is 0.0110682.
After 8486 training step(s), loss on training batch is 0.0105364.
After 8487 training step(s), loss on training batch is 0.0185533.
After 8488 training step(s), loss on training batch is 0.0111653.
After 8489 training step(s), loss on training batch is 0.0274918.
After 8490 training step(s), loss on training batch is 0.014587.
After 8491 training step(s), loss on training batch is 0.0299165.
After 8492 training step(s), loss on training batch is 0.0112701.
After 8493 training step(s), loss on training batch is 0.0213898.
After 8494 training step(s), loss on training batch is 0.0169739.
After 8495 training step(s), loss on training batch is 0.0104902.
After 8496 training step(s), loss on training batch is 0.012791.
After 8497 training step(s), loss on training batch is 0.013408.
After 8498 training step(s), loss on training batch is 0.0193715.
After 8499 training step(s), loss on training batch is 0.0149579.
After 8500 training step(s), loss on training batch is 0.0112239.
After 8501 training step(s), loss on training batch is 0.0153504.
After 8502 training step(s), loss on training batch is 0.0109663.
After 8503 training step(s), loss on training batch is 0.0131603.
After 8504 training step(s), loss on training batch is 0.0104332.
After 8505 training step(s), loss on training batch is 0.0176099.
After 8506 training step(s), loss on training batch is 0.0106.
After 8507 training step(s), loss on training batch is 0.0134429.
After 8508 training step(s), loss on training batch is 0.0131094.
After 8509 training step(s), loss on training batch is 0.0126711.
After 8510 training step(s), loss on training batch is 0.0150222.
After 8511 training step(s), loss on training batch is 0.0146613.
After 8512 training step(s), loss on training batch is 0.0100857.
After 8513 training step(s), loss on training batch is 0.0118115.
After 8514 training step(s), loss on training batch is 0.010836.
After 8515 training step(s), loss on training batch is 0.011061.
After 8516 training step(s), loss on training batch is 0.0130177.
After 8517 training step(s), loss on training batch is 0.0102027.
After 8518 training step(s), loss on training batch is 0.013926.
After 8519 training step(s), loss on training batch is 0.0144551.
After 8520 training step(s), loss on training batch is 0.0116789.
After 8521 training step(s), loss on training batch is 0.0121151.
After 8522 training step(s), loss on training batch is 0.0114647.
After 8523 training step(s), loss on training batch is 0.0101304.
After 8524 training step(s), loss on training batch is 0.0104231.
After 8525 training step(s), loss on training batch is 0.0103222.
After 8526 training step(s), loss on training batch is 0.00968858.
After 8527 training step(s), loss on training batch is 0.0112849.
After 8528 training step(s), loss on training batch is 0.0109751.
After 8529 training step(s), loss on training batch is 0.0130511.
After 8530 training step(s), loss on training batch is 0.0127094.
After 8531 training step(s), loss on training batch is 0.0184006.
After 8532 training step(s), loss on training batch is 0.0118263.
After 8533 training step(s), loss on training batch is 0.0114772.
After 8534 training step(s), loss on training batch is 0.00985058.
After 8535 training step(s), loss on training batch is 0.0111895.
After 8536 training step(s), loss on training batch is 0.0113324.
After 8537 training step(s), loss on training batch is 0.0238545.
After 8538 training step(s), loss on training batch is 0.0105794.
After 8539 training step(s), loss on training batch is 0.0172959.
After 8540 training step(s), loss on training batch is 0.0100094.
After 8541 training step(s), loss on training batch is 0.0150196.
After 8542 training step(s), loss on training batch is 0.0110562.
After 8543 training step(s), loss on training batch is 0.0145566.
After 8544 training step(s), loss on training batch is 0.0116741.
After 8545 training step(s), loss on training batch is 0.0103804.
After 8546 training step(s), loss on training batch is 0.0133682.
After 8547 training step(s), loss on training batch is 0.0215867.
After 8548 training step(s), loss on training batch is 0.0125835.
After 8549 training step(s), loss on training batch is 0.0116016.
After 8550 training step(s), loss on training batch is 0.00988114.
After 8551 training step(s), loss on training batch is 0.0111658.
After 8552 training step(s), loss on training batch is 0.0113945.
After 8553 training step(s), loss on training batch is 0.0115539.
After 8554 training step(s), loss on training batch is 0.0111437.
After 8555 training step(s), loss on training batch is 0.0103693.
After 8556 training step(s), loss on training batch is 0.0104948.
After 8557 training step(s), loss on training batch is 0.0113857.
After 8558 training step(s), loss on training batch is 0.0105105.
After 8559 training step(s), loss on training batch is 0.0152318.
After 8560 training step(s), loss on training batch is 0.0147565.
After 8561 training step(s), loss on training batch is 0.012896.
After 8562 training step(s), loss on training batch is 0.0151807.
After 8563 training step(s), loss on training batch is 0.0100117.
After 8564 training step(s), loss on training batch is 0.0098018.
After 8565 training step(s), loss on training batch is 0.00994356.
After 8566 training step(s), loss on training batch is 0.011576.
After 8567 training step(s), loss on training batch is 0.0105752.
After 8568 training step(s), loss on training batch is 0.0135969.
After 8569 training step(s), loss on training batch is 0.0122491.
After 8570 training step(s), loss on training batch is 0.00983779.
After 8571 training step(s), loss on training batch is 0.011508.
After 8572 training step(s), loss on training batch is 0.0132926.
After 8573 training step(s), loss on training batch is 0.0157268.
After 8574 training step(s), loss on training batch is 0.0251601.
After 8575 training step(s), loss on training batch is 0.0102012.
After 8576 training step(s), loss on training batch is 0.0119535.
After 8577 training step(s), loss on training batch is 0.0105577.
After 8578 training step(s), loss on training batch is 0.015246.
After 8579 training step(s), loss on training batch is 0.010332.
After 8580 training step(s), loss on training batch is 0.0174788.
After 8581 training step(s), loss on training batch is 0.0119945.
After 8582 training step(s), loss on training batch is 0.0139213.
After 8583 training step(s), loss on training batch is 0.0132142.
After 8584 training step(s), loss on training batch is 0.0131251.
After 8585 training step(s), loss on training batch is 0.0137208.
After 8586 training step(s), loss on training batch is 0.013613.
After 8587 training step(s), loss on training batch is 0.0099651.
After 8588 training step(s), loss on training batch is 0.0385539.
After 8589 training step(s), loss on training batch is 0.0134277.
After 8590 training step(s), loss on training batch is 0.0109036.
After 8591 training step(s), loss on training batch is 0.00971182.
After 8592 training step(s), loss on training batch is 0.0105172.
After 8593 training step(s), loss on training batch is 0.0261051.
After 8594 training step(s), loss on training batch is 0.00984855.
After 8595 training step(s), loss on training batch is 0.0106065.
After 8596 training step(s), loss on training batch is 0.0143201.
After 8597 training step(s), loss on training batch is 0.0115474.
After 8598 training step(s), loss on training batch is 0.0106248.
After 8599 training step(s), loss on training batch is 0.01096.
After 8600 training step(s), loss on training batch is 0.0101496.
After 8601 training step(s), loss on training batch is 0.0108416.
After 8602 training step(s), loss on training batch is 0.0119967.
After 8603 training step(s), loss on training batch is 0.0111151.
After 8604 training step(s), loss on training batch is 0.00999021.
After 8605 training step(s), loss on training batch is 0.0107766.
After 8606 training step(s), loss on training batch is 0.0101207.
After 8607 training step(s), loss on training batch is 0.0133119.
After 8608 training step(s), loss on training batch is 0.0102868.
After 8609 training step(s), loss on training batch is 0.0109891.
After 8610 training step(s), loss on training batch is 0.0116259.
After 8611 training step(s), loss on training batch is 0.0107805.
After 8612 training step(s), loss on training batch is 0.0106414.
After 8613 training step(s), loss on training batch is 0.0104458.
After 8614 training step(s), loss on training batch is 0.00966858.
After 8615 training step(s), loss on training batch is 0.0110501.
After 8616 training step(s), loss on training batch is 0.0100643.
After 8617 training step(s), loss on training batch is 0.0107987.
After 8618 training step(s), loss on training batch is 0.0110411.
After 8619 training step(s), loss on training batch is 0.010404.
After 8620 training step(s), loss on training batch is 0.0116491.
After 8621 training step(s), loss on training batch is 0.017168.
After 8622 training step(s), loss on training batch is 0.0171813.
After 8623 training step(s), loss on training batch is 0.0156499.
After 8624 training step(s), loss on training batch is 0.0131664.
After 8625 training step(s), loss on training batch is 0.0275764.
After 8626 training step(s), loss on training batch is 0.0163287.
After 8627 training step(s), loss on training batch is 0.0152415.
After 8628 training step(s), loss on training batch is 0.0182018.
After 8629 training step(s), loss on training batch is 0.0110465.
After 8630 training step(s), loss on training batch is 0.011926.
After 8631 training step(s), loss on training batch is 0.0122887.
After 8632 training step(s), loss on training batch is 0.0104201.
After 8633 training step(s), loss on training batch is 0.0104778.
After 8634 training step(s), loss on training batch is 0.00988627.
After 8635 training step(s), loss on training batch is 0.0123751.
After 8636 training step(s), loss on training batch is 0.0246536.
After 8637 training step(s), loss on training batch is 0.0150135.
After 8638 training step(s), loss on training batch is 0.0108924.
After 8639 training step(s), loss on training batch is 0.0174785.
After 8640 training step(s), loss on training batch is 0.0113307.
After 8641 training step(s), loss on training batch is 0.0138008.
After 8642 training step(s), loss on training batch is 0.0172164.
After 8643 training step(s), loss on training batch is 0.0129675.
After 8644 training step(s), loss on training batch is 0.00978406.
After 8645 training step(s), loss on training batch is 0.0117727.
After 8646 training step(s), loss on training batch is 0.0113382.
After 8647 training step(s), loss on training batch is 0.0111076.
After 8648 training step(s), loss on training batch is 0.0105302.
After 8649 training step(s), loss on training batch is 0.00988405.
After 8650 training step(s), loss on training batch is 0.00987035.
After 8651 training step(s), loss on training batch is 0.0111131.
After 8652 training step(s), loss on training batch is 0.0419704.
After 8653 training step(s), loss on training batch is 0.0150815.
After 8654 training step(s), loss on training batch is 0.0119171.
After 8655 training step(s), loss on training batch is 0.0109377.
After 8656 training step(s), loss on training batch is 0.0199495.
After 8657 training step(s), loss on training batch is 0.0242945.
After 8658 training step(s), loss on training batch is 0.00997906.
After 8659 training step(s), loss on training batch is 0.011028.
After 8660 training step(s), loss on training batch is 0.0218873.
After 8661 training step(s), loss on training batch is 0.0119486.
After 8662 training step(s), loss on training batch is 0.014003.
After 8663 training step(s), loss on training batch is 0.00980758.
After 8664 training step(s), loss on training batch is 0.0117375.
After 8665 training step(s), loss on training batch is 0.0166387.
After 8666 training step(s), loss on training batch is 0.024286.
After 8667 training step(s), loss on training batch is 0.0116031.
After 8668 training step(s), loss on training batch is 0.0124162.
After 8669 training step(s), loss on training batch is 0.0114337.
After 8670 training step(s), loss on training batch is 0.0126808.
After 8671 training step(s), loss on training batch is 0.016324.
After 8672 training step(s), loss on training batch is 0.0159803.
After 8673 training step(s), loss on training batch is 0.0114423.
After 8674 training step(s), loss on training batch is 0.00986335.
After 8675 training step(s), loss on training batch is 0.0103868.
After 8676 training step(s), loss on training batch is 0.0131096.
After 8677 training step(s), loss on training batch is 0.0127586.
After 8678 training step(s), loss on training batch is 0.0170778.
After 8679 training step(s), loss on training batch is 0.0167411.
After 8680 training step(s), loss on training batch is 0.0209681.
After 8681 training step(s), loss on training batch is 0.0101778.
After 8682 training step(s), loss on training batch is 0.0111892.
After 8683 training step(s), loss on training batch is 0.0141425.
After 8684 training step(s), loss on training batch is 0.0191302.
After 8685 training step(s), loss on training batch is 0.0126846.
After 8686 training step(s), loss on training batch is 0.0203117.
After 8687 training step(s), loss on training batch is 0.0195571.
After 8688 training step(s), loss on training batch is 0.0122087.
After 8689 training step(s), loss on training batch is 0.0121955.
After 8690 training step(s), loss on training batch is 0.0146292.
After 8691 training step(s), loss on training batch is 0.0115914.
After 8692 training step(s), loss on training batch is 0.0115819.
After 8693 training step(s), loss on training batch is 0.0103278.
After 8694 training step(s), loss on training batch is 0.0245085.
After 8695 training step(s), loss on training batch is 0.0130288.
After 8696 training step(s), loss on training batch is 0.0154157.
After 8697 training step(s), loss on training batch is 0.0152103.
After 8698 training step(s), loss on training batch is 0.013799.
After 8699 training step(s), loss on training batch is 0.00988091.
After 8700 training step(s), loss on training batch is 0.0133052.
After 8701 training step(s), loss on training batch is 0.0164665.
After 8702 training step(s), loss on training batch is 0.0191018.
After 8703 training step(s), loss on training batch is 0.0101672.
After 8704 training step(s), loss on training batch is 0.0131723.
After 8705 training step(s), loss on training batch is 0.0168629.
After 8706 training step(s), loss on training batch is 0.0114864.
After 8707 training step(s), loss on training batch is 0.012935.
After 8708 training step(s), loss on training batch is 0.0145407.
After 8709 training step(s), loss on training batch is 0.0096757.
After 8710 training step(s), loss on training batch is 0.01557.
After 8711 training step(s), loss on training batch is 0.0192821.
After 8712 training step(s), loss on training batch is 0.0146871.
After 8713 training step(s), loss on training batch is 0.0172562.
After 8714 training step(s), loss on training batch is 0.0115714.
After 8715 training step(s), loss on training batch is 0.0103616.
After 8716 training step(s), loss on training batch is 0.0175381.
After 8717 training step(s), loss on training batch is 0.0168549.
After 8718 training step(s), loss on training batch is 0.0130599.
After 8719 training step(s), loss on training batch is 0.0092374.
After 8720 training step(s), loss on training batch is 0.0105683.
After 8721 training step(s), loss on training batch is 0.0130578.
After 8722 training step(s), loss on training batch is 0.0128734.
After 8723 training step(s), loss on training batch is 0.0112812.
After 8724 training step(s), loss on training batch is 0.0092094.
After 8725 training step(s), loss on training batch is 0.0124419.
After 8726 training step(s), loss on training batch is 0.0101546.
After 8727 training step(s), loss on training batch is 0.0124015.
After 8728 training step(s), loss on training batch is 0.0124109.
After 8729 training step(s), loss on training batch is 0.012918.
After 8730 training step(s), loss on training batch is 0.0109174.
After 8731 training step(s), loss on training batch is 0.0105018.
After 8732 training step(s), loss on training batch is 0.0117157.
After 8733 training step(s), loss on training batch is 0.00956215.
After 8734 training step(s), loss on training batch is 0.010811.
After 8735 training step(s), loss on training batch is 0.00994833.
After 8736 training step(s), loss on training batch is 0.0183066.
After 8737 training step(s), loss on training batch is 0.0111702.
After 8738 training step(s), loss on training batch is 0.0110807.
After 8739 training step(s), loss on training batch is 0.0138038.
After 8740 training step(s), loss on training batch is 0.0105094.
After 8741 training step(s), loss on training batch is 0.00964374.
After 8742 training step(s), loss on training batch is 0.00963091.
After 8743 training step(s), loss on training batch is 0.0152552.
After 8744 training step(s), loss on training batch is 0.0151034.
After 8745 training step(s), loss on training batch is 0.0108195.
After 8746 training step(s), loss on training batch is 0.0145472.
After 8747 training step(s), loss on training batch is 0.0111519.
After 8748 training step(s), loss on training batch is 0.0108125.
After 8749 training step(s), loss on training batch is 0.0164935.
After 8750 training step(s), loss on training batch is 0.0137132.
After 8751 training step(s), loss on training batch is 0.00928668.
After 8752 training step(s), loss on training batch is 0.011748.
After 8753 training step(s), loss on training batch is 0.0119849.
After 8754 training step(s), loss on training batch is 0.0131585.
After 8755 training step(s), loss on training batch is 0.0122953.
After 8756 training step(s), loss on training batch is 0.00961383.
After 8757 training step(s), loss on training batch is 0.0097096.
After 8758 training step(s), loss on training batch is 0.00969327.
After 8759 training step(s), loss on training batch is 0.013386.
After 8760 training step(s), loss on training batch is 0.00929181.
After 8761 training step(s), loss on training batch is 0.011282.
After 8762 training step(s), loss on training batch is 0.010091.
After 8763 training step(s), loss on training batch is 0.0108741.
After 8764 training step(s), loss on training batch is 0.0110349.
After 8765 training step(s), loss on training batch is 0.0103196.
After 8766 training step(s), loss on training batch is 0.0131248.
After 8767 training step(s), loss on training batch is 0.010389.
After 8768 training step(s), loss on training batch is 0.00952673.
After 8769 training step(s), loss on training batch is 0.0135452.
After 8770 training step(s), loss on training batch is 0.0167263.
After 8771 training step(s), loss on training batch is 0.0138976.
After 8772 training step(s), loss on training batch is 0.0126001.
After 8773 training step(s), loss on training batch is 0.0149182.
After 8774 training step(s), loss on training batch is 0.01413.
After 8775 training step(s), loss on training batch is 0.0168148.
After 8776 training step(s), loss on training batch is 0.00909908.
After 8777 training step(s), loss on training batch is 0.00974992.
After 8778 training step(s), loss on training batch is 0.0138472.
After 8779 training step(s), loss on training batch is 0.00995421.
After 8780 training step(s), loss on training batch is 0.0242484.
After 8781 training step(s), loss on training batch is 0.0133657.
After 8782 training step(s), loss on training batch is 0.0295952.
After 8783 training step(s), loss on training batch is 0.0105667.
After 8784 training step(s), loss on training batch is 0.012471.
After 8785 training step(s), loss on training batch is 0.0119785.
After 8786 training step(s), loss on training batch is 0.0101791.
After 8787 training step(s), loss on training batch is 0.0139575.
After 8788 training step(s), loss on training batch is 0.0219434.
After 8789 training step(s), loss on training batch is 0.0102256.
After 8790 training step(s), loss on training batch is 0.0127726.
After 8791 training step(s), loss on training batch is 0.0145679.
After 8792 training step(s), loss on training batch is 0.0123939.
After 8793 training step(s), loss on training batch is 0.0117885.
After 8794 training step(s), loss on training batch is 0.0196899.
After 8795 training step(s), loss on training batch is 0.0128886.
After 8796 training step(s), loss on training batch is 0.0138918.
After 8797 training step(s), loss on training batch is 0.0129918.
After 8798 training step(s), loss on training batch is 0.012606.
After 8799 training step(s), loss on training batch is 0.015295.
After 8800 training step(s), loss on training batch is 0.0106048.
After 8801 training step(s), loss on training batch is 0.0107355.
After 8802 training step(s), loss on training batch is 0.0141056.
After 8803 training step(s), loss on training batch is 0.0105018.
After 8804 training step(s), loss on training batch is 0.0107624.
After 8805 training step(s), loss on training batch is 0.0250942.
After 8806 training step(s), loss on training batch is 0.00980207.
After 8807 training step(s), loss on training batch is 0.030946.
After 8808 training step(s), loss on training batch is 0.014074.
After 8809 training step(s), loss on training batch is 0.0124918.
After 8810 training step(s), loss on training batch is 0.0106647.
After 8811 training step(s), loss on training batch is 0.0139506.
After 8812 training step(s), loss on training batch is 0.0115062.
After 8813 training step(s), loss on training batch is 0.0094476.
After 8814 training step(s), loss on training batch is 0.0120629.
After 8815 training step(s), loss on training batch is 0.00997384.
After 8816 training step(s), loss on training batch is 0.0110654.
After 8817 training step(s), loss on training batch is 0.011975.
After 8818 training step(s), loss on training batch is 0.0128949.
After 8819 training step(s), loss on training batch is 0.00982711.
After 8820 training step(s), loss on training batch is 0.0109964.
After 8821 training step(s), loss on training batch is 0.0102702.
After 8822 training step(s), loss on training batch is 0.00993611.
After 8823 training step(s), loss on training batch is 0.0116693.
After 8824 training step(s), loss on training batch is 0.00938767.
After 8825 training step(s), loss on training batch is 0.0111165.
After 8826 training step(s), loss on training batch is 0.00902019.
After 8827 training step(s), loss on training batch is 0.00987089.
After 8828 training step(s), loss on training batch is 0.0092932.
After 8829 training step(s), loss on training batch is 0.0133019.
After 8830 training step(s), loss on training batch is 0.0097823.
After 8831 training step(s), loss on training batch is 0.00950613.
After 8832 training step(s), loss on training batch is 0.01214.
After 8833 training step(s), loss on training batch is 0.0130487.
After 8834 training step(s), loss on training batch is 0.0126145.
After 8835 training step(s), loss on training batch is 0.0109288.
After 8836 training step(s), loss on training batch is 0.00987094.
After 8837 training step(s), loss on training batch is 0.00979835.
After 8838 training step(s), loss on training batch is 0.00958216.
After 8839 training step(s), loss on training batch is 0.0100514.
After 8840 training step(s), loss on training batch is 0.0104589.
After 8841 training step(s), loss on training batch is 0.00924394.
After 8842 training step(s), loss on training batch is 0.0105124.
After 8843 training step(s), loss on training batch is 0.0165398.
After 8844 training step(s), loss on training batch is 0.0109769.
After 8845 training step(s), loss on training batch is 0.00998787.
After 8846 training step(s), loss on training batch is 0.0100161.
After 8847 training step(s), loss on training batch is 0.0107367.
After 8848 training step(s), loss on training batch is 0.00940501.
After 8849 training step(s), loss on training batch is 0.0108098.
After 8850 training step(s), loss on training batch is 0.00938755.
After 8851 training step(s), loss on training batch is 0.00926944.
After 8852 training step(s), loss on training batch is 0.0151126.
After 8853 training step(s), loss on training batch is 0.0106514.
After 8854 training step(s), loss on training batch is 0.0118849.
After 8855 training step(s), loss on training batch is 0.00963195.
After 8856 training step(s), loss on training batch is 0.0165401.
After 8857 training step(s), loss on training batch is 0.0116561.
After 8858 training step(s), loss on training batch is 0.00978217.
After 8859 training step(s), loss on training batch is 0.0104502.
After 8860 training step(s), loss on training batch is 0.0104054.
After 8861 training step(s), loss on training batch is 0.0117162.
After 8862 training step(s), loss on training batch is 0.0111801.
After 8863 training step(s), loss on training batch is 0.014149.
After 8864 training step(s), loss on training batch is 0.0133447.
After 8865 training step(s), loss on training batch is 0.0113133.
After 8866 training step(s), loss on training batch is 0.00971214.
After 8867 training step(s), loss on training batch is 0.0111961.
After 8868 training step(s), loss on training batch is 0.0102259.
After 8869 training step(s), loss on training batch is 0.0105584.
After 8870 training step(s), loss on training batch is 0.00941453.
After 8871 training step(s), loss on training batch is 0.0103245.
After 8872 training step(s), loss on training batch is 0.00933564.
After 8873 training step(s), loss on training batch is 0.00918596.
After 8874 training step(s), loss on training batch is 0.0110073.
After 8875 training step(s), loss on training batch is 0.00969692.
After 8876 training step(s), loss on training batch is 0.0131667.
After 8877 training step(s), loss on training batch is 0.0101391.
After 8878 training step(s), loss on training batch is 0.0108439.
After 8879 training step(s), loss on training batch is 0.0115629.
After 8880 training step(s), loss on training batch is 0.00966588.
After 8881 training step(s), loss on training batch is 0.010829.
After 8882 training step(s), loss on training batch is 0.0111039.
After 8883 training step(s), loss on training batch is 0.0143477.
After 8884 training step(s), loss on training batch is 0.00944363.
After 8885 training step(s), loss on training batch is 0.0117547.
After 8886 training step(s), loss on training batch is 0.00988227.
After 8887 training step(s), loss on training batch is 0.00918136.
After 8888 training step(s), loss on training batch is 0.0109158.
After 8889 training step(s), loss on training batch is 0.0115263.
After 8890 training step(s), loss on training batch is 0.00923741.
After 8891 training step(s), loss on training batch is 0.00965821.
After 8892 training step(s), loss on training batch is 0.0101721.
After 8893 training step(s), loss on training batch is 0.0105929.
After 8894 training step(s), loss on training batch is 0.0114399.
After 8895 training step(s), loss on training batch is 0.00997604.
After 8896 training step(s), loss on training batch is 0.0102947.
After 8897 training step(s), loss on training batch is 0.0100003.
After 8898 training step(s), loss on training batch is 0.0122192.
After 8899 training step(s), loss on training batch is 0.0440794.
After 8900 training step(s), loss on training batch is 0.0183711.
After 8901 training step(s), loss on training batch is 0.0109175.
After 8902 training step(s), loss on training batch is 0.0103312.
After 8903 training step(s), loss on training batch is 0.0105027.
After 8904 training step(s), loss on training batch is 0.0104418.
After 8905 training step(s), loss on training batch is 0.00913846.
After 8906 training step(s), loss on training batch is 0.00902285.
After 8907 training step(s), loss on training batch is 0.0120138.
After 8908 training step(s), loss on training batch is 0.0112951.
After 8909 training step(s), loss on training batch is 0.0100839.
After 8910 training step(s), loss on training batch is 0.0162497.
After 8911 training step(s), loss on training batch is 0.0136272.
After 8912 training step(s), loss on training batch is 0.0175268.
After 8913 training step(s), loss on training batch is 0.0112309.
After 8914 training step(s), loss on training batch is 0.0106827.
After 8915 training step(s), loss on training batch is 0.00983051.
After 8916 training step(s), loss on training batch is 0.0114711.
After 8917 training step(s), loss on training batch is 0.00995128.
After 8918 training step(s), loss on training batch is 0.0116234.
After 8919 training step(s), loss on training batch is 0.0126215.
After 8920 training step(s), loss on training batch is 0.0101306.
After 8921 training step(s), loss on training batch is 0.0105309.
After 8922 training step(s), loss on training batch is 0.0114255.
After 8923 training step(s), loss on training batch is 0.0111079.
After 8924 training step(s), loss on training batch is 0.0104486.
After 8925 training step(s), loss on training batch is 0.00906255.
After 8926 training step(s), loss on training batch is 0.00937429.
After 8927 training step(s), loss on training batch is 0.0101438.
After 8928 training step(s), loss on training batch is 0.0145184.
After 8929 training step(s), loss on training batch is 0.00985909.
After 8930 training step(s), loss on training batch is 0.0153863.
After 8931 training step(s), loss on training batch is 0.00972043.
After 8932 training step(s), loss on training batch is 0.0158176.
After 8933 training step(s), loss on training batch is 0.0107714.
After 8934 training step(s), loss on training batch is 0.0136379.
After 8935 training step(s), loss on training batch is 0.0139385.
After 8936 training step(s), loss on training batch is 0.00942233.
After 8937 training step(s), loss on training batch is 0.0120699.
After 8938 training step(s), loss on training batch is 0.0105756.
After 8939 training step(s), loss on training batch is 0.0100452.
After 8940 training step(s), loss on training batch is 0.0118815.
After 8941 training step(s), loss on training batch is 0.0102265.
After 8942 training step(s), loss on training batch is 0.00942994.
After 8943 training step(s), loss on training batch is 0.0120068.
After 8944 training step(s), loss on training batch is 0.0106816.
After 8945 training step(s), loss on training batch is 0.0112895.
After 8946 training step(s), loss on training batch is 0.0101665.
After 8947 training step(s), loss on training batch is 0.00984575.
After 8948 training step(s), loss on training batch is 0.00880425.
After 8949 training step(s), loss on training batch is 0.0123643.
After 8950 training step(s), loss on training batch is 0.01011.
After 8951 training step(s), loss on training batch is 0.0101595.
After 8952 training step(s), loss on training batch is 0.0095797.
After 8953 training step(s), loss on training batch is 0.0108677.
After 8954 training step(s), loss on training batch is 0.00915383.
After 8955 training step(s), loss on training batch is 0.0125735.
After 8956 training step(s), loss on training batch is 0.00982444.
After 8957 training step(s), loss on training batch is 0.0116044.
After 8958 training step(s), loss on training batch is 0.00942496.
After 8959 training step(s), loss on training batch is 0.0110639.
After 8960 training step(s), loss on training batch is 0.00970975.
After 8961 training step(s), loss on training batch is 0.00959828.
After 8962 training step(s), loss on training batch is 0.00947928.
After 8963 training step(s), loss on training batch is 0.00895913.
After 8964 training step(s), loss on training batch is 0.0110001.
After 8965 training step(s), loss on training batch is 0.0100227.
After 8966 training step(s), loss on training batch is 0.00953846.
After 8967 training step(s), loss on training batch is 0.00933141.
After 8968 training step(s), loss on training batch is 0.00915382.
After 8969 training step(s), loss on training batch is 0.0102462.
After 8970 training step(s), loss on training batch is 0.00911515.
After 8971 training step(s), loss on training batch is 0.0140247.
After 8972 training step(s), loss on training batch is 0.0323672.
After 8973 training step(s), loss on training batch is 0.0109167.
After 8974 training step(s), loss on training batch is 0.0256944.
After 8975 training step(s), loss on training batch is 0.0112982.
After 8976 training step(s), loss on training batch is 0.0108074.
After 8977 training step(s), loss on training batch is 0.0113677.
After 8978 training step(s), loss on training batch is 0.00969808.
After 8979 training step(s), loss on training batch is 0.00998539.
After 8980 training step(s), loss on training batch is 0.00938524.
After 8981 training step(s), loss on training batch is 0.0103425.
After 8982 training step(s), loss on training batch is 0.0111085.
After 8983 training step(s), loss on training batch is 0.00890333.
After 8984 training step(s), loss on training batch is 0.0127103.
After 8985 training step(s), loss on training batch is 0.008975.
After 8986 training step(s), loss on training batch is 0.0239212.
After 8987 training step(s), loss on training batch is 0.013666.
After 8988 training step(s), loss on training batch is 0.00924451.
After 8989 training step(s), loss on training batch is 0.0112047.
After 8990 training step(s), loss on training batch is 0.0115023.
After 8991 training step(s), loss on training batch is 0.010459.
After 8992 training step(s), loss on training batch is 0.0188273.
After 8993 training step(s), loss on training batch is 0.0133839.
After 8994 training step(s), loss on training batch is 0.0146847.
After 8995 training step(s), loss on training batch is 0.0102805.
After 8996 training step(s), loss on training batch is 0.0092662.
After 8997 training step(s), loss on training batch is 0.0191853.
After 8998 training step(s), loss on training batch is 0.0319409.
After 8999 training step(s), loss on training batch is 0.0132253.
After 9000 training step(s), loss on training batch is 0.010263.
After 9001 training step(s), loss on training batch is 0.010309.
After 9002 training step(s), loss on training batch is 0.0103256.
After 9003 training step(s), loss on training batch is 0.0186974.
After 9004 training step(s), loss on training batch is 0.0172401.
After 9005 training step(s), loss on training batch is 0.0105778.
After 9006 training step(s), loss on training batch is 0.0107628.
After 9007 training step(s), loss on training batch is 0.011512.
After 9008 training step(s), loss on training batch is 0.0134291.
After 9009 training step(s), loss on training batch is 0.0146093.
After 9010 training step(s), loss on training batch is 0.0158733.
After 9011 training step(s), loss on training batch is 0.0121458.
After 9012 training step(s), loss on training batch is 0.0100796.
After 9013 training step(s), loss on training batch is 0.0396125.
After 9014 training step(s), loss on training batch is 0.0236135.
After 9015 training step(s), loss on training batch is 0.0105297.
After 9016 training step(s), loss on training batch is 0.0186006.
After 9017 training step(s), loss on training batch is 0.0156348.
After 9018 training step(s), loss on training batch is 0.00980119.
After 9019 training step(s), loss on training batch is 0.00991971.
After 9020 training step(s), loss on training batch is 0.00951462.
After 9021 training step(s), loss on training batch is 0.0158928.
After 9022 training step(s), loss on training batch is 0.0121244.
After 9023 training step(s), loss on training batch is 0.011212.
After 9024 training step(s), loss on training batch is 0.00962041.
After 9025 training step(s), loss on training batch is 0.00986124.
After 9026 training step(s), loss on training batch is 0.0128677.
After 9027 training step(s), loss on training batch is 0.0112291.
After 9028 training step(s), loss on training batch is 0.0097666.
After 9029 training step(s), loss on training batch is 0.014369.
After 9030 training step(s), loss on training batch is 0.00966708.
After 9031 training step(s), loss on training batch is 0.0140563.
After 9032 training step(s), loss on training batch is 0.0118864.
After 9033 training step(s), loss on training batch is 0.0120124.
After 9034 training step(s), loss on training batch is 0.0106373.
After 9035 training step(s), loss on training batch is 0.0113106.
After 9036 training step(s), loss on training batch is 0.0110216.
After 9037 training step(s), loss on training batch is 0.0119498.
After 9038 training step(s), loss on training batch is 0.0101775.
After 9039 training step(s), loss on training batch is 0.0108171.
After 9040 training step(s), loss on training batch is 0.0093379.
After 9041 training step(s), loss on training batch is 0.0121946.
After 9042 training step(s), loss on training batch is 0.0100455.
After 9043 training step(s), loss on training batch is 0.00907284.
After 9044 training step(s), loss on training batch is 0.00890075.
After 9045 training step(s), loss on training batch is 0.0124954.
After 9046 training step(s), loss on training batch is 0.0105212.
After 9047 training step(s), loss on training batch is 0.0152289.
After 9048 training step(s), loss on training batch is 0.0132092.
After 9049 training step(s), loss on training batch is 0.00980169.
After 9050 training step(s), loss on training batch is 0.00931375.
After 9051 training step(s), loss on training batch is 0.0121885.
After 9052 training step(s), loss on training batch is 0.00868427.
After 9053 training step(s), loss on training batch is 0.0106687.
After 9054 training step(s), loss on training batch is 0.0114434.
After 9055 training step(s), loss on training batch is 0.0210919.
After 9056 training step(s), loss on training batch is 0.0167909.
After 9057 training step(s), loss on training batch is 0.0127273.
After 9058 training step(s), loss on training batch is 0.00964361.
After 9059 training step(s), loss on training batch is 0.0247318.
After 9060 training step(s), loss on training batch is 0.0167386.
After 9061 training step(s), loss on training batch is 0.0103858.
After 9062 training step(s), loss on training batch is 0.0101696.
After 9063 training step(s), loss on training batch is 0.0159876.
After 9064 training step(s), loss on training batch is 0.0188295.
After 9065 training step(s), loss on training batch is 0.011624.
After 9066 training step(s), loss on training batch is 0.0088722.
After 9067 training step(s), loss on training batch is 0.0114875.
After 9068 training step(s), loss on training batch is 0.00929857.
After 9069 training step(s), loss on training batch is 0.0100578.
After 9070 training step(s), loss on training batch is 0.00899744.
After 9071 training step(s), loss on training batch is 0.0100376.
After 9072 training step(s), loss on training batch is 0.0104553.
After 9073 training step(s), loss on training batch is 0.00976198.
After 9074 training step(s), loss on training batch is 0.00949388.
After 9075 training step(s), loss on training batch is 0.0099022.
After 9076 training step(s), loss on training batch is 0.0105968.
After 9077 training step(s), loss on training batch is 0.0123673.
After 9078 training step(s), loss on training batch is 0.0169709.
After 9079 training step(s), loss on training batch is 0.0148108.
After 9080 training step(s), loss on training batch is 0.0102225.
After 9081 training step(s), loss on training batch is 0.0125644.
After 9082 training step(s), loss on training batch is 0.0106191.
After 9083 training step(s), loss on training batch is 0.0099858.
After 9084 training step(s), loss on training batch is 0.00980159.
After 9085 training step(s), loss on training batch is 0.00875935.
After 9086 training step(s), loss on training batch is 0.00993767.
After 9087 training step(s), loss on training batch is 0.0133621.
After 9088 training step(s), loss on training batch is 0.0126689.
After 9089 training step(s), loss on training batch is 0.0112682.
After 9090 training step(s), loss on training batch is 0.0109969.
After 9091 training step(s), loss on training batch is 0.0118317.
After 9092 training step(s), loss on training batch is 0.0113605.
After 9093 training step(s), loss on training batch is 0.0102971.
After 9094 training step(s), loss on training batch is 0.00908761.
After 9095 training step(s), loss on training batch is 0.0110097.
After 9096 training step(s), loss on training batch is 0.0159885.
After 9097 training step(s), loss on training batch is 0.0111425.
After 9098 training step(s), loss on training batch is 0.0105162.
After 9099 training step(s), loss on training batch is 0.011066.
After 9100 training step(s), loss on training batch is 0.00978099.
After 9101 training step(s), loss on training batch is 0.0087561.
After 9102 training step(s), loss on training batch is 0.00916465.
After 9103 training step(s), loss on training batch is 0.0121559.
After 9104 training step(s), loss on training batch is 0.0101135.
After 9105 training step(s), loss on training batch is 0.00927116.
After 9106 training step(s), loss on training batch is 0.0105012.
After 9107 training step(s), loss on training batch is 0.00892584.
After 9108 training step(s), loss on training batch is 0.00946693.
After 9109 training step(s), loss on training batch is 0.0102877.
After 9110 training step(s), loss on training batch is 0.0204145.
After 9111 training step(s), loss on training batch is 0.0117042.
After 9112 training step(s), loss on training batch is 0.0134365.
After 9113 training step(s), loss on training batch is 0.00966212.
After 9114 training step(s), loss on training batch is 0.0163487.
After 9115 training step(s), loss on training batch is 0.010479.
After 9116 training step(s), loss on training batch is 0.0138591.
After 9117 training step(s), loss on training batch is 0.0115069.
After 9118 training step(s), loss on training batch is 0.0123257.
After 9119 training step(s), loss on training batch is 0.00945502.
After 9120 training step(s), loss on training batch is 0.00957002.
After 9121 training step(s), loss on training batch is 0.0109418.
After 9122 training step(s), loss on training batch is 0.0127676.
After 9123 training step(s), loss on training batch is 0.0114206.
After 9124 training step(s), loss on training batch is 0.0130206.
After 9125 training step(s), loss on training batch is 0.00944498.
After 9126 training step(s), loss on training batch is 0.0107239.
After 9127 training step(s), loss on training batch is 0.00936312.
After 9128 training step(s), loss on training batch is 0.0101018.
After 9129 training step(s), loss on training batch is 0.011114.
After 9130 training step(s), loss on training batch is 0.0121974.
After 9131 training step(s), loss on training batch is 0.0113384.
After 9132 training step(s), loss on training batch is 0.0104897.
After 9133 training step(s), loss on training batch is 0.0129756.
After 9134 training step(s), loss on training batch is 0.0103636.
After 9135 training step(s), loss on training batch is 0.0142599.
After 9136 training step(s), loss on training batch is 0.0109103.
After 9137 training step(s), loss on training batch is 0.00915788.
After 9138 training step(s), loss on training batch is 0.0304141.
After 9139 training step(s), loss on training batch is 0.0180587.
After 9140 training step(s), loss on training batch is 0.010191.
After 9141 training step(s), loss on training batch is 0.0113208.
After 9142 training step(s), loss on training batch is 0.00989508.
After 9143 training step(s), loss on training batch is 0.0137616.
After 9144 training step(s), loss on training batch is 0.0161387.
After 9145 training step(s), loss on training batch is 0.00993911.
After 9146 training step(s), loss on training batch is 0.0105295.
After 9147 training step(s), loss on training batch is 0.0152691.
After 9148 training step(s), loss on training batch is 0.0092915.
After 9149 training step(s), loss on training batch is 0.0101674.
After 9150 training step(s), loss on training batch is 0.0106432.
After 9151 training step(s), loss on training batch is 0.0100449.
After 9152 training step(s), loss on training batch is 0.00994729.
After 9153 training step(s), loss on training batch is 0.0126179.
After 9154 training step(s), loss on training batch is 0.00851616.
After 9155 training step(s), loss on training batch is 0.0114211.
After 9156 training step(s), loss on training batch is 0.00901982.
After 9157 training step(s), loss on training batch is 0.00940138.
After 9158 training step(s), loss on training batch is 0.010173.
After 9159 training step(s), loss on training batch is 0.0116009.
After 9160 training step(s), loss on training batch is 0.0102793.
After 9161 training step(s), loss on training batch is 0.00990245.
After 9162 training step(s), loss on training batch is 0.00960264.
After 9163 training step(s), loss on training batch is 0.0104562.
After 9164 training step(s), loss on training batch is 0.0112379.
After 9165 training step(s), loss on training batch is 0.0110221.
After 9166 training step(s), loss on training batch is 0.00911203.
After 9167 training step(s), loss on training batch is 0.00913664.
After 9168 training step(s), loss on training batch is 0.00936924.
After 9169 training step(s), loss on training batch is 0.00939851.
After 9170 training step(s), loss on training batch is 0.0103761.
After 9171 training step(s), loss on training batch is 0.0166982.
After 9172 training step(s), loss on training batch is 0.0119207.
After 9173 training step(s), loss on training batch is 0.0114872.
After 9174 training step(s), loss on training batch is 0.0112027.
After 9175 training step(s), loss on training batch is 0.0101143.
After 9176 training step(s), loss on training batch is 0.0091929.
After 9177 training step(s), loss on training batch is 0.0217292.
After 9178 training step(s), loss on training batch is 0.0575833.
After 9179 training step(s), loss on training batch is 0.0283257.
After 9180 training step(s), loss on training batch is 0.0466398.
After 9181 training step(s), loss on training batch is 0.0290908.
After 9182 training step(s), loss on training batch is 0.00888268.
After 9183 training step(s), loss on training batch is 0.0152442.
After 9184 training step(s), loss on training batch is 0.0127226.
After 9185 training step(s), loss on training batch is 0.00972836.
After 9186 training step(s), loss on training batch is 0.0101233.
After 9187 training step(s), loss on training batch is 0.0120217.
After 9188 training step(s), loss on training batch is 0.00970151.
After 9189 training step(s), loss on training batch is 0.0109934.
After 9190 training step(s), loss on training batch is 0.0100388.
After 9191 training step(s), loss on training batch is 0.0112397.
After 9192 training step(s), loss on training batch is 0.0125939.
After 9193 training step(s), loss on training batch is 0.009431.
After 9194 training step(s), loss on training batch is 0.0107657.
After 9195 training step(s), loss on training batch is 0.0120391.
After 9196 training step(s), loss on training batch is 0.00921929.
After 9197 training step(s), loss on training batch is 0.0119799.
After 9198 training step(s), loss on training batch is 0.0146366.
After 9199 training step(s), loss on training batch is 0.00917765.
After 9200 training step(s), loss on training batch is 0.0106634.
After 9201 training step(s), loss on training batch is 0.0106281.
After 9202 training step(s), loss on training batch is 0.0108048.
After 9203 training step(s), loss on training batch is 0.010495.
After 9204 training step(s), loss on training batch is 0.0107726.
After 9205 training step(s), loss on training batch is 0.023295.
After 9206 training step(s), loss on training batch is 0.0117089.
After 9207 training step(s), loss on training batch is 0.0229677.
After 9208 training step(s), loss on training batch is 0.0102626.
After 9209 training step(s), loss on training batch is 0.0149624.
After 9210 training step(s), loss on training batch is 0.00936434.
After 9211 training step(s), loss on training batch is 0.00994306.
After 9212 training step(s), loss on training batch is 0.0115938.
After 9213 training step(s), loss on training batch is 0.0111467.
After 9214 training step(s), loss on training batch is 0.013356.
After 9215 training step(s), loss on training batch is 0.0201911.
After 9216 training step(s), loss on training batch is 0.029348.
After 9217 training step(s), loss on training batch is 0.0147305.
After 9218 training step(s), loss on training batch is 0.0156173.
After 9219 training step(s), loss on training batch is 0.019714.
After 9220 training step(s), loss on training batch is 0.00950184.
After 9221 training step(s), loss on training batch is 0.0163763.
After 9222 training step(s), loss on training batch is 0.01535.
After 9223 training step(s), loss on training batch is 0.0159223.
After 9224 training step(s), loss on training batch is 0.0214596.
After 9225 training step(s), loss on training batch is 0.00892615.
After 9226 training step(s), loss on training batch is 0.0111314.
After 9227 training step(s), loss on training batch is 0.00928813.
After 9228 training step(s), loss on training batch is 0.0148549.
After 9229 training step(s), loss on training batch is 0.0173144.
After 9230 training step(s), loss on training batch is 0.00926061.
After 9231 training step(s), loss on training batch is 0.00885017.
After 9232 training step(s), loss on training batch is 0.00892684.
After 9233 training step(s), loss on training batch is 0.0105969.
After 9234 training step(s), loss on training batch is 0.0105586.
After 9235 training step(s), loss on training batch is 0.0150917.
After 9236 training step(s), loss on training batch is 0.0091157.
After 9237 training step(s), loss on training batch is 0.0101105.
After 9238 training step(s), loss on training batch is 0.009629.
After 9239 training step(s), loss on training batch is 0.00903721.
After 9240 training step(s), loss on training batch is 0.00971348.
After 9241 training step(s), loss on training batch is 0.0101796.
After 9242 training step(s), loss on training batch is 0.0101779.
After 9243 training step(s), loss on training batch is 0.00980653.
After 9244 training step(s), loss on training batch is 0.0104461.
After 9245 training step(s), loss on training batch is 0.0139911.
After 9246 training step(s), loss on training batch is 0.0122689.
After 9247 training step(s), loss on training batch is 0.00890779.
After 9248 training step(s), loss on training batch is 0.00953122.
After 9249 training step(s), loss on training batch is 0.0098503.
After 9250 training step(s), loss on training batch is 0.0106055.
After 9251 training step(s), loss on training batch is 0.0120416.
After 9252 training step(s), loss on training batch is 0.00901855.
After 9253 training step(s), loss on training batch is 0.0113557.
After 9254 training step(s), loss on training batch is 0.00846846.
After 9255 training step(s), loss on training batch is 0.00887504.
After 9256 training step(s), loss on training batch is 0.00875147.
After 9257 training step(s), loss on training batch is 0.0097772.
After 9258 training step(s), loss on training batch is 0.00931367.
After 9259 training step(s), loss on training batch is 0.00980309.
After 9260 training step(s), loss on training batch is 0.00977531.
After 9261 training step(s), loss on training batch is 0.00897572.
After 9262 training step(s), loss on training batch is 0.0097729.
After 9263 training step(s), loss on training batch is 0.00944929.
After 9264 training step(s), loss on training batch is 0.0103702.
After 9265 training step(s), loss on training batch is 0.00899714.
After 9266 training step(s), loss on training batch is 0.00890239.
After 9267 training step(s), loss on training batch is 0.0120664.
After 9268 training step(s), loss on training batch is 0.010795.
After 9269 training step(s), loss on training batch is 0.0114291.
After 9270 training step(s), loss on training batch is 0.00956897.
After 9271 training step(s), loss on training batch is 0.00857191.
After 9272 training step(s), loss on training batch is 0.0100637.
After 9273 training step(s), loss on training batch is 0.0100873.
After 9274 training step(s), loss on training batch is 0.00916892.
After 9275 training step(s), loss on training batch is 0.0128436.
After 9276 training step(s), loss on training batch is 0.00851916.
After 9277 training step(s), loss on training batch is 0.0100376.
After 9278 training step(s), loss on training batch is 0.0135507.
After 9279 training step(s), loss on training batch is 0.0104117.
After 9280 training step(s), loss on training batch is 0.00860498.
After 9281 training step(s), loss on training batch is 0.0100437.
After 9282 training step(s), loss on training batch is 0.00909385.
After 9283 training step(s), loss on training batch is 0.0108721.
After 9284 training step(s), loss on training batch is 0.00892225.
After 9285 training step(s), loss on training batch is 0.00934638.
After 9286 training step(s), loss on training batch is 0.00987616.
After 9287 training step(s), loss on training batch is 0.00866257.
After 9288 training step(s), loss on training batch is 0.0119476.
After 9289 training step(s), loss on training batch is 0.00992635.
After 9290 training step(s), loss on training batch is 0.0107948.
After 9291 training step(s), loss on training batch is 0.0102355.
After 9292 training step(s), loss on training batch is 0.00844294.
After 9293 training step(s), loss on training batch is 0.00873358.
After 9294 training step(s), loss on training batch is 0.0101003.
After 9295 training step(s), loss on training batch is 0.0115412.
After 9296 training step(s), loss on training batch is 0.0102299.
After 9297 training step(s), loss on training batch is 0.00998549.
After 9298 training step(s), loss on training batch is 0.00987987.
After 9299 training step(s), loss on training batch is 0.0103803.
After 9300 training step(s), loss on training batch is 0.010103.
After 9301 training step(s), loss on training batch is 0.0092164.
After 9302 training step(s), loss on training batch is 0.0132965.
After 9303 training step(s), loss on training batch is 0.0102324.
After 9304 training step(s), loss on training batch is 0.00968261.
After 9305 training step(s), loss on training batch is 0.0180019.
After 9306 training step(s), loss on training batch is 0.0144481.
After 9307 training step(s), loss on training batch is 0.0103374.
After 9308 training step(s), loss on training batch is 0.00913452.
After 9309 training step(s), loss on training batch is 0.0174344.
After 9310 training step(s), loss on training batch is 0.0135252.
After 9311 training step(s), loss on training batch is 0.00943528.
After 9312 training step(s), loss on training batch is 0.010794.
After 9313 training step(s), loss on training batch is 0.00880304.
After 9314 training step(s), loss on training batch is 0.0109084.
After 9315 training step(s), loss on training batch is 0.00962784.
After 9316 training step(s), loss on training batch is 0.0100176.
After 9317 training step(s), loss on training batch is 0.0114797.
After 9318 training step(s), loss on training batch is 0.0137415.
After 9319 training step(s), loss on training batch is 0.0108201.
After 9320 training step(s), loss on training batch is 0.0122101.
After 9321 training step(s), loss on training batch is 0.00937481.
After 9322 training step(s), loss on training batch is 0.0095142.
After 9323 training step(s), loss on training batch is 0.0103915.
After 9324 training step(s), loss on training batch is 0.00905867.
After 9325 training step(s), loss on training batch is 0.0123056.
After 9326 training step(s), loss on training batch is 0.0100672.
After 9327 training step(s), loss on training batch is 0.00888226.
After 9328 training step(s), loss on training batch is 0.0109266.
After 9329 training step(s), loss on training batch is 0.00848882.
After 9330 training step(s), loss on training batch is 0.0118664.
After 9331 training step(s), loss on training batch is 0.00870802.
After 9332 training step(s), loss on training batch is 0.00839189.
After 9333 training step(s), loss on training batch is 0.00872087.
After 9334 training step(s), loss on training batch is 0.00907988.
After 9335 training step(s), loss on training batch is 0.0092192.
After 9336 training step(s), loss on training batch is 0.0102005.
After 9337 training step(s), loss on training batch is 0.0106343.
After 9338 training step(s), loss on training batch is 0.00843354.
After 9339 training step(s), loss on training batch is 0.00937184.
After 9340 training step(s), loss on training batch is 0.0109745.
After 9341 training step(s), loss on training batch is 0.00949298.
After 9342 training step(s), loss on training batch is 0.00865827.
After 9343 training step(s), loss on training batch is 0.0105499.
After 9344 training step(s), loss on training batch is 0.00906526.
After 9345 training step(s), loss on training batch is 0.0102321.
After 9346 training step(s), loss on training batch is 0.00880803.
After 9347 training step(s), loss on training batch is 0.0108946.
After 9348 training step(s), loss on training batch is 0.045896.
After 9349 training step(s), loss on training batch is 0.00987719.
After 9350 training step(s), loss on training batch is 0.00939219.
After 9351 training step(s), loss on training batch is 0.00930076.
After 9352 training step(s), loss on training batch is 0.0102608.
After 9353 training step(s), loss on training batch is 0.0103824.
After 9354 training step(s), loss on training batch is 0.013008.
After 9355 training step(s), loss on training batch is 0.00842752.
After 9356 training step(s), loss on training batch is 0.00980068.
After 9357 training step(s), loss on training batch is 0.0113315.
After 9358 training step(s), loss on training batch is 0.0104531.
After 9359 training step(s), loss on training batch is 0.00915161.
After 9360 training step(s), loss on training batch is 0.00881659.
After 9361 training step(s), loss on training batch is 0.00910124.
After 9362 training step(s), loss on training batch is 0.00939867.
After 9363 training step(s), loss on training batch is 0.00941542.
After 9364 training step(s), loss on training batch is 0.0119908.
After 9365 training step(s), loss on training batch is 0.00970237.
After 9366 training step(s), loss on training batch is 0.0104942.
After 9367 training step(s), loss on training batch is 0.00933872.
After 9368 training step(s), loss on training batch is 0.00960683.
After 9369 training step(s), loss on training batch is 0.0096669.
After 9370 training step(s), loss on training batch is 0.00979179.
After 9371 training step(s), loss on training batch is 0.00917261.
After 9372 training step(s), loss on training batch is 0.00965757.
After 9373 training step(s), loss on training batch is 0.00961923.
After 9374 training step(s), loss on training batch is 0.00979836.
After 9375 training step(s), loss on training batch is 0.00998774.
After 9376 training step(s), loss on training batch is 0.00874931.
After 9377 training step(s), loss on training batch is 0.00974945.
After 9378 training step(s), loss on training batch is 0.00966878.
After 9379 training step(s), loss on training batch is 0.0104673.
After 9380 training step(s), loss on training batch is 0.00955572.
After 9381 training step(s), loss on training batch is 0.0104713.
After 9382 training step(s), loss on training batch is 0.00853504.
After 9383 training step(s), loss on training batch is 0.011597.
After 9384 training step(s), loss on training batch is 0.00858482.
After 9385 training step(s), loss on training batch is 0.00849821.
After 9386 training step(s), loss on training batch is 0.0102083.
After 9387 training step(s), loss on training batch is 0.00872053.
After 9388 training step(s), loss on training batch is 0.00907864.
After 9389 training step(s), loss on training batch is 0.00894864.
After 9390 training step(s), loss on training batch is 0.0096822.
After 9391 training step(s), loss on training batch is 0.00824268.
After 9392 training step(s), loss on training batch is 0.00868285.
After 9393 training step(s), loss on training batch is 0.0112333.
After 9394 training step(s), loss on training batch is 0.00907278.
After 9395 training step(s), loss on training batch is 0.0115541.
After 9396 training step(s), loss on training batch is 0.00853282.
After 9397 training step(s), loss on training batch is 0.00954953.
After 9398 training step(s), loss on training batch is 0.00989982.
After 9399 training step(s), loss on training batch is 0.00870891.
After 9400 training step(s), loss on training batch is 0.00913789.
After 9401 training step(s), loss on training batch is 0.00945882.
After 9402 training step(s), loss on training batch is 0.00922467.
After 9403 training step(s), loss on training batch is 0.00954659.
After 9404 training step(s), loss on training batch is 0.0122296.
After 9405 training step(s), loss on training batch is 0.00997644.
After 9406 training step(s), loss on training batch is 0.00880894.
After 9407 training step(s), loss on training batch is 0.0113844.
After 9408 training step(s), loss on training batch is 0.00957536.
After 9409 training step(s), loss on training batch is 0.00855702.
After 9410 training step(s), loss on training batch is 0.00912965.
After 9411 training step(s), loss on training batch is 0.00909037.
After 9412 training step(s), loss on training batch is 0.00848384.
After 9413 training step(s), loss on training batch is 0.00892473.
After 9414 training step(s), loss on training batch is 0.00960178.
After 9415 training step(s), loss on training batch is 0.00898471.
After 9416 training step(s), loss on training batch is 0.00998643.
After 9417 training step(s), loss on training batch is 0.00993624.
After 9418 training step(s), loss on training batch is 0.00865672.
After 9419 training step(s), loss on training batch is 0.0113043.
After 9420 training step(s), loss on training batch is 0.0122566.
After 9421 training step(s), loss on training batch is 0.0127186.
After 9422 training step(s), loss on training batch is 0.0116364.
After 9423 training step(s), loss on training batch is 0.00920442.
After 9424 training step(s), loss on training batch is 0.00940633.
After 9425 training step(s), loss on training batch is 0.00850602.
After 9426 training step(s), loss on training batch is 0.00842941.
After 9427 training step(s), loss on training batch is 0.0128102.
After 9428 training step(s), loss on training batch is 0.00875675.
After 9429 training step(s), loss on training batch is 0.00928238.
After 9430 training step(s), loss on training batch is 0.00920037.
After 9431 training step(s), loss on training batch is 0.00998246.
After 9432 training step(s), loss on training batch is 0.00837515.
After 9433 training step(s), loss on training batch is 0.0148404.
After 9434 training step(s), loss on training batch is 0.0145361.
After 9435 training step(s), loss on training batch is 0.0093062.
After 9436 training step(s), loss on training batch is 0.00903141.
After 9437 training step(s), loss on training batch is 0.0104032.
After 9438 training step(s), loss on training batch is 0.00960169.
After 9439 training step(s), loss on training batch is 0.00828461.
After 9440 training step(s), loss on training batch is 0.00809302.
After 9441 training step(s), loss on training batch is 0.0102023.
After 9442 training step(s), loss on training batch is 0.00994577.
After 9443 training step(s), loss on training batch is 0.00995255.
After 9444 training step(s), loss on training batch is 0.00842913.
After 9445 training step(s), loss on training batch is 0.00964153.
After 9446 training step(s), loss on training batch is 0.00870133.
After 9447 training step(s), loss on training batch is 0.0141495.
After 9448 training step(s), loss on training batch is 0.00931804.
After 9449 training step(s), loss on training batch is 0.00933386.
After 9450 training step(s), loss on training batch is 0.00901993.
After 9451 training step(s), loss on training batch is 0.0109441.
After 9452 training step(s), loss on training batch is 0.0100434.
After 9453 training step(s), loss on training batch is 0.00831089.
After 9454 training step(s), loss on training batch is 0.00813949.
After 9455 training step(s), loss on training batch is 0.0099273.
After 9456 training step(s), loss on training batch is 0.00869373.
After 9457 training step(s), loss on training batch is 0.00845849.
After 9458 training step(s), loss on training batch is 0.00946212.
After 9459 training step(s), loss on training batch is 0.0125156.
After 9460 training step(s), loss on training batch is 0.0100313.
After 9461 training step(s), loss on training batch is 0.00968745.
After 9462 training step(s), loss on training batch is 0.0112296.
After 9463 training step(s), loss on training batch is 0.00902855.
After 9464 training step(s), loss on training batch is 0.00899884.
After 9465 training step(s), loss on training batch is 0.00930083.
After 9466 training step(s), loss on training batch is 0.008436.
After 9467 training step(s), loss on training batch is 0.0086787.
After 9468 training step(s), loss on training batch is 0.00885081.
After 9469 training step(s), loss on training batch is 0.00861539.
After 9470 training step(s), loss on training batch is 0.00966782.
After 9471 training step(s), loss on training batch is 0.0113035.
After 9472 training step(s), loss on training batch is 0.0103638.
After 9473 training step(s), loss on training batch is 0.0111698.
After 9474 training step(s), loss on training batch is 0.00870616.
After 9475 training step(s), loss on training batch is 0.0098262.
After 9476 training step(s), loss on training batch is 0.0107193.
After 9477 training step(s), loss on training batch is 0.0107389.
After 9478 training step(s), loss on training batch is 0.00901743.
After 9479 training step(s), loss on training batch is 0.00992431.
After 9480 training step(s), loss on training batch is 0.00875211.
After 9481 training step(s), loss on training batch is 0.00884929.
After 9482 training step(s), loss on training batch is 0.0095389.
After 9483 training step(s), loss on training batch is 0.00871239.
After 9484 training step(s), loss on training batch is 0.00947122.
After 9485 training step(s), loss on training batch is 0.00900603.
After 9486 training step(s), loss on training batch is 0.0109142.
After 9487 training step(s), loss on training batch is 0.00884328.
After 9488 training step(s), loss on training batch is 0.00875551.
After 9489 training step(s), loss on training batch is 0.0125591.
After 9490 training step(s), loss on training batch is 0.0082755.
After 9491 training step(s), loss on training batch is 0.00931223.
After 9492 training step(s), loss on training batch is 0.0111606.
After 9493 training step(s), loss on training batch is 0.0109745.
After 9494 training step(s), loss on training batch is 0.0130973.
After 9495 training step(s), loss on training batch is 0.00839125.
After 9496 training step(s), loss on training batch is 0.00884491.
After 9497 training step(s), loss on training batch is 0.0156119.
After 9498 training step(s), loss on training batch is 0.00832995.
After 9499 training step(s), loss on training batch is 0.0123574.
After 9500 training step(s), loss on training batch is 0.0087173.
After 9501 training step(s), loss on training batch is 0.00818724.
After 9502 training step(s), loss on training batch is 0.00844806.
After 9503 training step(s), loss on training batch is 0.0189498.
After 9504 training step(s), loss on training batch is 0.00856518.
After 9505 training step(s), loss on training batch is 0.0112905.
After 9506 training step(s), loss on training batch is 0.0179286.
After 9507 training step(s), loss on training batch is 0.0130912.
After 9508 training step(s), loss on training batch is 0.00933389.
After 9509 training step(s), loss on training batch is 0.0207226.
After 9510 training step(s), loss on training batch is 0.0119009.
After 9511 training step(s), loss on training batch is 0.00889128.
After 9512 training step(s), loss on training batch is 0.0102646.
After 9513 training step(s), loss on training batch is 0.00962604.
After 9514 training step(s), loss on training batch is 0.00951087.
After 9515 training step(s), loss on training batch is 0.00988712.
After 9516 training step(s), loss on training batch is 0.0101981.
After 9517 training step(s), loss on training batch is 0.0097238.
After 9518 training step(s), loss on training batch is 0.0120546.
After 9519 training step(s), loss on training batch is 0.00872456.
After 9520 training step(s), loss on training batch is 0.00897919.
After 9521 training step(s), loss on training batch is 0.0153644.
After 9522 training step(s), loss on training batch is 0.0110409.
After 9523 training step(s), loss on training batch is 0.0297743.
After 9524 training step(s), loss on training batch is 0.0358967.
After 9525 training step(s), loss on training batch is 0.0107085.
After 9526 training step(s), loss on training batch is 0.0108323.
After 9527 training step(s), loss on training batch is 0.00941002.
After 9528 training step(s), loss on training batch is 0.00874294.
After 9529 training step(s), loss on training batch is 0.0084807.
After 9530 training step(s), loss on training batch is 0.00900276.
After 9531 training step(s), loss on training batch is 0.0100454.
After 9532 training step(s), loss on training batch is 0.00865779.
After 9533 training step(s), loss on training batch is 0.0191482.
After 9534 training step(s), loss on training batch is 0.0142198.
After 9535 training step(s), loss on training batch is 0.0153337.
After 9536 training step(s), loss on training batch is 0.00891727.
After 9537 training step(s), loss on training batch is 0.00941082.
After 9538 training step(s), loss on training batch is 0.00918385.
After 9539 training step(s), loss on training batch is 0.00881953.
After 9540 training step(s), loss on training batch is 0.01008.
After 9541 training step(s), loss on training batch is 0.00883192.
After 9542 training step(s), loss on training batch is 0.00936428.
After 9543 training step(s), loss on training batch is 0.00932194.
After 9544 training step(s), loss on training batch is 0.00982371.
After 9545 training step(s), loss on training batch is 0.00817612.
After 9546 training step(s), loss on training batch is 0.00809697.
After 9547 training step(s), loss on training batch is 0.008321.
After 9548 training step(s), loss on training batch is 0.00980563.
After 9549 training step(s), loss on training batch is 0.00899103.
After 9550 training step(s), loss on training batch is 0.00872015.
After 9551 training step(s), loss on training batch is 0.00872497.
After 9552 training step(s), loss on training batch is 0.00889983.
After 9553 training step(s), loss on training batch is 0.00836305.
After 9554 training step(s), loss on training batch is 0.00946095.
After 9555 training step(s), loss on training batch is 0.00919247.
After 9556 training step(s), loss on training batch is 0.0104248.
After 9557 training step(s), loss on training batch is 0.00945775.
After 9558 training step(s), loss on training batch is 0.00931083.
After 9559 training step(s), loss on training batch is 0.00998851.
After 9560 training step(s), loss on training batch is 0.0127165.
After 9561 training step(s), loss on training batch is 0.00916553.
After 9562 training step(s), loss on training batch is 0.01098.
After 9563 training step(s), loss on training batch is 0.00811089.
After 9564 training step(s), loss on training batch is 0.0108298.
After 9565 training step(s), loss on training batch is 0.00887663.
After 9566 training step(s), loss on training batch is 0.0101221.
After 9567 training step(s), loss on training batch is 0.00918694.
After 9568 training step(s), loss on training batch is 0.0107041.
After 9569 training step(s), loss on training batch is 0.00814849.
After 9570 training step(s), loss on training batch is 0.00914687.
After 9571 training step(s), loss on training batch is 0.0104791.
After 9572 training step(s), loss on training batch is 0.0191739.
After 9573 training step(s), loss on training batch is 0.042413.
After 9574 training step(s), loss on training batch is 0.0224449.
After 9575 training step(s), loss on training batch is 0.0141462.
After 9576 training step(s), loss on training batch is 0.00884819.
After 9577 training step(s), loss on training batch is 0.00969203.
After 9578 training step(s), loss on training batch is 0.0144035.
After 9579 training step(s), loss on training batch is 0.00929977.
After 9580 training step(s), loss on training batch is 0.0233431.
After 9581 training step(s), loss on training batch is 0.00934317.
After 9582 training step(s), loss on training batch is 0.00973486.
After 9583 training step(s), loss on training batch is 0.00887541.
After 9584 training step(s), loss on training batch is 0.0343052.
After 9585 training step(s), loss on training batch is 0.0187068.
After 9586 training step(s), loss on training batch is 0.00977669.
After 9587 training step(s), loss on training batch is 0.00895942.
After 9588 training step(s), loss on training batch is 0.0130897.
After 9589 training step(s), loss on training batch is 0.0081865.
After 9590 training step(s), loss on training batch is 0.0259584.
After 9591 training step(s), loss on training batch is 0.0342903.
After 9592 training step(s), loss on training batch is 0.0357919.
After 9593 training step(s), loss on training batch is 0.0098777.
After 9594 training step(s), loss on training batch is 0.0244469.
After 9595 training step(s), loss on training batch is 0.0093242.
After 9596 training step(s), loss on training batch is 0.00880163.
After 9597 training step(s), loss on training batch is 0.00802702.
After 9598 training step(s), loss on training batch is 0.0120181.
After 9599 training step(s), loss on training batch is 0.0100084.
After 9600 training step(s), loss on training batch is 0.00849619.
After 9601 training step(s), loss on training batch is 0.00980082.
After 9602 training step(s), loss on training batch is 0.0105596.
After 9603 training step(s), loss on training batch is 0.0428928.
After 9604 training step(s), loss on training batch is 0.010615.
After 9605 training step(s), loss on training batch is 0.0195439.
After 9606 training step(s), loss on training batch is 0.0078193.
After 9607 training step(s), loss on training batch is 0.01006.
After 9608 training step(s), loss on training batch is 0.00843259.
After 9609 training step(s), loss on training batch is 0.0100713.
After 9610 training step(s), loss on training batch is 0.0120116.
After 9611 training step(s), loss on training batch is 0.0110845.
After 9612 training step(s), loss on training batch is 0.0116521.
After 9613 training step(s), loss on training batch is 0.0100397.
After 9614 training step(s), loss on training batch is 0.00869088.
After 9615 training step(s), loss on training batch is 0.00864135.
After 9616 training step(s), loss on training batch is 0.0123081.
After 9617 training step(s), loss on training batch is 0.0147005.
After 9618 training step(s), loss on training batch is 0.0105741.
After 9619 training step(s), loss on training batch is 0.00897685.
After 9620 training step(s), loss on training batch is 0.00945222.
After 9621 training step(s), loss on training batch is 0.0114163.
After 9622 training step(s), loss on training batch is 0.0285885.
After 9623 training step(s), loss on training batch is 0.0199916.
After 9624 training step(s), loss on training batch is 0.010498.
After 9625 training step(s), loss on training batch is 0.0106009.
After 9626 training step(s), loss on training batch is 0.00857377.
After 9627 training step(s), loss on training batch is 0.00946339.
After 9628 training step(s), loss on training batch is 0.00933906.
After 9629 training step(s), loss on training batch is 0.0080692.
After 9630 training step(s), loss on training batch is 0.00951822.
After 9631 training step(s), loss on training batch is 0.0144158.
After 9632 training step(s), loss on training batch is 0.0145359.
After 9633 training step(s), loss on training batch is 0.0110124.
After 9634 training step(s), loss on training batch is 0.0106603.
After 9635 training step(s), loss on training batch is 0.00920027.
After 9636 training step(s), loss on training batch is 0.00893677.
After 9637 training step(s), loss on training batch is 0.00845606.
After 9638 training step(s), loss on training batch is 0.0100676.
After 9639 training step(s), loss on training batch is 0.00900156.
After 9640 training step(s), loss on training batch is 0.0127407.
After 9641 training step(s), loss on training batch is 0.00851605.
After 9642 training step(s), loss on training batch is 0.0121536.
After 9643 training step(s), loss on training batch is 0.00883575.
After 9644 training step(s), loss on training batch is 0.00822883.
After 9645 training step(s), loss on training batch is 0.00869271.
After 9646 training step(s), loss on training batch is 0.00791487.
After 9647 training step(s), loss on training batch is 0.0112606.
After 9648 training step(s), loss on training batch is 0.00870811.
After 9649 training step(s), loss on training batch is 0.00845298.
After 9650 training step(s), loss on training batch is 0.00844546.
After 9651 training step(s), loss on training batch is 0.00966786.
After 9652 training step(s), loss on training batch is 0.00860154.
After 9653 training step(s), loss on training batch is 0.00808297.
After 9654 training step(s), loss on training batch is 0.0200035.
After 9655 training step(s), loss on training batch is 0.010197.
After 9656 training step(s), loss on training batch is 0.0109289.
After 9657 training step(s), loss on training batch is 0.00887242.
After 9658 training step(s), loss on training batch is 0.0116111.
After 9659 training step(s), loss on training batch is 0.0106815.
After 9660 training step(s), loss on training batch is 0.0175157.
After 9661 training step(s), loss on training batch is 0.0147257.
After 9662 training step(s), loss on training batch is 0.0126324.
After 9663 training step(s), loss on training batch is 0.0134748.
After 9664 training step(s), loss on training batch is 0.0104879.
After 9665 training step(s), loss on training batch is 0.00940784.
After 9666 training step(s), loss on training batch is 0.0102952.
After 9667 training step(s), loss on training batch is 0.0103794.
After 9668 training step(s), loss on training batch is 0.0105333.
After 9669 training step(s), loss on training batch is 0.00961341.
After 9670 training step(s), loss on training batch is 0.00940261.
After 9671 training step(s), loss on training batch is 0.00893679.
After 9672 training step(s), loss on training batch is 0.0102956.
After 9673 training step(s), loss on training batch is 0.00948246.
After 9674 training step(s), loss on training batch is 0.0102975.
After 9675 training step(s), loss on training batch is 0.0085261.
After 9676 training step(s), loss on training batch is 0.00863264.
After 9677 training step(s), loss on training batch is 0.00905252.
After 9678 training step(s), loss on training batch is 0.00873134.
After 9679 training step(s), loss on training batch is 0.00840789.
After 9680 training step(s), loss on training batch is 0.00926254.
After 9681 training step(s), loss on training batch is 0.0108635.
After 9682 training step(s), loss on training batch is 0.00843414.
After 9683 training step(s), loss on training batch is 0.0089459.
After 9684 training step(s), loss on training batch is 0.00851239.
After 9685 training step(s), loss on training batch is 0.0100591.
After 9686 training step(s), loss on training batch is 0.00829031.
After 9687 training step(s), loss on training batch is 0.00905313.
After 9688 training step(s), loss on training batch is 0.00945117.
After 9689 training step(s), loss on training batch is 0.00951633.
After 9690 training step(s), loss on training batch is 0.00837892.
After 9691 training step(s), loss on training batch is 0.00939237.
After 9692 training step(s), loss on training batch is 0.00834067.
After 9693 training step(s), loss on training batch is 0.00910769.
After 9694 training step(s), loss on training batch is 0.017616.
After 9695 training step(s), loss on training batch is 0.011756.
After 9696 training step(s), loss on training batch is 0.0157708.
After 9697 training step(s), loss on training batch is 0.00891362.
After 9698 training step(s), loss on training batch is 0.00870991.
After 9699 training step(s), loss on training batch is 0.00816168.
After 9700 training step(s), loss on training batch is 0.0107832.
After 9701 training step(s), loss on training batch is 0.00824116.
After 9702 training step(s), loss on training batch is 0.0126309.
After 9703 training step(s), loss on training batch is 0.0109648.
After 9704 training step(s), loss on training batch is 0.0106892.
After 9705 training step(s), loss on training batch is 0.00813486.
After 9706 training step(s), loss on training batch is 0.00940763.
After 9707 training step(s), loss on training batch is 0.0093848.
After 9708 training step(s), loss on training batch is 0.00839131.
After 9709 training step(s), loss on training batch is 0.0101178.
After 9710 training step(s), loss on training batch is 0.00811459.
After 9711 training step(s), loss on training batch is 0.0115259.
After 9712 training step(s), loss on training batch is 0.0135402.
After 9713 training step(s), loss on training batch is 0.00947477.
After 9714 training step(s), loss on training batch is 0.00810025.
After 9715 training step(s), loss on training batch is 0.00885308.
After 9716 training step(s), loss on training batch is 0.0101752.
After 9717 training step(s), loss on training batch is 0.00821421.
After 9718 training step(s), loss on training batch is 0.00846.
After 9719 training step(s), loss on training batch is 0.00876225.
After 9720 training step(s), loss on training batch is 0.00796991.
After 9721 training step(s), loss on training batch is 0.0092141.
After 9722 training step(s), loss on training batch is 0.00813274.
After 9723 training step(s), loss on training batch is 0.00918057.
After 9724 training step(s), loss on training batch is 0.00962162.
After 9725 training step(s), loss on training batch is 0.00880604.
After 9726 training step(s), loss on training batch is 0.00969598.
After 9727 training step(s), loss on training batch is 0.010423.
After 9728 training step(s), loss on training batch is 0.00876565.
After 9729 training step(s), loss on training batch is 0.00853009.
After 9730 training step(s), loss on training batch is 0.00801461.
After 9731 training step(s), loss on training batch is 0.00843556.
After 9732 training step(s), loss on training batch is 0.00994809.
After 9733 training step(s), loss on training batch is 0.00984526.
After 9734 training step(s), loss on training batch is 0.00913327.
After 9735 training step(s), loss on training batch is 0.0117748.
After 9736 training step(s), loss on training batch is 0.00839494.
After 9737 training step(s), loss on training batch is 0.00905729.
After 9738 training step(s), loss on training batch is 0.00929287.
After 9739 training step(s), loss on training batch is 0.00946402.
After 9740 training step(s), loss on training batch is 0.0105612.
After 9741 training step(s), loss on training batch is 0.0132543.
After 9742 training step(s), loss on training batch is 0.0098326.
After 9743 training step(s), loss on training batch is 0.0082842.
After 9744 training step(s), loss on training batch is 0.0101409.
After 9745 training step(s), loss on training batch is 0.00846642.
After 9746 training step(s), loss on training batch is 0.00812791.
After 9747 training step(s), loss on training batch is 0.00839581.
After 9748 training step(s), loss on training batch is 0.00930085.
After 9749 training step(s), loss on training batch is 0.0171189.
After 9750 training step(s), loss on training batch is 0.00957607.
After 9751 training step(s), loss on training batch is 0.0113706.
After 9752 training step(s), loss on training batch is 0.00936562.
After 9753 training step(s), loss on training batch is 0.00807428.
After 9754 training step(s), loss on training batch is 0.0102191.
After 9755 training step(s), loss on training batch is 0.00823425.
After 9756 training step(s), loss on training batch is 0.00904557.
After 9757 training step(s), loss on training batch is 0.0085192.
After 9758 training step(s), loss on training batch is 0.0117895.
After 9759 training step(s), loss on training batch is 0.00956033.
After 9760 training step(s), loss on training batch is 0.00887448.
After 9761 training step(s), loss on training batch is 0.010207.
After 9762 training step(s), loss on training batch is 0.0106383.
After 9763 training step(s), loss on training batch is 0.0087258.
After 9764 training step(s), loss on training batch is 0.00955431.
After 9765 training step(s), loss on training batch is 0.0102394.
After 9766 training step(s), loss on training batch is 0.00811433.
After 9767 training step(s), loss on training batch is 0.0111049.
After 9768 training step(s), loss on training batch is 0.00892163.
After 9769 training step(s), loss on training batch is 0.0100286.
After 9770 training step(s), loss on training batch is 0.0087736.
After 9771 training step(s), loss on training batch is 0.00928094.
After 9772 training step(s), loss on training batch is 0.00902536.
After 9773 training step(s), loss on training batch is 0.00838885.
After 9774 training step(s), loss on training batch is 0.00912105.
After 9775 training step(s), loss on training batch is 0.00928438.
After 9776 training step(s), loss on training batch is 0.00880561.
After 9777 training step(s), loss on training batch is 0.00921026.
After 9778 training step(s), loss on training batch is 0.00822345.
After 9779 training step(s), loss on training batch is 0.00880334.
After 9780 training step(s), loss on training batch is 0.0089692.
After 9781 training step(s), loss on training batch is 0.00930084.
After 9782 training step(s), loss on training batch is 0.00989593.
After 9783 training step(s), loss on training batch is 0.00985659.
After 9784 training step(s), loss on training batch is 0.0087993.
After 9785 training step(s), loss on training batch is 0.0129832.
After 9786 training step(s), loss on training batch is 0.00898228.
After 9787 training step(s), loss on training batch is 0.008689.
After 9788 training step(s), loss on training batch is 0.00804967.
After 9789 training step(s), loss on training batch is 0.0091346.
After 9790 training step(s), loss on training batch is 0.00882179.
After 9791 training step(s), loss on training batch is 0.00861905.
After 9792 training step(s), loss on training batch is 0.00866664.
After 9793 training step(s), loss on training batch is 0.00803469.
After 9794 training step(s), loss on training batch is 0.00902143.
After 9795 training step(s), loss on training batch is 0.0100324.
After 9796 training step(s), loss on training batch is 0.0101656.
After 9797 training step(s), loss on training batch is 0.00863679.
After 9798 training step(s), loss on training batch is 0.00838642.
After 9799 training step(s), loss on training batch is 0.00856585.
After 9800 training step(s), loss on training batch is 0.00850554.
After 9801 training step(s), loss on training batch is 0.0153561.
After 9802 training step(s), loss on training batch is 0.0123019.
After 9803 training step(s), loss on training batch is 0.00955369.
After 9804 training step(s), loss on training batch is 0.0103758.
After 9805 training step(s), loss on training batch is 0.009197.
After 9806 training step(s), loss on training batch is 0.00947571.
After 9807 training step(s), loss on training batch is 0.00778172.
After 9808 training step(s), loss on training batch is 0.00800141.
After 9809 training step(s), loss on training batch is 0.0124164.
After 9810 training step(s), loss on training batch is 0.00880508.
After 9811 training step(s), loss on training batch is 0.00851291.
After 9812 training step(s), loss on training batch is 0.0119419.
After 9813 training step(s), loss on training batch is 0.00838648.
After 9814 training step(s), loss on training batch is 0.00924625.
After 9815 training step(s), loss on training batch is 0.00930822.
After 9816 training step(s), loss on training batch is 0.00902428.
After 9817 training step(s), loss on training batch is 0.00798423.
After 9818 training step(s), loss on training batch is 0.0122965.
After 9819 training step(s), loss on training batch is 0.0097117.
After 9820 training step(s), loss on training batch is 0.00967673.
After 9821 training step(s), loss on training batch is 0.0112381.
After 9822 training step(s), loss on training batch is 0.00938336.
After 9823 training step(s), loss on training batch is 0.00825378.
After 9824 training step(s), loss on training batch is 0.0111688.
After 9825 training step(s), loss on training batch is 0.00832491.
After 9826 training step(s), loss on training batch is 0.00987592.
After 9827 training step(s), loss on training batch is 0.00796485.
After 9828 training step(s), loss on training batch is 0.0101327.
After 9829 training step(s), loss on training batch is 0.00855015.
After 9830 training step(s), loss on training batch is 0.0104771.
After 9831 training step(s), loss on training batch is 0.0087308.
After 9832 training step(s), loss on training batch is 0.0131367.
After 9833 training step(s), loss on training batch is 0.0083023.
After 9834 training step(s), loss on training batch is 0.0112346.
After 9835 training step(s), loss on training batch is 0.00899945.
After 9836 training step(s), loss on training batch is 0.00801904.
After 9837 training step(s), loss on training batch is 0.0121135.
After 9838 training step(s), loss on training batch is 0.00984368.
After 9839 training step(s), loss on training batch is 0.00893804.
After 9840 training step(s), loss on training batch is 0.0112229.
After 9841 training step(s), loss on training batch is 0.0109712.
After 9842 training step(s), loss on training batch is 0.00890005.
After 9843 training step(s), loss on training batch is 0.00825788.
After 9844 training step(s), loss on training batch is 0.00921137.
After 9845 training step(s), loss on training batch is 0.00970833.
After 9846 training step(s), loss on training batch is 0.00966306.
After 9847 training step(s), loss on training batch is 0.0087434.
After 9848 training step(s), loss on training batch is 0.0303262.
After 9849 training step(s), loss on training batch is 0.0101741.
After 9850 training step(s), loss on training batch is 0.00905584.
After 9851 training step(s), loss on training batch is 0.00818798.
After 9852 training step(s), loss on training batch is 0.0100368.
After 9853 training step(s), loss on training batch is 0.00816843.
After 9854 training step(s), loss on training batch is 0.00887176.
After 9855 training step(s), loss on training batch is 0.00878698.
After 9856 training step(s), loss on training batch is 0.00885544.
After 9857 training step(s), loss on training batch is 0.0100163.
After 9858 training step(s), loss on training batch is 0.00934524.
After 9859 training step(s), loss on training batch is 0.0203565.
After 9860 training step(s), loss on training batch is 0.0136131.
After 9861 training step(s), loss on training batch is 0.0106018.
After 9862 training step(s), loss on training batch is 0.00986439.
After 9863 training step(s), loss on training batch is 0.00974475.
After 9864 training step(s), loss on training batch is 0.011891.
After 9865 training step(s), loss on training batch is 0.0146639.
After 9866 training step(s), loss on training batch is 0.0117729.
After 9867 training step(s), loss on training batch is 0.013036.
After 9868 training step(s), loss on training batch is 0.00871962.
After 9869 training step(s), loss on training batch is 0.0102724.
After 9870 training step(s), loss on training batch is 0.00957556.
After 9871 training step(s), loss on training batch is 0.00914872.
After 9872 training step(s), loss on training batch is 0.0128273.
After 9873 training step(s), loss on training batch is 0.00847628.
After 9874 training step(s), loss on training batch is 0.00984586.
After 9875 training step(s), loss on training batch is 0.00769628.
After 9876 training step(s), loss on training batch is 0.00947023.
After 9877 training step(s), loss on training batch is 0.00824749.
After 9878 training step(s), loss on training batch is 0.00832223.
After 9879 training step(s), loss on training batch is 0.0100855.
After 9880 training step(s), loss on training batch is 0.00883298.
After 9881 training step(s), loss on training batch is 0.00825886.
After 9882 training step(s), loss on training batch is 0.010009.
After 9883 training step(s), loss on training batch is 0.00874802.
After 9884 training step(s), loss on training batch is 0.00860467.
After 9885 training step(s), loss on training batch is 0.0144254.
After 9886 training step(s), loss on training batch is 0.0132888.
After 9887 training step(s), loss on training batch is 0.012865.
After 9888 training step(s), loss on training batch is 0.0145996.
After 9889 training step(s), loss on training batch is 0.0104174.
After 9890 training step(s), loss on training batch is 0.0114481.
After 9891 training step(s), loss on training batch is 0.00965496.
After 9892 training step(s), loss on training batch is 0.00830575.
After 9893 training step(s), loss on training batch is 0.00857994.
After 9894 training step(s), loss on training batch is 0.00881535.
After 9895 training step(s), loss on training batch is 0.00994516.
After 9896 training step(s), loss on training batch is 0.00994622.
After 9897 training step(s), loss on training batch is 0.00826359.
After 9898 training step(s), loss on training batch is 0.00834521.
After 9899 training step(s), loss on training batch is 0.0168994.
After 9900 training step(s), loss on training batch is 0.00756641.
After 9901 training step(s), loss on training batch is 0.0141591.
After 9902 training step(s), loss on training batch is 0.0118127.
After 9903 training step(s), loss on training batch is 0.00985068.
After 9904 training step(s), loss on training batch is 0.00802988.
After 9905 training step(s), loss on training batch is 0.0101769.
After 9906 training step(s), loss on training batch is 0.00979173.
After 9907 training step(s), loss on training batch is 0.00813842.
After 9908 training step(s), loss on training batch is 0.00797748.
After 9909 training step(s), loss on training batch is 0.0096657.
After 9910 training step(s), loss on training batch is 0.0123704.
After 9911 training step(s), loss on training batch is 0.00830055.
After 9912 training step(s), loss on training batch is 0.00925606.
After 9913 training step(s), loss on training batch is 0.00856608.
After 9914 training step(s), loss on training batch is 0.0100324.
After 9915 training step(s), loss on training batch is 0.00924461.
After 9916 training step(s), loss on training batch is 0.00859419.
After 9917 training step(s), loss on training batch is 0.0105114.
After 9918 training step(s), loss on training batch is 0.0099169.
After 9919 training step(s), loss on training batch is 0.00808448.
After 9920 training step(s), loss on training batch is 0.0105459.
After 9921 training step(s), loss on training batch is 0.00935007.
After 9922 training step(s), loss on training batch is 0.00863186.
After 9923 training step(s), loss on training batch is 0.0101805.
After 9924 training step(s), loss on training batch is 0.00767416.
After 9925 training step(s), loss on training batch is 0.0190222.
After 9926 training step(s), loss on training batch is 0.0110089.
After 9927 training step(s), loss on training batch is 0.00772865.
After 9928 training step(s), loss on training batch is 0.0102932.
After 9929 training step(s), loss on training batch is 0.0108028.
After 9930 training step(s), loss on training batch is 0.00793756.
After 9931 training step(s), loss on training batch is 0.00978146.
After 9932 training step(s), loss on training batch is 0.00807415.
After 9933 training step(s), loss on training batch is 0.00857864.
After 9934 training step(s), loss on training batch is 0.00831637.
After 9935 training step(s), loss on training batch is 0.00872575.
After 9936 training step(s), loss on training batch is 0.00795369.
After 9937 training step(s), loss on training batch is 0.00861109.
After 9938 training step(s), loss on training batch is 0.00835287.
After 9939 training step(s), loss on training batch is 0.00974732.
After 9940 training step(s), loss on training batch is 0.00761933.
After 9941 training step(s), loss on training batch is 0.00793967.
After 9942 training step(s), loss on training batch is 0.0108199.
After 9943 training step(s), loss on training batch is 0.00971363.
After 9944 training step(s), loss on training batch is 0.00841935.
After 9945 training step(s), loss on training batch is 0.00827217.
After 9946 training step(s), loss on training batch is 0.00982904.
After 9947 training step(s), loss on training batch is 0.00969048.
After 9948 training step(s), loss on training batch is 0.00909911.
After 9949 training step(s), loss on training batch is 0.00775899.
After 9950 training step(s), loss on training batch is 0.0083169.
After 9951 training step(s), loss on training batch is 0.00892966.
After 9952 training step(s), loss on training batch is 0.00912141.
After 9953 training step(s), loss on training batch is 0.00807276.
After 9954 training step(s), loss on training batch is 0.00782879.
After 9955 training step(s), loss on training batch is 0.00797411.
After 9956 training step(s), loss on training batch is 0.00759605.
After 9957 training step(s), loss on training batch is 0.00760864.
After 9958 training step(s), loss on training batch is 0.00910669.
After 9959 training step(s), loss on training batch is 0.00857794.
After 9960 training step(s), loss on training batch is 0.00815943.
After 9961 training step(s), loss on training batch is 0.00975048.
After 9962 training step(s), loss on training batch is 0.00804199.
After 9963 training step(s), loss on training batch is 0.00972115.
After 9964 training step(s), loss on training batch is 0.010473.
After 9965 training step(s), loss on training batch is 0.00947912.
After 9966 training step(s), loss on training batch is 0.0091729.
After 9967 training step(s), loss on training batch is 0.00833522.
After 9968 training step(s), loss on training batch is 0.0118523.
After 9969 training step(s), loss on training batch is 0.00778865.
After 9970 training step(s), loss on training batch is 0.00834326.
After 9971 training step(s), loss on training batch is 0.00892134.
After 9972 training step(s), loss on training batch is 0.00853037.
After 9973 training step(s), loss on training batch is 0.00843032.
After 9974 training step(s), loss on training batch is 0.00972457.
After 9975 training step(s), loss on training batch is 0.00808439.
After 9976 training step(s), loss on training batch is 0.00832819.
After 9977 training step(s), loss on training batch is 0.0159073.
After 9978 training step(s), loss on training batch is 0.0123276.
After 9979 training step(s), loss on training batch is 0.0107447.
After 9980 training step(s), loss on training batch is 0.0158719.
After 9981 training step(s), loss on training batch is 0.0079449.
After 9982 training step(s), loss on training batch is 0.0080992.
After 9983 training step(s), loss on training batch is 0.00872199.
After 9984 training step(s), loss on training batch is 0.00801489.
After 9985 training step(s), loss on training batch is 0.00812291.
After 9986 training step(s), loss on training batch is 0.00869511.
After 9987 training step(s), loss on training batch is 0.00890148.
After 9988 training step(s), loss on training batch is 0.00747845.
After 9989 training step(s), loss on training batch is 0.00885602.
After 9990 training step(s), loss on training batch is 0.00862201.
After 9991 training step(s), loss on training batch is 0.00999177.
After 9992 training step(s), loss on training batch is 0.00989089.
After 9993 training step(s), loss on training batch is 0.00900706.
After 9994 training step(s), loss on training batch is 0.00895112.
After 9995 training step(s), loss on training batch is 0.00766101.
After 9996 training step(s), loss on training batch is 0.00789827.
After 9997 training step(s), loss on training batch is 0.00920041.
After 9998 training step(s), loss on training batch is 0.0109092.
After 9999 training step(s), loss on training batch is 0.0143863.
After 10000 training step(s), loss on training batch is 0.00789397.
After 10001 training step(s), loss on training batch is 0.00794359.
After 10002 training step(s), loss on training batch is 0.0121693.
After 10003 training step(s), loss on training batch is 0.00896199.
After 10004 training step(s), loss on training batch is 0.00824856.
After 10005 training step(s), loss on training batch is 0.00747201.
After 10006 training step(s), loss on training batch is 0.00926032.
After 10007 training step(s), loss on training batch is 0.00757866.
After 10008 training step(s), loss on training batch is 0.00784978.
After 10009 training step(s), loss on training batch is 0.00843022.
After 10010 training step(s), loss on training batch is 0.00809461.
After 10011 training step(s), loss on training batch is 0.0086599.
After 10012 training step(s), loss on training batch is 0.00954719.
After 10013 training step(s), loss on training batch is 0.00829942.
After 10014 training step(s), loss on training batch is 0.0105812.
After 10015 training step(s), loss on training batch is 0.0104686.
After 10016 training step(s), loss on training batch is 0.00852996.
After 10017 training step(s), loss on training batch is 0.00856669.
After 10018 training step(s), loss on training batch is 0.0089943.
After 10019 training step(s), loss on training batch is 0.0100055.
After 10020 training step(s), loss on training batch is 0.00923636.
After 10021 training step(s), loss on training batch is 0.013725.
After 10022 training step(s), loss on training batch is 0.0189204.
After 10023 training step(s), loss on training batch is 0.0159954.
After 10024 training step(s), loss on training batch is 0.0104618.
After 10025 training step(s), loss on training batch is 0.00962342.
After 10026 training step(s), loss on training batch is 0.00958239.
After 10027 training step(s), loss on training batch is 0.0102632.
After 10028 training step(s), loss on training batch is 0.00861718.
After 10029 training step(s), loss on training batch is 0.00763164.
After 10030 training step(s), loss on training batch is 0.00891332.
After 10031 training step(s), loss on training batch is 0.00845344.
After 10032 training step(s), loss on training batch is 0.0081899.
After 10033 training step(s), loss on training batch is 0.00840215.
After 10034 training step(s), loss on training batch is 0.00794096.
After 10035 training step(s), loss on training batch is 0.00750792.
After 10036 training step(s), loss on training batch is 0.00920422.
After 10037 training step(s), loss on training batch is 0.00860737.
After 10038 training step(s), loss on training batch is 0.0079055.
After 10039 training step(s), loss on training batch is 0.00782904.
After 10040 training step(s), loss on training batch is 0.00796005.
After 10041 training step(s), loss on training batch is 0.00799775.
After 10042 training step(s), loss on training batch is 0.0124555.
After 10043 training step(s), loss on training batch is 0.0097601.
After 10044 training step(s), loss on training batch is 0.00763263.
After 10045 training step(s), loss on training batch is 0.00953831.
After 10046 training step(s), loss on training batch is 0.00884013.
After 10047 training step(s), loss on training batch is 0.00747704.
After 10048 training step(s), loss on training batch is 0.00847776.
After 10049 training step(s), loss on training batch is 0.00872983.
After 10050 training step(s), loss on training batch is 0.0087128.
After 10051 training step(s), loss on training batch is 0.0105798.
After 10052 training step(s), loss on training batch is 0.00786901.
After 10053 training step(s), loss on training batch is 0.00945367.
After 10054 training step(s), loss on training batch is 0.00858357.
After 10055 training step(s), loss on training batch is 0.0086042.
After 10056 training step(s), loss on training batch is 0.00761242.
After 10057 training step(s), loss on training batch is 0.0105091.
After 10058 training step(s), loss on training batch is 0.00805763.
After 10059 training step(s), loss on training batch is 0.00802534.
After 10060 training step(s), loss on training batch is 0.00844357.
After 10061 training step(s), loss on training batch is 0.0234901.
After 10062 training step(s), loss on training batch is 0.0121824.
After 10063 training step(s), loss on training batch is 0.0235231.
After 10064 training step(s), loss on training batch is 0.0081588.
After 10065 training step(s), loss on training batch is 0.00814369.
After 10066 training step(s), loss on training batch is 0.00894417.
After 10067 training step(s), loss on training batch is 0.00904983.
After 10068 training step(s), loss on training batch is 0.00849547.
After 10069 training step(s), loss on training batch is 0.00761112.
After 10070 training step(s), loss on training batch is 0.00888884.
After 10071 training step(s), loss on training batch is 0.00802333.
After 10072 training step(s), loss on training batch is 0.00871465.
After 10073 training step(s), loss on training batch is 0.00860462.
After 10074 training step(s), loss on training batch is 0.0101339.
After 10075 training step(s), loss on training batch is 0.00798367.
After 10076 training step(s), loss on training batch is 0.00857994.
After 10077 training step(s), loss on training batch is 0.00790022.
After 10078 training step(s), loss on training batch is 0.00885756.
After 10079 training step(s), loss on training batch is 0.00801278.
After 10080 training step(s), loss on training batch is 0.00845341.
After 10081 training step(s), loss on training batch is 0.00825336.
After 10082 training step(s), loss on training batch is 0.00912899.
After 10083 training step(s), loss on training batch is 0.00839039.
After 10084 training step(s), loss on training batch is 0.00835902.
After 10085 training step(s), loss on training batch is 0.00952212.
After 10086 training step(s), loss on training batch is 0.00877554.
After 10087 training step(s), loss on training batch is 0.008772.
After 10088 training step(s), loss on training batch is 0.00930465.
After 10089 training step(s), loss on training batch is 0.0104287.
After 10090 training step(s), loss on training batch is 0.00780096.
After 10091 training step(s), loss on training batch is 0.00833453.
After 10092 training step(s), loss on training batch is 0.00796142.
After 10093 training step(s), loss on training batch is 0.00911166.
After 10094 training step(s), loss on training batch is 0.00806852.
After 10095 training step(s), loss on training batch is 0.00882982.
After 10096 training step(s), loss on training batch is 0.00871916.
After 10097 training step(s), loss on training batch is 0.00831989.
After 10098 training step(s), loss on training batch is 0.0082628.
After 10099 training step(s), loss on training batch is 0.00826032.
After 10100 training step(s), loss on training batch is 0.00809285.
After 10101 training step(s), loss on training batch is 0.00951837.
After 10102 training step(s), loss on training batch is 0.00904599.
After 10103 training step(s), loss on training batch is 0.00786587.
After 10104 training step(s), loss on training batch is 0.00876975.
After 10105 training step(s), loss on training batch is 0.00812582.
After 10106 training step(s), loss on training batch is 0.00748195.
After 10107 training step(s), loss on training batch is 0.00758599.
After 10108 training step(s), loss on training batch is 0.00847395.
After 10109 training step(s), loss on training batch is 0.00804794.
After 10110 training step(s), loss on training batch is 0.0105434.
After 10111 training step(s), loss on training batch is 0.00890162.
After 10112 training step(s), loss on training batch is 0.00834503.
After 10113 training step(s), loss on training batch is 0.00819815.
After 10114 training step(s), loss on training batch is 0.0115467.
After 10115 training step(s), loss on training batch is 0.0087377.
After 10116 training step(s), loss on training batch is 0.00766952.
After 10117 training step(s), loss on training batch is 0.00817157.
After 10118 training step(s), loss on training batch is 0.00788802.
After 10119 training step(s), loss on training batch is 0.00847747.
After 10120 training step(s), loss on training batch is 0.00800584.
After 10121 training step(s), loss on training batch is 0.00820735.
After 10122 training step(s), loss on training batch is 0.00824139.
After 10123 training step(s), loss on training batch is 0.00885173.
After 10124 training step(s), loss on training batch is 0.00901093.
After 10125 training step(s), loss on training batch is 0.00864028.
After 10126 training step(s), loss on training batch is 0.00761914.
After 10127 training step(s), loss on training batch is 0.00874095.
After 10128 training step(s), loss on training batch is 0.00841423.
After 10129 training step(s), loss on training batch is 0.0102897.
After 10130 training step(s), loss on training batch is 0.00856237.
After 10131 training step(s), loss on training batch is 0.00744574.
After 10132 training step(s), loss on training batch is 0.00801078.
After 10133 training step(s), loss on training batch is 0.00787545.
After 10134 training step(s), loss on training batch is 0.00765751.
After 10135 training step(s), loss on training batch is 0.00764862.
After 10136 training step(s), loss on training batch is 0.00789601.
After 10137 training step(s), loss on training batch is 0.00861325.
After 10138 training step(s), loss on training batch is 0.0101796.
After 10139 training step(s), loss on training batch is 0.00773568.
After 10140 training step(s), loss on training batch is 0.00910196.
After 10141 training step(s), loss on training batch is 0.011289.
After 10142 training step(s), loss on training batch is 0.00856556.
After 10143 training step(s), loss on training batch is 0.00750693.
After 10144 training step(s), loss on training batch is 0.00985525.
After 10145 training step(s), loss on training batch is 0.0085787.
After 10146 training step(s), loss on training batch is 0.00874944.
After 10147 training step(s), loss on training batch is 0.0104716.
After 10148 training step(s), loss on training batch is 0.00840064.
After 10149 training step(s), loss on training batch is 0.00760764.
After 10150 training step(s), loss on training batch is 0.00861588.
After 10151 training step(s), loss on training batch is 0.00898437.
After 10152 training step(s), loss on training batch is 0.00802121.
After 10153 training step(s), loss on training batch is 0.00809232.
After 10154 training step(s), loss on training batch is 0.010114.
After 10155 training step(s), loss on training batch is 0.00736728.
After 10156 training step(s), loss on training batch is 0.0107692.
After 10157 training step(s), loss on training batch is 0.0139589.
After 10158 training step(s), loss on training batch is 0.00814314.
After 10159 training step(s), loss on training batch is 0.0110912.
After 10160 training step(s), loss on training batch is 0.012922.
After 10161 training step(s), loss on training batch is 0.0176379.
After 10162 training step(s), loss on training batch is 0.0112653.
After 10163 training step(s), loss on training batch is 0.00926571.
After 10164 training step(s), loss on training batch is 0.00950933.
After 10165 training step(s), loss on training batch is 0.00837656.
After 10166 training step(s), loss on training batch is 0.00750623.
After 10167 training step(s), loss on training batch is 0.0084292.
After 10168 training step(s), loss on training batch is 0.00975285.
After 10169 training step(s), loss on training batch is 0.00810627.
After 10170 training step(s), loss on training batch is 0.0083187.
After 10171 training step(s), loss on training batch is 0.00802653.
After 10172 training step(s), loss on training batch is 0.00803471.
After 10173 training step(s), loss on training batch is 0.0102109.
After 10174 training step(s), loss on training batch is 0.0102139.
After 10175 training step(s), loss on training batch is 0.00992781.
After 10176 training step(s), loss on training batch is 0.00808059.
After 10177 training step(s), loss on training batch is 0.00793074.
After 10178 training step(s), loss on training batch is 0.00861225.
After 10179 training step(s), loss on training batch is 0.00762335.
After 10180 training step(s), loss on training batch is 0.00945046.
After 10181 training step(s), loss on training batch is 0.0083101.
After 10182 training step(s), loss on training batch is 0.0081371.
After 10183 training step(s), loss on training batch is 0.00868477.
After 10184 training step(s), loss on training batch is 0.0075178.
After 10185 training step(s), loss on training batch is 0.00829346.
After 10186 training step(s), loss on training batch is 0.00770854.
After 10187 training step(s), loss on training batch is 0.00856403.
After 10188 training step(s), loss on training batch is 0.00731585.
After 10189 training step(s), loss on training batch is 0.00818273.
After 10190 training step(s), loss on training batch is 0.00785369.
After 10191 training step(s), loss on training batch is 0.0260756.
After 10192 training step(s), loss on training batch is 0.0100492.
After 10193 training step(s), loss on training batch is 0.00931211.
After 10194 training step(s), loss on training batch is 0.0175449.
After 10195 training step(s), loss on training batch is 0.00776491.
After 10196 training step(s), loss on training batch is 0.00890125.
After 10197 training step(s), loss on training batch is 0.00755195.
After 10198 training step(s), loss on training batch is 0.00793885.
After 10199 training step(s), loss on training batch is 0.00948528.
After 10200 training step(s), loss on training batch is 0.00873093.
After 10201 training step(s), loss on training batch is 0.00772002.
After 10202 training step(s), loss on training batch is 0.00748389.
After 10203 training step(s), loss on training batch is 0.00768549.
After 10204 training step(s), loss on training batch is 0.00790373.
After 10205 training step(s), loss on training batch is 0.00970214.
After 10206 training step(s), loss on training batch is 0.00722153.
After 10207 training step(s), loss on training batch is 0.00996763.
After 10208 training step(s), loss on training batch is 0.00859099.
After 10209 training step(s), loss on training batch is 0.00846857.
After 10210 training step(s), loss on training batch is 0.0084268.
After 10211 training step(s), loss on training batch is 0.00803514.
After 10212 training step(s), loss on training batch is 0.00943102.
After 10213 training step(s), loss on training batch is 0.0100561.
After 10214 training step(s), loss on training batch is 0.00821976.
After 10215 training step(s), loss on training batch is 0.00808438.
After 10216 training step(s), loss on training batch is 0.0152353.
After 10217 training step(s), loss on training batch is 0.00848036.
After 10218 training step(s), loss on training batch is 0.00770139.
After 10219 training step(s), loss on training batch is 0.0075456.
After 10220 training step(s), loss on training batch is 0.0131375.
After 10221 training step(s), loss on training batch is 0.0154056.
After 10222 training step(s), loss on training batch is 0.00860606.
After 10223 training step(s), loss on training batch is 0.00862993.
After 10224 training step(s), loss on training batch is 0.00788524.
After 10225 training step(s), loss on training batch is 0.00759563.
After 10226 training step(s), loss on training batch is 0.0077511.
After 10227 training step(s), loss on training batch is 0.00776845.
After 10228 training step(s), loss on training batch is 0.00821303.
After 10229 training step(s), loss on training batch is 0.00827618.
After 10230 training step(s), loss on training batch is 0.00755501.
After 10231 training step(s), loss on training batch is 0.00818415.
After 10232 training step(s), loss on training batch is 0.00830001.
After 10233 training step(s), loss on training batch is 0.00790878.
After 10234 training step(s), loss on training batch is 0.00771817.
After 10235 training step(s), loss on training batch is 0.00980992.
After 10236 training step(s), loss on training batch is 0.00756461.
After 10237 training step(s), loss on training batch is 0.00858161.
After 10238 training step(s), loss on training batch is 0.0074606.
After 10239 training step(s), loss on training batch is 0.0108582.
After 10240 training step(s), loss on training batch is 0.00871586.
After 10241 training step(s), loss on training batch is 0.00725239.
After 10242 training step(s), loss on training batch is 0.00731581.
After 10243 training step(s), loss on training batch is 0.00832107.
After 10244 training step(s), loss on training batch is 0.00732199.
After 10245 training step(s), loss on training batch is 0.00822019.
After 10246 training step(s), loss on training batch is 0.0090023.
After 10247 training step(s), loss on training batch is 0.00772914.
After 10248 training step(s), loss on training batch is 0.00921758.
After 10249 training step(s), loss on training batch is 0.00796969.
After 10250 training step(s), loss on training batch is 0.00760174.
After 10251 training step(s), loss on training batch is 0.00798067.
After 10252 training step(s), loss on training batch is 0.00754103.
After 10253 training step(s), loss on training batch is 0.00772846.
After 10254 training step(s), loss on training batch is 0.00837035.
After 10255 training step(s), loss on training batch is 0.00731592.
After 10256 training step(s), loss on training batch is 0.00843705.
After 10257 training step(s), loss on training batch is 0.00837389.
After 10258 training step(s), loss on training batch is 0.00793342.
After 10259 training step(s), loss on training batch is 0.0072086.
After 10260 training step(s), loss on training batch is 0.007728.
After 10261 training step(s), loss on training batch is 0.00827557.
After 10262 training step(s), loss on training batch is 0.00813726.
After 10263 training step(s), loss on training batch is 0.00822002.
After 10264 training step(s), loss on training batch is 0.00822407.
After 10265 training step(s), loss on training batch is 0.00829873.
After 10266 training step(s), loss on training batch is 0.0084466.
After 10267 training step(s), loss on training batch is 0.00733109.
After 10268 training step(s), loss on training batch is 0.00807454.
After 10269 training step(s), loss on training batch is 0.00833533.
After 10270 training step(s), loss on training batch is 0.0075852.
After 10271 training step(s), loss on training batch is 0.0074085.
After 10272 training step(s), loss on training batch is 0.00796015.
After 10273 training step(s), loss on training batch is 0.0072562.
After 10274 training step(s), loss on training batch is 0.00833971.
After 10275 training step(s), loss on training batch is 0.0147185.
After 10276 training step(s), loss on training batch is 0.00844617.
After 10277 training step(s), loss on training batch is 0.0109927.
After 10278 training step(s), loss on training batch is 0.0074254.
After 10279 training step(s), loss on training batch is 0.00809027.
After 10280 training step(s), loss on training batch is 0.00783834.
After 10281 training step(s), loss on training batch is 0.00787039.
After 10282 training step(s), loss on training batch is 0.0104908.
After 10283 training step(s), loss on training batch is 0.0100286.
After 10284 training step(s), loss on training batch is 0.00788412.
After 10285 training step(s), loss on training batch is 0.00753506.
After 10286 training step(s), loss on training batch is 0.00931562.
After 10287 training step(s), loss on training batch is 0.00809023.
After 10288 training step(s), loss on training batch is 0.00797542.
After 10289 training step(s), loss on training batch is 0.00857942.
After 10290 training step(s), loss on training batch is 0.00818707.
After 10291 training step(s), loss on training batch is 0.00812697.
After 10292 training step(s), loss on training batch is 0.00766294.
After 10293 training step(s), loss on training batch is 0.00977936.
After 10294 training step(s), loss on training batch is 0.00995843.
After 10295 training step(s), loss on training batch is 0.00767445.
After 10296 training step(s), loss on training batch is 0.0102913.
After 10297 training step(s), loss on training batch is 0.00742386.
After 10298 training step(s), loss on training batch is 0.00766865.
After 10299 training step(s), loss on training batch is 0.00732514.
After 10300 training step(s), loss on training batch is 0.00748129.
After 10301 training step(s), loss on training batch is 0.00802037.
After 10302 training step(s), loss on training batch is 0.0106963.
After 10303 training step(s), loss on training batch is 0.0103084.
After 10304 training step(s), loss on training batch is 0.00869792.
After 10305 training step(s), loss on training batch is 0.00831727.
After 10306 training step(s), loss on training batch is 0.00749652.
After 10307 training step(s), loss on training batch is 0.00927065.
After 10308 training step(s), loss on training batch is 0.00910251.
After 10309 training step(s), loss on training batch is 0.00759233.
After 10310 training step(s), loss on training batch is 0.0072041.
After 10311 training step(s), loss on training batch is 0.0081321.
After 10312 training step(s), loss on training batch is 0.0085714.
After 10313 training step(s), loss on training batch is 0.00841209.
After 10314 training step(s), loss on training batch is 0.00807137.
After 10315 training step(s), loss on training batch is 0.0076341.
After 10316 training step(s), loss on training batch is 0.00865666.
After 10317 training step(s), loss on training batch is 0.00799098.
After 10318 training step(s), loss on training batch is 0.00853321.
After 10319 training step(s), loss on training batch is 0.00805265.
After 10320 training step(s), loss on training batch is 0.0120944.
After 10321 training step(s), loss on training batch is 0.013095.
After 10322 training step(s), loss on training batch is 0.00973573.
After 10323 training step(s), loss on training batch is 0.00860312.
After 10324 training step(s), loss on training batch is 0.00923776.
After 10325 training step(s), loss on training batch is 0.00727128.
After 10326 training step(s), loss on training batch is 0.00758609.
After 10327 training step(s), loss on training batch is 0.00917995.
After 10328 training step(s), loss on training batch is 0.00728725.
After 10329 training step(s), loss on training batch is 0.00753001.
After 10330 training step(s), loss on training batch is 0.00740335.
After 10331 training step(s), loss on training batch is 0.00778228.
After 10332 training step(s), loss on training batch is 0.00769111.
After 10333 training step(s), loss on training batch is 0.00796734.
After 10334 training step(s), loss on training batch is 0.00769483.
After 10335 training step(s), loss on training batch is 0.00762002.
After 10336 training step(s), loss on training batch is 0.00989132.
After 10337 training step(s), loss on training batch is 0.00797743.
After 10338 training step(s), loss on training batch is 0.00836894.
After 10339 training step(s), loss on training batch is 0.00945758.
After 10340 training step(s), loss on training batch is 0.00937944.
After 10341 training step(s), loss on training batch is 0.00896983.
After 10342 training step(s), loss on training batch is 0.00885464.
After 10343 training step(s), loss on training batch is 0.00842416.
After 10344 training step(s), loss on training batch is 0.00839999.
After 10345 training step(s), loss on training batch is 0.00914747.
After 10346 training step(s), loss on training batch is 0.00877744.
After 10347 training step(s), loss on training batch is 0.00904704.
After 10348 training step(s), loss on training batch is 0.00793203.
After 10349 training step(s), loss on training batch is 0.00756567.
After 10350 training step(s), loss on training batch is 0.00887196.
After 10351 training step(s), loss on training batch is 0.00819165.
After 10352 training step(s), loss on training batch is 0.00750288.
After 10353 training step(s), loss on training batch is 0.00989123.
After 10354 training step(s), loss on training batch is 0.00742439.
After 10355 training step(s), loss on training batch is 0.011827.
After 10356 training step(s), loss on training batch is 0.00794529.
After 10357 training step(s), loss on training batch is 0.00807847.
After 10358 training step(s), loss on training batch is 0.00757825.
After 10359 training step(s), loss on training batch is 0.00718207.
After 10360 training step(s), loss on training batch is 0.00936795.
After 10361 training step(s), loss on training batch is 0.00982777.
After 10362 training step(s), loss on training batch is 0.0210924.
After 10363 training step(s), loss on training batch is 0.00839133.
After 10364 training step(s), loss on training batch is 0.00855013.
After 10365 training step(s), loss on training batch is 0.00781322.
After 10366 training step(s), loss on training batch is 0.00903425.
After 10367 training step(s), loss on training batch is 0.00815238.
After 10368 training step(s), loss on training batch is 0.0112577.
After 10369 training step(s), loss on training batch is 0.0103489.
After 10370 training step(s), loss on training batch is 0.00759905.
After 10371 training step(s), loss on training batch is 0.00794294.
After 10372 training step(s), loss on training batch is 0.00901737.
After 10373 training step(s), loss on training batch is 0.00808718.
After 10374 training step(s), loss on training batch is 0.00751001.
After 10375 training step(s), loss on training batch is 0.00896138.
After 10376 training step(s), loss on training batch is 0.00726038.
After 10377 training step(s), loss on training batch is 0.00859657.
After 10378 training step(s), loss on training batch is 0.00850246.
After 10379 training step(s), loss on training batch is 0.0101192.
After 10380 training step(s), loss on training batch is 0.00851403.
After 10381 training step(s), loss on training batch is 0.0072749.
After 10382 training step(s), loss on training batch is 0.00988861.
After 10383 training step(s), loss on training batch is 0.00900682.
After 10384 training step(s), loss on training batch is 0.0075304.
After 10385 training step(s), loss on training batch is 0.00722238.
After 10386 training step(s), loss on training batch is 0.0088268.
After 10387 training step(s), loss on training batch is 0.0106723.
After 10388 training step(s), loss on training batch is 0.00771448.
After 10389 training step(s), loss on training batch is 0.0103988.
After 10390 training step(s), loss on training batch is 0.0110217.
After 10391 training step(s), loss on training batch is 0.0075693.
After 10392 training step(s), loss on training batch is 0.0159274.
After 10393 training step(s), loss on training batch is 0.00985844.
After 10394 training step(s), loss on training batch is 0.0145549.
After 10395 training step(s), loss on training batch is 0.00970092.
After 10396 training step(s), loss on training batch is 0.00780997.
After 10397 training step(s), loss on training batch is 0.00839627.
After 10398 training step(s), loss on training batch is 0.0102642.
After 10399 training step(s), loss on training batch is 0.00814411.
After 10400 training step(s), loss on training batch is 0.00812692.
After 10401 training step(s), loss on training batch is 0.00791209.
After 10402 training step(s), loss on training batch is 0.00736674.
After 10403 training step(s), loss on training batch is 0.00925499.
After 10404 training step(s), loss on training batch is 0.00762006.
After 10405 training step(s), loss on training batch is 0.00735093.
After 10406 training step(s), loss on training batch is 0.00926151.
After 10407 training step(s), loss on training batch is 0.0107483.
After 10408 training step(s), loss on training batch is 0.00756182.
After 10409 training step(s), loss on training batch is 0.00794118.
After 10410 training step(s), loss on training batch is 0.00831926.
After 10411 training step(s), loss on training batch is 0.00960764.
After 10412 training step(s), loss on training batch is 0.0282135.
After 10413 training step(s), loss on training batch is 0.00777114.
After 10414 training step(s), loss on training batch is 0.00974287.
After 10415 training step(s), loss on training batch is 0.008373.
After 10416 training step(s), loss on training batch is 0.00955007.
After 10417 training step(s), loss on training batch is 0.00807392.
After 10418 training step(s), loss on training batch is 0.00794949.
After 10419 training step(s), loss on training batch is 0.0100501.
After 10420 training step(s), loss on training batch is 0.00922938.
After 10421 training step(s), loss on training batch is 0.00848463.
After 10422 training step(s), loss on training batch is 0.00941707.
After 10423 training step(s), loss on training batch is 0.00734072.
After 10424 training step(s), loss on training batch is 0.00868276.
After 10425 training step(s), loss on training batch is 0.00812727.
After 10426 training step(s), loss on training batch is 0.00781104.
After 10427 training step(s), loss on training batch is 0.00760567.
After 10428 training step(s), loss on training batch is 0.00912596.
After 10429 training step(s), loss on training batch is 0.00772559.
After 10430 training step(s), loss on training batch is 0.00847936.
After 10431 training step(s), loss on training batch is 0.0079343.
After 10432 training step(s), loss on training batch is 0.00823405.
After 10433 training step(s), loss on training batch is 0.00799458.
After 10434 training step(s), loss on training batch is 0.00868305.
After 10435 training step(s), loss on training batch is 0.00784247.
After 10436 training step(s), loss on training batch is 0.0132337.
After 10437 training step(s), loss on training batch is 0.0107251.
After 10438 training step(s), loss on training batch is 0.00764409.
After 10439 training step(s), loss on training batch is 0.00716303.
After 10440 training step(s), loss on training batch is 0.00924853.
After 10441 training step(s), loss on training batch is 0.00849001.
After 10442 training step(s), loss on training batch is 0.00826347.
After 10443 training step(s), loss on training batch is 0.00737893.
After 10444 training step(s), loss on training batch is 0.00964139.
After 10445 training step(s), loss on training batch is 0.00903223.
After 10446 training step(s), loss on training batch is 0.00748177.
After 10447 training step(s), loss on training batch is 0.00840253.
After 10448 training step(s), loss on training batch is 0.00776362.
After 10449 training step(s), loss on training batch is 0.00741831.
After 10450 training step(s), loss on training batch is 0.00895613.
After 10451 training step(s), loss on training batch is 0.00899757.
After 10452 training step(s), loss on training batch is 0.00791725.
After 10453 training step(s), loss on training batch is 0.00924079.
After 10454 training step(s), loss on training batch is 0.00827298.
After 10455 training step(s), loss on training batch is 0.00746088.
After 10456 training step(s), loss on training batch is 0.00693555.
After 10457 training step(s), loss on training batch is 0.00839262.
After 10458 training step(s), loss on training batch is 0.00719456.
After 10459 training step(s), loss on training batch is 0.00938634.
After 10460 training step(s), loss on training batch is 0.00762808.
After 10461 training step(s), loss on training batch is 0.00730212.
After 10462 training step(s), loss on training batch is 0.00750631.
After 10463 training step(s), loss on training batch is 0.00755122.
After 10464 training step(s), loss on training batch is 0.0083743.
After 10465 training step(s), loss on training batch is 0.00941388.
After 10466 training step(s), loss on training batch is 0.00822966.
After 10467 training step(s), loss on training batch is 0.00757086.
After 10468 training step(s), loss on training batch is 0.00930305.
After 10469 training step(s), loss on training batch is 0.00907851.
After 10470 training step(s), loss on training batch is 0.00796125.
After 10471 training step(s), loss on training batch is 0.00743399.
After 10472 training step(s), loss on training batch is 0.00752013.
After 10473 training step(s), loss on training batch is 0.00716269.
After 10474 training step(s), loss on training batch is 0.00950408.
After 10475 training step(s), loss on training batch is 0.00945656.
After 10476 training step(s), loss on training batch is 0.00721215.
After 10477 training step(s), loss on training batch is 0.007461.
After 10478 training step(s), loss on training batch is 0.00772979.
After 10479 training step(s), loss on training batch is 0.00746612.
After 10480 training step(s), loss on training batch is 0.00897272.
After 10481 training step(s), loss on training batch is 0.00871199.
After 10482 training step(s), loss on training batch is 0.00997389.
After 10483 training step(s), loss on training batch is 0.00770597.
After 10484 training step(s), loss on training batch is 0.00829293.
After 10485 training step(s), loss on training batch is 0.00836882.
After 10486 training step(s), loss on training batch is 0.00696147.
After 10487 training step(s), loss on training batch is 0.00836409.
After 10488 training step(s), loss on training batch is 0.0076087.
After 10489 training step(s), loss on training batch is 0.00743565.
After 10490 training step(s), loss on training batch is 0.00779996.
After 10491 training step(s), loss on training batch is 0.00748269.
After 10492 training step(s), loss on training batch is 0.00796056.
After 10493 training step(s), loss on training batch is 0.00759875.
After 10494 training step(s), loss on training batch is 0.00735782.
After 10495 training step(s), loss on training batch is 0.00947355.
After 10496 training step(s), loss on training batch is 0.0083956.
After 10497 training step(s), loss on training batch is 0.0081219.
After 10498 training step(s), loss on training batch is 0.00720172.
After 10499 training step(s), loss on training batch is 0.00826632.
After 10500 training step(s), loss on training batch is 0.00767028.
After 10501 training step(s), loss on training batch is 0.00833047.
After 10502 training step(s), loss on training batch is 0.00991995.
After 10503 training step(s), loss on training batch is 0.00796832.
After 10504 training step(s), loss on training batch is 0.00843524.
After 10505 training step(s), loss on training batch is 0.007472.
After 10506 training step(s), loss on training batch is 0.00735545.
After 10507 training step(s), loss on training batch is 0.0074983.
After 10508 training step(s), loss on training batch is 0.0080642.
After 10509 training step(s), loss on training batch is 0.00770011.
After 10510 training step(s), loss on training batch is 0.00714281.
After 10511 training step(s), loss on training batch is 0.00749904.
After 10512 training step(s), loss on training batch is 0.00751322.
After 10513 training step(s), loss on training batch is 0.00682486.
After 10514 training step(s), loss on training batch is 0.0073668.
After 10515 training step(s), loss on training batch is 0.00724339.
After 10516 training step(s), loss on training batch is 0.00707564.
After 10517 training step(s), loss on training batch is 0.00919533.
After 10518 training step(s), loss on training batch is 0.00804413.
After 10519 training step(s), loss on training batch is 0.00866492.
After 10520 training step(s), loss on training batch is 0.00719538.
After 10521 training step(s), loss on training batch is 0.00803959.
After 10522 training step(s), loss on training batch is 0.00948258.
After 10523 training step(s), loss on training batch is 0.00766454.
After 10524 training step(s), loss on training batch is 0.00894116.
After 10525 training step(s), loss on training batch is 0.00793593.
After 10526 training step(s), loss on training batch is 0.00935135.
After 10527 training step(s), loss on training batch is 0.0086993.
After 10528 training step(s), loss on training batch is 0.00702761.
After 10529 training step(s), loss on training batch is 0.00967446.
After 10530 training step(s), loss on training batch is 0.00766855.
After 10531 training step(s), loss on training batch is 0.00803863.
After 10532 training step(s), loss on training batch is 0.00812351.
After 10533 training step(s), loss on training batch is 0.0079853.
After 10534 training step(s), loss on training batch is 0.00727395.
After 10535 training step(s), loss on training batch is 0.0079582.
After 10536 training step(s), loss on training batch is 0.00815707.
After 10537 training step(s), loss on training batch is 0.00940916.
After 10538 training step(s), loss on training batch is 0.00779.
After 10539 training step(s), loss on training batch is 0.00794632.
After 10540 training step(s), loss on training batch is 0.00713402.
After 10541 training step(s), loss on training batch is 0.00946358.
After 10542 training step(s), loss on training batch is 0.00774802.
After 10543 training step(s), loss on training batch is 0.00749297.
After 10544 training step(s), loss on training batch is 0.00883071.
After 10545 training step(s), loss on training batch is 0.00731767.
After 10546 training step(s), loss on training batch is 0.00714749.
After 10547 training step(s), loss on training batch is 0.00762335.
After 10548 training step(s), loss on training batch is 0.00802595.
After 10549 training step(s), loss on training batch is 0.00727339.
After 10550 training step(s), loss on training batch is 0.0074545.
After 10551 training step(s), loss on training batch is 0.00710599.
After 10552 training step(s), loss on training batch is 0.00720852.
After 10553 training step(s), loss on training batch is 0.00839645.
After 10554 training step(s), loss on training batch is 0.00739665.
After 10555 training step(s), loss on training batch is 0.00743969.
After 10556 training step(s), loss on training batch is 0.00752786.
After 10557 training step(s), loss on training batch is 0.00707902.
After 10558 training step(s), loss on training batch is 0.0081047.
After 10559 training step(s), loss on training batch is 0.00711335.
After 10560 training step(s), loss on training batch is 0.0085351.
After 10561 training step(s), loss on training batch is 0.00724522.
After 10562 training step(s), loss on training batch is 0.0109146.
After 10563 training step(s), loss on training batch is 0.00811983.
After 10564 training step(s), loss on training batch is 0.0092546.
After 10565 training step(s), loss on training batch is 0.00713393.
After 10566 training step(s), loss on training batch is 0.00697483.
After 10567 training step(s), loss on training batch is 0.00872708.
After 10568 training step(s), loss on training batch is 0.00813407.
After 10569 training step(s), loss on training batch is 0.00746155.
After 10570 training step(s), loss on training batch is 0.00749738.
After 10571 training step(s), loss on training batch is 0.00880756.
After 10572 training step(s), loss on training batch is 0.00791542.
After 10573 training step(s), loss on training batch is 0.0075677.
After 10574 training step(s), loss on training batch is 0.00881971.
After 10575 training step(s), loss on training batch is 0.00709337.
After 10576 training step(s), loss on training batch is 0.00898882.
After 10577 training step(s), loss on training batch is 0.00798871.
After 10578 training step(s), loss on training batch is 0.00935985.
After 10579 training step(s), loss on training batch is 0.00783287.
After 10580 training step(s), loss on training batch is 0.00747256.
After 10581 training step(s), loss on training batch is 0.00725269.
After 10582 training step(s), loss on training batch is 0.00778661.
After 10583 training step(s), loss on training batch is 0.00853737.
After 10584 training step(s), loss on training batch is 0.00718393.
After 10585 training step(s), loss on training batch is 0.00731688.
After 10586 training step(s), loss on training batch is 0.00770236.
After 10587 training step(s), loss on training batch is 0.00769711.
After 10588 training step(s), loss on training batch is 0.00835872.
After 10589 training step(s), loss on training batch is 0.00735416.
After 10590 training step(s), loss on training batch is 0.008955.
After 10591 training step(s), loss on training batch is 0.00721434.
After 10592 training step(s), loss on training batch is 0.00719486.
After 10593 training step(s), loss on training batch is 0.00857537.
After 10594 training step(s), loss on training batch is 0.00700698.
After 10595 training step(s), loss on training batch is 0.00738278.
After 10596 training step(s), loss on training batch is 0.00754818.
After 10597 training step(s), loss on training batch is 0.0080611.
After 10598 training step(s), loss on training batch is 0.00936759.
After 10599 training step(s), loss on training batch is 0.00763024.
After 10600 training step(s), loss on training batch is 0.00824981.
After 10601 training step(s), loss on training batch is 0.00837094.
After 10602 training step(s), loss on training batch is 0.00691536.
After 10603 training step(s), loss on training batch is 0.00887405.
After 10604 training step(s), loss on training batch is 0.00753701.
After 10605 training step(s), loss on training batch is 0.00862542.
After 10606 training step(s), loss on training batch is 0.00719645.
After 10607 training step(s), loss on training batch is 0.00798679.
After 10608 training step(s), loss on training batch is 0.00769807.
After 10609 training step(s), loss on training batch is 0.00942854.
After 10610 training step(s), loss on training batch is 0.00708375.
After 10611 training step(s), loss on training batch is 0.00710555.
After 10612 training step(s), loss on training batch is 0.00777243.
After 10613 training step(s), loss on training batch is 0.00767158.
After 10614 training step(s), loss on training batch is 0.00793433.
After 10615 training step(s), loss on training batch is 0.0086411.
After 10616 training step(s), loss on training batch is 0.00761139.
After 10617 training step(s), loss on training batch is 0.00714485.
After 10618 training step(s), loss on training batch is 0.00729011.
After 10619 training step(s), loss on training batch is 0.00975149.
After 10620 training step(s), loss on training batch is 0.00689915.
After 10621 training step(s), loss on training batch is 0.00711418.
After 10622 training step(s), loss on training batch is 0.00741633.
After 10623 training step(s), loss on training batch is 0.00794071.
After 10624 training step(s), loss on training batch is 0.00718762.
After 10625 training step(s), loss on training batch is 0.00744824.
After 10626 training step(s), loss on training batch is 0.00738584.
After 10627 training step(s), loss on training batch is 0.00708565.
After 10628 training step(s), loss on training batch is 0.00803368.
After 10629 training step(s), loss on training batch is 0.00757365.
After 10630 training step(s), loss on training batch is 0.00712861.
After 10631 training step(s), loss on training batch is 0.00892773.
After 10632 training step(s), loss on training batch is 0.00997901.
After 10633 training step(s), loss on training batch is 0.0106506.
After 10634 training step(s), loss on training batch is 0.00729236.
After 10635 training step(s), loss on training batch is 0.00783791.
After 10636 training step(s), loss on training batch is 0.00758413.
After 10637 training step(s), loss on training batch is 0.00911023.
After 10638 training step(s), loss on training batch is 0.00779729.
After 10639 training step(s), loss on training batch is 0.00752155.
After 10640 training step(s), loss on training batch is 0.00756288.
After 10641 training step(s), loss on training batch is 0.0072203.
After 10642 training step(s), loss on training batch is 0.0100447.
After 10643 training step(s), loss on training batch is 0.0138774.
After 10644 training step(s), loss on training batch is 0.00765032.
After 10645 training step(s), loss on training batch is 0.0089327.
After 10646 training step(s), loss on training batch is 0.00812786.
After 10647 training step(s), loss on training batch is 0.00934519.
After 10648 training step(s), loss on training batch is 0.00840238.
After 10649 training step(s), loss on training batch is 0.00880747.
After 10650 training step(s), loss on training batch is 0.00934371.
After 10651 training step(s), loss on training batch is 0.00788965.
After 10652 training step(s), loss on training batch is 0.00751797.
After 10653 training step(s), loss on training batch is 0.00801814.
After 10654 training step(s), loss on training batch is 0.00924771.
After 10655 training step(s), loss on training batch is 0.00794257.
After 10656 training step(s), loss on training batch is 0.0106248.
After 10657 training step(s), loss on training batch is 0.00929972.
After 10658 training step(s), loss on training batch is 0.00853815.
After 10659 training step(s), loss on training batch is 0.00712255.
After 10660 training step(s), loss on training batch is 0.0112195.
After 10661 training step(s), loss on training batch is 0.0073155.
After 10662 training step(s), loss on training batch is 0.00727056.
After 10663 training step(s), loss on training batch is 0.00737394.
After 10664 training step(s), loss on training batch is 0.0081314.
After 10665 training step(s), loss on training batch is 0.00770473.
After 10666 training step(s), loss on training batch is 0.00736836.
After 10667 training step(s), loss on training batch is 0.00854849.
After 10668 training step(s), loss on training batch is 0.00981666.
After 10669 training step(s), loss on training batch is 0.00683617.
After 10670 training step(s), loss on training batch is 0.00835787.
After 10671 training step(s), loss on training batch is 0.00696548.
After 10672 training step(s), loss on training batch is 0.00827892.
After 10673 training step(s), loss on training batch is 0.00787806.
After 10674 training step(s), loss on training batch is 0.00887502.
After 10675 training step(s), loss on training batch is 0.0114395.
After 10676 training step(s), loss on training batch is 0.0109853.
After 10677 training step(s), loss on training batch is 0.00769892.
After 10678 training step(s), loss on training batch is 0.00827136.
After 10679 training step(s), loss on training batch is 0.00776295.
After 10680 training step(s), loss on training batch is 0.00747675.
After 10681 training step(s), loss on training batch is 0.00824695.
After 10682 training step(s), loss on training batch is 0.00712125.
After 10683 training step(s), loss on training batch is 0.0069392.
After 10684 training step(s), loss on training batch is 0.00712422.
After 10685 training step(s), loss on training batch is 0.00832078.
After 10686 training step(s), loss on training batch is 0.00817045.
After 10687 training step(s), loss on training batch is 0.00722217.
After 10688 training step(s), loss on training batch is 0.00908551.
After 10689 training step(s), loss on training batch is 0.00868625.
After 10690 training step(s), loss on training batch is 0.00929748.
After 10691 training step(s), loss on training batch is 0.00807469.
After 10692 training step(s), loss on training batch is 0.00756474.
After 10693 training step(s), loss on training batch is 0.00720918.
After 10694 training step(s), loss on training batch is 0.0122819.
After 10695 training step(s), loss on training batch is 0.0086659.
After 10696 training step(s), loss on training batch is 0.00994053.
After 10697 training step(s), loss on training batch is 0.00805731.
After 10698 training step(s), loss on training batch is 0.0076714.
After 10699 training step(s), loss on training batch is 0.00768698.
After 10700 training step(s), loss on training batch is 0.00816288.
After 10701 training step(s), loss on training batch is 0.00726429.
After 10702 training step(s), loss on training batch is 0.00808234.
After 10703 training step(s), loss on training batch is 0.0112382.
After 10704 training step(s), loss on training batch is 0.00840182.
After 10705 training step(s), loss on training batch is 0.00742696.
After 10706 training step(s), loss on training batch is 0.00952864.
After 10707 training step(s), loss on training batch is 0.010897.
After 10708 training step(s), loss on training batch is 0.00811207.
After 10709 training step(s), loss on training batch is 0.00715881.
After 10710 training step(s), loss on training batch is 0.00846201.
After 10711 training step(s), loss on training batch is 0.00839872.
After 10712 training step(s), loss on training batch is 0.00820699.
After 10713 training step(s), loss on training batch is 0.00783178.
After 10714 training step(s), loss on training batch is 0.00705128.
After 10715 training step(s), loss on training batch is 0.0072775.
After 10716 training step(s), loss on training batch is 0.00813746.
After 10717 training step(s), loss on training batch is 0.00742907.
After 10718 training step(s), loss on training batch is 0.00707877.
After 10719 training step(s), loss on training batch is 0.00776276.
After 10720 training step(s), loss on training batch is 0.00765416.
After 10721 training step(s), loss on training batch is 0.00739841.
After 10722 training step(s), loss on training batch is 0.0171437.
After 10723 training step(s), loss on training batch is 0.00938401.
After 10724 training step(s), loss on training batch is 0.00722724.
After 10725 training step(s), loss on training batch is 0.00709581.
After 10726 training step(s), loss on training batch is 0.00704866.
After 10727 training step(s), loss on training batch is 0.0071388.
After 10728 training step(s), loss on training batch is 0.00796929.
After 10729 training step(s), loss on training batch is 0.0070692.
After 10730 training step(s), loss on training batch is 0.00698785.
After 10731 training step(s), loss on training batch is 0.00696469.
After 10732 training step(s), loss on training batch is 0.00680853.
After 10733 training step(s), loss on training batch is 0.00752866.
After 10734 training step(s), loss on training batch is 0.00841466.
After 10735 training step(s), loss on training batch is 0.00728358.
After 10736 training step(s), loss on training batch is 0.00722214.
After 10737 training step(s), loss on training batch is 0.00772317.
After 10738 training step(s), loss on training batch is 0.0078948.
After 10739 training step(s), loss on training batch is 0.00730412.
After 10740 training step(s), loss on training batch is 0.00713097.
After 10741 training step(s), loss on training batch is 0.0107958.
After 10742 training step(s), loss on training batch is 0.0076283.
After 10743 training step(s), loss on training batch is 0.00698731.
After 10744 training step(s), loss on training batch is 0.0117489.
After 10745 training step(s), loss on training batch is 0.00819315.
After 10746 training step(s), loss on training batch is 0.00782898.
After 10747 training step(s), loss on training batch is 0.008093.
After 10748 training step(s), loss on training batch is 0.0153316.
After 10749 training step(s), loss on training batch is 0.00963944.
After 10750 training step(s), loss on training batch is 0.007847.
After 10751 training step(s), loss on training batch is 0.00745654.
After 10752 training step(s), loss on training batch is 0.0076968.
After 10753 training step(s), loss on training batch is 0.00697559.
After 10754 training step(s), loss on training batch is 0.00828616.
After 10755 training step(s), loss on training batch is 0.00712791.
After 10756 training step(s), loss on training batch is 0.00799682.
After 10757 training step(s), loss on training batch is 0.00912156.
After 10758 training step(s), loss on training batch is 0.00692956.
After 10759 training step(s), loss on training batch is 0.00867815.
After 10760 training step(s), loss on training batch is 0.00813553.
After 10761 training step(s), loss on training batch is 0.00849134.
After 10762 training step(s), loss on training batch is 0.00747036.
After 10763 training step(s), loss on training batch is 0.00749437.
After 10764 training step(s), loss on training batch is 0.0081853.
After 10765 training step(s), loss on training batch is 0.00706197.
After 10766 training step(s), loss on training batch is 0.00775766.
After 10767 training step(s), loss on training batch is 0.00679479.
After 10768 training step(s), loss on training batch is 0.00746476.
After 10769 training step(s), loss on training batch is 0.00920331.
After 10770 training step(s), loss on training batch is 0.00774362.
After 10771 training step(s), loss on training batch is 0.00769977.
After 10772 training step(s), loss on training batch is 0.00751604.
After 10773 training step(s), loss on training batch is 0.0109584.
After 10774 training step(s), loss on training batch is 0.00741108.
After 10775 training step(s), loss on training batch is 0.0073833.
After 10776 training step(s), loss on training batch is 0.00770698.
After 10777 training step(s), loss on training batch is 0.00901133.
After 10778 training step(s), loss on training batch is 0.00780375.
After 10779 training step(s), loss on training batch is 0.0072028.
After 10780 training step(s), loss on training batch is 0.00674121.
After 10781 training step(s), loss on training batch is 0.00820317.
After 10782 training step(s), loss on training batch is 0.00715154.
After 10783 training step(s), loss on training batch is 0.00736732.
After 10784 training step(s), loss on training batch is 0.00828508.
After 10785 training step(s), loss on training batch is 0.00707526.
After 10786 training step(s), loss on training batch is 0.00935882.
After 10787 training step(s), loss on training batch is 0.0082028.
After 10788 training step(s), loss on training batch is 0.00829293.
After 10789 training step(s), loss on training batch is 0.0099778.
After 10790 training step(s), loss on training batch is 0.00882249.
After 10791 training step(s), loss on training batch is 0.00890145.
After 10792 training step(s), loss on training batch is 0.00745829.
After 10793 training step(s), loss on training batch is 0.00743816.
After 10794 training step(s), loss on training batch is 0.00700916.
After 10795 training step(s), loss on training batch is 0.00772482.
After 10796 training step(s), loss on training batch is 0.00753433.
After 10797 training step(s), loss on training batch is 0.00732274.
After 10798 training step(s), loss on training batch is 0.0071282.
After 10799 training step(s), loss on training batch is 0.00758616.
After 10800 training step(s), loss on training batch is 0.00761069.
After 10801 training step(s), loss on training batch is 0.00871478.
After 10802 training step(s), loss on training batch is 0.0121323.
After 10803 training step(s), loss on training batch is 0.00753337.
After 10804 training step(s), loss on training batch is 0.00814721.
After 10805 training step(s), loss on training batch is 0.00738989.
After 10806 training step(s), loss on training batch is 0.00743083.
After 10807 training step(s), loss on training batch is 0.0115199.
After 10808 training step(s), loss on training batch is 0.00676052.
After 10809 training step(s), loss on training batch is 0.00709131.
After 10810 training step(s), loss on training batch is 0.00813693.
After 10811 training step(s), loss on training batch is 0.00694353.
After 10812 training step(s), loss on training batch is 0.00777053.
After 10813 training step(s), loss on training batch is 0.00725173.
After 10814 training step(s), loss on training batch is 0.00708995.
After 10815 training step(s), loss on training batch is 0.00724928.
After 10816 training step(s), loss on training batch is 0.00902787.
After 10817 training step(s), loss on training batch is 0.00704768.
After 10818 training step(s), loss on training batch is 0.00658583.
After 10819 training step(s), loss on training batch is 0.013373.
After 10820 training step(s), loss on training batch is 0.00784473.
After 10821 training step(s), loss on training batch is 0.00776936.
After 10822 training step(s), loss on training batch is 0.0127481.
After 10823 training step(s), loss on training batch is 0.00785269.
After 10824 training step(s), loss on training batch is 0.00750824.
After 10825 training step(s), loss on training batch is 0.00886578.
After 10826 training step(s), loss on training batch is 0.00749869.
After 10827 training step(s), loss on training batch is 0.00904286.
After 10828 training step(s), loss on training batch is 0.00877252.
After 10829 training step(s), loss on training batch is 0.00691212.
After 10830 training step(s), loss on training batch is 0.00738376.
After 10831 training step(s), loss on training batch is 0.00791397.
After 10832 training step(s), loss on training batch is 0.00740665.
After 10833 training step(s), loss on training batch is 0.00744523.
After 10834 training step(s), loss on training batch is 0.00721782.
After 10835 training step(s), loss on training batch is 0.00750684.
After 10836 training step(s), loss on training batch is 0.00748559.
After 10837 training step(s), loss on training batch is 0.00703084.
After 10838 training step(s), loss on training batch is 0.00735162.
After 10839 training step(s), loss on training batch is 0.00753146.
After 10840 training step(s), loss on training batch is 0.00810304.
After 10841 training step(s), loss on training batch is 0.00778464.
After 10842 training step(s), loss on training batch is 0.00698934.
After 10843 training step(s), loss on training batch is 0.00719077.
After 10844 training step(s), loss on training batch is 0.006686.
After 10845 training step(s), loss on training batch is 0.00842148.
After 10846 training step(s), loss on training batch is 0.007027.
After 10847 training step(s), loss on training batch is 0.00744471.
After 10848 training step(s), loss on training batch is 0.00794782.
After 10849 training step(s), loss on training batch is 0.00732069.
After 10850 training step(s), loss on training batch is 0.00718674.
After 10851 training step(s), loss on training batch is 0.00785714.
After 10852 training step(s), loss on training batch is 0.00688287.
After 10853 training step(s), loss on training batch is 0.00839083.
After 10854 training step(s), loss on training batch is 0.00788383.
After 10855 training step(s), loss on training batch is 0.00665983.
After 10856 training step(s), loss on training batch is 0.00707115.
After 10857 training step(s), loss on training batch is 0.00695421.
After 10858 training step(s), loss on training batch is 0.00737743.
After 10859 training step(s), loss on training batch is 0.00715871.
After 10860 training step(s), loss on training batch is 0.00785108.
After 10861 training step(s), loss on training batch is 0.00707818.
After 10862 training step(s), loss on training batch is 0.00706397.
After 10863 training step(s), loss on training batch is 0.00674344.
After 10864 training step(s), loss on training batch is 0.00811671.
After 10865 training step(s), loss on training batch is 0.00810194.
After 10866 training step(s), loss on training batch is 0.00826901.
After 10867 training step(s), loss on training batch is 0.00690391.
After 10868 training step(s), loss on training batch is 0.00878355.
After 10869 training step(s), loss on training batch is 0.00812967.
After 10870 training step(s), loss on training batch is 0.00893846.
After 10871 training step(s), loss on training batch is 0.0105757.
After 10872 training step(s), loss on training batch is 0.0101352.
After 10873 training step(s), loss on training batch is 0.00689358.
After 10874 training step(s), loss on training batch is 0.00760116.
After 10875 training step(s), loss on training batch is 0.00809884.
After 10876 training step(s), loss on training batch is 0.00720056.
After 10877 training step(s), loss on training batch is 0.00905822.
After 10878 training step(s), loss on training batch is 0.00941907.
After 10879 training step(s), loss on training batch is 0.00730422.
After 10880 training step(s), loss on training batch is 0.00813713.
After 10881 training step(s), loss on training batch is 0.00763578.
After 10882 training step(s), loss on training batch is 0.0074861.
After 10883 training step(s), loss on training batch is 0.00720365.
After 10884 training step(s), loss on training batch is 0.00837569.
After 10885 training step(s), loss on training batch is 0.00703976.
After 10886 training step(s), loss on training batch is 0.00704518.
After 10887 training step(s), loss on training batch is 0.00789981.
After 10888 training step(s), loss on training batch is 0.00667775.
After 10889 training step(s), loss on training batch is 0.00695546.
After 10890 training step(s), loss on training batch is 0.00729783.
After 10891 training step(s), loss on training batch is 0.00894384.
After 10892 training step(s), loss on training batch is 0.00791427.
After 10893 training step(s), loss on training batch is 0.00873007.
After 10894 training step(s), loss on training batch is 0.00667336.
After 10895 training step(s), loss on training batch is 0.00773766.
After 10896 training step(s), loss on training batch is 0.00714068.
After 10897 training step(s), loss on training batch is 0.0075005.
After 10898 training step(s), loss on training batch is 0.00729715.
After 10899 training step(s), loss on training batch is 0.00774844.
After 10900 training step(s), loss on training batch is 0.00758141.
After 10901 training step(s), loss on training batch is 0.00672914.
After 10902 training step(s), loss on training batch is 0.00763411.
After 10903 training step(s), loss on training batch is 0.00695428.
After 10904 training step(s), loss on training batch is 0.00675708.
After 10905 training step(s), loss on training batch is 0.00706517.
After 10906 training step(s), loss on training batch is 0.00689565.
After 10907 training step(s), loss on training batch is 0.00655006.
After 10908 training step(s), loss on training batch is 0.00799711.
After 10909 training step(s), loss on training batch is 0.00798808.
After 10910 training step(s), loss on training batch is 0.00727837.
After 10911 training step(s), loss on training batch is 0.00955701.
After 10912 training step(s), loss on training batch is 0.00781756.
After 10913 training step(s), loss on training batch is 0.00820684.
After 10914 training step(s), loss on training batch is 0.00702784.
After 10915 training step(s), loss on training batch is 0.0080454.
After 10916 training step(s), loss on training batch is 0.00705116.
After 10917 training step(s), loss on training batch is 0.00666955.
After 10918 training step(s), loss on training batch is 0.00656246.
After 10919 training step(s), loss on training batch is 0.0100063.
After 10920 training step(s), loss on training batch is 0.00895094.
After 10921 training step(s), loss on training batch is 0.00679912.
After 10922 training step(s), loss on training batch is 0.00981805.
After 10923 training step(s), loss on training batch is 0.0160834.
After 10924 training step(s), loss on training batch is 0.0199353.
After 10925 training step(s), loss on training batch is 0.0151682.
After 10926 training step(s), loss on training batch is 0.0069285.
After 10927 training step(s), loss on training batch is 0.00829087.
After 10928 training step(s), loss on training batch is 0.00927651.
After 10929 training step(s), loss on training batch is 0.00801406.
After 10930 training step(s), loss on training batch is 0.0109858.
After 10931 training step(s), loss on training batch is 0.00825227.
After 10932 training step(s), loss on training batch is 0.00920648.
After 10933 training step(s), loss on training batch is 0.0075534.
After 10934 training step(s), loss on training batch is 0.00818881.
After 10935 training step(s), loss on training batch is 0.00773658.
After 10936 training step(s), loss on training batch is 0.0079783.
After 10937 training step(s), loss on training batch is 0.00727687.
After 10938 training step(s), loss on training batch is 0.00688158.
After 10939 training step(s), loss on training batch is 0.011476.
After 10940 training step(s), loss on training batch is 0.00866128.
After 10941 training step(s), loss on training batch is 0.00944628.
After 10942 training step(s), loss on training batch is 0.00672998.
After 10943 training step(s), loss on training batch is 0.00770166.
After 10944 training step(s), loss on training batch is 0.00694263.
After 10945 training step(s), loss on training batch is 0.00688466.
After 10946 training step(s), loss on training batch is 0.00700623.
After 10947 training step(s), loss on training batch is 0.00737987.
After 10948 training step(s), loss on training batch is 0.00705309.
After 10949 training step(s), loss on training batch is 0.00982517.
After 10950 training step(s), loss on training batch is 0.00724133.
After 10951 training step(s), loss on training batch is 0.00723697.
After 10952 training step(s), loss on training batch is 0.00760704.
After 10953 training step(s), loss on training batch is 0.0081474.
After 10954 training step(s), loss on training batch is 0.0078273.
After 10955 training step(s), loss on training batch is 0.00898038.
After 10956 training step(s), loss on training batch is 0.00888529.
After 10957 training step(s), loss on training batch is 0.0085052.
After 10958 training step(s), loss on training batch is 0.00704326.
After 10959 training step(s), loss on training batch is 0.00902095.
After 10960 training step(s), loss on training batch is 0.00755362.
After 10961 training step(s), loss on training batch is 0.00751542.
After 10962 training step(s), loss on training batch is 0.00894653.
After 10963 training step(s), loss on training batch is 0.00697736.
After 10964 training step(s), loss on training batch is 0.00785183.
After 10965 training step(s), loss on training batch is 0.00715165.
After 10966 training step(s), loss on training batch is 0.00891836.
After 10967 training step(s), loss on training batch is 0.00810112.
After 10968 training step(s), loss on training batch is 0.00667877.
After 10969 training step(s), loss on training batch is 0.00712651.
After 10970 training step(s), loss on training batch is 0.00935582.
After 10971 training step(s), loss on training batch is 0.0066841.
After 10972 training step(s), loss on training batch is 0.00915059.
After 10973 training step(s), loss on training batch is 0.00864143.
After 10974 training step(s), loss on training batch is 0.00807631.
After 10975 training step(s), loss on training batch is 0.00738016.
After 10976 training step(s), loss on training batch is 0.00727966.
After 10977 training step(s), loss on training batch is 0.00757447.
After 10978 training step(s), loss on training batch is 0.0115846.
After 10979 training step(s), loss on training batch is 0.00728939.
After 10980 training step(s), loss on training batch is 0.00696221.
After 10981 training step(s), loss on training batch is 0.00720921.
After 10982 training step(s), loss on training batch is 0.00654392.
After 10983 training step(s), loss on training batch is 0.00760343.
After 10984 training step(s), loss on training batch is 0.00798623.
After 10985 training step(s), loss on training batch is 0.00678914.
After 10986 training step(s), loss on training batch is 0.00683127.
After 10987 training step(s), loss on training batch is 0.00707147.
After 10988 training step(s), loss on training batch is 0.00693838.
After 10989 training step(s), loss on training batch is 0.00799244.
After 10990 training step(s), loss on training batch is 0.0137503.
After 10991 training step(s), loss on training batch is 0.00784741.
After 10992 training step(s), loss on training batch is 0.00722859.
After 10993 training step(s), loss on training batch is 0.00771314.
After 10994 training step(s), loss on training batch is 0.0078108.
After 10995 training step(s), loss on training batch is 0.00652338.
After 10996 training step(s), loss on training batch is 0.00726423.
After 10997 training step(s), loss on training batch is 0.0157095.
After 10998 training step(s), loss on training batch is 0.00811128.
After 10999 training step(s), loss on training batch is 0.00734998.
After 11000 training step(s), loss on training batch is 0.0069034.
After 11001 training step(s), loss on training batch is 0.00958007.
After 11002 training step(s), loss on training batch is 0.00941126.
After 11003 training step(s), loss on training batch is 0.00691367.
After 11004 training step(s), loss on training batch is 0.00932195.
After 11005 training step(s), loss on training batch is 0.00654906.
After 11006 training step(s), loss on training batch is 0.00731482.
After 11007 training step(s), loss on training batch is 0.00729477.
After 11008 training step(s), loss on training batch is 0.00738478.
After 11009 training step(s), loss on training batch is 0.0072312.
After 11010 training step(s), loss on training batch is 0.00927338.
After 11011 training step(s), loss on training batch is 0.00723805.
After 11012 training step(s), loss on training batch is 0.00720386.
After 11013 training step(s), loss on training batch is 0.0065034.
After 11014 training step(s), loss on training batch is 0.0073452.
After 11015 training step(s), loss on training batch is 0.00679224.
After 11016 training step(s), loss on training batch is 0.00779969.
After 11017 training step(s), loss on training batch is 0.00842519.
After 11018 training step(s), loss on training batch is 0.00749273.
After 11019 training step(s), loss on training batch is 0.00672578.
After 11020 training step(s), loss on training batch is 0.00681572.
After 11021 training step(s), loss on training batch is 0.00724258.
After 11022 training step(s), loss on training batch is 0.00799533.
After 11023 training step(s), loss on training batch is 0.00696233.
After 11024 training step(s), loss on training batch is 0.00705564.
After 11025 training step(s), loss on training batch is 0.00797037.
After 11026 training step(s), loss on training batch is 0.00687315.
After 11027 training step(s), loss on training batch is 0.00700023.
After 11028 training step(s), loss on training batch is 0.00705329.
After 11029 training step(s), loss on training batch is 0.00675816.
After 11030 training step(s), loss on training batch is 0.00696525.
After 11031 training step(s), loss on training batch is 0.00676554.
After 11032 training step(s), loss on training batch is 0.00669399.
After 11033 training step(s), loss on training batch is 0.00703994.
After 11034 training step(s), loss on training batch is 0.00651947.
After 11035 training step(s), loss on training batch is 0.00759534.
After 11036 training step(s), loss on training batch is 0.00767165.
After 11037 training step(s), loss on training batch is 0.0076894.
After 11038 training step(s), loss on training batch is 0.00682262.
After 11039 training step(s), loss on training batch is 0.00824681.
After 11040 training step(s), loss on training batch is 0.00952877.
After 11041 training step(s), loss on training batch is 0.00784273.
After 11042 training step(s), loss on training batch is 0.00748728.
After 11043 training step(s), loss on training batch is 0.00659758.
After 11044 training step(s), loss on training batch is 0.006633.
After 11045 training step(s), loss on training batch is 0.0065869.
After 11046 training step(s), loss on training batch is 0.0078438.
After 11047 training step(s), loss on training batch is 0.00773454.
After 11048 training step(s), loss on training batch is 0.00796696.
After 11049 training step(s), loss on training batch is 0.00691505.
After 11050 training step(s), loss on training batch is 0.00745559.
After 11051 training step(s), loss on training batch is 0.0080338.
After 11052 training step(s), loss on training batch is 0.00662351.
After 11053 training step(s), loss on training batch is 0.00678209.
After 11054 training step(s), loss on training batch is 0.00715175.
After 11055 training step(s), loss on training batch is 0.00688378.
After 11056 training step(s), loss on training batch is 0.00663493.
After 11057 training step(s), loss on training batch is 0.00755755.
After 11058 training step(s), loss on training batch is 0.007262.
After 11059 training step(s), loss on training batch is 0.00761196.
After 11060 training step(s), loss on training batch is 0.00717439.
After 11061 training step(s), loss on training batch is 0.00743592.
After 11062 training step(s), loss on training batch is 0.00776213.
After 11063 training step(s), loss on training batch is 0.00897888.
After 11064 training step(s), loss on training batch is 0.00725781.
After 11065 training step(s), loss on training batch is 0.00644515.
After 11066 training step(s), loss on training batch is 0.0103115.
After 11067 training step(s), loss on training batch is 0.00877181.
After 11068 training step(s), loss on training batch is 0.00726524.
After 11069 training step(s), loss on training batch is 0.00871718.
After 11070 training step(s), loss on training batch is 0.00713013.
After 11071 training step(s), loss on training batch is 0.00764351.
After 11072 training step(s), loss on training batch is 0.00739634.
After 11073 training step(s), loss on training batch is 0.00675868.
After 11074 training step(s), loss on training batch is 0.00701317.
After 11075 training step(s), loss on training batch is 0.00695819.
After 11076 training step(s), loss on training batch is 0.00762636.
After 11077 training step(s), loss on training batch is 0.00672692.
After 11078 training step(s), loss on training batch is 0.00754125.
After 11079 training step(s), loss on training batch is 0.00869176.
After 11080 training step(s), loss on training batch is 0.00761749.
After 11081 training step(s), loss on training batch is 0.00948884.
After 11082 training step(s), loss on training batch is 0.00678819.
After 11083 training step(s), loss on training batch is 0.00783094.
After 11084 training step(s), loss on training batch is 0.00674217.
After 11085 training step(s), loss on training batch is 0.00929746.
After 11086 training step(s), loss on training batch is 0.00806677.
After 11087 training step(s), loss on training batch is 0.00768421.
After 11088 training step(s), loss on training batch is 0.00789654.
After 11089 training step(s), loss on training batch is 0.00713048.
After 11090 training step(s), loss on training batch is 0.00655581.
After 11091 training step(s), loss on training batch is 0.00680176.
After 11092 training step(s), loss on training batch is 0.0067394.
After 11093 training step(s), loss on training batch is 0.00791087.
After 11094 training step(s), loss on training batch is 0.00719397.
After 11095 training step(s), loss on training batch is 0.00682919.
After 11096 training step(s), loss on training batch is 0.00804538.
After 11097 training step(s), loss on training batch is 0.00691567.
After 11098 training step(s), loss on training batch is 0.00687774.
After 11099 training step(s), loss on training batch is 0.0067972.
After 11100 training step(s), loss on training batch is 0.00662618.
After 11101 training step(s), loss on training batch is 0.00660748.
After 11102 training step(s), loss on training batch is 0.00656913.
After 11103 training step(s), loss on training batch is 0.00694827.
After 11104 training step(s), loss on training batch is 0.00765884.
After 11105 training step(s), loss on training batch is 0.00680164.
After 11106 training step(s), loss on training batch is 0.00765312.
After 11107 training step(s), loss on training batch is 0.006819.
After 11108 training step(s), loss on training batch is 0.00664108.
After 11109 training step(s), loss on training batch is 0.00675984.
After 11110 training step(s), loss on training batch is 0.00688666.
After 11111 training step(s), loss on training batch is 0.00698983.
After 11112 training step(s), loss on training batch is 0.00709702.
After 11113 training step(s), loss on training batch is 0.00666948.
After 11114 training step(s), loss on training batch is 0.00816221.
After 11115 training step(s), loss on training batch is 0.00753212.
After 11116 training step(s), loss on training batch is 0.00728926.
After 11117 training step(s), loss on training batch is 0.00679321.
After 11118 training step(s), loss on training batch is 0.00774842.
After 11119 training step(s), loss on training batch is 0.00699634.
After 11120 training step(s), loss on training batch is 0.00708674.
After 11121 training step(s), loss on training batch is 0.00779072.
After 11122 training step(s), loss on training batch is 0.0069691.
After 11123 training step(s), loss on training batch is 0.0072365.
After 11124 training step(s), loss on training batch is 0.00702398.
After 11125 training step(s), loss on training batch is 0.00737727.
After 11126 training step(s), loss on training batch is 0.00642705.
After 11127 training step(s), loss on training batch is 0.00669746.
After 11128 training step(s), loss on training batch is 0.00678728.
After 11129 training step(s), loss on training batch is 0.00669286.
After 11130 training step(s), loss on training batch is 0.00871501.
After 11131 training step(s), loss on training batch is 0.00643744.
After 11132 training step(s), loss on training batch is 0.00650672.
After 11133 training step(s), loss on training batch is 0.0068738.
After 11134 training step(s), loss on training batch is 0.00738926.
After 11135 training step(s), loss on training batch is 0.0070353.
After 11136 training step(s), loss on training batch is 0.00665373.
After 11137 training step(s), loss on training batch is 0.00705474.
After 11138 training step(s), loss on training batch is 0.0072135.
After 11139 training step(s), loss on training batch is 0.00752212.
After 11140 training step(s), loss on training batch is 0.0100355.
After 11141 training step(s), loss on training batch is 0.00693112.
After 11142 training step(s), loss on training batch is 0.0070724.
After 11143 training step(s), loss on training batch is 0.0074369.
After 11144 training step(s), loss on training batch is 0.00664339.
After 11145 training step(s), loss on training batch is 0.00766305.
After 11146 training step(s), loss on training batch is 0.0075413.
After 11147 training step(s), loss on training batch is 0.00801555.
After 11148 training step(s), loss on training batch is 0.0114575.
After 11149 training step(s), loss on training batch is 0.0211913.
After 11150 training step(s), loss on training batch is 0.00680465.
After 11151 training step(s), loss on training batch is 0.00677882.
After 11152 training step(s), loss on training batch is 0.00772565.
After 11153 training step(s), loss on training batch is 0.00929316.
After 11154 training step(s), loss on training batch is 0.00836857.
After 11155 training step(s), loss on training batch is 0.00641638.
After 11156 training step(s), loss on training batch is 0.00730393.
After 11157 training step(s), loss on training batch is 0.0068021.
After 11158 training step(s), loss on training batch is 0.00778907.
After 11159 training step(s), loss on training batch is 0.00719119.
After 11160 training step(s), loss on training batch is 0.00674996.
After 11161 training step(s), loss on training batch is 0.0078204.
After 11162 training step(s), loss on training batch is 0.00744591.
After 11163 training step(s), loss on training batch is 0.0091931.
After 11164 training step(s), loss on training batch is 0.00719287.
After 11165 training step(s), loss on training batch is 0.00656384.
After 11166 training step(s), loss on training batch is 0.00802345.
After 11167 training step(s), loss on training batch is 0.00808092.
After 11168 training step(s), loss on training batch is 0.00668995.
After 11169 training step(s), loss on training batch is 0.0157632.
After 11170 training step(s), loss on training batch is 0.00711297.
After 11171 training step(s), loss on training batch is 0.0108137.
After 11172 training step(s), loss on training batch is 0.0067819.
After 11173 training step(s), loss on training batch is 0.00685092.
After 11174 training step(s), loss on training batch is 0.00678047.
After 11175 training step(s), loss on training batch is 0.00648223.
After 11176 training step(s), loss on training batch is 0.00648982.
After 11177 training step(s), loss on training batch is 0.0068382.
After 11178 training step(s), loss on training batch is 0.00729248.
After 11179 training step(s), loss on training batch is 0.00845606.
After 11180 training step(s), loss on training batch is 0.0070281.
After 11181 training step(s), loss on training batch is 0.00747279.
After 11182 training step(s), loss on training batch is 0.00829582.
After 11183 training step(s), loss on training batch is 0.00671194.
After 11184 training step(s), loss on training batch is 0.00726865.
After 11185 training step(s), loss on training batch is 0.00655976.
After 11186 training step(s), loss on training batch is 0.00690755.
After 11187 training step(s), loss on training batch is 0.00745584.
After 11188 training step(s), loss on training batch is 0.00775434.
After 11189 training step(s), loss on training batch is 0.00685874.
After 11190 training step(s), loss on training batch is 0.00687761.
After 11191 training step(s), loss on training batch is 0.00824933.
After 11192 training step(s), loss on training batch is 0.00766604.
After 11193 training step(s), loss on training batch is 0.00677309.
After 11194 training step(s), loss on training batch is 0.00824665.
After 11195 training step(s), loss on training batch is 0.00715579.
After 11196 training step(s), loss on training batch is 0.00755674.
After 11197 training step(s), loss on training batch is 0.00731851.
After 11198 training step(s), loss on training batch is 0.00688664.
After 11199 training step(s), loss on training batch is 0.00670744.
After 11200 training step(s), loss on training batch is 0.00741439.
After 11201 training step(s), loss on training batch is 0.00787556.
After 11202 training step(s), loss on training batch is 0.00797437.
After 11203 training step(s), loss on training batch is 0.00676464.
After 11204 training step(s), loss on training batch is 0.00706096.
After 11205 training step(s), loss on training batch is 0.00714511.
After 11206 training step(s), loss on training batch is 0.00847603.
After 11207 training step(s), loss on training batch is 0.00688929.
After 11208 training step(s), loss on training batch is 0.00696354.
After 11209 training step(s), loss on training batch is 0.00868764.
After 11210 training step(s), loss on training batch is 0.00837489.
After 11211 training step(s), loss on training batch is 0.0072086.
After 11212 training step(s), loss on training batch is 0.0116328.
After 11213 training step(s), loss on training batch is 0.0168665.
After 11214 training step(s), loss on training batch is 0.0218284.
After 11215 training step(s), loss on training batch is 0.00770699.
After 11216 training step(s), loss on training batch is 0.00902492.
After 11217 training step(s), loss on training batch is 0.00801323.
After 11218 training step(s), loss on training batch is 0.00921787.
After 11219 training step(s), loss on training batch is 0.00659256.
After 11220 training step(s), loss on training batch is 0.00696823.
After 11221 training step(s), loss on training batch is 0.00801105.
After 11222 training step(s), loss on training batch is 0.00686619.
After 11223 training step(s), loss on training batch is 0.0127517.
After 11224 training step(s), loss on training batch is 0.00698047.
After 11225 training step(s), loss on training batch is 0.00697422.
After 11226 training step(s), loss on training batch is 0.00872182.
After 11227 training step(s), loss on training batch is 0.01005.
After 11228 training step(s), loss on training batch is 0.0077045.
After 11229 training step(s), loss on training batch is 0.00750174.
After 11230 training step(s), loss on training batch is 0.00749237.
After 11231 training step(s), loss on training batch is 0.00729205.
After 11232 training step(s), loss on training batch is 0.00664728.
After 11233 training step(s), loss on training batch is 0.00875265.
After 11234 training step(s), loss on training batch is 0.00952481.
After 11235 training step(s), loss on training batch is 0.00727688.
After 11236 training step(s), loss on training batch is 0.00855051.
After 11237 training step(s), loss on training batch is 0.00851756.
After 11238 training step(s), loss on training batch is 0.00903817.
After 11239 training step(s), loss on training batch is 0.00825732.
After 11240 training step(s), loss on training batch is 0.00810934.
After 11241 training step(s), loss on training batch is 0.00658256.
After 11242 training step(s), loss on training batch is 0.0073101.
After 11243 training step(s), loss on training batch is 0.00841384.
After 11244 training step(s), loss on training batch is 0.00699519.
After 11245 training step(s), loss on training batch is 0.00768417.
After 11246 training step(s), loss on training batch is 0.00667651.
After 11247 training step(s), loss on training batch is 0.00744984.
After 11248 training step(s), loss on training batch is 0.00673372.
After 11249 training step(s), loss on training batch is 0.0068928.
After 11250 training step(s), loss on training batch is 0.0069204.
After 11251 training step(s), loss on training batch is 0.00788566.
After 11252 training step(s), loss on training batch is 0.00821163.
After 11253 training step(s), loss on training batch is 0.00802569.
After 11254 training step(s), loss on training batch is 0.00629617.
After 11255 training step(s), loss on training batch is 0.00819032.
After 11256 training step(s), loss on training batch is 0.00760294.
After 11257 training step(s), loss on training batch is 0.00710012.
After 11258 training step(s), loss on training batch is 0.00667721.
After 11259 training step(s), loss on training batch is 0.0267545.
After 11260 training step(s), loss on training batch is 0.00826944.
After 11261 training step(s), loss on training batch is 0.00705731.
After 11262 training step(s), loss on training batch is 0.0103574.
After 11263 training step(s), loss on training batch is 0.00903243.
After 11264 training step(s), loss on training batch is 0.00916279.
After 11265 training step(s), loss on training batch is 0.00688596.
After 11266 training step(s), loss on training batch is 0.0127415.
After 11267 training step(s), loss on training batch is 0.00982928.
After 11268 training step(s), loss on training batch is 0.00751847.
After 11269 training step(s), loss on training batch is 0.00852794.
After 11270 training step(s), loss on training batch is 0.0079162.
After 11271 training step(s), loss on training batch is 0.0184972.
After 11272 training step(s), loss on training batch is 0.00740261.
After 11273 training step(s), loss on training batch is 0.0074891.
After 11274 training step(s), loss on training batch is 0.00843578.
After 11275 training step(s), loss on training batch is 0.00913432.
After 11276 training step(s), loss on training batch is 0.0112921.
After 11277 training step(s), loss on training batch is 0.00670445.
After 11278 training step(s), loss on training batch is 0.00988384.
After 11279 training step(s), loss on training batch is 0.00801453.
After 11280 training step(s), loss on training batch is 0.00631374.
After 11281 training step(s), loss on training batch is 0.00777177.
After 11282 training step(s), loss on training batch is 0.00692795.
After 11283 training step(s), loss on training batch is 0.00759167.
After 11284 training step(s), loss on training batch is 0.00621476.
After 11285 training step(s), loss on training batch is 0.00653912.
After 11286 training step(s), loss on training batch is 0.00794555.
After 11287 training step(s), loss on training batch is 0.00709489.
After 11288 training step(s), loss on training batch is 0.00772141.
After 11289 training step(s), loss on training batch is 0.00726567.
After 11290 training step(s), loss on training batch is 0.00675127.
After 11291 training step(s), loss on training batch is 0.00705166.
After 11292 training step(s), loss on training batch is 0.00679611.
After 11293 training step(s), loss on training batch is 0.00682595.
After 11294 training step(s), loss on training batch is 0.0080181.
After 11295 training step(s), loss on training batch is 0.00667987.
After 11296 training step(s), loss on training batch is 0.0070263.
After 11297 training step(s), loss on training batch is 0.0073005.
After 11298 training step(s), loss on training batch is 0.00642324.
After 11299 training step(s), loss on training batch is 0.00651323.
After 11300 training step(s), loss on training batch is 0.00720962.
After 11301 training step(s), loss on training batch is 0.00687358.
After 11302 training step(s), loss on training batch is 0.00710362.
After 11303 training step(s), loss on training batch is 0.0070035.
After 11304 training step(s), loss on training batch is 0.00681389.
After 11305 training step(s), loss on training batch is 0.00680031.
After 11306 training step(s), loss on training batch is 0.0077946.
After 11307 training step(s), loss on training batch is 0.0080585.
After 11308 training step(s), loss on training batch is 0.00939279.
After 11309 training step(s), loss on training batch is 0.0076115.
After 11310 training step(s), loss on training batch is 0.00703591.
After 11311 training step(s), loss on training batch is 0.00713703.
After 11312 training step(s), loss on training batch is 0.00739419.
After 11313 training step(s), loss on training batch is 0.00690765.
After 11314 training step(s), loss on training batch is 0.00833364.
After 11315 training step(s), loss on training batch is 0.00799778.
After 11316 training step(s), loss on training batch is 0.0082336.
After 11317 training step(s), loss on training batch is 0.0070006.
After 11318 training step(s), loss on training batch is 0.00679501.
After 11319 training step(s), loss on training batch is 0.00662249.
After 11320 training step(s), loss on training batch is 0.00819841.
After 11321 training step(s), loss on training batch is 0.00708734.
After 11322 training step(s), loss on training batch is 0.00804446.
After 11323 training step(s), loss on training batch is 0.00694919.
After 11324 training step(s), loss on training batch is 0.00698994.
After 11325 training step(s), loss on training batch is 0.00726534.
After 11326 training step(s), loss on training batch is 0.00715705.
After 11327 training step(s), loss on training batch is 0.00644534.
After 11328 training step(s), loss on training batch is 0.00718835.
After 11329 training step(s), loss on training batch is 0.00679157.
After 11330 training step(s), loss on training batch is 0.00656533.
After 11331 training step(s), loss on training batch is 0.00673181.
After 11332 training step(s), loss on training batch is 0.00827925.
After 11333 training step(s), loss on training batch is 0.0070099.
After 11334 training step(s), loss on training batch is 0.00747496.
After 11335 training step(s), loss on training batch is 0.00661251.
After 11336 training step(s), loss on training batch is 0.0064993.
After 11337 training step(s), loss on training batch is 0.00790926.
After 11338 training step(s), loss on training batch is 0.00673115.
After 11339 training step(s), loss on training batch is 0.00661286.
After 11340 training step(s), loss on training batch is 0.00791973.
After 11341 training step(s), loss on training batch is 0.00660207.
After 11342 training step(s), loss on training batch is 0.00782085.
After 11343 training step(s), loss on training batch is 0.00679176.
After 11344 training step(s), loss on training batch is 0.00657435.
After 11345 training step(s), loss on training batch is 0.00643639.
After 11346 training step(s), loss on training batch is 0.00664381.
After 11347 training step(s), loss on training batch is 0.00763435.
After 11348 training step(s), loss on training batch is 0.00651595.
After 11349 training step(s), loss on training batch is 0.00741655.
After 11350 training step(s), loss on training batch is 0.00677715.
After 11351 training step(s), loss on training batch is 0.00679073.
After 11352 training step(s), loss on training batch is 0.00783736.
After 11353 training step(s), loss on training batch is 0.00722566.
After 11354 training step(s), loss on training batch is 0.00889317.
After 11355 training step(s), loss on training batch is 0.00790968.
After 11356 training step(s), loss on training batch is 0.00681846.
After 11357 training step(s), loss on training batch is 0.00795933.
After 11358 training step(s), loss on training batch is 0.0068631.
After 11359 training step(s), loss on training batch is 0.00619907.
After 11360 training step(s), loss on training batch is 0.00868171.
After 11361 training step(s), loss on training batch is 0.00639753.
After 11362 training step(s), loss on training batch is 0.00687439.
After 11363 training step(s), loss on training batch is 0.00735899.
After 11364 training step(s), loss on training batch is 0.00768454.
After 11365 training step(s), loss on training batch is 0.00625938.
After 11366 training step(s), loss on training batch is 0.00812606.
After 11367 training step(s), loss on training batch is 0.0128216.
After 11368 training step(s), loss on training batch is 0.00708101.
After 11369 training step(s), loss on training batch is 0.00768121.
After 11370 training step(s), loss on training batch is 0.00691628.
After 11371 training step(s), loss on training batch is 0.00640769.
After 11372 training step(s), loss on training batch is 0.00691452.
After 11373 training step(s), loss on training batch is 0.00767149.
After 11374 training step(s), loss on training batch is 0.00890356.
After 11375 training step(s), loss on training batch is 0.00677946.
After 11376 training step(s), loss on training batch is 0.00652917.
After 11377 training step(s), loss on training batch is 0.00682962.
After 11378 training step(s), loss on training batch is 0.00647405.
After 11379 training step(s), loss on training batch is 0.0081671.
After 11380 training step(s), loss on training batch is 0.0075077.
After 11381 training step(s), loss on training batch is 0.00830368.
After 11382 training step(s), loss on training batch is 0.00711967.
After 11383 training step(s), loss on training batch is 0.0084737.
After 11384 training step(s), loss on training batch is 0.00639953.
After 11385 training step(s), loss on training batch is 0.00651906.
After 11386 training step(s), loss on training batch is 0.00692137.
After 11387 training step(s), loss on training batch is 0.00646078.
After 11388 training step(s), loss on training batch is 0.007813.
After 11389 training step(s), loss on training batch is 0.00652824.
After 11390 training step(s), loss on training batch is 0.00730068.
After 11391 training step(s), loss on training batch is 0.00615317.
After 11392 training step(s), loss on training batch is 0.00684014.
After 11393 training step(s), loss on training batch is 0.00817819.
After 11394 training step(s), loss on training batch is 0.00712029.
After 11395 training step(s), loss on training batch is 0.00766264.
After 11396 training step(s), loss on training batch is 0.00783564.
After 11397 training step(s), loss on training batch is 0.00630045.
After 11398 training step(s), loss on training batch is 0.00789969.
After 11399 training step(s), loss on training batch is 0.00775102.
After 11400 training step(s), loss on training batch is 0.00818211.
After 11401 training step(s), loss on training batch is 0.00908229.
After 11402 training step(s), loss on training batch is 0.00743979.
After 11403 training step(s), loss on training batch is 0.00693893.
After 11404 training step(s), loss on training batch is 0.00663158.
After 11405 training step(s), loss on training batch is 0.00629345.
After 11406 training step(s), loss on training batch is 0.00650692.
After 11407 training step(s), loss on training batch is 0.0124553.
After 11408 training step(s), loss on training batch is 0.00784151.
After 11409 training step(s), loss on training batch is 0.00664899.
After 11410 training step(s), loss on training batch is 0.00827841.
After 11411 training step(s), loss on training batch is 0.00979739.
After 11412 training step(s), loss on training batch is 0.00673584.
After 11413 training step(s), loss on training batch is 0.00654715.
After 11414 training step(s), loss on training batch is 0.00654723.
After 11415 training step(s), loss on training batch is 0.00865634.
After 11416 training step(s), loss on training batch is 0.00649374.
After 11417 training step(s), loss on training batch is 0.00752611.
After 11418 training step(s), loss on training batch is 0.00793564.
After 11419 training step(s), loss on training batch is 0.00750329.
After 11420 training step(s), loss on training batch is 0.0098224.
After 11421 training step(s), loss on training batch is 0.00690788.
After 11422 training step(s), loss on training batch is 0.00637602.
After 11423 training step(s), loss on training batch is 0.00722943.
After 11424 training step(s), loss on training batch is 0.00686905.
After 11425 training step(s), loss on training batch is 0.00737835.
After 11426 training step(s), loss on training batch is 0.006675.
After 11427 training step(s), loss on training batch is 0.00744778.
After 11428 training step(s), loss on training batch is 0.00626902.
After 11429 training step(s), loss on training batch is 0.00684812.
After 11430 training step(s), loss on training batch is 0.00671021.
After 11431 training step(s), loss on training batch is 0.00749761.
After 11432 training step(s), loss on training batch is 0.00650761.
After 11433 training step(s), loss on training batch is 0.00660764.
After 11434 training step(s), loss on training batch is 0.00629444.
After 11435 training step(s), loss on training batch is 0.00723496.
After 11436 training step(s), loss on training batch is 0.00628792.
After 11437 training step(s), loss on training batch is 0.00677103.
After 11438 training step(s), loss on training batch is 0.0066756.
After 11439 training step(s), loss on training batch is 0.00706299.
After 11440 training step(s), loss on training batch is 0.00652768.
After 11441 training step(s), loss on training batch is 0.0068648.
After 11442 training step(s), loss on training batch is 0.00761841.
After 11443 training step(s), loss on training batch is 0.00746912.
After 11444 training step(s), loss on training batch is 0.00739525.
After 11445 training step(s), loss on training batch is 0.00703351.
After 11446 training step(s), loss on training batch is 0.00783279.
After 11447 training step(s), loss on training batch is 0.0068078.
After 11448 training step(s), loss on training batch is 0.0068595.
After 11449 training step(s), loss on training batch is 0.00649783.
After 11450 training step(s), loss on training batch is 0.00750237.
After 11451 training step(s), loss on training batch is 0.00684002.
After 11452 training step(s), loss on training batch is 0.00687656.
After 11453 training step(s), loss on training batch is 0.00673739.
After 11454 training step(s), loss on training batch is 0.00697559.
After 11455 training step(s), loss on training batch is 0.00620372.
After 11456 training step(s), loss on training batch is 0.00668051.
After 11457 training step(s), loss on training batch is 0.00609782.
After 11458 training step(s), loss on training batch is 0.00707529.
After 11459 training step(s), loss on training batch is 0.00682743.
After 11460 training step(s), loss on training batch is 0.0062777.
After 11461 training step(s), loss on training batch is 0.00698214.
After 11462 training step(s), loss on training batch is 0.00679231.
After 11463 training step(s), loss on training batch is 0.0069386.
After 11464 training step(s), loss on training batch is 0.00764132.
After 11465 training step(s), loss on training batch is 0.0067688.
After 11466 training step(s), loss on training batch is 0.00911742.
After 11467 training step(s), loss on training batch is 0.00669946.
After 11468 training step(s), loss on training batch is 0.00685513.
After 11469 training step(s), loss on training batch is 0.00675059.
After 11470 training step(s), loss on training batch is 0.00655186.
After 11471 training step(s), loss on training batch is 0.00812453.
After 11472 training step(s), loss on training batch is 0.00734713.
After 11473 training step(s), loss on training batch is 0.00816695.
After 11474 training step(s), loss on training batch is 0.00716902.
After 11475 training step(s), loss on training batch is 0.00891308.
After 11476 training step(s), loss on training batch is 0.00650479.
After 11477 training step(s), loss on training batch is 0.00697509.
After 11478 training step(s), loss on training batch is 0.00684409.
After 11479 training step(s), loss on training batch is 0.00782027.
After 11480 training step(s), loss on training batch is 0.00789994.
After 11481 training step(s), loss on training batch is 0.00880282.
After 11482 training step(s), loss on training batch is 0.0077552.
After 11483 training step(s), loss on training batch is 0.0090337.
After 11484 training step(s), loss on training batch is 0.00745301.
After 11485 training step(s), loss on training batch is 0.00677879.
After 11486 training step(s), loss on training batch is 0.00683876.
After 11487 training step(s), loss on training batch is 0.00652659.
After 11488 training step(s), loss on training batch is 0.00702455.
After 11489 training step(s), loss on training batch is 0.00701029.
After 11490 training step(s), loss on training batch is 0.0066408.
After 11491 training step(s), loss on training batch is 0.00792972.
After 11492 training step(s), loss on training batch is 0.00643044.
After 11493 training step(s), loss on training batch is 0.00663856.
After 11494 training step(s), loss on training batch is 0.00663407.
After 11495 training step(s), loss on training batch is 0.0070682.
After 11496 training step(s), loss on training batch is 0.00683461.
After 11497 training step(s), loss on training batch is 0.0205991.
After 11498 training step(s), loss on training batch is 0.00900427.
After 11499 training step(s), loss on training batch is 0.0064845.
After 11500 training step(s), loss on training batch is 0.00752389.
After 11501 training step(s), loss on training batch is 0.00663375.
After 11502 training step(s), loss on training batch is 0.00672043.
After 11503 training step(s), loss on training batch is 0.00698028.
After 11504 training step(s), loss on training batch is 0.00665607.
After 11505 training step(s), loss on training batch is 0.00683922.
After 11506 training step(s), loss on training batch is 0.00892065.
After 11507 training step(s), loss on training batch is 0.00622673.
After 11508 training step(s), loss on training batch is 0.0068514.
After 11509 training step(s), loss on training batch is 0.00628904.
After 11510 training step(s), loss on training batch is 0.00691194.
After 11511 training step(s), loss on training batch is 0.00712578.
After 11512 training step(s), loss on training batch is 0.00739945.
After 11513 training step(s), loss on training batch is 0.00935579.
After 11514 training step(s), loss on training batch is 0.00705147.
After 11515 training step(s), loss on training batch is 0.00964568.
After 11516 training step(s), loss on training batch is 0.00784481.
After 11517 training step(s), loss on training batch is 0.0069766.
After 11518 training step(s), loss on training batch is 0.00694234.
After 11519 training step(s), loss on training batch is 0.00727195.
After 11520 training step(s), loss on training batch is 0.00742618.
After 11521 training step(s), loss on training batch is 0.00758555.
After 11522 training step(s), loss on training batch is 0.00654157.
After 11523 training step(s), loss on training batch is 0.00670167.
After 11524 training step(s), loss on training batch is 0.00761185.
After 11525 training step(s), loss on training batch is 0.00690266.
After 11526 training step(s), loss on training batch is 0.00658812.
After 11527 training step(s), loss on training batch is 0.00662567.
After 11528 training step(s), loss on training batch is 0.00812686.
After 11529 training step(s), loss on training batch is 0.00703546.
After 11530 training step(s), loss on training batch is 0.00673076.
After 11531 training step(s), loss on training batch is 0.00670451.
After 11532 training step(s), loss on training batch is 0.0064706.
After 11533 training step(s), loss on training batch is 0.00705952.
After 11534 training step(s), loss on training batch is 0.00648683.
After 11535 training step(s), loss on training batch is 0.00677515.
After 11536 training step(s), loss on training batch is 0.00653321.
After 11537 training step(s), loss on training batch is 0.00637896.
After 11538 training step(s), loss on training batch is 0.00700564.
After 11539 training step(s), loss on training batch is 0.00673341.
After 11540 training step(s), loss on training batch is 0.00761905.
After 11541 training step(s), loss on training batch is 0.00745796.
After 11542 training step(s), loss on training batch is 0.00691038.
After 11543 training step(s), loss on training batch is 0.00670169.
After 11544 training step(s), loss on training batch is 0.00652487.
After 11545 training step(s), loss on training batch is 0.00725096.
After 11546 training step(s), loss on training batch is 0.00695027.
After 11547 training step(s), loss on training batch is 0.00642943.
After 11548 training step(s), loss on training batch is 0.00737964.
After 11549 training step(s), loss on training batch is 0.00690749.
After 11550 training step(s), loss on training batch is 0.011356.
After 11551 training step(s), loss on training batch is 0.0318382.
After 11552 training step(s), loss on training batch is 0.00698844.
After 11553 training step(s), loss on training batch is 0.00644538.
After 11554 training step(s), loss on training batch is 0.00694246.
After 11555 training step(s), loss on training batch is 0.00639874.
After 11556 training step(s), loss on training batch is 0.0071976.
After 11557 training step(s), loss on training batch is 0.00809238.
After 11558 training step(s), loss on training batch is 0.00709782.
After 11559 training step(s), loss on training batch is 0.00657908.
After 11560 training step(s), loss on training batch is 0.00649557.
After 11561 training step(s), loss on training batch is 0.00737094.
After 11562 training step(s), loss on training batch is 0.0067831.
After 11563 training step(s), loss on training batch is 0.00744234.
After 11564 training step(s), loss on training batch is 0.00620123.
After 11565 training step(s), loss on training batch is 0.00715318.
After 11566 training step(s), loss on training batch is 0.0063186.
After 11567 training step(s), loss on training batch is 0.00721413.
After 11568 training step(s), loss on training batch is 0.00635703.
After 11569 training step(s), loss on training batch is 0.0063485.
After 11570 training step(s), loss on training batch is 0.00747618.
After 11571 training step(s), loss on training batch is 0.00685788.
After 11572 training step(s), loss on training batch is 0.00620658.
After 11573 training step(s), loss on training batch is 0.00698441.
After 11574 training step(s), loss on training batch is 0.00654189.
After 11575 training step(s), loss on training batch is 0.00707286.
After 11576 training step(s), loss on training batch is 0.00733172.
After 11577 training step(s), loss on training batch is 0.00659939.
After 11578 training step(s), loss on training batch is 0.00676511.
After 11579 training step(s), loss on training batch is 0.00755974.
After 11580 training step(s), loss on training batch is 0.0070274.
After 11581 training step(s), loss on training batch is 0.00669114.
After 11582 training step(s), loss on training batch is 0.00739648.
After 11583 training step(s), loss on training batch is 0.00663712.
After 11584 training step(s), loss on training batch is 0.00656309.
After 11585 training step(s), loss on training batch is 0.006699.
After 11586 training step(s), loss on training batch is 0.00660981.
After 11587 training step(s), loss on training batch is 0.00641878.
After 11588 training step(s), loss on training batch is 0.00790841.
After 11589 training step(s), loss on training batch is 0.00772012.
After 11590 training step(s), loss on training batch is 0.00729834.
After 11591 training step(s), loss on training batch is 0.00654952.
After 11592 training step(s), loss on training batch is 0.00611466.
After 11593 training step(s), loss on training batch is 0.00766706.
After 11594 training step(s), loss on training batch is 0.00719225.
After 11595 training step(s), loss on training batch is 0.00719826.
After 11596 training step(s), loss on training batch is 0.00670315.
After 11597 training step(s), loss on training batch is 0.00675956.
After 11598 training step(s), loss on training batch is 0.00791172.
After 11599 training step(s), loss on training batch is 0.00680705.
After 11600 training step(s), loss on training batch is 0.00637068.
After 11601 training step(s), loss on training batch is 0.00694796.
After 11602 training step(s), loss on training batch is 0.00653165.
After 11603 training step(s), loss on training batch is 0.00668068.
After 11604 training step(s), loss on training batch is 0.00620765.
After 11605 training step(s), loss on training batch is 0.00663534.
After 11606 training step(s), loss on training batch is 0.00607976.
After 11607 training step(s), loss on training batch is 0.00654428.
After 11608 training step(s), loss on training batch is 0.0060537.
After 11609 training step(s), loss on training batch is 0.00679297.
After 11610 training step(s), loss on training batch is 0.00642566.
After 11611 training step(s), loss on training batch is 0.00631647.
After 11612 training step(s), loss on training batch is 0.00644551.
After 11613 training step(s), loss on training batch is 0.00649266.
After 11614 training step(s), loss on training batch is 0.0105963.
After 11615 training step(s), loss on training batch is 0.00636271.
After 11616 training step(s), loss on training batch is 0.00659365.
After 11617 training step(s), loss on training batch is 0.00648401.
After 11618 training step(s), loss on training batch is 0.0064611.
After 11619 training step(s), loss on training batch is 0.00725572.
After 11620 training step(s), loss on training batch is 0.00743837.
After 11621 training step(s), loss on training batch is 0.00663635.
After 11622 training step(s), loss on training batch is 0.00623334.
After 11623 training step(s), loss on training batch is 0.00740423.
After 11624 training step(s), loss on training batch is 0.00656592.
After 11625 training step(s), loss on training batch is 0.00718866.
After 11626 training step(s), loss on training batch is 0.00646592.
After 11627 training step(s), loss on training batch is 0.00636098.
After 11628 training step(s), loss on training batch is 0.00785416.
After 11629 training step(s), loss on training batch is 0.00704816.
After 11630 training step(s), loss on training batch is 0.00665326.
After 11631 training step(s), loss on training batch is 0.00713778.
After 11632 training step(s), loss on training batch is 0.00719528.
After 11633 training step(s), loss on training batch is 0.00620709.
After 11634 training step(s), loss on training batch is 0.00723889.
After 11635 training step(s), loss on training batch is 0.00664174.
After 11636 training step(s), loss on training batch is 0.00607076.
After 11637 training step(s), loss on training batch is 0.00688312.
After 11638 training step(s), loss on training batch is 0.00647974.
After 11639 training step(s), loss on training batch is 0.0060107.
After 11640 training step(s), loss on training batch is 0.00702558.
After 11641 training step(s), loss on training batch is 0.00665843.
After 11642 training step(s), loss on training batch is 0.00777548.
After 11643 training step(s), loss on training batch is 0.00659399.
After 11644 training step(s), loss on training batch is 0.006449.
After 11645 training step(s), loss on training batch is 0.00644783.
After 11646 training step(s), loss on training batch is 0.0112083.
After 11647 training step(s), loss on training batch is 0.00696819.
After 11648 training step(s), loss on training batch is 0.00613248.
After 11649 training step(s), loss on training batch is 0.0063273.
After 11650 training step(s), loss on training batch is 0.00747666.
After 11651 training step(s), loss on training batch is 0.00728325.
After 11652 training step(s), loss on training batch is 0.00671543.
After 11653 training step(s), loss on training batch is 0.00712435.
After 11654 training step(s), loss on training batch is 0.00706751.
After 11655 training step(s), loss on training batch is 0.00743309.
After 11656 training step(s), loss on training batch is 0.00974032.
After 11657 training step(s), loss on training batch is 0.00670502.
After 11658 training step(s), loss on training batch is 0.00680577.
After 11659 training step(s), loss on training batch is 0.00718546.
After 11660 training step(s), loss on training batch is 0.00687699.
After 11661 training step(s), loss on training batch is 0.0072864.
After 11662 training step(s), loss on training batch is 0.00733653.
After 11663 training step(s), loss on training batch is 0.00650898.
After 11664 training step(s), loss on training batch is 0.00658894.
After 11665 training step(s), loss on training batch is 0.00731881.
After 11666 training step(s), loss on training batch is 0.00610445.
After 11667 training step(s), loss on training batch is 0.00773776.
After 11668 training step(s), loss on training batch is 0.00719269.
After 11669 training step(s), loss on training batch is 0.00714338.
After 11670 training step(s), loss on training batch is 0.0063371.
After 11671 training step(s), loss on training batch is 0.00634038.
After 11672 training step(s), loss on training batch is 0.00639824.
After 11673 training step(s), loss on training batch is 0.00751327.
After 11674 training step(s), loss on training batch is 0.00778353.
After 11675 training step(s), loss on training batch is 0.00686564.
After 11676 training step(s), loss on training batch is 0.00643953.
After 11677 training step(s), loss on training batch is 0.00646811.
After 11678 training step(s), loss on training batch is 0.0069678.
After 11679 training step(s), loss on training batch is 0.00763342.
After 11680 training step(s), loss on training batch is 0.00651414.
After 11681 training step(s), loss on training batch is 0.00636266.
After 11682 training step(s), loss on training batch is 0.00635613.
After 11683 training step(s), loss on training batch is 0.00713091.
After 11684 training step(s), loss on training batch is 0.00921109.
After 11685 training step(s), loss on training batch is 0.00876652.
After 11686 training step(s), loss on training batch is 0.00619063.
After 11687 training step(s), loss on training batch is 0.00646452.
After 11688 training step(s), loss on training batch is 0.00690683.
After 11689 training step(s), loss on training batch is 0.00647012.
After 11690 training step(s), loss on training batch is 0.00945071.
After 11691 training step(s), loss on training batch is 0.00822513.
After 11692 training step(s), loss on training batch is 0.00689316.
After 11693 training step(s), loss on training batch is 0.00808718.
After 11694 training step(s), loss on training batch is 0.00633427.
After 11695 training step(s), loss on training batch is 0.0063079.
After 11696 training step(s), loss on training batch is 0.00719585.
After 11697 training step(s), loss on training batch is 0.00626075.
After 11698 training step(s), loss on training batch is 0.00676577.
After 11699 training step(s), loss on training batch is 0.00641314.
After 11700 training step(s), loss on training batch is 0.00605521.
After 11701 training step(s), loss on training batch is 0.00735024.
After 11702 training step(s), loss on training batch is 0.0065998.
After 11703 training step(s), loss on training batch is 0.00647665.
After 11704 training step(s), loss on training batch is 0.00660702.
After 11705 training step(s), loss on training batch is 0.00756161.
After 11706 training step(s), loss on training batch is 0.00723997.
After 11707 training step(s), loss on training batch is 0.00642659.
After 11708 training step(s), loss on training batch is 0.00756795.
After 11709 training step(s), loss on training batch is 0.00661871.
After 11710 training step(s), loss on training batch is 0.0084814.
After 11711 training step(s), loss on training batch is 0.00631123.
After 11712 training step(s), loss on training batch is 0.00653243.
After 11713 training step(s), loss on training batch is 0.00878587.
After 11714 training step(s), loss on training batch is 0.00668209.
After 11715 training step(s), loss on training batch is 0.00621369.
After 11716 training step(s), loss on training batch is 0.00673161.
After 11717 training step(s), loss on training batch is 0.00714953.
After 11718 training step(s), loss on training batch is 0.00614905.
After 11719 training step(s), loss on training batch is 0.00644615.
After 11720 training step(s), loss on training batch is 0.0117105.
After 11721 training step(s), loss on training batch is 0.00700543.
After 11722 training step(s), loss on training batch is 0.0072818.
After 11723 training step(s), loss on training batch is 0.00682392.
After 11724 training step(s), loss on training batch is 0.00746341.
After 11725 training step(s), loss on training batch is 0.00703794.
After 11726 training step(s), loss on training batch is 0.00843906.
After 11727 training step(s), loss on training batch is 0.00770297.
After 11728 training step(s), loss on training batch is 0.0108076.
After 11729 training step(s), loss on training batch is 0.00870332.
After 11730 training step(s), loss on training batch is 0.00664565.
After 11731 training step(s), loss on training batch is 0.00728118.
After 11732 training step(s), loss on training batch is 0.0059323.
After 11733 training step(s), loss on training batch is 0.00660365.
After 11734 training step(s), loss on training batch is 0.00741178.
After 11735 training step(s), loss on training batch is 0.00692591.
After 11736 training step(s), loss on training batch is 0.00608498.
After 11737 training step(s), loss on training batch is 0.00761466.
After 11738 training step(s), loss on training batch is 0.00760337.
After 11739 training step(s), loss on training batch is 0.00734956.
After 11740 training step(s), loss on training batch is 0.00839734.
After 11741 training step(s), loss on training batch is 0.00725876.
After 11742 training step(s), loss on training batch is 0.00785311.
After 11743 training step(s), loss on training batch is 0.00731422.
After 11744 training step(s), loss on training batch is 0.00711563.
After 11745 training step(s), loss on training batch is 0.00606656.
After 11746 training step(s), loss on training batch is 0.00653589.
After 11747 training step(s), loss on training batch is 0.0101282.
After 11748 training step(s), loss on training batch is 0.00765852.
After 11749 training step(s), loss on training batch is 0.00664746.
After 11750 training step(s), loss on training batch is 0.00612938.
After 11751 training step(s), loss on training batch is 0.00701536.
After 11752 training step(s), loss on training batch is 0.00679348.
After 11753 training step(s), loss on training batch is 0.00620505.
After 11754 training step(s), loss on training batch is 0.0086097.
After 11755 training step(s), loss on training batch is 0.00639008.
After 11756 training step(s), loss on training batch is 0.00677938.
After 11757 training step(s), loss on training batch is 0.00677725.
After 11758 training step(s), loss on training batch is 0.00663141.
After 11759 training step(s), loss on training batch is 0.00734406.
After 11760 training step(s), loss on training batch is 0.00724014.
After 11761 training step(s), loss on training batch is 0.00699123.
After 11762 training step(s), loss on training batch is 0.00692691.
After 11763 training step(s), loss on training batch is 0.00657208.
After 11764 training step(s), loss on training batch is 0.00674739.
After 11765 training step(s), loss on training batch is 0.00694052.
After 11766 training step(s), loss on training batch is 0.00614648.
After 11767 training step(s), loss on training batch is 0.00659911.
After 11768 training step(s), loss on training batch is 0.0080445.
After 11769 training step(s), loss on training batch is 0.00635128.
After 11770 training step(s), loss on training batch is 0.00643246.
After 11771 training step(s), loss on training batch is 0.00894229.
After 11772 training step(s), loss on training batch is 0.00884943.
After 11773 training step(s), loss on training batch is 0.00625257.
After 11774 training step(s), loss on training batch is 0.0082321.
After 11775 training step(s), loss on training batch is 0.00634215.
After 11776 training step(s), loss on training batch is 0.0068315.
After 11777 training step(s), loss on training batch is 0.00703558.
After 11778 training step(s), loss on training batch is 0.00607991.
After 11779 training step(s), loss on training batch is 0.0072943.
After 11780 training step(s), loss on training batch is 0.00703749.
After 11781 training step(s), loss on training batch is 0.00679515.
After 11782 training step(s), loss on training batch is 0.00630067.
After 11783 training step(s), loss on training batch is 0.00680822.
After 11784 training step(s), loss on training batch is 0.00642783.
After 11785 training step(s), loss on training batch is 0.00626538.
After 11786 training step(s), loss on training batch is 0.00727298.
After 11787 training step(s), loss on training batch is 0.0061271.
After 11788 training step(s), loss on training batch is 0.00684028.
After 11789 training step(s), loss on training batch is 0.00636294.
After 11790 training step(s), loss on training batch is 0.00843712.
After 11791 training step(s), loss on training batch is 0.00752256.
After 11792 training step(s), loss on training batch is 0.0065565.
After 11793 training step(s), loss on training batch is 0.00654948.
After 11794 training step(s), loss on training batch is 0.00592518.
After 11795 training step(s), loss on training batch is 0.00770478.
After 11796 training step(s), loss on training batch is 0.00846647.
After 11797 training step(s), loss on training batch is 0.00693173.
After 11798 training step(s), loss on training batch is 0.0080479.
After 11799 training step(s), loss on training batch is 0.00619161.
After 11800 training step(s), loss on training batch is 0.00659162.
After 11801 training step(s), loss on training batch is 0.00595774.
After 11802 training step(s), loss on training batch is 0.0107623.
After 11803 training step(s), loss on training batch is 0.00696505.
After 11804 training step(s), loss on training batch is 0.00619197.
After 11805 training step(s), loss on training batch is 0.00614808.
After 11806 training step(s), loss on training batch is 0.00608315.
After 11807 training step(s), loss on training batch is 0.00718385.
After 11808 training step(s), loss on training batch is 0.00686577.
After 11809 training step(s), loss on training batch is 0.00611525.
After 11810 training step(s), loss on training batch is 0.00676127.
After 11811 training step(s), loss on training batch is 0.00710995.
After 11812 training step(s), loss on training batch is 0.00626883.
After 11813 training step(s), loss on training batch is 0.00889135.
After 11814 training step(s), loss on training batch is 0.00658789.
After 11815 training step(s), loss on training batch is 0.00635214.
After 11816 training step(s), loss on training batch is 0.0152103.
After 11817 training step(s), loss on training batch is 0.00664947.
After 11818 training step(s), loss on training batch is 0.00812686.
After 11819 training step(s), loss on training batch is 0.00730502.
After 11820 training step(s), loss on training batch is 0.00643922.
After 11821 training step(s), loss on training batch is 0.00736074.
After 11822 training step(s), loss on training batch is 0.0106899.
After 11823 training step(s), loss on training batch is 0.00669635.
After 11824 training step(s), loss on training batch is 0.00624119.
After 11825 training step(s), loss on training batch is 0.00657111.
After 11826 training step(s), loss on training batch is 0.00721945.
After 11827 training step(s), loss on training batch is 0.00632796.
After 11828 training step(s), loss on training batch is 0.00608696.
After 11829 training step(s), loss on training batch is 0.00600414.
After 11830 training step(s), loss on training batch is 0.00728839.
After 11831 training step(s), loss on training batch is 0.00635754.
After 11832 training step(s), loss on training batch is 0.00688664.
After 11833 training step(s), loss on training batch is 0.00694102.
After 11834 training step(s), loss on training batch is 0.00806065.
After 11835 training step(s), loss on training batch is 0.00664578.
After 11836 training step(s), loss on training batch is 0.0059757.
After 11837 training step(s), loss on training batch is 0.00701759.
After 11838 training step(s), loss on training batch is 0.00689646.
After 11839 training step(s), loss on training batch is 0.00618343.
After 11840 training step(s), loss on training batch is 0.00623791.
After 11841 training step(s), loss on training batch is 0.00625657.
After 11842 training step(s), loss on training batch is 0.00643544.
After 11843 training step(s), loss on training batch is 0.00606579.
After 11844 training step(s), loss on training batch is 0.00671314.
After 11845 training step(s), loss on training batch is 0.00660999.
After 11846 training step(s), loss on training batch is 0.0067015.
After 11847 training step(s), loss on training batch is 0.00611571.
After 11848 training step(s), loss on training batch is 0.0065342.
After 11849 training step(s), loss on training batch is 0.00665501.
After 11850 training step(s), loss on training batch is 0.00617437.
After 11851 training step(s), loss on training batch is 0.00637269.
After 11852 training step(s), loss on training batch is 0.00865419.
After 11853 training step(s), loss on training batch is 0.00641555.
After 11854 training step(s), loss on training batch is 0.00625382.
After 11855 training step(s), loss on training batch is 0.00612012.
After 11856 training step(s), loss on training batch is 0.00762256.
After 11857 training step(s), loss on training batch is 0.0068456.
After 11858 training step(s), loss on training batch is 0.00689225.
After 11859 training step(s), loss on training batch is 0.00783968.
After 11860 training step(s), loss on training batch is 0.00787981.
After 11861 training step(s), loss on training batch is 0.00625922.
After 11862 training step(s), loss on training batch is 0.0063836.
After 11863 training step(s), loss on training batch is 0.00654431.
After 11864 training step(s), loss on training batch is 0.00640287.
After 11865 training step(s), loss on training batch is 0.00712652.
After 11866 training step(s), loss on training batch is 0.00629478.
After 11867 training step(s), loss on training batch is 0.00604163.
After 11868 training step(s), loss on training batch is 0.00693168.
After 11869 training step(s), loss on training batch is 0.00718138.
After 11870 training step(s), loss on training batch is 0.00624838.
After 11871 training step(s), loss on training batch is 0.00649664.
After 11872 training step(s), loss on training batch is 0.00693835.
After 11873 training step(s), loss on training batch is 0.00702454.
After 11874 training step(s), loss on training batch is 0.00846578.
After 11875 training step(s), loss on training batch is 0.0059799.
After 11876 training step(s), loss on training batch is 0.0076682.
After 11877 training step(s), loss on training batch is 0.00629171.
After 11878 training step(s), loss on training batch is 0.00614609.
After 11879 training step(s), loss on training batch is 0.00590209.
After 11880 training step(s), loss on training batch is 0.00620784.
After 11881 training step(s), loss on training batch is 0.0063812.
After 11882 training step(s), loss on training batch is 0.00670649.
After 11883 training step(s), loss on training batch is 0.0060555.
After 11884 training step(s), loss on training batch is 0.00614307.
After 11885 training step(s), loss on training batch is 0.0061599.
After 11886 training step(s), loss on training batch is 0.00666246.
After 11887 training step(s), loss on training batch is 0.00762028.
After 11888 training step(s), loss on training batch is 0.00616818.
After 11889 training step(s), loss on training batch is 0.00660704.
After 11890 training step(s), loss on training batch is 0.00602984.
After 11891 training step(s), loss on training batch is 0.00598229.
After 11892 training step(s), loss on training batch is 0.00589893.
After 11893 training step(s), loss on training batch is 0.00648217.
After 11894 training step(s), loss on training batch is 0.00631049.
After 11895 training step(s), loss on training batch is 0.00636922.
After 11896 training step(s), loss on training batch is 0.00625925.
After 11897 training step(s), loss on training batch is 0.00600445.
After 11898 training step(s), loss on training batch is 0.00607737.
After 11899 training step(s), loss on training batch is 0.00618739.
After 11900 training step(s), loss on training batch is 0.00643297.
After 11901 training step(s), loss on training batch is 0.0127404.
After 11902 training step(s), loss on training batch is 0.00688714.
After 11903 training step(s), loss on training batch is 0.00670952.
After 11904 training step(s), loss on training batch is 0.00790431.
After 11905 training step(s), loss on training batch is 0.00661784.
After 11906 training step(s), loss on training batch is 0.00647712.
After 11907 training step(s), loss on training batch is 0.00609281.
After 11908 training step(s), loss on training batch is 0.00604515.
After 11909 training step(s), loss on training batch is 0.00679405.
After 11910 training step(s), loss on training batch is 0.00717261.
After 11911 training step(s), loss on training batch is 0.00572904.
After 11912 training step(s), loss on training batch is 0.00676812.
After 11913 training step(s), loss on training batch is 0.00706408.
After 11914 training step(s), loss on training batch is 0.00708444.
After 11915 training step(s), loss on training batch is 0.00615449.
After 11916 training step(s), loss on training batch is 0.00617346.
After 11917 training step(s), loss on training batch is 0.00611967.
After 11918 training step(s), loss on training batch is 0.00645091.
After 11919 training step(s), loss on training batch is 0.0090254.
After 11920 training step(s), loss on training batch is 0.00727115.
After 11921 training step(s), loss on training batch is 0.0058835.
After 11922 training step(s), loss on training batch is 0.00597502.
After 11923 training step(s), loss on training batch is 0.0114314.
After 11924 training step(s), loss on training batch is 0.00664717.
After 11925 training step(s), loss on training batch is 0.00622093.
After 11926 training step(s), loss on training batch is 0.00617031.
After 11927 training step(s), loss on training batch is 0.00737049.
After 11928 training step(s), loss on training batch is 0.00617097.
After 11929 training step(s), loss on training batch is 0.00820751.
After 11930 training step(s), loss on training batch is 0.00620916.
After 11931 training step(s), loss on training batch is 0.00641889.
After 11932 training step(s), loss on training batch is 0.008428.
After 11933 training step(s), loss on training batch is 0.00710816.
After 11934 training step(s), loss on training batch is 0.00621965.
After 11935 training step(s), loss on training batch is 0.0057525.
After 11936 training step(s), loss on training batch is 0.00600049.
After 11937 training step(s), loss on training batch is 0.00675986.
After 11938 training step(s), loss on training batch is 0.00679516.
After 11939 training step(s), loss on training batch is 0.00656755.
After 11940 training step(s), loss on training batch is 0.00696171.
After 11941 training step(s), loss on training batch is 0.00614504.
After 11942 training step(s), loss on training batch is 0.00646396.
After 11943 training step(s), loss on training batch is 0.00700648.
After 11944 training step(s), loss on training batch is 0.00727825.
After 11945 training step(s), loss on training batch is 0.00636447.
After 11946 training step(s), loss on training batch is 0.00682223.
After 11947 training step(s), loss on training batch is 0.00634272.
After 11948 training step(s), loss on training batch is 0.00657436.
After 11949 training step(s), loss on training batch is 0.00617811.
After 11950 training step(s), loss on training batch is 0.0118007.
After 11951 training step(s), loss on training batch is 0.00931288.
After 11952 training step(s), loss on training batch is 0.00645323.
After 11953 training step(s), loss on training batch is 0.0063339.
After 11954 training step(s), loss on training batch is 0.00600444.
After 11955 training step(s), loss on training batch is 0.00612.
After 11956 training step(s), loss on training batch is 0.00764616.
After 11957 training step(s), loss on training batch is 0.00631069.
After 11958 training step(s), loss on training batch is 0.00680253.
After 11959 training step(s), loss on training batch is 0.0106404.
After 11960 training step(s), loss on training batch is 0.0067656.
After 11961 training step(s), loss on training batch is 0.00720864.
After 11962 training step(s), loss on training batch is 0.0206478.
After 11963 training step(s), loss on training batch is 0.00983258.
After 11964 training step(s), loss on training batch is 0.00679143.
After 11965 training step(s), loss on training batch is 0.00611896.
After 11966 training step(s), loss on training batch is 0.00633366.
After 11967 training step(s), loss on training batch is 0.00649654.
After 11968 training step(s), loss on training batch is 0.0067166.
After 11969 training step(s), loss on training batch is 0.00695795.
After 11970 training step(s), loss on training batch is 0.00602444.
After 11971 training step(s), loss on training batch is 0.00633114.
After 11972 training step(s), loss on training batch is 0.0062085.
After 11973 training step(s), loss on training batch is 0.00616173.
After 11974 training step(s), loss on training batch is 0.00716694.
After 11975 training step(s), loss on training batch is 0.00671498.
After 11976 training step(s), loss on training batch is 0.0064085.
After 11977 training step(s), loss on training batch is 0.00662819.
After 11978 training step(s), loss on training batch is 0.00652817.
After 11979 training step(s), loss on training batch is 0.00725827.
After 11980 training step(s), loss on training batch is 0.00648255.
After 11981 training step(s), loss on training batch is 0.00685068.
After 11982 training step(s), loss on training batch is 0.00677535.
After 11983 training step(s), loss on training batch is 0.00623037.
After 11984 training step(s), loss on training batch is 0.00576865.
After 11985 training step(s), loss on training batch is 0.00656972.
After 11986 training step(s), loss on training batch is 0.00746785.
After 11987 training step(s), loss on training batch is 0.00691939.
After 11988 training step(s), loss on training batch is 0.00638631.
After 11989 training step(s), loss on training batch is 0.007906.
After 11990 training step(s), loss on training batch is 0.00716594.
After 11991 training step(s), loss on training batch is 0.00633053.
After 11992 training step(s), loss on training batch is 0.00624102.
After 11993 training step(s), loss on training batch is 0.00622755.
After 11994 training step(s), loss on training batch is 0.00683888.
After 11995 training step(s), loss on training batch is 0.00760186.
After 11996 training step(s), loss on training batch is 0.00694341.
After 11997 training step(s), loss on training batch is 0.00714676.
After 11998 training step(s), loss on training batch is 0.00632182.
After 11999 training step(s), loss on training batch is 0.00618304.
After 12000 training step(s), loss on training batch is 0.00604564.
After 12001 training step(s), loss on training batch is 0.00717228.
After 12002 training step(s), loss on training batch is 0.00682243.
After 12003 training step(s), loss on training batch is 0.00718229.
After 12004 training step(s), loss on training batch is 0.0068331.
After 12005 training step(s), loss on training batch is 0.00592793.
After 12006 training step(s), loss on training batch is 0.00584341.
After 12007 training step(s), loss on training batch is 0.00619118.
After 12008 training step(s), loss on training batch is 0.00611959.
After 12009 training step(s), loss on training batch is 0.0059232.
After 12010 training step(s), loss on training batch is 0.00649456.
After 12011 training step(s), loss on training batch is 0.00665973.
After 12012 training step(s), loss on training batch is 0.00583489.
After 12013 training step(s), loss on training batch is 0.00599788.
After 12014 training step(s), loss on training batch is 0.00671243.
After 12015 training step(s), loss on training batch is 0.00616221.
After 12016 training step(s), loss on training batch is 0.00642386.
After 12017 training step(s), loss on training batch is 0.00827304.
After 12018 training step(s), loss on training batch is 0.00630594.
After 12019 training step(s), loss on training batch is 0.00630988.
After 12020 training step(s), loss on training batch is 0.00881854.
After 12021 training step(s), loss on training batch is 0.0063012.
After 12022 training step(s), loss on training batch is 0.0061884.
After 12023 training step(s), loss on training batch is 0.00678335.
After 12024 training step(s), loss on training batch is 0.00941753.
After 12025 training step(s), loss on training batch is 0.00724696.
After 12026 training step(s), loss on training batch is 0.006502.
After 12027 training step(s), loss on training batch is 0.00720091.
After 12028 training step(s), loss on training batch is 0.00600597.
After 12029 training step(s), loss on training batch is 0.00609915.
After 12030 training step(s), loss on training batch is 0.00680035.
After 12031 training step(s), loss on training batch is 0.00686763.
After 12032 training step(s), loss on training batch is 0.00575876.
After 12033 training step(s), loss on training batch is 0.00671951.
After 12034 training step(s), loss on training batch is 0.00637559.
After 12035 training step(s), loss on training batch is 0.00682843.
After 12036 training step(s), loss on training batch is 0.00601913.
After 12037 training step(s), loss on training batch is 0.00706317.
After 12038 training step(s), loss on training batch is 0.0068455.
After 12039 training step(s), loss on training batch is 0.005726.
After 12040 training step(s), loss on training batch is 0.00575267.
After 12041 training step(s), loss on training batch is 0.00639185.
After 12042 training step(s), loss on training batch is 0.00665301.
After 12043 training step(s), loss on training batch is 0.00610703.
After 12044 training step(s), loss on training batch is 0.00721722.
After 12045 training step(s), loss on training batch is 0.00601631.
After 12046 training step(s), loss on training batch is 0.0060373.
After 12047 training step(s), loss on training batch is 0.00781986.
After 12048 training step(s), loss on training batch is 0.00649878.
After 12049 training step(s), loss on training batch is 0.00655086.
After 12050 training step(s), loss on training batch is 0.00627604.
After 12051 training step(s), loss on training batch is 0.00597985.
After 12052 training step(s), loss on training batch is 0.00603894.
After 12053 training step(s), loss on training batch is 0.0066048.
After 12054 training step(s), loss on training batch is 0.0135698.
After 12055 training step(s), loss on training batch is 0.00612021.
After 12056 training step(s), loss on training batch is 0.00697289.
After 12057 training step(s), loss on training batch is 0.00648422.
After 12058 training step(s), loss on training batch is 0.00714044.
After 12059 training step(s), loss on training batch is 0.00769277.
After 12060 training step(s), loss on training batch is 0.00613954.
After 12061 training step(s), loss on training batch is 0.00583013.
After 12062 training step(s), loss on training batch is 0.00784166.
After 12063 training step(s), loss on training batch is 0.00611702.
After 12064 training step(s), loss on training batch is 0.0063944.
After 12065 training step(s), loss on training batch is 0.00724338.
After 12066 training step(s), loss on training batch is 0.00701897.
After 12067 training step(s), loss on training batch is 0.00610873.
After 12068 training step(s), loss on training batch is 0.00632742.
After 12069 training step(s), loss on training batch is 0.00661323.
After 12070 training step(s), loss on training batch is 0.00712829.
After 12071 training step(s), loss on training batch is 0.0065842.
After 12072 training step(s), loss on training batch is 0.00598809.
After 12073 training step(s), loss on training batch is 0.00622884.
After 12074 training step(s), loss on training batch is 0.00846132.
After 12075 training step(s), loss on training batch is 0.00630472.
After 12076 training step(s), loss on training batch is 0.00700815.
After 12077 training step(s), loss on training batch is 0.0074662.
After 12078 training step(s), loss on training batch is 0.0070873.
After 12079 training step(s), loss on training batch is 0.0062295.
After 12080 training step(s), loss on training batch is 0.00613938.
After 12081 training step(s), loss on training batch is 0.00656436.
After 12082 training step(s), loss on training batch is 0.00593754.
After 12083 training step(s), loss on training batch is 0.00645383.
After 12084 training step(s), loss on training batch is 0.00649352.
After 12085 training step(s), loss on training batch is 0.00852041.
After 12086 training step(s), loss on training batch is 0.00687743.
After 12087 training step(s), loss on training batch is 0.00644182.
After 12088 training step(s), loss on training batch is 0.00679019.
After 12089 training step(s), loss on training batch is 0.00576768.
After 12090 training step(s), loss on training batch is 0.00693227.
After 12091 training step(s), loss on training batch is 0.00604107.
After 12092 training step(s), loss on training batch is 0.00609151.
After 12093 training step(s), loss on training batch is 0.00610088.
After 12094 training step(s), loss on training batch is 0.00731432.
After 12095 training step(s), loss on training batch is 0.00631126.
After 12096 training step(s), loss on training batch is 0.00622201.
After 12097 training step(s), loss on training batch is 0.00603744.
After 12098 training step(s), loss on training batch is 0.00639772.
After 12099 training step(s), loss on training batch is 0.00728535.
After 12100 training step(s), loss on training batch is 0.00642215.
After 12101 training step(s), loss on training batch is 0.00606117.
After 12102 training step(s), loss on training batch is 0.00829414.
After 12103 training step(s), loss on training batch is 0.00614902.
After 12104 training step(s), loss on training batch is 0.00645965.
After 12105 training step(s), loss on training batch is 0.00606882.
After 12106 training step(s), loss on training batch is 0.00603463.
After 12107 training step(s), loss on training batch is 0.00684754.
After 12108 training step(s), loss on training batch is 0.00635225.
After 12109 training step(s), loss on training batch is 0.00653193.
After 12110 training step(s), loss on training batch is 0.00613117.
After 12111 training step(s), loss on training batch is 0.00602901.
After 12112 training step(s), loss on training batch is 0.00620365.
After 12113 training step(s), loss on training batch is 0.00687592.
After 12114 training step(s), loss on training batch is 0.00619688.
After 12115 training step(s), loss on training batch is 0.00598536.
After 12116 training step(s), loss on training batch is 0.00621007.
After 12117 training step(s), loss on training batch is 0.00652956.
After 12118 training step(s), loss on training batch is 0.00588427.
After 12119 training step(s), loss on training batch is 0.00640433.
After 12120 training step(s), loss on training batch is 0.00641871.
After 12121 training step(s), loss on training batch is 0.00660019.
After 12122 training step(s), loss on training batch is 0.00574998.
After 12123 training step(s), loss on training batch is 0.00676096.
After 12124 training step(s), loss on training batch is 0.0059605.
After 12125 training step(s), loss on training batch is 0.00672107.
After 12126 training step(s), loss on training batch is 0.00635224.
After 12127 training step(s), loss on training batch is 0.00636359.
After 12128 training step(s), loss on training batch is 0.00683635.
After 12129 training step(s), loss on training batch is 0.00604434.
After 12130 training step(s), loss on training batch is 0.00591629.
After 12131 training step(s), loss on training batch is 0.00709449.
After 12132 training step(s), loss on training batch is 0.0059718.
After 12133 training step(s), loss on training batch is 0.00709993.
After 12134 training step(s), loss on training batch is 0.00607036.
After 12135 training step(s), loss on training batch is 0.00665872.
After 12136 training step(s), loss on training batch is 0.00591752.
After 12137 training step(s), loss on training batch is 0.00588089.
After 12138 training step(s), loss on training batch is 0.00649231.
After 12139 training step(s), loss on training batch is 0.00632714.
After 12140 training step(s), loss on training batch is 0.00700651.
After 12141 training step(s), loss on training batch is 0.00769002.
After 12142 training step(s), loss on training batch is 0.00669553.
After 12143 training step(s), loss on training batch is 0.00653639.
After 12144 training step(s), loss on training batch is 0.00659456.
After 12145 training step(s), loss on training batch is 0.00651152.
After 12146 training step(s), loss on training batch is 0.00683335.
After 12147 training step(s), loss on training batch is 0.00741539.
After 12148 training step(s), loss on training batch is 0.00592159.
After 12149 training step(s), loss on training batch is 0.00615239.
After 12150 training step(s), loss on training batch is 0.0067358.
After 12151 training step(s), loss on training batch is 0.00612457.
After 12152 training step(s), loss on training batch is 0.0070269.
After 12153 training step(s), loss on training batch is 0.006056.
After 12154 training step(s), loss on training batch is 0.00603195.
After 12155 training step(s), loss on training batch is 0.00602107.
After 12156 training step(s), loss on training batch is 0.00578894.
After 12157 training step(s), loss on training batch is 0.00628421.
After 12158 training step(s), loss on training batch is 0.00638222.
After 12159 training step(s), loss on training batch is 0.00616939.
After 12160 training step(s), loss on training batch is 0.00669429.
After 12161 training step(s), loss on training batch is 0.00662014.
After 12162 training step(s), loss on training batch is 0.0058925.
After 12163 training step(s), loss on training batch is 0.00623297.
After 12164 training step(s), loss on training batch is 0.00598103.
After 12165 training step(s), loss on training batch is 0.00591437.
After 12166 training step(s), loss on training batch is 0.0056676.
After 12167 training step(s), loss on training batch is 0.00667259.
After 12168 training step(s), loss on training batch is 0.00637951.
After 12169 training step(s), loss on training batch is 0.00631285.
After 12170 training step(s), loss on training batch is 0.00622998.
After 12171 training step(s), loss on training batch is 0.00597298.
After 12172 training step(s), loss on training batch is 0.00632932.
After 12173 training step(s), loss on training batch is 0.010847.
After 12174 training step(s), loss on training batch is 0.00670575.
After 12175 training step(s), loss on training batch is 0.00644873.
After 12176 training step(s), loss on training batch is 0.00655455.
After 12177 training step(s), loss on training batch is 0.0201629.
After 12178 training step(s), loss on training batch is 0.00941414.
After 12179 training step(s), loss on training batch is 0.0066019.
After 12180 training step(s), loss on training batch is 0.00631007.
After 12181 training step(s), loss on training batch is 0.00771881.
After 12182 training step(s), loss on training batch is 0.00713757.
After 12183 training step(s), loss on training batch is 0.00643189.
After 12184 training step(s), loss on training batch is 0.00573366.
After 12185 training step(s), loss on training batch is 0.00700387.
After 12186 training step(s), loss on training batch is 0.00663145.
After 12187 training step(s), loss on training batch is 0.00588682.
After 12188 training step(s), loss on training batch is 0.00615127.
After 12189 training step(s), loss on training batch is 0.00619601.
After 12190 training step(s), loss on training batch is 0.00605979.
After 12191 training step(s), loss on training batch is 0.0251699.
After 12192 training step(s), loss on training batch is 0.011882.
After 12193 training step(s), loss on training batch is 0.00612487.
After 12194 training step(s), loss on training batch is 0.00657913.
After 12195 training step(s), loss on training batch is 0.00638712.
After 12196 training step(s), loss on training batch is 0.00579187.
After 12197 training step(s), loss on training batch is 0.00615245.
After 12198 training step(s), loss on training batch is 0.00577272.
After 12199 training step(s), loss on training batch is 0.00775267.
After 12200 training step(s), loss on training batch is 0.00669631.
After 12201 training step(s), loss on training batch is 0.00978591.
After 12202 training step(s), loss on training batch is 0.00591408.
After 12203 training step(s), loss on training batch is 0.00762645.
After 12204 training step(s), loss on training batch is 0.0116036.
After 12205 training step(s), loss on training batch is 0.00667869.
After 12206 training step(s), loss on training batch is 0.00696092.
After 12207 training step(s), loss on training batch is 0.00708791.
After 12208 training step(s), loss on training batch is 0.00678445.
After 12209 training step(s), loss on training batch is 0.00702882.
After 12210 training step(s), loss on training batch is 0.00593235.
After 12211 training step(s), loss on training batch is 0.00582441.
After 12212 training step(s), loss on training batch is 0.00776138.
After 12213 training step(s), loss on training batch is 0.0056177.
After 12214 training step(s), loss on training batch is 0.00594936.
After 12215 training step(s), loss on training batch is 0.00685227.
After 12216 training step(s), loss on training batch is 0.00592796.
After 12217 training step(s), loss on training batch is 0.00581881.
After 12218 training step(s), loss on training batch is 0.00655611.
After 12219 training step(s), loss on training batch is 0.0077802.
After 12220 training step(s), loss on training batch is 0.00633124.
After 12221 training step(s), loss on training batch is 0.00587564.
After 12222 training step(s), loss on training batch is 0.00607193.
After 12223 training step(s), loss on training batch is 0.00594405.
After 12224 training step(s), loss on training batch is 0.00628966.
After 12225 training step(s), loss on training batch is 0.00638641.
After 12226 training step(s), loss on training batch is 0.006447.
After 12227 training step(s), loss on training batch is 0.00632954.
After 12228 training step(s), loss on training batch is 0.00597855.
After 12229 training step(s), loss on training batch is 0.00583005.
After 12230 training step(s), loss on training batch is 0.00656264.
After 12231 training step(s), loss on training batch is 0.0092163.
After 12232 training step(s), loss on training batch is 0.00901196.
After 12233 training step(s), loss on training batch is 0.00648621.
After 12234 training step(s), loss on training batch is 0.00604909.
After 12235 training step(s), loss on training batch is 0.00717535.
After 12236 training step(s), loss on training batch is 0.00776479.
After 12237 training step(s), loss on training batch is 0.00590888.
After 12238 training step(s), loss on training batch is 0.00646681.
After 12239 training step(s), loss on training batch is 0.00583293.
After 12240 training step(s), loss on training batch is 0.00664003.
After 12241 training step(s), loss on training batch is 0.00619762.
After 12242 training step(s), loss on training batch is 0.0150595.
After 12243 training step(s), loss on training batch is 0.00709944.
After 12244 training step(s), loss on training batch is 0.00618351.
After 12245 training step(s), loss on training batch is 0.00649575.
After 12246 training step(s), loss on training batch is 0.00613109.
After 12247 training step(s), loss on training batch is 0.0101517.
After 12248 training step(s), loss on training batch is 0.00618723.
After 12249 training step(s), loss on training batch is 0.0059694.
After 12250 training step(s), loss on training batch is 0.00610962.
After 12251 training step(s), loss on training batch is 0.00715972.
After 12252 training step(s), loss on training batch is 0.00762705.
After 12253 training step(s), loss on training batch is 0.00648732.
After 12254 training step(s), loss on training batch is 0.00588069.
After 12255 training step(s), loss on training batch is 0.00592675.
After 12256 training step(s), loss on training batch is 0.00723751.
After 12257 training step(s), loss on training batch is 0.00615886.
After 12258 training step(s), loss on training batch is 0.00620427.
After 12259 training step(s), loss on training batch is 0.00640879.
After 12260 training step(s), loss on training batch is 0.00564765.
After 12261 training step(s), loss on training batch is 0.00672668.
After 12262 training step(s), loss on training batch is 0.00672698.
After 12263 training step(s), loss on training batch is 0.00572446.
After 12264 training step(s), loss on training batch is 0.00665463.
After 12265 training step(s), loss on training batch is 0.00565975.
After 12266 training step(s), loss on training batch is 0.0066361.
After 12267 training step(s), loss on training batch is 0.00625808.
After 12268 training step(s), loss on training batch is 0.00732332.
After 12269 training step(s), loss on training batch is 0.00648524.
After 12270 training step(s), loss on training batch is 0.00600415.
After 12271 training step(s), loss on training batch is 0.00837675.
After 12272 training step(s), loss on training batch is 0.00611434.
After 12273 training step(s), loss on training batch is 0.00560059.
After 12274 training step(s), loss on training batch is 0.00581917.
After 12275 training step(s), loss on training batch is 0.00607077.
After 12276 training step(s), loss on training batch is 0.00579883.
After 12277 training step(s), loss on training batch is 0.00665487.
After 12278 training step(s), loss on training batch is 0.00594047.
After 12279 training step(s), loss on training batch is 0.0062441.
After 12280 training step(s), loss on training batch is 0.00931575.
After 12281 training step(s), loss on training batch is 0.00633874.
After 12282 training step(s), loss on training batch is 0.00619815.
After 12283 training step(s), loss on training batch is 0.0062287.
After 12284 training step(s), loss on training batch is 0.00671104.
After 12285 training step(s), loss on training batch is 0.00611005.
After 12286 training step(s), loss on training batch is 0.0075323.
After 12287 training step(s), loss on training batch is 0.0077512.
After 12288 training step(s), loss on training batch is 0.00701512.
After 12289 training step(s), loss on training batch is 0.00603596.
After 12290 training step(s), loss on training batch is 0.0067606.
After 12291 training step(s), loss on training batch is 0.00664501.
After 12292 training step(s), loss on training batch is 0.0091674.
After 12293 training step(s), loss on training batch is 0.00718644.
After 12294 training step(s), loss on training batch is 0.00680132.
After 12295 training step(s), loss on training batch is 0.00620051.
After 12296 training step(s), loss on training batch is 0.00682633.
After 12297 training step(s), loss on training batch is 0.00622468.
After 12298 training step(s), loss on training batch is 0.00587692.
After 12299 training step(s), loss on training batch is 0.00587098.
After 12300 training step(s), loss on training batch is 0.00742632.
After 12301 training step(s), loss on training batch is 0.00594935.
After 12302 training step(s), loss on training batch is 0.00658506.
After 12303 training step(s), loss on training batch is 0.00634482.
After 12304 training step(s), loss on training batch is 0.00620597.
After 12305 training step(s), loss on training batch is 0.00709129.
After 12306 training step(s), loss on training batch is 0.00632562.
After 12307 training step(s), loss on training batch is 0.00740873.
After 12308 training step(s), loss on training batch is 0.00626697.
After 12309 training step(s), loss on training batch is 0.00583784.
After 12310 training step(s), loss on training batch is 0.00633043.
After 12311 training step(s), loss on training batch is 0.00620662.
After 12312 training step(s), loss on training batch is 0.00570104.
After 12313 training step(s), loss on training batch is 0.00737328.
After 12314 training step(s), loss on training batch is 0.00669722.
After 12315 training step(s), loss on training batch is 0.00584465.
After 12316 training step(s), loss on training batch is 0.00644251.
After 12317 training step(s), loss on training batch is 0.00642322.
After 12318 training step(s), loss on training batch is 0.00761727.
After 12319 training step(s), loss on training batch is 0.0062177.
After 12320 training step(s), loss on training batch is 0.0062603.
After 12321 training step(s), loss on training batch is 0.00879196.
After 12322 training step(s), loss on training batch is 0.00693604.
After 12323 training step(s), loss on training batch is 0.00607789.
After 12324 training step(s), loss on training batch is 0.00649157.
After 12325 training step(s), loss on training batch is 0.00652386.
After 12326 training step(s), loss on training batch is 0.00625165.
After 12327 training step(s), loss on training batch is 0.00726948.
After 12328 training step(s), loss on training batch is 0.00581961.
After 12329 training step(s), loss on training batch is 0.00685055.
After 12330 training step(s), loss on training batch is 0.00562941.
After 12331 training step(s), loss on training batch is 0.00618614.
After 12332 training step(s), loss on training batch is 0.00580007.
After 12333 training step(s), loss on training batch is 0.00581365.
After 12334 training step(s), loss on training batch is 0.00651934.
After 12335 training step(s), loss on training batch is 0.00583701.
After 12336 training step(s), loss on training batch is 0.00629859.
After 12337 training step(s), loss on training batch is 0.00638047.
After 12338 training step(s), loss on training batch is 0.00622771.
After 12339 training step(s), loss on training batch is 0.00619427.
After 12340 training step(s), loss on training batch is 0.0058665.
After 12341 training step(s), loss on training batch is 0.00704607.
After 12342 training step(s), loss on training batch is 0.00576989.
After 12343 training step(s), loss on training batch is 0.00649331.
After 12344 training step(s), loss on training batch is 0.00679105.
After 12345 training step(s), loss on training batch is 0.00580988.
After 12346 training step(s), loss on training batch is 0.00586535.
After 12347 training step(s), loss on training batch is 0.00616795.
After 12348 training step(s), loss on training batch is 0.00733691.
After 12349 training step(s), loss on training batch is 0.00554366.
After 12350 training step(s), loss on training batch is 0.00621176.
After 12351 training step(s), loss on training batch is 0.00640208.
After 12352 training step(s), loss on training batch is 0.00640898.
After 12353 training step(s), loss on training batch is 0.00586259.
After 12354 training step(s), loss on training batch is 0.00576581.
After 12355 training step(s), loss on training batch is 0.00612539.
After 12356 training step(s), loss on training batch is 0.00574113.
After 12357 training step(s), loss on training batch is 0.00552726.
After 12358 training step(s), loss on training batch is 0.00662193.
After 12359 training step(s), loss on training batch is 0.00723062.
After 12360 training step(s), loss on training batch is 0.0063596.
After 12361 training step(s), loss on training batch is 0.00583919.
After 12362 training step(s), loss on training batch is 0.00628092.
After 12363 training step(s), loss on training batch is 0.0057533.
After 12364 training step(s), loss on training batch is 0.00600417.
After 12365 training step(s), loss on training batch is 0.00689123.
After 12366 training step(s), loss on training batch is 0.0059152.
After 12367 training step(s), loss on training batch is 0.00566843.
After 12368 training step(s), loss on training batch is 0.00702147.
After 12369 training step(s), loss on training batch is 0.00658573.
After 12370 training step(s), loss on training batch is 0.00649174.
After 12371 training step(s), loss on training batch is 0.00556478.
After 12372 training step(s), loss on training batch is 0.00606882.
After 12373 training step(s), loss on training batch is 0.00737298.
After 12374 training step(s), loss on training batch is 0.00776379.
After 12375 training step(s), loss on training batch is 0.00622449.
After 12376 training step(s), loss on training batch is 0.00707495.
After 12377 training step(s), loss on training batch is 0.00580956.
After 12378 training step(s), loss on training batch is 0.00715367.
After 12379 training step(s), loss on training batch is 0.00567497.
After 12380 training step(s), loss on training batch is 0.00604661.
After 12381 training step(s), loss on training batch is 0.00672775.
After 12382 training step(s), loss on training batch is 0.00570376.
After 12383 training step(s), loss on training batch is 0.0057732.
After 12384 training step(s), loss on training batch is 0.00584783.
After 12385 training step(s), loss on training batch is 0.00706636.
After 12386 training step(s), loss on training batch is 0.00631444.
After 12387 training step(s), loss on training batch is 0.00632274.
After 12388 training step(s), loss on training batch is 0.00880834.
After 12389 training step(s), loss on training batch is 0.0059997.
After 12390 training step(s), loss on training batch is 0.00605832.
After 12391 training step(s), loss on training batch is 0.00592337.
After 12392 training step(s), loss on training batch is 0.00610371.
After 12393 training step(s), loss on training batch is 0.00642002.
After 12394 training step(s), loss on training batch is 0.00571721.
After 12395 training step(s), loss on training batch is 0.00591368.
After 12396 training step(s), loss on training batch is 0.00606319.
After 12397 training step(s), loss on training batch is 0.00657734.
After 12398 training step(s), loss on training batch is 0.0057681.
After 12399 training step(s), loss on training batch is 0.00602091.
After 12400 training step(s), loss on training batch is 0.00589284.
After 12401 training step(s), loss on training batch is 0.0062655.
After 12402 training step(s), loss on training batch is 0.00577759.
After 12403 training step(s), loss on training batch is 0.00602028.
After 12404 training step(s), loss on training batch is 0.00609584.
After 12405 training step(s), loss on training batch is 0.00576423.
After 12406 training step(s), loss on training batch is 0.00622554.
After 12407 training step(s), loss on training batch is 0.00549652.
After 12408 training step(s), loss on training batch is 0.0090739.
After 12409 training step(s), loss on training batch is 0.0058198.
After 12410 training step(s), loss on training batch is 0.00634466.
After 12411 training step(s), loss on training batch is 0.00801517.
After 12412 training step(s), loss on training batch is 0.00648887.
After 12413 training step(s), loss on training batch is 0.00675798.
After 12414 training step(s), loss on training batch is 0.00569585.
After 12415 training step(s), loss on training batch is 0.00591097.
After 12416 training step(s), loss on training batch is 0.00628769.
After 12417 training step(s), loss on training batch is 0.006117.
After 12418 training step(s), loss on training batch is 0.00686456.
After 12419 training step(s), loss on training batch is 0.00665412.
After 12420 training step(s), loss on training batch is 0.00600171.
After 12421 training step(s), loss on training batch is 0.00603534.
After 12422 training step(s), loss on training batch is 0.00630795.
After 12423 training step(s), loss on training batch is 0.00610839.
After 12424 training step(s), loss on training batch is 0.00590471.
After 12425 training step(s), loss on training batch is 0.00574288.
After 12426 training step(s), loss on training batch is 0.00609629.
After 12427 training step(s), loss on training batch is 0.00665072.
After 12428 training step(s), loss on training batch is 0.00598072.
After 12429 training step(s), loss on training batch is 0.0060509.
After 12430 training step(s), loss on training batch is 0.00558145.
After 12431 training step(s), loss on training batch is 0.00647629.
After 12432 training step(s), loss on training batch is 0.00720859.
After 12433 training step(s), loss on training batch is 0.00641582.
After 12434 training step(s), loss on training batch is 0.00636184.
After 12435 training step(s), loss on training batch is 0.00696142.
After 12436 training step(s), loss on training batch is 0.00620982.
After 12437 training step(s), loss on training batch is 0.00649872.
After 12438 training step(s), loss on training batch is 0.00615551.
After 12439 training step(s), loss on training batch is 0.00606171.
After 12440 training step(s), loss on training batch is 0.00590692.
After 12441 training step(s), loss on training batch is 0.0105374.
After 12442 training step(s), loss on training batch is 0.00586421.
After 12443 training step(s), loss on training batch is 0.00596724.
After 12444 training step(s), loss on training batch is 0.00628786.
After 12445 training step(s), loss on training batch is 0.0070518.
After 12446 training step(s), loss on training batch is 0.00585313.
After 12447 training step(s), loss on training batch is 0.00670811.
After 12448 training step(s), loss on training batch is 0.00677884.
After 12449 training step(s), loss on training batch is 0.00601259.
After 12450 training step(s), loss on training batch is 0.00716248.
After 12451 training step(s), loss on training batch is 0.00564141.
After 12452 training step(s), loss on training batch is 0.00614686.
After 12453 training step(s), loss on training batch is 0.00599712.
After 12454 training step(s), loss on training batch is 0.00585957.
After 12455 training step(s), loss on training batch is 0.00625857.
After 12456 training step(s), loss on training batch is 0.00680961.
After 12457 training step(s), loss on training batch is 0.00606461.
After 12458 training step(s), loss on training batch is 0.00601727.
After 12459 training step(s), loss on training batch is 0.00622858.
After 12460 training step(s), loss on training batch is 0.00664123.
After 12461 training step(s), loss on training batch is 0.00662868.
After 12462 training step(s), loss on training batch is 0.00576362.
After 12463 training step(s), loss on training batch is 0.00562902.
After 12464 training step(s), loss on training batch is 0.00602438.
After 12465 training step(s), loss on training batch is 0.00586251.
After 12466 training step(s), loss on training batch is 0.00605426.
After 12467 training step(s), loss on training batch is 0.00559025.
After 12468 training step(s), loss on training batch is 0.00599191.
After 12469 training step(s), loss on training batch is 0.00662036.
After 12470 training step(s), loss on training batch is 0.00593335.
After 12471 training step(s), loss on training batch is 0.0060599.
After 12472 training step(s), loss on training batch is 0.0062418.
After 12473 training step(s), loss on training batch is 0.00588139.
After 12474 training step(s), loss on training batch is 0.00569102.
After 12475 training step(s), loss on training batch is 0.0056126.
After 12476 training step(s), loss on training batch is 0.00539908.
After 12477 training step(s), loss on training batch is 0.00670498.
After 12478 training step(s), loss on training batch is 0.00669349.
After 12479 training step(s), loss on training batch is 0.00587273.
After 12480 training step(s), loss on training batch is 0.00600286.
After 12481 training step(s), loss on training batch is 0.00625937.
After 12482 training step(s), loss on training batch is 0.00618484.
After 12483 training step(s), loss on training batch is 0.00588852.
After 12484 training step(s), loss on training batch is 0.00641243.
After 12485 training step(s), loss on training batch is 0.00603637.
After 12486 training step(s), loss on training batch is 0.00547363.
After 12487 training step(s), loss on training batch is 0.00616318.
After 12488 training step(s), loss on training batch is 0.0062648.
After 12489 training step(s), loss on training batch is 0.00571153.
After 12490 training step(s), loss on training batch is 0.00569664.
After 12491 training step(s), loss on training batch is 0.00548832.
After 12492 training step(s), loss on training batch is 0.0063152.
After 12493 training step(s), loss on training batch is 0.00599837.
After 12494 training step(s), loss on training batch is 0.00658501.
After 12495 training step(s), loss on training batch is 0.00586637.
After 12496 training step(s), loss on training batch is 0.00565213.
After 12497 training step(s), loss on training batch is 0.00610894.
After 12498 training step(s), loss on training batch is 0.00595087.
After 12499 training step(s), loss on training batch is 0.00678143.
After 12500 training step(s), loss on training batch is 0.00709746.
After 12501 training step(s), loss on training batch is 0.00611574.
After 12502 training step(s), loss on training batch is 0.00622229.
After 12503 training step(s), loss on training batch is 0.00600607.
After 12504 training step(s), loss on training batch is 0.00567795.
After 12505 training step(s), loss on training batch is 0.00595162.
After 12506 training step(s), loss on training batch is 0.0063113.
After 12507 training step(s), loss on training batch is 0.0065356.
After 12508 training step(s), loss on training batch is 0.00575873.
After 12509 training step(s), loss on training batch is 0.00609539.
After 12510 training step(s), loss on training batch is 0.00693128.
After 12511 training step(s), loss on training batch is 0.0063714.
After 12512 training step(s), loss on training batch is 0.00589233.
After 12513 training step(s), loss on training batch is 0.00642117.
After 12514 training step(s), loss on training batch is 0.0057088.
After 12515 training step(s), loss on training batch is 0.00614123.
After 12516 training step(s), loss on training batch is 0.00680835.
After 12517 training step(s), loss on training batch is 0.00698973.
After 12518 training step(s), loss on training batch is 0.00576447.
After 12519 training step(s), loss on training batch is 0.00576486.
After 12520 training step(s), loss on training batch is 0.0072768.
After 12521 training step(s), loss on training batch is 0.00577724.
After 12522 training step(s), loss on training batch is 0.0067699.
After 12523 training step(s), loss on training batch is 0.00610334.
After 12524 training step(s), loss on training batch is 0.00606076.
After 12525 training step(s), loss on training batch is 0.00681456.
After 12526 training step(s), loss on training batch is 0.00549626.
After 12527 training step(s), loss on training batch is 0.00588895.
After 12528 training step(s), loss on training batch is 0.00639265.
After 12529 training step(s), loss on training batch is 0.00629881.
After 12530 training step(s), loss on training batch is 0.00650261.
After 12531 training step(s), loss on training batch is 0.00656261.
After 12532 training step(s), loss on training batch is 0.00563018.
After 12533 training step(s), loss on training batch is 0.00561902.
After 12534 training step(s), loss on training batch is 0.00609993.
After 12535 training step(s), loss on training batch is 0.00572275.
After 12536 training step(s), loss on training batch is 0.00567715.
After 12537 training step(s), loss on training batch is 0.00710345.
After 12538 training step(s), loss on training batch is 0.00810847.
After 12539 training step(s), loss on training batch is 0.00738788.
After 12540 training step(s), loss on training batch is 0.00630138.
After 12541 training step(s), loss on training batch is 0.00659271.
After 12542 training step(s), loss on training batch is 0.00667141.
After 12543 training step(s), loss on training batch is 0.00565658.
After 12544 training step(s), loss on training batch is 0.00565079.
After 12545 training step(s), loss on training batch is 0.00746375.
After 12546 training step(s), loss on training batch is 0.0065457.
After 12547 training step(s), loss on training batch is 0.00643002.
After 12548 training step(s), loss on training batch is 0.00747851.
After 12549 training step(s), loss on training batch is 0.0060138.
After 12550 training step(s), loss on training batch is 0.00586089.
After 12551 training step(s), loss on training batch is 0.0072504.
After 12552 training step(s), loss on training batch is 0.00577442.
After 12553 training step(s), loss on training batch is 0.00598884.
After 12554 training step(s), loss on training batch is 0.00612579.
After 12555 training step(s), loss on training batch is 0.00661794.
After 12556 training step(s), loss on training batch is 0.00576315.
After 12557 training step(s), loss on training batch is 0.00622668.
After 12558 training step(s), loss on training batch is 0.00594011.
After 12559 training step(s), loss on training batch is 0.00628866.
After 12560 training step(s), loss on training batch is 0.00631136.
After 12561 training step(s), loss on training batch is 0.00664257.
After 12562 training step(s), loss on training batch is 0.00556227.
After 12563 training step(s), loss on training batch is 0.00640858.
After 12564 training step(s), loss on training batch is 0.00577186.
After 12565 training step(s), loss on training batch is 0.00554684.
After 12566 training step(s), loss on training batch is 0.00663562.
After 12567 training step(s), loss on training batch is 0.00568194.
After 12568 training step(s), loss on training batch is 0.0056592.
After 12569 training step(s), loss on training batch is 0.00594383.
After 12570 training step(s), loss on training batch is 0.00719661.
After 12571 training step(s), loss on training batch is 0.00671444.
After 12572 training step(s), loss on training batch is 0.00630725.
After 12573 training step(s), loss on training batch is 0.00641036.
After 12574 training step(s), loss on training batch is 0.00558244.
After 12575 training step(s), loss on training batch is 0.00554358.
After 12576 training step(s), loss on training batch is 0.00677822.
After 12577 training step(s), loss on training batch is 0.00531991.
After 12578 training step(s), loss on training batch is 0.00683141.
After 12579 training step(s), loss on training batch is 0.00672431.
After 12580 training step(s), loss on training batch is 0.00576341.
After 12581 training step(s), loss on training batch is 0.00678884.
After 12582 training step(s), loss on training batch is 0.015479.
After 12583 training step(s), loss on training batch is 0.00683864.
After 12584 training step(s), loss on training batch is 0.0062085.
After 12585 training step(s), loss on training batch is 0.00612933.
After 12586 training step(s), loss on training batch is 0.00610481.
After 12587 training step(s), loss on training batch is 0.00543676.
After 12588 training step(s), loss on training batch is 0.00624452.
After 12589 training step(s), loss on training batch is 0.00739797.
After 12590 training step(s), loss on training batch is 0.00575787.
After 12591 training step(s), loss on training batch is 0.00572932.
After 12592 training step(s), loss on training batch is 0.00567546.
After 12593 training step(s), loss on training batch is 0.0061812.
After 12594 training step(s), loss on training batch is 0.00612695.
After 12595 training step(s), loss on training batch is 0.00605831.
After 12596 training step(s), loss on training batch is 0.00557351.
After 12597 training step(s), loss on training batch is 0.00611738.
After 12598 training step(s), loss on training batch is 0.00624396.
After 12599 training step(s), loss on training batch is 0.00644613.
After 12600 training step(s), loss on training batch is 0.00561557.
After 12601 training step(s), loss on training batch is 0.00639581.
After 12602 training step(s), loss on training batch is 0.00654126.
After 12603 training step(s), loss on training batch is 0.00576775.
After 12604 training step(s), loss on training batch is 0.0104725.
After 12605 training step(s), loss on training batch is 0.0130142.
After 12606 training step(s), loss on training batch is 0.0161519.
After 12607 training step(s), loss on training batch is 0.00658789.
After 12608 training step(s), loss on training batch is 0.00624253.
After 12609 training step(s), loss on training batch is 0.00611517.
After 12610 training step(s), loss on training batch is 0.00614089.
After 12611 training step(s), loss on training batch is 0.00591393.
After 12612 training step(s), loss on training batch is 0.00663024.
After 12613 training step(s), loss on training batch is 0.00745278.
After 12614 training step(s), loss on training batch is 0.00639407.
After 12615 training step(s), loss on training batch is 0.00575639.
After 12616 training step(s), loss on training batch is 0.00842455.
After 12617 training step(s), loss on training batch is 0.0075417.
After 12618 training step(s), loss on training batch is 0.0056253.
After 12619 training step(s), loss on training batch is 0.00557091.
After 12620 training step(s), loss on training batch is 0.00601485.
After 12621 training step(s), loss on training batch is 0.0060305.
After 12622 training step(s), loss on training batch is 0.00617299.
After 12623 training step(s), loss on training batch is 0.00607553.
After 12624 training step(s), loss on training batch is 0.00740497.
After 12625 training step(s), loss on training batch is 0.00568351.
After 12626 training step(s), loss on training batch is 0.00570939.
After 12627 training step(s), loss on training batch is 0.0066571.
After 12628 training step(s), loss on training batch is 0.00591908.
After 12629 training step(s), loss on training batch is 0.00861449.
After 12630 training step(s), loss on training batch is 0.00658798.
After 12631 training step(s), loss on training batch is 0.00646331.
After 12632 training step(s), loss on training batch is 0.00612677.
After 12633 training step(s), loss on training batch is 0.00597005.
After 12634 training step(s), loss on training batch is 0.00824109.
After 12635 training step(s), loss on training batch is 0.00604603.
After 12636 training step(s), loss on training batch is 0.00693623.
After 12637 training step(s), loss on training batch is 0.00619272.
After 12638 training step(s), loss on training batch is 0.00976516.
After 12639 training step(s), loss on training batch is 0.00748691.
After 12640 training step(s), loss on training batch is 0.00634453.
After 12641 training step(s), loss on training batch is 0.00630811.
After 12642 training step(s), loss on training batch is 0.00619181.
After 12643 training step(s), loss on training batch is 0.0059252.
After 12644 training step(s), loss on training batch is 0.0068115.
After 12645 training step(s), loss on training batch is 0.00538099.
After 12646 training step(s), loss on training batch is 0.00702748.
After 12647 training step(s), loss on training batch is 0.0078016.
After 12648 training step(s), loss on training batch is 0.0054598.
After 12649 training step(s), loss on training batch is 0.00722748.
After 12650 training step(s), loss on training batch is 0.00628494.
After 12651 training step(s), loss on training batch is 0.00669523.
After 12652 training step(s), loss on training batch is 0.00574155.
After 12653 training step(s), loss on training batch is 0.00545725.
After 12654 training step(s), loss on training batch is 0.00601042.
After 12655 training step(s), loss on training batch is 0.00640769.
After 12656 training step(s), loss on training batch is 0.00596197.
After 12657 training step(s), loss on training batch is 0.00555896.
After 12658 training step(s), loss on training batch is 0.00664952.
After 12659 training step(s), loss on training batch is 0.00722333.
After 12660 training step(s), loss on training batch is 0.0076638.
After 12661 training step(s), loss on training batch is 0.00563719.
After 12662 training step(s), loss on training batch is 0.00558261.
After 12663 training step(s), loss on training batch is 0.00672566.
After 12664 training step(s), loss on training batch is 0.00639882.
After 12665 training step(s), loss on training batch is 0.00533811.
After 12666 training step(s), loss on training batch is 0.00624091.
After 12667 training step(s), loss on training batch is 0.00570306.
After 12668 training step(s), loss on training batch is 0.00615148.
After 12669 training step(s), loss on training batch is 0.00617855.
After 12670 training step(s), loss on training batch is 0.00821988.
After 12671 training step(s), loss on training batch is 0.006914.
After 12672 training step(s), loss on training batch is 0.00584558.
After 12673 training step(s), loss on training batch is 0.00561553.
After 12674 training step(s), loss on training batch is 0.00602709.
After 12675 training step(s), loss on training batch is 0.00576867.
After 12676 training step(s), loss on training batch is 0.00619229.
After 12677 training step(s), loss on training batch is 0.00608389.
After 12678 training step(s), loss on training batch is 0.00669623.
After 12679 training step(s), loss on training batch is 0.00662396.
After 12680 training step(s), loss on training batch is 0.00689589.
After 12681 training step(s), loss on training batch is 0.00575026.
After 12682 training step(s), loss on training batch is 0.00622448.
After 12683 training step(s), loss on training batch is 0.00610202.
After 12684 training step(s), loss on training batch is 0.00589532.
After 12685 training step(s), loss on training batch is 0.00665315.
After 12686 training step(s), loss on training batch is 0.00635014.
After 12687 training step(s), loss on training batch is 0.00551753.
After 12688 training step(s), loss on training batch is 0.00577311.
After 12689 training step(s), loss on training batch is 0.00605379.
After 12690 training step(s), loss on training batch is 0.00619531.
After 12691 training step(s), loss on training batch is 0.00683456.
After 12692 training step(s), loss on training batch is 0.00630998.
After 12693 training step(s), loss on training batch is 0.0059289.
After 12694 training step(s), loss on training batch is 0.00573339.
After 12695 training step(s), loss on training batch is 0.00669203.
After 12696 training step(s), loss on training batch is 0.00537093.
After 12697 training step(s), loss on training batch is 0.00580175.
After 12698 training step(s), loss on training batch is 0.00590194.
After 12699 training step(s), loss on training batch is 0.00577592.
After 12700 training step(s), loss on training batch is 0.00550881.
After 12701 training step(s), loss on training batch is 0.00555168.
After 12702 training step(s), loss on training batch is 0.00569017.
After 12703 training step(s), loss on training batch is 0.00558534.
After 12704 training step(s), loss on training batch is 0.0056444.
After 12705 training step(s), loss on training batch is 0.00550841.
After 12706 training step(s), loss on training batch is 0.00559538.
After 12707 training step(s), loss on training batch is 0.00589106.
After 12708 training step(s), loss on training batch is 0.00609414.
After 12709 training step(s), loss on training batch is 0.00660107.
After 12710 training step(s), loss on training batch is 0.00551248.
After 12711 training step(s), loss on training batch is 0.00665296.
After 12712 training step(s), loss on training batch is 0.00866684.
After 12713 training step(s), loss on training batch is 0.00582249.
After 12714 training step(s), loss on training batch is 0.00554498.
After 12715 training step(s), loss on training batch is 0.00770172.
After 12716 training step(s), loss on training batch is 0.00564569.
After 12717 training step(s), loss on training batch is 0.00661221.
After 12718 training step(s), loss on training batch is 0.00602228.
After 12719 training step(s), loss on training batch is 0.00615273.
After 12720 training step(s), loss on training batch is 0.00642107.
After 12721 training step(s), loss on training batch is 0.00583611.
After 12722 training step(s), loss on training batch is 0.00593432.
After 12723 training step(s), loss on training batch is 0.00560778.
After 12724 training step(s), loss on training batch is 0.00597191.
After 12725 training step(s), loss on training batch is 0.00571016.
After 12726 training step(s), loss on training batch is 0.00577716.
After 12727 training step(s), loss on training batch is 0.00558113.
After 12728 training step(s), loss on training batch is 0.00765562.
After 12729 training step(s), loss on training batch is 0.00584588.
After 12730 training step(s), loss on training batch is 0.00588459.
After 12731 training step(s), loss on training batch is 0.00575354.
After 12732 training step(s), loss on training batch is 0.00578568.
After 12733 training step(s), loss on training batch is 0.00597821.
After 12734 training step(s), loss on training batch is 0.00578276.
After 12735 training step(s), loss on training batch is 0.00686165.
After 12736 training step(s), loss on training batch is 0.0065822.
After 12737 training step(s), loss on training batch is 0.00547324.
After 12738 training step(s), loss on training batch is 0.00531577.
After 12739 training step(s), loss on training batch is 0.00550128.
After 12740 training step(s), loss on training batch is 0.00659995.
After 12741 training step(s), loss on training batch is 0.00628221.
After 12742 training step(s), loss on training batch is 0.00587851.
After 12743 training step(s), loss on training batch is 0.00789511.
After 12744 training step(s), loss on training batch is 0.00624635.
After 12745 training step(s), loss on training batch is 0.00624134.
After 12746 training step(s), loss on training batch is 0.00601124.
After 12747 training step(s), loss on training batch is 0.00577709.
After 12748 training step(s), loss on training batch is 0.00578525.
After 12749 training step(s), loss on training batch is 0.00604995.
After 12750 training step(s), loss on training batch is 0.0061316.
After 12751 training step(s), loss on training batch is 0.00591698.
After 12752 training step(s), loss on training batch is 0.00563443.
After 12753 training step(s), loss on training batch is 0.00592704.
After 12754 training step(s), loss on training batch is 0.00563796.
After 12755 training step(s), loss on training batch is 0.0068009.
After 12756 training step(s), loss on training batch is 0.00578918.
After 12757 training step(s), loss on training batch is 0.00533323.
After 12758 training step(s), loss on training batch is 0.00603752.
After 12759 training step(s), loss on training batch is 0.0067178.
After 12760 training step(s), loss on training batch is 0.00631169.
After 12761 training step(s), loss on training batch is 0.00595305.
After 12762 training step(s), loss on training batch is 0.00600339.
After 12763 training step(s), loss on training batch is 0.00828604.
After 12764 training step(s), loss on training batch is 0.00623425.
After 12765 training step(s), loss on training batch is 0.00582695.
After 12766 training step(s), loss on training batch is 0.00730586.
After 12767 training step(s), loss on training batch is 0.00699028.
After 12768 training step(s), loss on training batch is 0.00624942.
After 12769 training step(s), loss on training batch is 0.00566086.
After 12770 training step(s), loss on training batch is 0.0060359.
After 12771 training step(s), loss on training batch is 0.00562722.
After 12772 training step(s), loss on training batch is 0.00595167.
After 12773 training step(s), loss on training batch is 0.00548818.
After 12774 training step(s), loss on training batch is 0.00687036.
After 12775 training step(s), loss on training batch is 0.00638944.
After 12776 training step(s), loss on training batch is 0.00737211.
After 12777 training step(s), loss on training batch is 0.00566082.
After 12778 training step(s), loss on training batch is 0.00549645.
After 12779 training step(s), loss on training batch is 0.00553127.
After 12780 training step(s), loss on training batch is 0.00573518.
After 12781 training step(s), loss on training batch is 0.00821369.
After 12782 training step(s), loss on training batch is 0.00588807.
After 12783 training step(s), loss on training batch is 0.00630828.
After 12784 training step(s), loss on training batch is 0.0106822.
After 12785 training step(s), loss on training batch is 0.0322037.
After 12786 training step(s), loss on training batch is 0.0668906.
After 12787 training step(s), loss on training batch is 0.0371282.
After 12788 training step(s), loss on training batch is 0.0320689.
After 12789 training step(s), loss on training batch is 0.00867591.
After 12790 training step(s), loss on training batch is 0.0074914.
After 12791 training step(s), loss on training batch is 0.00737825.
After 12792 training step(s), loss on training batch is 0.00654726.
After 12793 training step(s), loss on training batch is 0.0106224.
After 12794 training step(s), loss on training batch is 0.00600064.
After 12795 training step(s), loss on training batch is 0.00652225.
After 12796 training step(s), loss on training batch is 0.00597176.
After 12797 training step(s), loss on training batch is 0.00592784.
After 12798 training step(s), loss on training batch is 0.00546531.
After 12799 training step(s), loss on training batch is 0.00623987.
After 12800 training step(s), loss on training batch is 0.0123279.
After 12801 training step(s), loss on training batch is 0.00656311.
After 12802 training step(s), loss on training batch is 0.00583767.
After 12803 training step(s), loss on training batch is 0.00947168.
After 12804 training step(s), loss on training batch is 0.00745125.
After 12805 training step(s), loss on training batch is 0.0062522.
After 12806 training step(s), loss on training batch is 0.00567623.
After 12807 training step(s), loss on training batch is 0.00683044.
After 12808 training step(s), loss on training batch is 0.00596685.
After 12809 training step(s), loss on training batch is 0.00669371.
After 12810 training step(s), loss on training batch is 0.00653821.
After 12811 training step(s), loss on training batch is 0.00635574.
After 12812 training step(s), loss on training batch is 0.0106829.
After 12813 training step(s), loss on training batch is 0.0063697.
After 12814 training step(s), loss on training batch is 0.00646361.
After 12815 training step(s), loss on training batch is 0.00618461.
After 12816 training step(s), loss on training batch is 0.00640066.
After 12817 training step(s), loss on training batch is 0.00652027.
After 12818 training step(s), loss on training batch is 0.00536457.
After 12819 training step(s), loss on training batch is 0.00633321.
After 12820 training step(s), loss on training batch is 0.00548953.
After 12821 training step(s), loss on training batch is 0.00593444.
After 12822 training step(s), loss on training batch is 0.00576081.
After 12823 training step(s), loss on training batch is 0.00625274.
After 12824 training step(s), loss on training batch is 0.00597333.
After 12825 training step(s), loss on training batch is 0.00607614.
After 12826 training step(s), loss on training batch is 0.00627529.
After 12827 training step(s), loss on training batch is 0.0100059.
After 12828 training step(s), loss on training batch is 0.0057825.
After 12829 training step(s), loss on training batch is 0.0055883.
After 12830 training step(s), loss on training batch is 0.00838217.
After 12831 training step(s), loss on training batch is 0.00794866.
After 12832 training step(s), loss on training batch is 0.00573006.
After 12833 training step(s), loss on training batch is 0.00623874.
After 12834 training step(s), loss on training batch is 0.00567548.
After 12835 training step(s), loss on training batch is 0.00643815.
After 12836 training step(s), loss on training batch is 0.00561809.
After 12837 training step(s), loss on training batch is 0.0057165.
After 12838 training step(s), loss on training batch is 0.00581015.
After 12839 training step(s), loss on training batch is 0.00632107.
After 12840 training step(s), loss on training batch is 0.00614705.
After 12841 training step(s), loss on training batch is 0.00601078.
After 12842 training step(s), loss on training batch is 0.00782017.
After 12843 training step(s), loss on training batch is 0.00579124.
After 12844 training step(s), loss on training batch is 0.00525263.
After 12845 training step(s), loss on training batch is 0.00681303.
After 12846 training step(s), loss on training batch is 0.00569027.
After 12847 training step(s), loss on training batch is 0.00693682.
After 12848 training step(s), loss on training batch is 0.00679045.
After 12849 training step(s), loss on training batch is 0.00639492.
After 12850 training step(s), loss on training batch is 0.00565968.
After 12851 training step(s), loss on training batch is 0.00635139.
After 12852 training step(s), loss on training batch is 0.00544202.
After 12853 training step(s), loss on training batch is 0.00582308.
After 12854 training step(s), loss on training batch is 0.00615568.
After 12855 training step(s), loss on training batch is 0.00725067.
After 12856 training step(s), loss on training batch is 0.00613706.
After 12857 training step(s), loss on training batch is 0.00552082.
After 12858 training step(s), loss on training batch is 0.00549838.
After 12859 training step(s), loss on training batch is 0.00674838.
After 12860 training step(s), loss on training batch is 0.0069931.
After 12861 training step(s), loss on training batch is 0.00561884.
After 12862 training step(s), loss on training batch is 0.00580955.
After 12863 training step(s), loss on training batch is 0.00628375.
After 12864 training step(s), loss on training batch is 0.00590029.
After 12865 training step(s), loss on training batch is 0.0056474.
After 12866 training step(s), loss on training batch is 0.00602066.
After 12867 training step(s), loss on training batch is 0.00606449.
After 12868 training step(s), loss on training batch is 0.00535991.
After 12869 training step(s), loss on training batch is 0.00571799.
After 12870 training step(s), loss on training batch is 0.00602234.
After 12871 training step(s), loss on training batch is 0.00613202.
After 12872 training step(s), loss on training batch is 0.00536116.
After 12873 training step(s), loss on training batch is 0.00597799.
After 12874 training step(s), loss on training batch is 0.005716.
After 12875 training step(s), loss on training batch is 0.00603173.
After 12876 training step(s), loss on training batch is 0.00636782.
After 12877 training step(s), loss on training batch is 0.00707212.
After 12878 training step(s), loss on training batch is 0.00639477.
After 12879 training step(s), loss on training batch is 0.006346.
After 12880 training step(s), loss on training batch is 0.00572254.
After 12881 training step(s), loss on training batch is 0.00716082.
After 12882 training step(s), loss on training batch is 0.00586008.
After 12883 training step(s), loss on training batch is 0.00560097.
After 12884 training step(s), loss on training batch is 0.00650554.
After 12885 training step(s), loss on training batch is 0.0100529.
After 12886 training step(s), loss on training batch is 0.00821178.
After 12887 training step(s), loss on training batch is 0.00879583.
After 12888 training step(s), loss on training batch is 0.00586796.
After 12889 training step(s), loss on training batch is 0.00550538.
After 12890 training step(s), loss on training batch is 0.00728989.
After 12891 training step(s), loss on training batch is 0.00725228.
After 12892 training step(s), loss on training batch is 0.00623839.
After 12893 training step(s), loss on training batch is 0.00539524.
After 12894 training step(s), loss on training batch is 0.00530383.
After 12895 training step(s), loss on training batch is 0.00700021.
After 12896 training step(s), loss on training batch is 0.00917672.
After 12897 training step(s), loss on training batch is 0.00602374.
After 12898 training step(s), loss on training batch is 0.00720588.
After 12899 training step(s), loss on training batch is 0.00626315.
After 12900 training step(s), loss on training batch is 0.00974455.
After 12901 training step(s), loss on training batch is 0.00629949.
After 12902 training step(s), loss on training batch is 0.00538097.
After 12903 training step(s), loss on training batch is 0.00589863.
After 12904 training step(s), loss on training batch is 0.00567316.
After 12905 training step(s), loss on training batch is 0.00561579.
After 12906 training step(s), loss on training batch is 0.00694682.
After 12907 training step(s), loss on training batch is 0.00775823.
After 12908 training step(s), loss on training batch is 0.00625378.
After 12909 training step(s), loss on training batch is 0.00582732.
After 12910 training step(s), loss on training batch is 0.00586012.
After 12911 training step(s), loss on training batch is 0.00558635.
After 12912 training step(s), loss on training batch is 0.00590371.
After 12913 training step(s), loss on training batch is 0.00641636.
After 12914 training step(s), loss on training batch is 0.00541199.
After 12915 training step(s), loss on training batch is 0.0145598.
After 12916 training step(s), loss on training batch is 0.00625412.
After 12917 training step(s), loss on training batch is 0.00651401.
After 12918 training step(s), loss on training batch is 0.00593353.
After 12919 training step(s), loss on training batch is 0.00627731.
After 12920 training step(s), loss on training batch is 0.00761087.
After 12921 training step(s), loss on training batch is 0.00530769.
After 12922 training step(s), loss on training batch is 0.00624312.
After 12923 training step(s), loss on training batch is 0.00659902.
After 12924 training step(s), loss on training batch is 0.00588348.
After 12925 training step(s), loss on training batch is 0.0069786.
After 12926 training step(s), loss on training batch is 0.00558204.
After 12927 training step(s), loss on training batch is 0.0057245.
After 12928 training step(s), loss on training batch is 0.00604871.
After 12929 training step(s), loss on training batch is 0.00561954.
After 12930 training step(s), loss on training batch is 0.00875011.
After 12931 training step(s), loss on training batch is 0.00699724.
After 12932 training step(s), loss on training batch is 0.00551154.
After 12933 training step(s), loss on training batch is 0.0081094.
After 12934 training step(s), loss on training batch is 0.00561343.
After 12935 training step(s), loss on training batch is 0.00553321.
After 12936 training step(s), loss on training batch is 0.00630697.
After 12937 training step(s), loss on training batch is 0.00738296.
After 12938 training step(s), loss on training batch is 0.0085992.
After 12939 training step(s), loss on training batch is 0.00554019.
After 12940 training step(s), loss on training batch is 0.00622719.
After 12941 training step(s), loss on training batch is 0.00596112.
After 12942 training step(s), loss on training batch is 0.00613761.
After 12943 training step(s), loss on training batch is 0.00687135.
After 12944 training step(s), loss on training batch is 0.00568358.
After 12945 training step(s), loss on training batch is 0.00557151.
After 12946 training step(s), loss on training batch is 0.00599022.
After 12947 training step(s), loss on training batch is 0.00563151.
After 12948 training step(s), loss on training batch is 0.00576956.
After 12949 training step(s), loss on training batch is 0.00684169.
After 12950 training step(s), loss on training batch is 0.00562662.
After 12951 training step(s), loss on training batch is 0.00593998.
After 12952 training step(s), loss on training batch is 0.00594656.
After 12953 training step(s), loss on training batch is 0.00575557.
After 12954 training step(s), loss on training batch is 0.00672635.
After 12955 training step(s), loss on training batch is 0.00599597.
After 12956 training step(s), loss on training batch is 0.00598054.
After 12957 training step(s), loss on training batch is 0.00617946.
After 12958 training step(s), loss on training batch is 0.00874463.
After 12959 training step(s), loss on training batch is 0.00629119.
After 12960 training step(s), loss on training batch is 0.00573187.
After 12961 training step(s), loss on training batch is 0.00569453.
After 12962 training step(s), loss on training batch is 0.00618936.
After 12963 training step(s), loss on training batch is 0.00742223.
After 12964 training step(s), loss on training batch is 0.00598748.
After 12965 training step(s), loss on training batch is 0.00554319.
After 12966 training step(s), loss on training batch is 0.00569591.
After 12967 training step(s), loss on training batch is 0.00584104.
After 12968 training step(s), loss on training batch is 0.00599841.
After 12969 training step(s), loss on training batch is 0.00633012.
After 12970 training step(s), loss on training batch is 0.00625186.
After 12971 training step(s), loss on training batch is 0.00581149.
After 12972 training step(s), loss on training batch is 0.00587087.
After 12973 training step(s), loss on training batch is 0.00547752.
After 12974 training step(s), loss on training batch is 0.00586874.
After 12975 training step(s), loss on training batch is 0.00561276.
After 12976 training step(s), loss on training batch is 0.00550322.
After 12977 training step(s), loss on training batch is 0.00981188.
After 12978 training step(s), loss on training batch is 0.00732596.
After 12979 training step(s), loss on training batch is 0.00523709.
After 12980 training step(s), loss on training batch is 0.00548588.
After 12981 training step(s), loss on training batch is 0.00965877.
After 12982 training step(s), loss on training batch is 0.00571332.
After 12983 training step(s), loss on training batch is 0.00617519.
After 12984 training step(s), loss on training batch is 0.00670434.
After 12985 training step(s), loss on training batch is 0.00530744.
After 12986 training step(s), loss on training batch is 0.00543817.
After 12987 training step(s), loss on training batch is 0.0067939.
After 12988 training step(s), loss on training batch is 0.00572816.
After 12989 training step(s), loss on training batch is 0.00638045.
After 12990 training step(s), loss on training batch is 0.00591861.
After 12991 training step(s), loss on training batch is 0.00567685.
After 12992 training step(s), loss on training batch is 0.0053881.
After 12993 training step(s), loss on training batch is 0.00615328.
After 12994 training step(s), loss on training batch is 0.00559573.
After 12995 training step(s), loss on training batch is 0.00579362.
After 12996 training step(s), loss on training batch is 0.00565513.
After 12997 training step(s), loss on training batch is 0.00569611.
After 12998 training step(s), loss on training batch is 0.00799935.
After 12999 training step(s), loss on training batch is 0.00596812.
After 13000 training step(s), loss on training batch is 0.00594094.
After 13001 training step(s), loss on training batch is 0.00586353.
After 13002 training step(s), loss on training batch is 0.0052544.
After 13003 training step(s), loss on training batch is 0.0059878.
After 13004 training step(s), loss on training batch is 0.0057773.
After 13005 training step(s), loss on training batch is 0.00628468.
After 13006 training step(s), loss on training batch is 0.00649271.
After 13007 training step(s), loss on training batch is 0.00546744.
After 13008 training step(s), loss on training batch is 0.00582022.
After 13009 training step(s), loss on training batch is 0.00634325.
After 13010 training step(s), loss on training batch is 0.0056482.
After 13011 training step(s), loss on training batch is 0.00575883.
After 13012 training step(s), loss on training batch is 0.00555596.
After 13013 training step(s), loss on training batch is 0.00561789.
After 13014 training step(s), loss on training batch is 0.00550045.
After 13015 training step(s), loss on training batch is 0.0060189.
After 13016 training step(s), loss on training batch is 0.00776654.
After 13017 training step(s), loss on training batch is 0.00573776.
After 13018 training step(s), loss on training batch is 0.00969375.
After 13019 training step(s), loss on training batch is 0.00585005.
After 13020 training step(s), loss on training batch is 0.00682623.
After 13021 training step(s), loss on training batch is 0.00528998.
After 13022 training step(s), loss on training batch is 0.00572847.
After 13023 training step(s), loss on training batch is 0.00563693.
After 13024 training step(s), loss on training batch is 0.00601153.
After 13025 training step(s), loss on training batch is 0.00568897.
After 13026 training step(s), loss on training batch is 0.00640342.
After 13027 training step(s), loss on training batch is 0.00597633.
After 13028 training step(s), loss on training batch is 0.00557581.
After 13029 training step(s), loss on training batch is 0.00545705.
After 13030 training step(s), loss on training batch is 0.00563147.
After 13031 training step(s), loss on training batch is 0.00662219.
After 13032 training step(s), loss on training batch is 0.00642993.
After 13033 training step(s), loss on training batch is 0.00663606.
After 13034 training step(s), loss on training batch is 0.00542254.
After 13035 training step(s), loss on training batch is 0.00616526.
After 13036 training step(s), loss on training batch is 0.0075173.
After 13037 training step(s), loss on training batch is 0.00933381.
After 13038 training step(s), loss on training batch is 0.00576599.
After 13039 training step(s), loss on training batch is 0.0054681.
After 13040 training step(s), loss on training batch is 0.00695598.
After 13041 training step(s), loss on training batch is 0.00595468.
After 13042 training step(s), loss on training batch is 0.00524769.
After 13043 training step(s), loss on training batch is 0.00603378.
After 13044 training step(s), loss on training batch is 0.00531655.
After 13045 training step(s), loss on training batch is 0.00570494.
After 13046 training step(s), loss on training batch is 0.00548745.
After 13047 training step(s), loss on training batch is 0.00612811.
After 13048 training step(s), loss on training batch is 0.00576943.
After 13049 training step(s), loss on training batch is 0.0071081.
After 13050 training step(s), loss on training batch is 0.00565328.
After 13051 training step(s), loss on training batch is 0.00612274.
After 13052 training step(s), loss on training batch is 0.00588612.
After 13053 training step(s), loss on training batch is 0.00614579.
After 13054 training step(s), loss on training batch is 0.00580863.
After 13055 training step(s), loss on training batch is 0.00617307.
After 13056 training step(s), loss on training batch is 0.0058407.
After 13057 training step(s), loss on training batch is 0.00687527.
After 13058 training step(s), loss on training batch is 0.00547431.
After 13059 training step(s), loss on training batch is 0.00540718.
After 13060 training step(s), loss on training batch is 0.00577261.
After 13061 training step(s), loss on training batch is 0.00542998.
After 13062 training step(s), loss on training batch is 0.00525792.
After 13063 training step(s), loss on training batch is 0.00731727.
After 13064 training step(s), loss on training batch is 0.00583221.
After 13065 training step(s), loss on training batch is 0.00874706.
After 13066 training step(s), loss on training batch is 0.00602188.
After 13067 training step(s), loss on training batch is 0.00639141.
After 13068 training step(s), loss on training batch is 0.00593081.
After 13069 training step(s), loss on training batch is 0.00634416.
After 13070 training step(s), loss on training batch is 0.0060216.
After 13071 training step(s), loss on training batch is 0.00604868.
After 13072 training step(s), loss on training batch is 0.0060783.
After 13073 training step(s), loss on training batch is 0.00599528.
After 13074 training step(s), loss on training batch is 0.00516805.
After 13075 training step(s), loss on training batch is 0.00551355.
After 13076 training step(s), loss on training batch is 0.00591541.
After 13077 training step(s), loss on training batch is 0.00548641.
After 13078 training step(s), loss on training batch is 0.00618921.
After 13079 training step(s), loss on training batch is 0.00599487.
After 13080 training step(s), loss on training batch is 0.0055092.
After 13081 training step(s), loss on training batch is 0.00556546.
After 13082 training step(s), loss on training batch is 0.00528847.
After 13083 training step(s), loss on training batch is 0.00564448.
After 13084 training step(s), loss on training batch is 0.00564286.
After 13085 training step(s), loss on training batch is 0.00610902.
After 13086 training step(s), loss on training batch is 0.00620624.
After 13087 training step(s), loss on training batch is 0.00611044.
After 13088 training step(s), loss on training batch is 0.00583578.
After 13089 training step(s), loss on training batch is 0.00561641.
After 13090 training step(s), loss on training batch is 0.00551889.
After 13091 training step(s), loss on training batch is 0.00522931.
After 13092 training step(s), loss on training batch is 0.00643632.
After 13093 training step(s), loss on training batch is 0.00572139.
After 13094 training step(s), loss on training batch is 0.00598468.
After 13095 training step(s), loss on training batch is 0.00559742.
After 13096 training step(s), loss on training batch is 0.00537577.
After 13097 training step(s), loss on training batch is 0.00564737.
After 13098 training step(s), loss on training batch is 0.00632051.
After 13099 training step(s), loss on training batch is 0.0059764.
After 13100 training step(s), loss on training batch is 0.00542772.
After 13101 training step(s), loss on training batch is 0.00602219.
After 13102 training step(s), loss on training batch is 0.00557552.
After 13103 training step(s), loss on training batch is 0.00635099.
After 13104 training step(s), loss on training batch is 0.00594698.
After 13105 training step(s), loss on training batch is 0.00657743.
After 13106 training step(s), loss on training batch is 0.00702224.
After 13107 training step(s), loss on training batch is 0.00556026.
After 13108 training step(s), loss on training batch is 0.00580674.
After 13109 training step(s), loss on training batch is 0.00551378.
After 13110 training step(s), loss on training batch is 0.00595322.
After 13111 training step(s), loss on training batch is 0.0064272.
After 13112 training step(s), loss on training batch is 0.00568036.
After 13113 training step(s), loss on training batch is 0.00534538.
After 13114 training step(s), loss on training batch is 0.00671223.
After 13115 training step(s), loss on training batch is 0.00611951.
After 13116 training step(s), loss on training batch is 0.00585179.
After 13117 training step(s), loss on training batch is 0.00506754.
After 13118 training step(s), loss on training batch is 0.00568359.
After 13119 training step(s), loss on training batch is 0.00619577.
After 13120 training step(s), loss on training batch is 0.00556687.
After 13121 training step(s), loss on training batch is 0.00638325.
After 13122 training step(s), loss on training batch is 0.0052806.
After 13123 training step(s), loss on training batch is 0.0117707.
After 13124 training step(s), loss on training batch is 0.0177278.
After 13125 training step(s), loss on training batch is 0.00666303.
After 13126 training step(s), loss on training batch is 0.00731724.
After 13127 training step(s), loss on training batch is 0.00662331.
After 13128 training step(s), loss on training batch is 0.00530368.
After 13129 training step(s), loss on training batch is 0.00547033.
After 13130 training step(s), loss on training batch is 0.00595876.
After 13131 training step(s), loss on training batch is 0.00532722.
After 13132 training step(s), loss on training batch is 0.00519438.
After 13133 training step(s), loss on training batch is 0.00545469.
After 13134 training step(s), loss on training batch is 0.00685712.
After 13135 training step(s), loss on training batch is 0.00569771.
After 13136 training step(s), loss on training batch is 0.00595366.
After 13137 training step(s), loss on training batch is 0.00657937.
After 13138 training step(s), loss on training batch is 0.00531193.
After 13139 training step(s), loss on training batch is 0.00549173.
After 13140 training step(s), loss on training batch is 0.00637018.
After 13141 training step(s), loss on training batch is 0.00559279.
After 13142 training step(s), loss on training batch is 0.00558591.
After 13143 training step(s), loss on training batch is 0.00571714.
After 13144 training step(s), loss on training batch is 0.00636296.
After 13145 training step(s), loss on training batch is 0.00541953.
After 13146 training step(s), loss on training batch is 0.00616432.
After 13147 training step(s), loss on training batch is 0.00678746.
After 13148 training step(s), loss on training batch is 0.00614896.
After 13149 training step(s), loss on training batch is 0.0063175.
After 13150 training step(s), loss on training batch is 0.00555601.
After 13151 training step(s), loss on training batch is 0.0056713.
After 13152 training step(s), loss on training batch is 0.00740656.
After 13153 training step(s), loss on training batch is 0.00582872.
After 13154 training step(s), loss on training batch is 0.00534984.
After 13155 training step(s), loss on training batch is 0.00618562.
After 13156 training step(s), loss on training batch is 0.0066542.
After 13157 training step(s), loss on training batch is 0.00558021.
After 13158 training step(s), loss on training batch is 0.00719364.
After 13159 training step(s), loss on training batch is 0.00590976.
After 13160 training step(s), loss on training batch is 0.00826663.
After 13161 training step(s), loss on training batch is 0.00700013.
After 13162 training step(s), loss on training batch is 0.00543584.
After 13163 training step(s), loss on training batch is 0.00646488.
After 13164 training step(s), loss on training batch is 0.00559121.
After 13165 training step(s), loss on training batch is 0.00897985.
After 13166 training step(s), loss on training batch is 0.00623104.
After 13167 training step(s), loss on training batch is 0.00666606.
After 13168 training step(s), loss on training batch is 0.00574139.
After 13169 training step(s), loss on training batch is 0.00536016.
After 13170 training step(s), loss on training batch is 0.00588209.
After 13171 training step(s), loss on training batch is 0.00614024.
After 13172 training step(s), loss on training batch is 0.00619298.
After 13173 training step(s), loss on training batch is 0.00595801.
After 13174 training step(s), loss on training batch is 0.00532133.
After 13175 training step(s), loss on training batch is 0.00585064.
After 13176 training step(s), loss on training batch is 0.00655555.
After 13177 training step(s), loss on training batch is 0.00533799.
After 13178 training step(s), loss on training batch is 0.00774.
After 13179 training step(s), loss on training batch is 0.00567338.
After 13180 training step(s), loss on training batch is 0.00729955.
After 13181 training step(s), loss on training batch is 0.00594933.
After 13182 training step(s), loss on training batch is 0.00709103.
After 13183 training step(s), loss on training batch is 0.00524296.
After 13184 training step(s), loss on training batch is 0.00523238.
After 13185 training step(s), loss on training batch is 0.00527739.
After 13186 training step(s), loss on training batch is 0.00729702.
After 13187 training step(s), loss on training batch is 0.00623298.
After 13188 training step(s), loss on training batch is 0.00608075.
After 13189 training step(s), loss on training batch is 0.0075341.
After 13190 training step(s), loss on training batch is 0.00565122.
After 13191 training step(s), loss on training batch is 0.00555474.
After 13192 training step(s), loss on training batch is 0.00554595.
After 13193 training step(s), loss on training batch is 0.00547377.
After 13194 training step(s), loss on training batch is 0.00611583.
After 13195 training step(s), loss on training batch is 0.00526194.
After 13196 training step(s), loss on training batch is 0.00576889.
After 13197 training step(s), loss on training batch is 0.0058318.
After 13198 training step(s), loss on training batch is 0.00676552.
After 13199 training step(s), loss on training batch is 0.00548938.
After 13200 training step(s), loss on training batch is 0.00689117.
After 13201 training step(s), loss on training batch is 0.0073644.
After 13202 training step(s), loss on training batch is 0.00587486.
After 13203 training step(s), loss on training batch is 0.00536462.
After 13204 training step(s), loss on training batch is 0.00529665.
After 13205 training step(s), loss on training batch is 0.00611136.
After 13206 training step(s), loss on training batch is 0.00587246.
After 13207 training step(s), loss on training batch is 0.00672312.
After 13208 training step(s), loss on training batch is 0.00539784.
After 13209 training step(s), loss on training batch is 0.0118884.
After 13210 training step(s), loss on training batch is 0.011445.
After 13211 training step(s), loss on training batch is 0.0061353.
After 13212 training step(s), loss on training batch is 0.00571647.
After 13213 training step(s), loss on training batch is 0.0102057.
After 13214 training step(s), loss on training batch is 0.00625207.
After 13215 training step(s), loss on training batch is 0.00576014.
After 13216 training step(s), loss on training batch is 0.006056.
After 13217 training step(s), loss on training batch is 0.00573768.
After 13218 training step(s), loss on training batch is 0.00545355.
After 13219 training step(s), loss on training batch is 0.00710563.
After 13220 training step(s), loss on training batch is 0.00600201.
After 13221 training step(s), loss on training batch is 0.00558511.
After 13222 training step(s), loss on training batch is 0.00517461.
After 13223 training step(s), loss on training batch is 0.00720268.
After 13224 training step(s), loss on training batch is 0.00871602.
After 13225 training step(s), loss on training batch is 0.00604267.
After 13226 training step(s), loss on training batch is 0.00572375.
After 13227 training step(s), loss on training batch is 0.00578133.
After 13228 training step(s), loss on training batch is 0.00606553.
After 13229 training step(s), loss on training batch is 0.00534959.
After 13230 training step(s), loss on training batch is 0.00530649.
After 13231 training step(s), loss on training batch is 0.00557367.
After 13232 training step(s), loss on training batch is 0.00531717.
After 13233 training step(s), loss on training batch is 0.00545957.
After 13234 training step(s), loss on training batch is 0.00581193.
After 13235 training step(s), loss on training batch is 0.00541257.
After 13236 training step(s), loss on training batch is 0.00551756.
After 13237 training step(s), loss on training batch is 0.00624833.
After 13238 training step(s), loss on training batch is 0.00553612.
After 13239 training step(s), loss on training batch is 0.00571307.
After 13240 training step(s), loss on training batch is 0.00590638.
After 13241 training step(s), loss on training batch is 0.00554503.
After 13242 training step(s), loss on training batch is 0.0052999.
After 13243 training step(s), loss on training batch is 0.00556031.
After 13244 training step(s), loss on training batch is 0.00616702.
After 13245 training step(s), loss on training batch is 0.00570497.
After 13246 training step(s), loss on training batch is 0.00661881.
After 13247 training step(s), loss on training batch is 0.00540162.
After 13248 training step(s), loss on training batch is 0.00545268.
After 13249 training step(s), loss on training batch is 0.00582058.
After 13250 training step(s), loss on training batch is 0.00573218.
After 13251 training step(s), loss on training batch is 0.00558606.
After 13252 training step(s), loss on training batch is 0.00555492.
After 13253 training step(s), loss on training batch is 0.00655413.
After 13254 training step(s), loss on training batch is 0.00557466.
After 13255 training step(s), loss on training batch is 0.00696181.
After 13256 training step(s), loss on training batch is 0.00632244.
After 13257 training step(s), loss on training batch is 0.00605464.
After 13258 training step(s), loss on training batch is 0.00581961.
After 13259 training step(s), loss on training batch is 0.00587325.
After 13260 training step(s), loss on training batch is 0.00551644.
After 13261 training step(s), loss on training batch is 0.00597065.
After 13262 training step(s), loss on training batch is 0.00553262.
After 13263 training step(s), loss on training batch is 0.00533198.
After 13264 training step(s), loss on training batch is 0.00560463.
After 13265 training step(s), loss on training batch is 0.00633238.
After 13266 training step(s), loss on training batch is 0.00551287.
After 13267 training step(s), loss on training batch is 0.00537321.
After 13268 training step(s), loss on training batch is 0.00565829.
After 13269 training step(s), loss on training batch is 0.00617019.
After 13270 training step(s), loss on training batch is 0.00593933.
After 13271 training step(s), loss on training batch is 0.00601469.
After 13272 training step(s), loss on training batch is 0.00611303.
After 13273 training step(s), loss on training batch is 0.00522579.
After 13274 training step(s), loss on training batch is 0.00539165.
After 13275 training step(s), loss on training batch is 0.00529814.
After 13276 training step(s), loss on training batch is 0.00515836.
After 13277 training step(s), loss on training batch is 0.00631254.
After 13278 training step(s), loss on training batch is 0.00565896.
After 13279 training step(s), loss on training batch is 0.00520628.
After 13280 training step(s), loss on training batch is 0.00570767.
After 13281 training step(s), loss on training batch is 0.00614783.
After 13282 training step(s), loss on training batch is 0.00600202.
After 13283 training step(s), loss on training batch is 0.00572699.
After 13284 training step(s), loss on training batch is 0.00526359.
After 13285 training step(s), loss on training batch is 0.00639699.
After 13286 training step(s), loss on training batch is 0.00583758.
After 13287 training step(s), loss on training batch is 0.00742949.
After 13288 training step(s), loss on training batch is 0.0055738.
After 13289 training step(s), loss on training batch is 0.00650246.
After 13290 training step(s), loss on training batch is 0.00603397.
After 13291 training step(s), loss on training batch is 0.00594152.
After 13292 training step(s), loss on training batch is 0.00628628.
After 13293 training step(s), loss on training batch is 0.00566848.
After 13294 training step(s), loss on training batch is 0.0065481.
After 13295 training step(s), loss on training batch is 0.00623252.
After 13296 training step(s), loss on training batch is 0.00566356.
After 13297 training step(s), loss on training batch is 0.00517519.
After 13298 training step(s), loss on training batch is 0.00547406.
After 13299 training step(s), loss on training batch is 0.00543664.
After 13300 training step(s), loss on training batch is 0.00554702.
After 13301 training step(s), loss on training batch is 0.00509532.
After 13302 training step(s), loss on training batch is 0.00675312.
After 13303 training step(s), loss on training batch is 0.00612594.
After 13304 training step(s), loss on training batch is 0.00556208.
After 13305 training step(s), loss on training batch is 0.00529455.
After 13306 training step(s), loss on training batch is 0.00561548.
After 13307 training step(s), loss on training batch is 0.00662158.
After 13308 training step(s), loss on training batch is 0.00530025.
After 13309 training step(s), loss on training batch is 0.00590526.
After 13310 training step(s), loss on training batch is 0.00618716.
After 13311 training step(s), loss on training batch is 0.00554709.
After 13312 training step(s), loss on training batch is 0.00552055.
After 13313 training step(s), loss on training batch is 0.00539242.
After 13314 training step(s), loss on training batch is 0.00544956.
After 13315 training step(s), loss on training batch is 0.00589693.
After 13316 training step(s), loss on training batch is 0.00532738.
After 13317 training step(s), loss on training batch is 0.00569807.
After 13318 training step(s), loss on training batch is 0.00735619.
After 13319 training step(s), loss on training batch is 0.00650497.
After 13320 training step(s), loss on training batch is 0.00693127.
After 13321 training step(s), loss on training batch is 0.00545491.
After 13322 training step(s), loss on training batch is 0.00569001.
After 13323 training step(s), loss on training batch is 0.00567786.
After 13324 training step(s), loss on training batch is 0.00522743.
After 13325 training step(s), loss on training batch is 0.00591473.
After 13326 training step(s), loss on training batch is 0.00593787.
After 13327 training step(s), loss on training batch is 0.00573682.
After 13328 training step(s), loss on training batch is 0.00678031.
After 13329 training step(s), loss on training batch is 0.0051737.
After 13330 training step(s), loss on training batch is 0.00619432.
After 13331 training step(s), loss on training batch is 0.00506274.
After 13332 training step(s), loss on training batch is 0.00587077.
After 13333 training step(s), loss on training batch is 0.00554505.
After 13334 training step(s), loss on training batch is 0.0056617.
After 13335 training step(s), loss on training batch is 0.00543196.
After 13336 training step(s), loss on training batch is 0.00617458.
After 13337 training step(s), loss on training batch is 0.00549198.
After 13338 training step(s), loss on training batch is 0.00643451.
After 13339 training step(s), loss on training batch is 0.00616375.
After 13340 training step(s), loss on training batch is 0.00523291.
After 13341 training step(s), loss on training batch is 0.0059001.
After 13342 training step(s), loss on training batch is 0.00546662.
After 13343 training step(s), loss on training batch is 0.00559145.
After 13344 training step(s), loss on training batch is 0.00602014.
After 13345 training step(s), loss on training batch is 0.00638845.
After 13346 training step(s), loss on training batch is 0.00531718.
After 13347 training step(s), loss on training batch is 0.00613324.
After 13348 training step(s), loss on training batch is 0.00578629.
After 13349 training step(s), loss on training batch is 0.00542329.
After 13350 training step(s), loss on training batch is 0.00707482.
After 13351 training step(s), loss on training batch is 0.00534466.
After 13352 training step(s), loss on training batch is 0.00542642.
After 13353 training step(s), loss on training batch is 0.0053785.
After 13354 training step(s), loss on training batch is 0.00529616.
After 13355 training step(s), loss on training batch is 0.0058189.
After 13356 training step(s), loss on training batch is 0.00620153.
After 13357 training step(s), loss on training batch is 0.00568584.
After 13358 training step(s), loss on training batch is 0.00563846.
After 13359 training step(s), loss on training batch is 0.00600016.
After 13360 training step(s), loss on training batch is 0.00543752.
After 13361 training step(s), loss on training batch is 0.0053336.
After 13362 training step(s), loss on training batch is 0.0051839.
After 13363 training step(s), loss on training batch is 0.00629526.
After 13364 training step(s), loss on training batch is 0.00584334.
After 13365 training step(s), loss on training batch is 0.00571216.
After 13366 training step(s), loss on training batch is 0.00513152.
After 13367 training step(s), loss on training batch is 0.00558213.
After 13368 training step(s), loss on training batch is 0.00542318.
After 13369 training step(s), loss on training batch is 0.00842162.
After 13370 training step(s), loss on training batch is 0.00585875.
After 13371 training step(s), loss on training batch is 0.00573814.
After 13372 training step(s), loss on training batch is 0.00669047.
After 13373 training step(s), loss on training batch is 0.0055609.
After 13374 training step(s), loss on training batch is 0.00596312.
After 13375 training step(s), loss on training batch is 0.00562268.
After 13376 training step(s), loss on training batch is 0.00503104.
After 13377 training step(s), loss on training batch is 0.0053644.
After 13378 training step(s), loss on training batch is 0.00573335.
After 13379 training step(s), loss on training batch is 0.00544258.
After 13380 training step(s), loss on training batch is 0.00576755.
After 13381 training step(s), loss on training batch is 0.00593506.
After 13382 training step(s), loss on training batch is 0.00547268.
After 13383 training step(s), loss on training batch is 0.00532553.
After 13384 training step(s), loss on training batch is 0.00611836.
After 13385 training step(s), loss on training batch is 0.00608126.
After 13386 training step(s), loss on training batch is 0.00543405.
After 13387 training step(s), loss on training batch is 0.00600512.
After 13388 training step(s), loss on training batch is 0.00523667.
After 13389 training step(s), loss on training batch is 0.00595702.
After 13390 training step(s), loss on training batch is 0.00545853.
After 13391 training step(s), loss on training batch is 0.00656796.
After 13392 training step(s), loss on training batch is 0.00549758.
After 13393 training step(s), loss on training batch is 0.00698078.
After 13394 training step(s), loss on training batch is 0.00591409.
After 13395 training step(s), loss on training batch is 0.00537545.
After 13396 training step(s), loss on training batch is 0.00587204.
After 13397 training step(s), loss on training batch is 0.00608161.
After 13398 training step(s), loss on training batch is 0.0058299.
After 13399 training step(s), loss on training batch is 0.00573031.
After 13400 training step(s), loss on training batch is 0.00538263.
After 13401 training step(s), loss on training batch is 0.00532753.
After 13402 training step(s), loss on training batch is 0.0057037.
After 13403 training step(s), loss on training batch is 0.00528043.
After 13404 training step(s), loss on training batch is 0.00583028.
After 13405 training step(s), loss on training batch is 0.0105093.
After 13406 training step(s), loss on training batch is 0.00575246.
After 13407 training step(s), loss on training batch is 0.00528783.
After 13408 training step(s), loss on training batch is 0.00574544.
After 13409 training step(s), loss on training batch is 0.00589621.
After 13410 training step(s), loss on training batch is 0.0070924.
After 13411 training step(s), loss on training batch is 0.00943549.
After 13412 training step(s), loss on training batch is 0.00605655.
After 13413 training step(s), loss on training batch is 0.0057735.
After 13414 training step(s), loss on training batch is 0.00605054.
After 13415 training step(s), loss on training batch is 0.00519618.
After 13416 training step(s), loss on training batch is 0.005392.
After 13417 training step(s), loss on training batch is 0.00542701.
After 13418 training step(s), loss on training batch is 0.00519727.
After 13419 training step(s), loss on training batch is 0.00538485.
After 13420 training step(s), loss on training batch is 0.0056144.
After 13421 training step(s), loss on training batch is 0.00595556.
After 13422 training step(s), loss on training batch is 0.00518122.
After 13423 training step(s), loss on training batch is 0.00521655.
After 13424 training step(s), loss on training batch is 0.00541379.
After 13425 training step(s), loss on training batch is 0.00556355.
After 13426 training step(s), loss on training batch is 0.00568161.
After 13427 training step(s), loss on training batch is 0.0059314.
After 13428 training step(s), loss on training batch is 0.00639567.
After 13429 training step(s), loss on training batch is 0.00672535.
After 13430 training step(s), loss on training batch is 0.00668799.
After 13431 training step(s), loss on training batch is 0.00540937.
After 13432 training step(s), loss on training batch is 0.00503514.
After 13433 training step(s), loss on training batch is 0.00784741.
After 13434 training step(s), loss on training batch is 0.00643078.
After 13435 training step(s), loss on training batch is 0.00536571.
After 13436 training step(s), loss on training batch is 0.00561089.
After 13437 training step(s), loss on training batch is 0.00624014.
After 13438 training step(s), loss on training batch is 0.00543845.
After 13439 training step(s), loss on training batch is 0.00563754.
After 13440 training step(s), loss on training batch is 0.00656664.
After 13441 training step(s), loss on training batch is 0.00571706.
After 13442 training step(s), loss on training batch is 0.00545342.
After 13443 training step(s), loss on training batch is 0.00567099.
After 13444 training step(s), loss on training batch is 0.0065142.
After 13445 training step(s), loss on training batch is 0.00566113.
After 13446 training step(s), loss on training batch is 0.00526958.
After 13447 training step(s), loss on training batch is 0.00546516.
After 13448 training step(s), loss on training batch is 0.00518015.
After 13449 training step(s), loss on training batch is 0.00606862.
After 13450 training step(s), loss on training batch is 0.00651805.
After 13451 training step(s), loss on training batch is 0.00565319.
After 13452 training step(s), loss on training batch is 0.00699572.
After 13453 training step(s), loss on training batch is 0.00609665.
After 13454 training step(s), loss on training batch is 0.00535705.
After 13455 training step(s), loss on training batch is 0.00551822.
After 13456 training step(s), loss on training batch is 0.00559324.
After 13457 training step(s), loss on training batch is 0.00525093.
After 13458 training step(s), loss on training batch is 0.00522432.
After 13459 training step(s), loss on training batch is 0.00530209.
After 13460 training step(s), loss on training batch is 0.0066162.
After 13461 training step(s), loss on training batch is 0.0062113.
After 13462 training step(s), loss on training batch is 0.00597625.
After 13463 training step(s), loss on training batch is 0.00519132.
After 13464 training step(s), loss on training batch is 0.00572886.
After 13465 training step(s), loss on training batch is 0.00520719.
After 13466 training step(s), loss on training batch is 0.00586142.
After 13467 training step(s), loss on training batch is 0.00506076.
After 13468 training step(s), loss on training batch is 0.00565179.
After 13469 training step(s), loss on training batch is 0.00524941.
After 13470 training step(s), loss on training batch is 0.00576009.
After 13471 training step(s), loss on training batch is 0.0056481.
After 13472 training step(s), loss on training batch is 0.00718132.
After 13473 training step(s), loss on training batch is 0.00540107.
After 13474 training step(s), loss on training batch is 0.00637075.
After 13475 training step(s), loss on training batch is 0.00660035.
After 13476 training step(s), loss on training batch is 0.00599712.
After 13477 training step(s), loss on training batch is 0.00749911.
After 13478 training step(s), loss on training batch is 0.00514782.
After 13479 training step(s), loss on training batch is 0.00553383.
After 13480 training step(s), loss on training batch is 0.00549119.
After 13481 training step(s), loss on training batch is 0.00663511.
After 13482 training step(s), loss on training batch is 0.00580326.
After 13483 training step(s), loss on training batch is 0.0050369.
After 13484 training step(s), loss on training batch is 0.00611295.
After 13485 training step(s), loss on training batch is 0.00550707.
After 13486 training step(s), loss on training batch is 0.00549575.
After 13487 training step(s), loss on training batch is 0.00514091.
After 13488 training step(s), loss on training batch is 0.00546975.
After 13489 training step(s), loss on training batch is 0.00534176.
After 13490 training step(s), loss on training batch is 0.00576289.
After 13491 training step(s), loss on training batch is 0.00553621.
After 13492 training step(s), loss on training batch is 0.00532252.
After 13493 training step(s), loss on training batch is 0.00564676.
After 13494 training step(s), loss on training batch is 0.0061519.
After 13495 training step(s), loss on training batch is 0.00508637.
After 13496 training step(s), loss on training batch is 0.00524967.
After 13497 training step(s), loss on training batch is 0.00556.
After 13498 training step(s), loss on training batch is 0.00491531.
After 13499 training step(s), loss on training batch is 0.00546573.
After 13500 training step(s), loss on training batch is 0.00552667.
After 13501 training step(s), loss on training batch is 0.00590338.
After 13502 training step(s), loss on training batch is 0.00537082.
After 13503 training step(s), loss on training batch is 0.00520428.
After 13504 training step(s), loss on training batch is 0.00541767.
After 13505 training step(s), loss on training batch is 0.00582042.
After 13506 training step(s), loss on training batch is 0.00534602.
After 13507 training step(s), loss on training batch is 0.00547816.
After 13508 training step(s), loss on training batch is 0.0053537.
After 13509 training step(s), loss on training batch is 0.0057074.
After 13510 training step(s), loss on training batch is 0.00549443.
After 13511 training step(s), loss on training batch is 0.00532046.
After 13512 training step(s), loss on training batch is 0.00578119.
After 13513 training step(s), loss on training batch is 0.00558619.
After 13514 training step(s), loss on training batch is 0.00631215.
After 13515 training step(s), loss on training batch is 0.0058133.
After 13516 training step(s), loss on training batch is 0.00534325.
After 13517 training step(s), loss on training batch is 0.00588847.
After 13518 training step(s), loss on training batch is 0.00498864.
After 13519 training step(s), loss on training batch is 0.00623948.
After 13520 training step(s), loss on training batch is 0.00557847.
After 13521 training step(s), loss on training batch is 0.00580548.
After 13522 training step(s), loss on training batch is 0.0065378.
After 13523 training step(s), loss on training batch is 0.00744753.
After 13524 training step(s), loss on training batch is 0.00683292.
After 13525 training step(s), loss on training batch is 0.00563428.
After 13526 training step(s), loss on training batch is 0.00505406.
After 13527 training step(s), loss on training batch is 0.00515332.
After 13528 training step(s), loss on training batch is 0.00576023.
After 13529 training step(s), loss on training batch is 0.00510328.
After 13530 training step(s), loss on training batch is 0.00488751.
After 13531 training step(s), loss on training batch is 0.00558385.
After 13532 training step(s), loss on training batch is 0.00574486.
After 13533 training step(s), loss on training batch is 0.00600765.
After 13534 training step(s), loss on training batch is 0.00554325.
After 13535 training step(s), loss on training batch is 0.00495424.
After 13536 training step(s), loss on training batch is 0.00555351.
After 13537 training step(s), loss on training batch is 0.00541425.
After 13538 training step(s), loss on training batch is 0.00728442.
After 13539 training step(s), loss on training batch is 0.00599938.
After 13540 training step(s), loss on training batch is 0.00546408.
After 13541 training step(s), loss on training batch is 0.00552238.
After 13542 training step(s), loss on training batch is 0.00552461.
After 13543 training step(s), loss on training batch is 0.00583256.
After 13544 training step(s), loss on training batch is 0.00561526.
After 13545 training step(s), loss on training batch is 0.00515317.
After 13546 training step(s), loss on training batch is 0.00634152.
After 13547 training step(s), loss on training batch is 0.00604235.
After 13548 training step(s), loss on training batch is 0.00528517.
After 13549 training step(s), loss on training batch is 0.00569989.
After 13550 training step(s), loss on training batch is 0.00521296.
After 13551 training step(s), loss on training batch is 0.00548669.
After 13552 training step(s), loss on training batch is 0.00576815.
After 13553 training step(s), loss on training batch is 0.00500334.
After 13554 training step(s), loss on training batch is 0.00514542.
After 13555 training step(s), loss on training batch is 0.00580186.
After 13556 training step(s), loss on training batch is 0.00564803.
After 13557 training step(s), loss on training batch is 0.00492108.
After 13558 training step(s), loss on training batch is 0.00550933.
After 13559 training step(s), loss on training batch is 0.00588739.
After 13560 training step(s), loss on training batch is 0.0060462.
After 13561 training step(s), loss on training batch is 0.00524176.
After 13562 training step(s), loss on training batch is 0.00621728.
After 13563 training step(s), loss on training batch is 0.00539799.
After 13564 training step(s), loss on training batch is 0.00613248.
After 13565 training step(s), loss on training batch is 0.00541411.
After 13566 training step(s), loss on training batch is 0.0052845.
After 13567 training step(s), loss on training batch is 0.00520803.
After 13568 training step(s), loss on training batch is 0.00642721.
After 13569 training step(s), loss on training batch is 0.00531692.
After 13570 training step(s), loss on training batch is 0.00634881.
After 13571 training step(s), loss on training batch is 0.00596335.
After 13572 training step(s), loss on training batch is 0.00497393.
After 13573 training step(s), loss on training batch is 0.00493997.
After 13574 training step(s), loss on training batch is 0.00559685.
After 13575 training step(s), loss on training batch is 0.00652811.
After 13576 training step(s), loss on training batch is 0.00664151.
After 13577 training step(s), loss on training batch is 0.00540593.
After 13578 training step(s), loss on training batch is 0.00543768.
After 13579 training step(s), loss on training batch is 0.00519322.
After 13580 training step(s), loss on training batch is 0.00521361.
After 13581 training step(s), loss on training batch is 0.00519381.
After 13582 training step(s), loss on training batch is 0.0061635.
After 13583 training step(s), loss on training batch is 0.00545076.
After 13584 training step(s), loss on training batch is 0.00518894.
After 13585 training step(s), loss on training batch is 0.00666996.
After 13586 training step(s), loss on training batch is 0.00522448.
After 13587 training step(s), loss on training batch is 0.00568813.
After 13588 training step(s), loss on training batch is 0.00525944.
After 13589 training step(s), loss on training batch is 0.00605434.
After 13590 training step(s), loss on training batch is 0.00501562.
After 13591 training step(s), loss on training batch is 0.00505727.
After 13592 training step(s), loss on training batch is 0.00546748.
After 13593 training step(s), loss on training batch is 0.00518468.
After 13594 training step(s), loss on training batch is 0.0050438.
After 13595 training step(s), loss on training batch is 0.00518717.
After 13596 training step(s), loss on training batch is 0.00679957.
After 13597 training step(s), loss on training batch is 0.0055246.
After 13598 training step(s), loss on training batch is 0.00578623.
After 13599 training step(s), loss on training batch is 0.00535059.
After 13600 training step(s), loss on training batch is 0.00552629.
After 13601 training step(s), loss on training batch is 0.00503648.
After 13602 training step(s), loss on training batch is 0.00579357.
After 13603 training step(s), loss on training batch is 0.00555016.
After 13604 training step(s), loss on training batch is 0.00519793.
After 13605 training step(s), loss on training batch is 0.00490945.
After 13606 training step(s), loss on training batch is 0.00584442.
After 13607 training step(s), loss on training batch is 0.00566247.
After 13608 training step(s), loss on training batch is 0.00580671.
After 13609 training step(s), loss on training batch is 0.00576112.
After 13610 training step(s), loss on training batch is 0.00519858.
After 13611 training step(s), loss on training batch is 0.00605351.
After 13612 training step(s), loss on training batch is 0.00516848.
After 13613 training step(s), loss on training batch is 0.00540201.
After 13614 training step(s), loss on training batch is 0.00550023.
After 13615 training step(s), loss on training batch is 0.00603937.
After 13616 training step(s), loss on training batch is 0.0058079.
After 13617 training step(s), loss on training batch is 0.00540901.
After 13618 training step(s), loss on training batch is 0.00532265.
After 13619 training step(s), loss on training batch is 0.00615201.
After 13620 training step(s), loss on training batch is 0.00620332.
After 13621 training step(s), loss on training batch is 0.00550416.
After 13622 training step(s), loss on training batch is 0.00549293.
After 13623 training step(s), loss on training batch is 0.00580103.
After 13624 training step(s), loss on training batch is 0.00581137.
After 13625 training step(s), loss on training batch is 0.00493641.
After 13626 training step(s), loss on training batch is 0.00573873.
After 13627 training step(s), loss on training batch is 0.00516436.
After 13628 training step(s), loss on training batch is 0.00565131.
After 13629 training step(s), loss on training batch is 0.00502899.
After 13630 training step(s), loss on training batch is 0.00561279.
After 13631 training step(s), loss on training batch is 0.00742397.
After 13632 training step(s), loss on training batch is 0.00570867.
After 13633 training step(s), loss on training batch is 0.00542445.
After 13634 training step(s), loss on training batch is 0.00577887.
After 13635 training step(s), loss on training batch is 0.00599816.
After 13636 training step(s), loss on training batch is 0.00630816.
After 13637 training step(s), loss on training batch is 0.00567736.
After 13638 training step(s), loss on training batch is 0.00516844.
After 13639 training step(s), loss on training batch is 0.00616498.
After 13640 training step(s), loss on training batch is 0.0058492.
After 13641 training step(s), loss on training batch is 0.00564599.
After 13642 training step(s), loss on training batch is 0.00519897.
After 13643 training step(s), loss on training batch is 0.0102271.
After 13644 training step(s), loss on training batch is 0.00698961.
After 13645 training step(s), loss on training batch is 0.0107223.
After 13646 training step(s), loss on training batch is 0.00615858.
After 13647 training step(s), loss on training batch is 0.00901624.
After 13648 training step(s), loss on training batch is 0.00518461.
After 13649 training step(s), loss on training batch is 0.00570054.
After 13650 training step(s), loss on training batch is 0.00579377.
After 13651 training step(s), loss on training batch is 0.00556587.
After 13652 training step(s), loss on training batch is 0.00577288.
After 13653 training step(s), loss on training batch is 0.00562556.
After 13654 training step(s), loss on training batch is 0.00538379.
After 13655 training step(s), loss on training batch is 0.00668083.
After 13656 training step(s), loss on training batch is 0.00594068.
After 13657 training step(s), loss on training batch is 0.00625673.
After 13658 training step(s), loss on training batch is 0.00547893.
After 13659 training step(s), loss on training batch is 0.00503787.
After 13660 training step(s), loss on training batch is 0.00526923.
After 13661 training step(s), loss on training batch is 0.00538306.
After 13662 training step(s), loss on training batch is 0.00755581.
After 13663 training step(s), loss on training batch is 0.00708622.
After 13664 training step(s), loss on training batch is 0.00510341.
After 13665 training step(s), loss on training batch is 0.00544117.
After 13666 training step(s), loss on training batch is 0.00645.
After 13667 training step(s), loss on training batch is 0.00554258.
After 13668 training step(s), loss on training batch is 0.00537913.
After 13669 training step(s), loss on training batch is 0.00535458.
After 13670 training step(s), loss on training batch is 0.00520357.
After 13671 training step(s), loss on training batch is 0.00743852.
After 13672 training step(s), loss on training batch is 0.00525004.
After 13673 training step(s), loss on training batch is 0.00513444.
After 13674 training step(s), loss on training batch is 0.00489498.
After 13675 training step(s), loss on training batch is 0.00538417.
After 13676 training step(s), loss on training batch is 0.00590082.
After 13677 training step(s), loss on training batch is 0.00650798.
After 13678 training step(s), loss on training batch is 0.00575005.
After 13679 training step(s), loss on training batch is 0.00573864.
After 13680 training step(s), loss on training batch is 0.00589132.
After 13681 training step(s), loss on training batch is 0.00602983.
After 13682 training step(s), loss on training batch is 0.0052205.
After 13683 training step(s), loss on training batch is 0.00567023.
After 13684 training step(s), loss on training batch is 0.00493971.
After 13685 training step(s), loss on training batch is 0.0052219.
After 13686 training step(s), loss on training batch is 0.00535756.
After 13687 training step(s), loss on training batch is 0.00553025.
After 13688 training step(s), loss on training batch is 0.00557672.
After 13689 training step(s), loss on training batch is 0.00590953.
After 13690 training step(s), loss on training batch is 0.00551738.
After 13691 training step(s), loss on training batch is 0.00521105.
After 13692 training step(s), loss on training batch is 0.00626576.
After 13693 training step(s), loss on training batch is 0.00508463.
After 13694 training step(s), loss on training batch is 0.00752014.
After 13695 training step(s), loss on training batch is 0.00556156.
After 13696 training step(s), loss on training batch is 0.00510244.
After 13697 training step(s), loss on training batch is 0.0060272.
After 13698 training step(s), loss on training batch is 0.00601738.
After 13699 training step(s), loss on training batch is 0.00530878.
After 13700 training step(s), loss on training batch is 0.0057156.
After 13701 training step(s), loss on training batch is 0.00500247.
After 13702 training step(s), loss on training batch is 0.00543432.
After 13703 training step(s), loss on training batch is 0.00604969.
After 13704 training step(s), loss on training batch is 0.00517969.
After 13705 training step(s), loss on training batch is 0.00638989.
After 13706 training step(s), loss on training batch is 0.00498022.
After 13707 training step(s), loss on training batch is 0.00542885.
After 13708 training step(s), loss on training batch is 0.00527618.
After 13709 training step(s), loss on training batch is 0.00612645.
After 13710 training step(s), loss on training batch is 0.00502858.
After 13711 training step(s), loss on training batch is 0.00526188.
After 13712 training step(s), loss on training batch is 0.00533712.
After 13713 training step(s), loss on training batch is 0.00490726.
After 13714 training step(s), loss on training batch is 0.00509656.
After 13715 training step(s), loss on training batch is 0.00533306.
After 13716 training step(s), loss on training batch is 0.00552057.
After 13717 training step(s), loss on training batch is 0.00501767.
After 13718 training step(s), loss on training batch is 0.00564191.
After 13719 training step(s), loss on training batch is 0.00555146.
After 13720 training step(s), loss on training batch is 0.00657816.
After 13721 training step(s), loss on training batch is 0.00499784.
After 13722 training step(s), loss on training batch is 0.00499148.
After 13723 training step(s), loss on training batch is 0.0056008.
After 13724 training step(s), loss on training batch is 0.00631554.
After 13725 training step(s), loss on training batch is 0.00613967.
After 13726 training step(s), loss on training batch is 0.00536.
After 13727 training step(s), loss on training batch is 0.00671207.
After 13728 training step(s), loss on training batch is 0.00535205.
After 13729 training step(s), loss on training batch is 0.0051994.
After 13730 training step(s), loss on training batch is 0.0057105.
After 13731 training step(s), loss on training batch is 0.00512921.
After 13732 training step(s), loss on training batch is 0.0055042.
After 13733 training step(s), loss on training batch is 0.00572448.
After 13734 training step(s), loss on training batch is 0.00562937.
After 13735 training step(s), loss on training batch is 0.00571148.
After 13736 training step(s), loss on training batch is 0.00550753.
After 13737 training step(s), loss on training batch is 0.0050246.
After 13738 training step(s), loss on training batch is 0.00549055.
After 13739 training step(s), loss on training batch is 0.00543388.
After 13740 training step(s), loss on training batch is 0.00575922.
After 13741 training step(s), loss on training batch is 0.0058536.
After 13742 training step(s), loss on training batch is 0.00540217.
After 13743 training step(s), loss on training batch is 0.0057778.
After 13744 training step(s), loss on training batch is 0.00504241.
After 13745 training step(s), loss on training batch is 0.00534377.
After 13746 training step(s), loss on training batch is 0.00529826.
After 13747 training step(s), loss on training batch is 0.0056197.
After 13748 training step(s), loss on training batch is 0.0118596.
After 13749 training step(s), loss on training batch is 0.0183932.
After 13750 training step(s), loss on training batch is 0.00771151.
After 13751 training step(s), loss on training batch is 0.00542169.
After 13752 training step(s), loss on training batch is 0.00530214.
After 13753 training step(s), loss on training batch is 0.0055129.
After 13754 training step(s), loss on training batch is 0.00530898.
After 13755 training step(s), loss on training batch is 0.00519976.
After 13756 training step(s), loss on training batch is 0.00700381.
After 13757 training step(s), loss on training batch is 0.00568356.
After 13758 training step(s), loss on training batch is 0.00503656.
After 13759 training step(s), loss on training batch is 0.005177.
After 13760 training step(s), loss on training batch is 0.00631663.
After 13761 training step(s), loss on training batch is 0.00596968.
After 13762 training step(s), loss on training batch is 0.0050831.
After 13763 training step(s), loss on training batch is 0.00571631.
After 13764 training step(s), loss on training batch is 0.0055642.
After 13765 training step(s), loss on training batch is 0.00542776.
After 13766 training step(s), loss on training batch is 0.00489978.
After 13767 training step(s), loss on training batch is 0.00512435.
After 13768 training step(s), loss on training batch is 0.00582349.
After 13769 training step(s), loss on training batch is 0.00695055.
After 13770 training step(s), loss on training batch is 0.00600014.
After 13771 training step(s), loss on training batch is 0.00504577.
After 13772 training step(s), loss on training batch is 0.00549117.
After 13773 training step(s), loss on training batch is 0.00846161.
After 13774 training step(s), loss on training batch is 0.00526299.
After 13775 training step(s), loss on training batch is 0.00558568.
After 13776 training step(s), loss on training batch is 0.00524506.
After 13777 training step(s), loss on training batch is 0.00554498.
After 13778 training step(s), loss on training batch is 0.00485332.
After 13779 training step(s), loss on training batch is 0.00543992.
After 13780 training step(s), loss on training batch is 0.00552898.
After 13781 training step(s), loss on training batch is 0.00489588.
After 13782 training step(s), loss on training batch is 0.00513083.
After 13783 training step(s), loss on training batch is 0.00558496.
After 13784 training step(s), loss on training batch is 0.00570634.
After 13785 training step(s), loss on training batch is 0.00542034.
After 13786 training step(s), loss on training batch is 0.00673146.
After 13787 training step(s), loss on training batch is 0.00633574.
After 13788 training step(s), loss on training batch is 0.00509728.
After 13789 training step(s), loss on training batch is 0.00564143.
After 13790 training step(s), loss on training batch is 0.00526496.
After 13791 training step(s), loss on training batch is 0.00502592.
After 13792 training step(s), loss on training batch is 0.0060318.
After 13793 training step(s), loss on training batch is 0.00531289.
After 13794 training step(s), loss on training batch is 0.00504836.
After 13795 training step(s), loss on training batch is 0.00524516.
After 13796 training step(s), loss on training batch is 0.00519519.
After 13797 training step(s), loss on training batch is 0.00562276.
After 13798 training step(s), loss on training batch is 0.00556736.
After 13799 training step(s), loss on training batch is 0.00535637.
After 13800 training step(s), loss on training batch is 0.0052175.
After 13801 training step(s), loss on training batch is 0.00542558.
After 13802 training step(s), loss on training batch is 0.00688314.
After 13803 training step(s), loss on training batch is 0.00640788.
After 13804 training step(s), loss on training batch is 0.0050906.
After 13805 training step(s), loss on training batch is 0.00572479.
After 13806 training step(s), loss on training batch is 0.00537707.
After 13807 training step(s), loss on training batch is 0.00563539.
After 13808 training step(s), loss on training batch is 0.00598148.
After 13809 training step(s), loss on training batch is 0.00517968.
After 13810 training step(s), loss on training batch is 0.00644177.
After 13811 training step(s), loss on training batch is 0.0050999.
After 13812 training step(s), loss on training batch is 0.00484125.
After 13813 training step(s), loss on training batch is 0.00717093.
After 13814 training step(s), loss on training batch is 0.00529657.
After 13815 training step(s), loss on training batch is 0.00515903.
After 13816 training step(s), loss on training batch is 0.00513711.
After 13817 training step(s), loss on training batch is 0.00546867.
After 13818 training step(s), loss on training batch is 0.00899417.
After 13819 training step(s), loss on training batch is 0.00493576.
After 13820 training step(s), loss on training batch is 0.00507936.
After 13821 training step(s), loss on training batch is 0.00523261.
After 13822 training step(s), loss on training batch is 0.00573434.
After 13823 training step(s), loss on training batch is 0.00530808.
After 13824 training step(s), loss on training batch is 0.00603381.
After 13825 training step(s), loss on training batch is 0.00512773.
After 13826 training step(s), loss on training batch is 0.00510449.
After 13827 training step(s), loss on training batch is 0.00645822.
After 13828 training step(s), loss on training batch is 0.0053928.
After 13829 training step(s), loss on training batch is 0.00494434.
After 13830 training step(s), loss on training batch is 0.0068079.
After 13831 training step(s), loss on training batch is 0.00501372.
After 13832 training step(s), loss on training batch is 0.00634157.
After 13833 training step(s), loss on training batch is 0.00551135.
After 13834 training step(s), loss on training batch is 0.00587214.
After 13835 training step(s), loss on training batch is 0.00517474.
After 13836 training step(s), loss on training batch is 0.00541734.
After 13837 training step(s), loss on training batch is 0.00562834.
After 13838 training step(s), loss on training batch is 0.00795822.
After 13839 training step(s), loss on training batch is 0.0051023.
After 13840 training step(s), loss on training batch is 0.00529788.
After 13841 training step(s), loss on training batch is 0.00511975.
After 13842 training step(s), loss on training batch is 0.00784246.
After 13843 training step(s), loss on training batch is 0.00518574.
After 13844 training step(s), loss on training batch is 0.00601837.
After 13845 training step(s), loss on training batch is 0.00522715.
After 13846 training step(s), loss on training batch is 0.00525224.
After 13847 training step(s), loss on training batch is 0.00558686.
After 13848 training step(s), loss on training batch is 0.00553356.
After 13849 training step(s), loss on training batch is 0.00570353.
After 13850 training step(s), loss on training batch is 0.00526047.
After 13851 training step(s), loss on training batch is 0.00584842.
After 13852 training step(s), loss on training batch is 0.00555719.
After 13853 training step(s), loss on training batch is 0.0052423.
After 13854 training step(s), loss on training batch is 0.00559338.
After 13855 training step(s), loss on training batch is 0.00510476.
After 13856 training step(s), loss on training batch is 0.00569738.
After 13857 training step(s), loss on training batch is 0.00501376.
After 13858 training step(s), loss on training batch is 0.00537561.
After 13859 training step(s), loss on training batch is 0.00575224.
After 13860 training step(s), loss on training batch is 0.00606167.
After 13861 training step(s), loss on training batch is 0.00534991.
After 13862 training step(s), loss on training batch is 0.00498335.
After 13863 training step(s), loss on training batch is 0.00502676.
After 13864 training step(s), loss on training batch is 0.00595353.
After 13865 training step(s), loss on training batch is 0.00631839.
After 13866 training step(s), loss on training batch is 0.00522537.
After 13867 training step(s), loss on training batch is 0.00492209.
After 13868 training step(s), loss on training batch is 0.0071127.
After 13869 training step(s), loss on training batch is 0.00961488.
After 13870 training step(s), loss on training batch is 0.0065694.
After 13871 training step(s), loss on training batch is 0.00566046.
After 13872 training step(s), loss on training batch is 0.00481699.
After 13873 training step(s), loss on training batch is 0.00487901.
After 13874 training step(s), loss on training batch is 0.00651382.
After 13875 training step(s), loss on training batch is 0.00527508.
After 13876 training step(s), loss on training batch is 0.00532496.
After 13877 training step(s), loss on training batch is 0.00512884.
After 13878 training step(s), loss on training batch is 0.00503447.
After 13879 training step(s), loss on training batch is 0.00555473.
After 13880 training step(s), loss on training batch is 0.00508855.
After 13881 training step(s), loss on training batch is 0.00501645.
After 13882 training step(s), loss on training batch is 0.00583947.
After 13883 training step(s), loss on training batch is 0.00519776.
After 13884 training step(s), loss on training batch is 0.0051019.
After 13885 training step(s), loss on training batch is 0.00577341.
After 13886 training step(s), loss on training batch is 0.00479471.
After 13887 training step(s), loss on training batch is 0.00537187.
After 13888 training step(s), loss on training batch is 0.00609697.
After 13889 training step(s), loss on training batch is 0.00494132.
After 13890 training step(s), loss on training batch is 0.00566718.
After 13891 training step(s), loss on training batch is 0.00541074.
After 13892 training step(s), loss on training batch is 0.0051521.
After 13893 training step(s), loss on training batch is 0.00512204.
After 13894 training step(s), loss on training batch is 0.00515512.
After 13895 training step(s), loss on training batch is 0.00605539.
After 13896 training step(s), loss on training batch is 0.00530079.
After 13897 training step(s), loss on training batch is 0.00522142.
After 13898 training step(s), loss on training batch is 0.0053724.
After 13899 training step(s), loss on training batch is 0.005575.
After 13900 training step(s), loss on training batch is 0.00614029.
After 13901 training step(s), loss on training batch is 0.00653296.
After 13902 training step(s), loss on training batch is 0.00575694.
After 13903 training step(s), loss on training batch is 0.0053644.
After 13904 training step(s), loss on training batch is 0.0055743.
After 13905 training step(s), loss on training batch is 0.00644218.
After 13906 training step(s), loss on training batch is 0.00521243.
After 13907 training step(s), loss on training batch is 0.00617667.
After 13908 training step(s), loss on training batch is 0.00544246.
After 13909 training step(s), loss on training batch is 0.00524304.
After 13910 training step(s), loss on training batch is 0.00493248.
After 13911 training step(s), loss on training batch is 0.0054882.
After 13912 training step(s), loss on training batch is 0.0054312.
After 13913 training step(s), loss on training batch is 0.00550961.
After 13914 training step(s), loss on training batch is 0.0048673.
After 13915 training step(s), loss on training batch is 0.00537612.
After 13916 training step(s), loss on training batch is 0.00561236.
After 13917 training step(s), loss on training batch is 0.00554124.
After 13918 training step(s), loss on training batch is 0.00487958.
After 13919 training step(s), loss on training batch is 0.00624104.
After 13920 training step(s), loss on training batch is 0.00499546.
After 13921 training step(s), loss on training batch is 0.0060815.
After 13922 training step(s), loss on training batch is 0.00673746.
After 13923 training step(s), loss on training batch is 0.00496254.
After 13924 training step(s), loss on training batch is 0.00673186.
After 13925 training step(s), loss on training batch is 0.00503367.
After 13926 training step(s), loss on training batch is 0.00540252.
After 13927 training step(s), loss on training batch is 0.00534895.
After 13928 training step(s), loss on training batch is 0.0065787.
After 13929 training step(s), loss on training batch is 0.00536131.
After 13930 training step(s), loss on training batch is 0.00553755.
After 13931 training step(s), loss on training batch is 0.00610495.
After 13932 training step(s), loss on training batch is 0.00571378.
After 13933 training step(s), loss on training batch is 0.00599063.
After 13934 training step(s), loss on training batch is 0.00675901.
After 13935 training step(s), loss on training batch is 0.00513309.
After 13936 training step(s), loss on training batch is 0.00524553.
After 13937 training step(s), loss on training batch is 0.00523564.
After 13938 training step(s), loss on training batch is 0.00639595.
After 13939 training step(s), loss on training batch is 0.00502302.
After 13940 training step(s), loss on training batch is 0.00520233.
After 13941 training step(s), loss on training batch is 0.00524072.
After 13942 training step(s), loss on training batch is 0.00547932.
After 13943 training step(s), loss on training batch is 0.00608016.
After 13944 training step(s), loss on training batch is 0.00556514.
After 13945 training step(s), loss on training batch is 0.00521628.
After 13946 training step(s), loss on training batch is 0.00591311.
After 13947 training step(s), loss on training batch is 0.00547364.
After 13948 training step(s), loss on training batch is 0.00651955.
After 13949 training step(s), loss on training batch is 0.00471509.
After 13950 training step(s), loss on training batch is 0.00514413.
After 13951 training step(s), loss on training batch is 0.00509176.
After 13952 training step(s), loss on training batch is 0.00495503.
After 13953 training step(s), loss on training batch is 0.00542986.
After 13954 training step(s), loss on training batch is 0.00555037.
After 13955 training step(s), loss on training batch is 0.00534347.
After 13956 training step(s), loss on training batch is 0.00525422.
After 13957 training step(s), loss on training batch is 0.00544927.
After 13958 training step(s), loss on training batch is 0.0049254.
After 13959 training step(s), loss on training batch is 0.00624188.
After 13960 training step(s), loss on training batch is 0.00549269.
After 13961 training step(s), loss on training batch is 0.00517349.
After 13962 training step(s), loss on training batch is 0.00496284.
After 13963 training step(s), loss on training batch is 0.00563667.
After 13964 training step(s), loss on training batch is 0.00508637.
After 13965 training step(s), loss on training batch is 0.00503732.
After 13966 training step(s), loss on training batch is 0.00638732.
After 13967 training step(s), loss on training batch is 0.00576795.
After 13968 training step(s), loss on training batch is 0.00568676.
After 13969 training step(s), loss on training batch is 0.00615203.
After 13970 training step(s), loss on training batch is 0.00575437.
After 13971 training step(s), loss on training batch is 0.00685305.
After 13972 training step(s), loss on training batch is 0.00584901.
After 13973 training step(s), loss on training batch is 0.0060973.
After 13974 training step(s), loss on training batch is 0.00498697.
After 13975 training step(s), loss on training batch is 0.00612713.
After 13976 training step(s), loss on training batch is 0.00500268.
After 13977 training step(s), loss on training batch is 0.00529931.
After 13978 training step(s), loss on training batch is 0.00496505.
After 13979 training step(s), loss on training batch is 0.00605936.
After 13980 training step(s), loss on training batch is 0.00502634.
After 13981 training step(s), loss on training batch is 0.00598358.
After 13982 training step(s), loss on training batch is 0.00516493.
After 13983 training step(s), loss on training batch is 0.00628586.
After 13984 training step(s), loss on training batch is 0.00488528.
After 13985 training step(s), loss on training batch is 0.00524568.
After 13986 training step(s), loss on training batch is 0.00603568.
After 13987 training step(s), loss on training batch is 0.00477288.
After 13988 training step(s), loss on training batch is 0.00538534.
After 13989 training step(s), loss on training batch is 0.00518598.
After 13990 training step(s), loss on training batch is 0.00515774.
After 13991 training step(s), loss on training batch is 0.00516334.
After 13992 training step(s), loss on training batch is 0.00485167.
After 13993 training step(s), loss on training batch is 0.00633491.
After 13994 training step(s), loss on training batch is 0.0100616.
After 13995 training step(s), loss on training batch is 0.00680345.
After 13996 training step(s), loss on training batch is 0.00514339.
After 13997 training step(s), loss on training batch is 0.00524744.
After 13998 training step(s), loss on training batch is 0.00525866.
After 13999 training step(s), loss on training batch is 0.00582555.
After 14000 training step(s), loss on training batch is 0.00530073.
After 14001 training step(s), loss on training batch is 0.00491481.
After 14002 training step(s), loss on training batch is 0.00548897.
After 14003 training step(s), loss on training batch is 0.00556218.
After 14004 training step(s), loss on training batch is 0.00575369.
After 14005 training step(s), loss on training batch is 0.00473788.
After 14006 training step(s), loss on training batch is 0.00571028.
After 14007 training step(s), loss on training batch is 0.00531635.
After 14008 training step(s), loss on training batch is 0.00726714.
After 14009 training step(s), loss on training batch is 0.00557235.
After 14010 training step(s), loss on training batch is 0.00509872.
After 14011 training step(s), loss on training batch is 0.0055478.
After 14012 training step(s), loss on training batch is 0.00555027.
After 14013 training step(s), loss on training batch is 0.00554186.
After 14014 training step(s), loss on training batch is 0.00496388.
After 14015 training step(s), loss on training batch is 0.00518199.
After 14016 training step(s), loss on training batch is 0.00571162.
After 14017 training step(s), loss on training batch is 0.0079071.
After 14018 training step(s), loss on training batch is 0.00570916.
After 14019 training step(s), loss on training batch is 0.00539662.
After 14020 training step(s), loss on training batch is 0.00568723.
After 14021 training step(s), loss on training batch is 0.00516517.
After 14022 training step(s), loss on training batch is 0.00576323.
After 14023 training step(s), loss on training batch is 0.00483311.
After 14024 training step(s), loss on training batch is 0.00576495.
After 14025 training step(s), loss on training batch is 0.00549092.
After 14026 training step(s), loss on training batch is 0.00845268.
After 14027 training step(s), loss on training batch is 0.00519188.
After 14028 training step(s), loss on training batch is 0.00566329.
After 14029 training step(s), loss on training batch is 0.0136659.
After 14030 training step(s), loss on training batch is 0.00591587.
After 14031 training step(s), loss on training batch is 0.00491659.
After 14032 training step(s), loss on training batch is 0.00629341.
After 14033 training step(s), loss on training batch is 0.0051898.
After 14034 training step(s), loss on training batch is 0.00483277.
After 14035 training step(s), loss on training batch is 0.0060764.
After 14036 training step(s), loss on training batch is 0.00520912.
After 14037 training step(s), loss on training batch is 0.00552587.
After 14038 training step(s), loss on training batch is 0.00482284.
After 14039 training step(s), loss on training batch is 0.00496719.
After 14040 training step(s), loss on training batch is 0.0050295.
After 14041 training step(s), loss on training batch is 0.00575116.
After 14042 training step(s), loss on training batch is 0.00559532.
After 14043 training step(s), loss on training batch is 0.00499466.
After 14044 training step(s), loss on training batch is 0.00501369.
After 14045 training step(s), loss on training batch is 0.00536181.
After 14046 training step(s), loss on training batch is 0.00525178.
After 14047 training step(s), loss on training batch is 0.00542297.
After 14048 training step(s), loss on training batch is 0.00529996.
After 14049 training step(s), loss on training batch is 0.00532548.
After 14050 training step(s), loss on training batch is 0.00532462.
After 14051 training step(s), loss on training batch is 0.00509444.
After 14052 training step(s), loss on training batch is 0.00487203.
After 14053 training step(s), loss on training batch is 0.0060131.
After 14054 training step(s), loss on training batch is 0.00502521.
After 14055 training step(s), loss on training batch is 0.00572594.
After 14056 training step(s), loss on training batch is 0.00501681.
After 14057 training step(s), loss on training batch is 0.0054199.
After 14058 training step(s), loss on training batch is 0.00538114.
After 14059 training step(s), loss on training batch is 0.00613665.
After 14060 training step(s), loss on training batch is 0.0053586.
After 14061 training step(s), loss on training batch is 0.00510943.
After 14062 training step(s), loss on training batch is 0.0052053.
After 14063 training step(s), loss on training batch is 0.00590269.
After 14064 training step(s), loss on training batch is 0.00505731.
After 14065 training step(s), loss on training batch is 0.00570332.
After 14066 training step(s), loss on training batch is 0.0052376.
After 14067 training step(s), loss on training batch is 0.00518303.
After 14068 training step(s), loss on training batch is 0.00496397.
After 14069 training step(s), loss on training batch is 0.00481231.
After 14070 training step(s), loss on training batch is 0.00531701.
After 14071 training step(s), loss on training batch is 0.0047335.
After 14072 training step(s), loss on training batch is 0.00605921.
After 14073 training step(s), loss on training batch is 0.00552273.
After 14074 training step(s), loss on training batch is 0.00578675.
After 14075 training step(s), loss on training batch is 0.00527173.
After 14076 training step(s), loss on training batch is 0.0055612.
After 14077 training step(s), loss on training batch is 0.00517627.
After 14078 training step(s), loss on training batch is 0.00622244.
After 14079 training step(s), loss on training batch is 0.00561624.
After 14080 training step(s), loss on training batch is 0.00557031.
After 14081 training step(s), loss on training batch is 0.00494414.
After 14082 training step(s), loss on training batch is 0.00482776.
After 14083 training step(s), loss on training batch is 0.0047842.
After 14084 training step(s), loss on training batch is 0.00553918.
After 14085 training step(s), loss on training batch is 0.00467584.
After 14086 training step(s), loss on training batch is 0.0050714.
After 14087 training step(s), loss on training batch is 0.00532126.
After 14088 training step(s), loss on training batch is 0.00491387.
After 14089 training step(s), loss on training batch is 0.00569447.
After 14090 training step(s), loss on training batch is 0.00658841.
After 14091 training step(s), loss on training batch is 0.00590331.
After 14092 training step(s), loss on training batch is 0.00556075.
After 14093 training step(s), loss on training batch is 0.00715052.
After 14094 training step(s), loss on training batch is 0.00488546.
After 14095 training step(s), loss on training batch is 0.00517335.
After 14096 training step(s), loss on training batch is 0.00521022.
After 14097 training step(s), loss on training batch is 0.00519271.
After 14098 training step(s), loss on training batch is 0.00496453.
After 14099 training step(s), loss on training batch is 0.00480038.
After 14100 training step(s), loss on training batch is 0.00493187.
After 14101 training step(s), loss on training batch is 0.00507338.
After 14102 training step(s), loss on training batch is 0.00543177.
After 14103 training step(s), loss on training batch is 0.0050228.
After 14104 training step(s), loss on training batch is 0.00579406.
After 14105 training step(s), loss on training batch is 0.0055185.
After 14106 training step(s), loss on training batch is 0.00558643.
After 14107 training step(s), loss on training batch is 0.00521173.
After 14108 training step(s), loss on training batch is 0.00500166.
After 14109 training step(s), loss on training batch is 0.00562567.
After 14110 training step(s), loss on training batch is 0.0049517.
After 14111 training step(s), loss on training batch is 0.00539837.
After 14112 training step(s), loss on training batch is 0.00587248.
After 14113 training step(s), loss on training batch is 0.00514214.
After 14114 training step(s), loss on training batch is 0.00548675.
After 14115 training step(s), loss on training batch is 0.0053604.
After 14116 training step(s), loss on training batch is 0.0050044.
After 14117 training step(s), loss on training batch is 0.00525457.
After 14118 training step(s), loss on training batch is 0.00559105.
After 14119 training step(s), loss on training batch is 0.00522598.
After 14120 training step(s), loss on training batch is 0.00554165.
After 14121 training step(s), loss on training batch is 0.00499908.
After 14122 training step(s), loss on training batch is 0.00536085.
After 14123 training step(s), loss on training batch is 0.00483137.
After 14124 training step(s), loss on training batch is 0.00497479.
After 14125 training step(s), loss on training batch is 0.00505254.
After 14126 training step(s), loss on training batch is 0.00493115.
After 14127 training step(s), loss on training batch is 0.00509295.
After 14128 training step(s), loss on training batch is 0.0058424.
After 14129 training step(s), loss on training batch is 0.00564475.
After 14130 training step(s), loss on training batch is 0.00557129.
After 14131 training step(s), loss on training batch is 0.00503509.
After 14132 training step(s), loss on training batch is 0.0055443.
After 14133 training step(s), loss on training batch is 0.00552634.
After 14134 training step(s), loss on training batch is 0.00888434.
After 14135 training step(s), loss on training batch is 0.00658251.
After 14136 training step(s), loss on training batch is 0.00598387.
After 14137 training step(s), loss on training batch is 0.00541186.
After 14138 training step(s), loss on training batch is 0.0054387.
After 14139 training step(s), loss on training batch is 0.00606648.
After 14140 training step(s), loss on training batch is 0.00605113.
After 14141 training step(s), loss on training batch is 0.00552836.
After 14142 training step(s), loss on training batch is 0.00505378.
After 14143 training step(s), loss on training batch is 0.00546199.
After 14144 training step(s), loss on training batch is 0.00588367.
After 14145 training step(s), loss on training batch is 0.00504118.
After 14146 training step(s), loss on training batch is 0.00541355.
After 14147 training step(s), loss on training batch is 0.00502638.
After 14148 training step(s), loss on training batch is 0.00500529.
After 14149 training step(s), loss on training batch is 0.00528385.
After 14150 training step(s), loss on training batch is 0.00553096.
After 14151 training step(s), loss on training batch is 0.00529803.
After 14152 training step(s), loss on training batch is 0.00553419.
After 14153 training step(s), loss on training batch is 0.00547028.
After 14154 training step(s), loss on training batch is 0.00550047.
After 14155 training step(s), loss on training batch is 0.00592884.
After 14156 training step(s), loss on training batch is 0.00577625.
After 14157 training step(s), loss on training batch is 0.00465841.
After 14158 training step(s), loss on training batch is 0.00532522.
After 14159 training step(s), loss on training batch is 0.00565761.
After 14160 training step(s), loss on training batch is 0.00582097.
After 14161 training step(s), loss on training batch is 0.00569805.
After 14162 training step(s), loss on training batch is 0.00500165.
After 14163 training step(s), loss on training batch is 0.00547165.
After 14164 training step(s), loss on training batch is 0.00570604.
After 14165 training step(s), loss on training batch is 0.00528254.
After 14166 training step(s), loss on training batch is 0.00540403.
After 14167 training step(s), loss on training batch is 0.00558527.
After 14168 training step(s), loss on training batch is 0.00541685.
After 14169 training step(s), loss on training batch is 0.0050851.
After 14170 training step(s), loss on training batch is 0.00488956.
After 14171 training step(s), loss on training batch is 0.00521679.
After 14172 training step(s), loss on training batch is 0.00523717.
After 14173 training step(s), loss on training batch is 0.00473281.
After 14174 training step(s), loss on training batch is 0.00562825.
After 14175 training step(s), loss on training batch is 0.00516075.
After 14176 training step(s), loss on training batch is 0.00478153.
After 14177 training step(s), loss on training batch is 0.00483544.
After 14178 training step(s), loss on training batch is 0.00545465.
After 14179 training step(s), loss on training batch is 0.00532683.
After 14180 training step(s), loss on training batch is 0.00513203.
After 14181 training step(s), loss on training batch is 0.00482544.
After 14182 training step(s), loss on training batch is 0.00599878.
After 14183 training step(s), loss on training batch is 0.00531486.
After 14184 training step(s), loss on training batch is 0.00495933.
After 14185 training step(s), loss on training batch is 0.00478768.
After 14186 training step(s), loss on training batch is 0.00579996.
After 14187 training step(s), loss on training batch is 0.00511802.
After 14188 training step(s), loss on training batch is 0.00496901.
After 14189 training step(s), loss on training batch is 0.00586303.
After 14190 training step(s), loss on training batch is 0.00539205.
After 14191 training step(s), loss on training batch is 0.00538054.
After 14192 training step(s), loss on training batch is 0.00528603.
After 14193 training step(s), loss on training batch is 0.0050783.
After 14194 training step(s), loss on training batch is 0.00507837.
After 14195 training step(s), loss on training batch is 0.0056141.
After 14196 training step(s), loss on training batch is 0.00537269.
After 14197 training step(s), loss on training batch is 0.00502967.
After 14198 training step(s), loss on training batch is 0.0053253.
After 14199 training step(s), loss on training batch is 0.00485223.
After 14200 training step(s), loss on training batch is 0.00568442.
After 14201 training step(s), loss on training batch is 0.0049272.
After 14202 training step(s), loss on training batch is 0.00738776.
After 14203 training step(s), loss on training batch is 0.00706806.
After 14204 training step(s), loss on training batch is 0.00958375.
After 14205 training step(s), loss on training batch is 0.0049947.
After 14206 training step(s), loss on training batch is 0.0152203.
After 14207 training step(s), loss on training batch is 0.00856592.
After 14208 training step(s), loss on training batch is 0.00639796.
After 14209 training step(s), loss on training batch is 0.00580981.
After 14210 training step(s), loss on training batch is 0.00612324.
After 14211 training step(s), loss on training batch is 0.00671698.
After 14212 training step(s), loss on training batch is 0.00550502.
After 14213 training step(s), loss on training batch is 0.00498838.
After 14214 training step(s), loss on training batch is 0.0052565.
After 14215 training step(s), loss on training batch is 0.00553711.
After 14216 training step(s), loss on training batch is 0.00555123.
After 14217 training step(s), loss on training batch is 0.0050846.
After 14218 training step(s), loss on training batch is 0.00526087.
After 14219 training step(s), loss on training batch is 0.0060477.
After 14220 training step(s), loss on training batch is 0.00480493.
After 14221 training step(s), loss on training batch is 0.00542506.
After 14222 training step(s), loss on training batch is 0.0052104.
After 14223 training step(s), loss on training batch is 0.00635727.
After 14224 training step(s), loss on training batch is 0.00560207.
After 14225 training step(s), loss on training batch is 0.00505532.
After 14226 training step(s), loss on training batch is 0.00580673.
After 14227 training step(s), loss on training batch is 0.00556347.
After 14228 training step(s), loss on training batch is 0.00494839.
After 14229 training step(s), loss on training batch is 0.0049824.
After 14230 training step(s), loss on training batch is 0.0054058.
After 14231 training step(s), loss on training batch is 0.0108628.
After 14232 training step(s), loss on training batch is 0.00478471.
After 14233 training step(s), loss on training batch is 0.0290407.
After 14234 training step(s), loss on training batch is 0.0382909.
After 14235 training step(s), loss on training batch is 0.0136224.
After 14236 training step(s), loss on training batch is 0.0104131.
After 14237 training step(s), loss on training batch is 0.0117338.
After 14238 training step(s), loss on training batch is 0.00545908.
After 14239 training step(s), loss on training batch is 0.00468372.
After 14240 training step(s), loss on training batch is 0.00555558.
After 14241 training step(s), loss on training batch is 0.00622052.
After 14242 training step(s), loss on training batch is 0.0060611.
After 14243 training step(s), loss on training batch is 0.00643739.
After 14244 training step(s), loss on training batch is 0.00540631.
After 14245 training step(s), loss on training batch is 0.00552168.
After 14246 training step(s), loss on training batch is 0.00555226.
After 14247 training step(s), loss on training batch is 0.0052584.
After 14248 training step(s), loss on training batch is 0.0056945.
After 14249 training step(s), loss on training batch is 0.00588489.
After 14250 training step(s), loss on training batch is 0.00524952.
After 14251 training step(s), loss on training batch is 0.00583734.
After 14252 training step(s), loss on training batch is 0.00477183.
After 14253 training step(s), loss on training batch is 0.00594484.
After 14254 training step(s), loss on training batch is 0.00486949.
After 14255 training step(s), loss on training batch is 0.00545597.
After 14256 training step(s), loss on training batch is 0.0050653.
After 14257 training step(s), loss on training batch is 0.00532875.
After 14258 training step(s), loss on training batch is 0.00631735.
After 14259 training step(s), loss on training batch is 0.00577736.
After 14260 training step(s), loss on training batch is 0.00598611.
After 14261 training step(s), loss on training batch is 0.00603265.
After 14262 training step(s), loss on training batch is 0.0083116.
After 14263 training step(s), loss on training batch is 0.00614782.
After 14264 training step(s), loss on training batch is 0.00551527.
After 14265 training step(s), loss on training batch is 0.00507187.
After 14266 training step(s), loss on training batch is 0.00637707.
After 14267 training step(s), loss on training batch is 0.00521831.
After 14268 training step(s), loss on training batch is 0.006409.
After 14269 training step(s), loss on training batch is 0.00535.
After 14270 training step(s), loss on training batch is 0.00534636.
After 14271 training step(s), loss on training batch is 0.00555211.
After 14272 training step(s), loss on training batch is 0.00703927.
After 14273 training step(s), loss on training batch is 0.00488086.
After 14274 training step(s), loss on training batch is 0.00495813.
After 14275 training step(s), loss on training batch is 0.0048556.
After 14276 training step(s), loss on training batch is 0.00533313.
After 14277 training step(s), loss on training batch is 0.00534554.
After 14278 training step(s), loss on training batch is 0.00592611.
After 14279 training step(s), loss on training batch is 0.00532761.
After 14280 training step(s), loss on training batch is 0.00556332.
After 14281 training step(s), loss on training batch is 0.00561246.
After 14282 training step(s), loss on training batch is 0.00557549.
After 14283 training step(s), loss on training batch is 0.00955121.
After 14284 training step(s), loss on training batch is 0.015004.
After 14285 training step(s), loss on training batch is 0.00563043.
After 14286 training step(s), loss on training batch is 0.0065118.
After 14287 training step(s), loss on training batch is 0.00535132.
After 14288 training step(s), loss on training batch is 0.0102425.
After 14289 training step(s), loss on training batch is 0.0064364.
After 14290 training step(s), loss on training batch is 0.00540959.
After 14291 training step(s), loss on training batch is 0.00544332.
After 14292 training step(s), loss on training batch is 0.00471715.
After 14293 training step(s), loss on training batch is 0.00742649.
After 14294 training step(s), loss on training batch is 0.00512425.
After 14295 training step(s), loss on training batch is 0.00577408.
After 14296 training step(s), loss on training batch is 0.00535382.
After 14297 training step(s), loss on training batch is 0.0106197.
After 14298 training step(s), loss on training batch is 0.00748422.
After 14299 training step(s), loss on training batch is 0.00516997.
After 14300 training step(s), loss on training batch is 0.00576196.
After 14301 training step(s), loss on training batch is 0.00502967.
After 14302 training step(s), loss on training batch is 0.0063096.
After 14303 training step(s), loss on training batch is 0.00528707.
After 14304 training step(s), loss on training batch is 0.00487525.
After 14305 training step(s), loss on training batch is 0.00655927.
After 14306 training step(s), loss on training batch is 0.00513656.
After 14307 training step(s), loss on training batch is 0.00549382.
After 14308 training step(s), loss on training batch is 0.00758231.
After 14309 training step(s), loss on training batch is 0.00479088.
After 14310 training step(s), loss on training batch is 0.00522078.
After 14311 training step(s), loss on training batch is 0.00488856.
After 14312 training step(s), loss on training batch is 0.00551479.
After 14313 training step(s), loss on training batch is 0.00519129.
After 14314 training step(s), loss on training batch is 0.00578808.
After 14315 training step(s), loss on training batch is 0.00510572.
After 14316 training step(s), loss on training batch is 0.0049008.
After 14317 training step(s), loss on training batch is 0.00526072.
After 14318 training step(s), loss on training batch is 0.00694491.
After 14319 training step(s), loss on training batch is 0.00538331.
After 14320 training step(s), loss on training batch is 0.00524498.
After 14321 training step(s), loss on training batch is 0.00492269.
After 14322 training step(s), loss on training batch is 0.00551527.
After 14323 training step(s), loss on training batch is 0.00515405.
After 14324 training step(s), loss on training batch is 0.00582567.
After 14325 training step(s), loss on training batch is 0.00483723.
After 14326 training step(s), loss on training batch is 0.0051288.
After 14327 training step(s), loss on training batch is 0.00610231.
After 14328 training step(s), loss on training batch is 0.00478879.
After 14329 training step(s), loss on training batch is 0.00547094.
After 14330 training step(s), loss on training batch is 0.00583239.
After 14331 training step(s), loss on training batch is 0.00534133.
After 14332 training step(s), loss on training batch is 0.00544657.
After 14333 training step(s), loss on training batch is 0.00530552.
After 14334 training step(s), loss on training batch is 0.00559234.
After 14335 training step(s), loss on training batch is 0.0061097.
After 14336 training step(s), loss on training batch is 0.00573677.
After 14337 training step(s), loss on training batch is 0.00565147.
After 14338 training step(s), loss on training batch is 0.0055004.
After 14339 training step(s), loss on training batch is 0.00524025.
After 14340 training step(s), loss on training batch is 0.00503744.
After 14341 training step(s), loss on training batch is 0.00490833.
After 14342 training step(s), loss on training batch is 0.0055001.
After 14343 training step(s), loss on training batch is 0.00530326.
After 14344 training step(s), loss on training batch is 0.00554121.
After 14345 training step(s), loss on training batch is 0.00495209.
After 14346 training step(s), loss on training batch is 0.00502051.
After 14347 training step(s), loss on training batch is 0.00540771.
After 14348 training step(s), loss on training batch is 0.00501521.
After 14349 training step(s), loss on training batch is 0.00465957.
After 14350 training step(s), loss on training batch is 0.00657644.
After 14351 training step(s), loss on training batch is 0.00542863.
After 14352 training step(s), loss on training batch is 0.0060219.
After 14353 training step(s), loss on training batch is 0.00512094.
After 14354 training step(s), loss on training batch is 0.00504534.
After 14355 training step(s), loss on training batch is 0.00501968.
After 14356 training step(s), loss on training batch is 0.00531203.
After 14357 training step(s), loss on training batch is 0.00520266.
After 14358 training step(s), loss on training batch is 0.00514326.
After 14359 training step(s), loss on training batch is 0.00711461.
After 14360 training step(s), loss on training batch is 0.00525327.
After 14361 training step(s), loss on training batch is 0.00520207.
After 14362 training step(s), loss on training batch is 0.0048648.
After 14363 training step(s), loss on training batch is 0.00588406.
After 14364 training step(s), loss on training batch is 0.00612126.
After 14365 training step(s), loss on training batch is 0.00472504.
After 14366 training step(s), loss on training batch is 0.00522996.
After 14367 training step(s), loss on training batch is 0.00672983.
After 14368 training step(s), loss on training batch is 0.00535551.
After 14369 training step(s), loss on training batch is 0.0072349.
After 14370 training step(s), loss on training batch is 0.00598663.
After 14371 training step(s), loss on training batch is 0.00517392.
After 14372 training step(s), loss on training batch is 0.00522012.
After 14373 training step(s), loss on training batch is 0.00502207.
After 14374 training step(s), loss on training batch is 0.0057021.
After 14375 training step(s), loss on training batch is 0.00620919.
After 14376 training step(s), loss on training batch is 0.00597074.
After 14377 training step(s), loss on training batch is 0.00468632.
After 14378 training step(s), loss on training batch is 0.00559097.
After 14379 training step(s), loss on training batch is 0.00491428.
After 14380 training step(s), loss on training batch is 0.0052535.
After 14381 training step(s), loss on training batch is 0.00546582.
After 14382 training step(s), loss on training batch is 0.00543015.
After 14383 training step(s), loss on training batch is 0.00545787.
After 14384 training step(s), loss on training batch is 0.00524913.
After 14385 training step(s), loss on training batch is 0.00536638.
After 14386 training step(s), loss on training batch is 0.00510656.
After 14387 training step(s), loss on training batch is 0.00548554.
After 14388 training step(s), loss on training batch is 0.00536782.
After 14389 training step(s), loss on training batch is 0.00533164.
After 14390 training step(s), loss on training batch is 0.00496846.
After 14391 training step(s), loss on training batch is 0.00556142.
After 14392 training step(s), loss on training batch is 0.00525558.
After 14393 training step(s), loss on training batch is 0.00534925.
After 14394 training step(s), loss on training batch is 0.00486994.
After 14395 training step(s), loss on training batch is 0.00504918.
After 14396 training step(s), loss on training batch is 0.00646329.
After 14397 training step(s), loss on training batch is 0.00468069.
After 14398 training step(s), loss on training batch is 0.00539647.
After 14399 training step(s), loss on training batch is 0.00516303.
After 14400 training step(s), loss on training batch is 0.00601238.
After 14401 training step(s), loss on training batch is 0.00481115.
After 14402 training step(s), loss on training batch is 0.00486516.
After 14403 training step(s), loss on training batch is 0.00510767.
After 14404 training step(s), loss on training batch is 0.00484723.
After 14405 training step(s), loss on training batch is 0.00514236.
After 14406 training step(s), loss on training batch is 0.00604953.
After 14407 training step(s), loss on training batch is 0.00508706.
After 14408 training step(s), loss on training batch is 0.00513417.
After 14409 training step(s), loss on training batch is 0.00531324.
After 14410 training step(s), loss on training batch is 0.0057364.
After 14411 training step(s), loss on training batch is 0.00470078.
After 14412 training step(s), loss on training batch is 0.0065805.
After 14413 training step(s), loss on training batch is 0.00615544.
After 14414 training step(s), loss on training batch is 0.00498858.
After 14415 training step(s), loss on training batch is 0.00560622.
After 14416 training step(s), loss on training batch is 0.00516646.
After 14417 training step(s), loss on training batch is 0.00544252.
After 14418 training step(s), loss on training batch is 0.00570841.
After 14419 training step(s), loss on training batch is 0.00581847.
After 14420 training step(s), loss on training batch is 0.00551447.
After 14421 training step(s), loss on training batch is 0.0050363.
After 14422 training step(s), loss on training batch is 0.00657187.
After 14423 training step(s), loss on training batch is 0.00603825.
After 14424 training step(s), loss on training batch is 0.00497553.
After 14425 training step(s), loss on training batch is 0.0054468.
After 14426 training step(s), loss on training batch is 0.00541592.
After 14427 training step(s), loss on training batch is 0.00519886.
After 14428 training step(s), loss on training batch is 0.00477849.
After 14429 training step(s), loss on training batch is 0.00503476.
After 14430 training step(s), loss on training batch is 0.00521203.
After 14431 training step(s), loss on training batch is 0.00501583.
After 14432 training step(s), loss on training batch is 0.00575525.
After 14433 training step(s), loss on training batch is 0.00470053.
After 14434 training step(s), loss on training batch is 0.00504081.
After 14435 training step(s), loss on training batch is 0.00539238.
After 14436 training step(s), loss on training batch is 0.0051815.
After 14437 training step(s), loss on training batch is 0.00620033.
After 14438 training step(s), loss on training batch is 0.00604126.
After 14439 training step(s), loss on training batch is 0.00486276.
After 14440 training step(s), loss on training batch is 0.00539102.
After 14441 training step(s), loss on training batch is 0.00477448.
After 14442 training step(s), loss on training batch is 0.00511439.
After 14443 training step(s), loss on training batch is 0.0048253.
After 14444 training step(s), loss on training batch is 0.00631291.
After 14445 training step(s), loss on training batch is 0.0049218.
After 14446 training step(s), loss on training batch is 0.00504168.
After 14447 training step(s), loss on training batch is 0.00483356.
After 14448 training step(s), loss on training batch is 0.00558251.
After 14449 training step(s), loss on training batch is 0.00553309.
After 14450 training step(s), loss on training batch is 0.00500909.
After 14451 training step(s), loss on training batch is 0.00513594.
After 14452 training step(s), loss on training batch is 0.00497306.
After 14453 training step(s), loss on training batch is 0.00593055.
After 14454 training step(s), loss on training batch is 0.00483112.
After 14455 training step(s), loss on training batch is 0.00465192.
After 14456 training step(s), loss on training batch is 0.00486056.
After 14457 training step(s), loss on training batch is 0.00584977.
After 14458 training step(s), loss on training batch is 0.00509963.
After 14459 training step(s), loss on training batch is 0.00479715.
After 14460 training step(s), loss on training batch is 0.00508754.
After 14461 training step(s), loss on training batch is 0.00522392.
After 14462 training step(s), loss on training batch is 0.00499622.
After 14463 training step(s), loss on training batch is 0.00577441.
After 14464 training step(s), loss on training batch is 0.00660705.
After 14465 training step(s), loss on training batch is 0.00538202.
After 14466 training step(s), loss on training batch is 0.00490677.
After 14467 training step(s), loss on training batch is 0.00516136.
After 14468 training step(s), loss on training batch is 0.00501599.
After 14469 training step(s), loss on training batch is 0.00595748.
After 14470 training step(s), loss on training batch is 0.00621202.
After 14471 training step(s), loss on training batch is 0.00889286.
After 14472 training step(s), loss on training batch is 0.00647035.
After 14473 training step(s), loss on training batch is 0.00548761.
After 14474 training step(s), loss on training batch is 0.00471913.
After 14475 training step(s), loss on training batch is 0.00476692.
After 14476 training step(s), loss on training batch is 0.00494575.
After 14477 training step(s), loss on training batch is 0.00536415.
After 14478 training step(s), loss on training batch is 0.00524311.
After 14479 training step(s), loss on training batch is 0.00468297.
After 14480 training step(s), loss on training batch is 0.00602926.
After 14481 training step(s), loss on training batch is 0.00462834.
After 14482 training step(s), loss on training batch is 0.00762607.
After 14483 training step(s), loss on training batch is 0.00680366.
After 14484 training step(s), loss on training batch is 0.00501646.
After 14485 training step(s), loss on training batch is 0.00533289.
After 14486 training step(s), loss on training batch is 0.00514302.
After 14487 training step(s), loss on training batch is 0.00495638.
After 14488 training step(s), loss on training batch is 0.00523208.
After 14489 training step(s), loss on training batch is 0.00493586.
After 14490 training step(s), loss on training batch is 0.00521404.
After 14491 training step(s), loss on training batch is 0.00499194.
After 14492 training step(s), loss on training batch is 0.00523125.
After 14493 training step(s), loss on training batch is 0.00533377.
After 14494 training step(s), loss on training batch is 0.00529961.
After 14495 training step(s), loss on training batch is 0.00573517.
After 14496 training step(s), loss on training batch is 0.00496853.
After 14497 training step(s), loss on training batch is 0.00484473.
After 14498 training step(s), loss on training batch is 0.00578678.
After 14499 training step(s), loss on training batch is 0.00567391.
After 14500 training step(s), loss on training batch is 0.00558001.
After 14501 training step(s), loss on training batch is 0.00521974.
After 14502 training step(s), loss on training batch is 0.00582429.
After 14503 training step(s), loss on training batch is 0.00503312.
After 14504 training step(s), loss on training batch is 0.0052815.
After 14505 training step(s), loss on training batch is 0.00487899.
After 14506 training step(s), loss on training batch is 0.00508698.
After 14507 training step(s), loss on training batch is 0.00564903.
After 14508 training step(s), loss on training batch is 0.0047407.
After 14509 training step(s), loss on training batch is 0.00507694.
After 14510 training step(s), loss on training batch is 0.0051719.
After 14511 training step(s), loss on training batch is 0.00496693.
After 14512 training step(s), loss on training batch is 0.00573855.
After 14513 training step(s), loss on training batch is 0.00572135.
After 14514 training step(s), loss on training batch is 0.00644375.
After 14515 training step(s), loss on training batch is 0.00519822.
After 14516 training step(s), loss on training batch is 0.00589727.
After 14517 training step(s), loss on training batch is 0.00484875.
After 14518 training step(s), loss on training batch is 0.00564671.
After 14519 training step(s), loss on training batch is 0.00523342.
After 14520 training step(s), loss on training batch is 0.00534205.
After 14521 training step(s), loss on training batch is 0.00577529.
After 14522 training step(s), loss on training batch is 0.00501357.
After 14523 training step(s), loss on training batch is 0.00525394.
After 14524 training step(s), loss on training batch is 0.0054097.
After 14525 training step(s), loss on training batch is 0.00461967.
After 14526 training step(s), loss on training batch is 0.00716421.
After 14527 training step(s), loss on training batch is 0.00709407.
After 14528 training step(s), loss on training batch is 0.00576482.
After 14529 training step(s), loss on training batch is 0.00522556.
After 14530 training step(s), loss on training batch is 0.0053186.
After 14531 training step(s), loss on training batch is 0.00528688.
After 14532 training step(s), loss on training batch is 0.00711405.
After 14533 training step(s), loss on training batch is 0.005434.
After 14534 training step(s), loss on training batch is 0.00458804.
After 14535 training step(s), loss on training batch is 0.00577479.
After 14536 training step(s), loss on training batch is 0.00522297.
After 14537 training step(s), loss on training batch is 0.00486131.
After 14538 training step(s), loss on training batch is 0.00622743.
After 14539 training step(s), loss on training batch is 0.00522909.
After 14540 training step(s), loss on training batch is 0.00600101.
After 14541 training step(s), loss on training batch is 0.00522066.
After 14542 training step(s), loss on training batch is 0.00501029.
After 14543 training step(s), loss on training batch is 0.00570781.
After 14544 training step(s), loss on training batch is 0.00499711.
After 14545 training step(s), loss on training batch is 0.00722328.
After 14546 training step(s), loss on training batch is 0.00477077.
After 14547 training step(s), loss on training batch is 0.00521.
After 14548 training step(s), loss on training batch is 0.00477637.
After 14549 training step(s), loss on training batch is 0.00804554.
After 14550 training step(s), loss on training batch is 0.00576069.
After 14551 training step(s), loss on training batch is 0.00555481.
After 14552 training step(s), loss on training batch is 0.00569126.
After 14553 training step(s), loss on training batch is 0.00814408.
After 14554 training step(s), loss on training batch is 0.00506206.
After 14555 training step(s), loss on training batch is 0.00575666.
After 14556 training step(s), loss on training batch is 0.00596683.
After 14557 training step(s), loss on training batch is 0.00585261.
After 14558 training step(s), loss on training batch is 0.00527712.
After 14559 training step(s), loss on training batch is 0.00485615.
After 14560 training step(s), loss on training batch is 0.00597666.
After 14561 training step(s), loss on training batch is 0.00546881.
After 14562 training step(s), loss on training batch is 0.00538128.
After 14563 training step(s), loss on training batch is 0.00613353.
After 14564 training step(s), loss on training batch is 0.00507469.
After 14565 training step(s), loss on training batch is 0.00491388.
After 14566 training step(s), loss on training batch is 0.0067498.
After 14567 training step(s), loss on training batch is 0.00465553.
After 14568 training step(s), loss on training batch is 0.00507416.
After 14569 training step(s), loss on training batch is 0.00495488.
After 14570 training step(s), loss on training batch is 0.00530126.
After 14571 training step(s), loss on training batch is 0.00467651.
After 14572 training step(s), loss on training batch is 0.00553131.
After 14573 training step(s), loss on training batch is 0.00508981.
After 14574 training step(s), loss on training batch is 0.0052897.
After 14575 training step(s), loss on training batch is 0.00516677.
After 14576 training step(s), loss on training batch is 0.00517773.
After 14577 training step(s), loss on training batch is 0.00527341.
After 14578 training step(s), loss on training batch is 0.00482102.
After 14579 training step(s), loss on training batch is 0.00453709.
After 14580 training step(s), loss on training batch is 0.00580532.
After 14581 training step(s), loss on training batch is 0.00552661.
After 14582 training step(s), loss on training batch is 0.00545933.
After 14583 training step(s), loss on training batch is 0.00504848.
After 14584 training step(s), loss on training batch is 0.00479191.
After 14585 training step(s), loss on training batch is 0.00516122.
After 14586 training step(s), loss on training batch is 0.00560004.
After 14587 training step(s), loss on training batch is 0.00481888.
After 14588 training step(s), loss on training batch is 0.00464013.
After 14589 training step(s), loss on training batch is 0.00487914.
After 14590 training step(s), loss on training batch is 0.00746859.
After 14591 training step(s), loss on training batch is 0.00495506.
After 14592 training step(s), loss on training batch is 0.00535424.
After 14593 training step(s), loss on training batch is 0.00488448.
After 14594 training step(s), loss on training batch is 0.00459191.
After 14595 training step(s), loss on training batch is 0.00508272.
After 14596 training step(s), loss on training batch is 0.00472449.
After 14597 training step(s), loss on training batch is 0.00480891.
After 14598 training step(s), loss on training batch is 0.00506926.
After 14599 training step(s), loss on training batch is 0.0054316.
After 14600 training step(s), loss on training batch is 0.0051226.
After 14601 training step(s), loss on training batch is 0.00532595.
After 14602 training step(s), loss on training batch is 0.00471683.
After 14603 training step(s), loss on training batch is 0.00461451.
After 14604 training step(s), loss on training batch is 0.00569786.
After 14605 training step(s), loss on training batch is 0.00502789.
After 14606 training step(s), loss on training batch is 0.00464611.
After 14607 training step(s), loss on training batch is 0.00542913.
After 14608 training step(s), loss on training batch is 0.00484402.
After 14609 training step(s), loss on training batch is 0.00505866.
After 14610 training step(s), loss on training batch is 0.00578415.
After 14611 training step(s), loss on training batch is 0.004944.
After 14612 training step(s), loss on training batch is 0.00598908.
After 14613 training step(s), loss on training batch is 0.00493132.
After 14614 training step(s), loss on training batch is 0.00506907.
After 14615 training step(s), loss on training batch is 0.00480074.
After 14616 training step(s), loss on training batch is 0.00472658.
After 14617 training step(s), loss on training batch is 0.00541607.
After 14618 training step(s), loss on training batch is 0.00475492.
After 14619 training step(s), loss on training batch is 0.00461636.
After 14620 training step(s), loss on training batch is 0.00586896.
After 14621 training step(s), loss on training batch is 0.00520971.
After 14622 training step(s), loss on training batch is 0.00519153.
After 14623 training step(s), loss on training batch is 0.00477055.
After 14624 training step(s), loss on training batch is 0.00453975.
After 14625 training step(s), loss on training batch is 0.00481177.
After 14626 training step(s), loss on training batch is 0.00546417.
After 14627 training step(s), loss on training batch is 0.00572283.
After 14628 training step(s), loss on training batch is 0.00536973.
After 14629 training step(s), loss on training batch is 0.00502487.
After 14630 training step(s), loss on training batch is 0.00959863.
After 14631 training step(s), loss on training batch is 0.00508546.
After 14632 training step(s), loss on training batch is 0.00525199.
After 14633 training step(s), loss on training batch is 0.00479956.
After 14634 training step(s), loss on training batch is 0.00577135.
After 14635 training step(s), loss on training batch is 0.00471087.
After 14636 training step(s), loss on training batch is 0.00516401.
After 14637 training step(s), loss on training batch is 0.00462758.
After 14638 training step(s), loss on training batch is 0.00535446.
After 14639 training step(s), loss on training batch is 0.0051308.
After 14640 training step(s), loss on training batch is 0.00506106.
After 14641 training step(s), loss on training batch is 0.00480267.
After 14642 training step(s), loss on training batch is 0.00502199.
After 14643 training step(s), loss on training batch is 0.00496618.
After 14644 training step(s), loss on training batch is 0.00471367.
After 14645 training step(s), loss on training batch is 0.00472933.
After 14646 training step(s), loss on training batch is 0.00487145.
After 14647 training step(s), loss on training batch is 0.00600964.
After 14648 training step(s), loss on training batch is 0.00509479.
After 14649 training step(s), loss on training batch is 0.00564514.
After 14650 training step(s), loss on training batch is 0.00529362.
After 14651 training step(s), loss on training batch is 0.00532547.
After 14652 training step(s), loss on training batch is 0.00538223.
After 14653 training step(s), loss on training batch is 0.00538902.
After 14654 training step(s), loss on training batch is 0.00470675.
After 14655 training step(s), loss on training batch is 0.00472604.
After 14656 training step(s), loss on training batch is 0.00600625.
After 14657 training step(s), loss on training batch is 0.0054614.
After 14658 training step(s), loss on training batch is 0.00517593.
After 14659 training step(s), loss on training batch is 0.00467576.
After 14660 training step(s), loss on training batch is 0.00486819.
After 14661 training step(s), loss on training batch is 0.00533052.
After 14662 training step(s), loss on training batch is 0.00482659.
After 14663 training step(s), loss on training batch is 0.0055346.
After 14664 training step(s), loss on training batch is 0.00482803.
After 14665 training step(s), loss on training batch is 0.00518234.
After 14666 training step(s), loss on training batch is 0.00493217.
After 14667 training step(s), loss on training batch is 0.00529535.
After 14668 training step(s), loss on training batch is 0.00470742.
After 14669 training step(s), loss on training batch is 0.00473953.
After 14670 training step(s), loss on training batch is 0.00480016.
After 14671 training step(s), loss on training batch is 0.004782.
After 14672 training step(s), loss on training batch is 0.00518343.
After 14673 training step(s), loss on training batch is 0.00499164.
After 14674 training step(s), loss on training batch is 0.00527309.
After 14675 training step(s), loss on training batch is 0.00508713.
After 14676 training step(s), loss on training batch is 0.00693795.
After 14677 training step(s), loss on training batch is 0.00500894.
After 14678 training step(s), loss on training batch is 0.00473312.
After 14679 training step(s), loss on training batch is 0.00581586.
After 14680 training step(s), loss on training batch is 0.00502045.
After 14681 training step(s), loss on training batch is 0.00482248.
After 14682 training step(s), loss on training batch is 0.0054474.
After 14683 training step(s), loss on training batch is 0.00489948.
After 14684 training step(s), loss on training batch is 0.00591559.
After 14685 training step(s), loss on training batch is 0.00463051.
After 14686 training step(s), loss on training batch is 0.00454986.
After 14687 training step(s), loss on training batch is 0.00503417.
After 14688 training step(s), loss on training batch is 0.00617314.
After 14689 training step(s), loss on training batch is 0.00766003.
After 14690 training step(s), loss on training batch is 0.00507741.
After 14691 training step(s), loss on training batch is 0.00718599.
After 14692 training step(s), loss on training batch is 0.00483912.
After 14693 training step(s), loss on training batch is 0.00536989.
After 14694 training step(s), loss on training batch is 0.00543609.
After 14695 training step(s), loss on training batch is 0.00538807.
After 14696 training step(s), loss on training batch is 0.00615068.
After 14697 training step(s), loss on training batch is 0.00499034.
After 14698 training step(s), loss on training batch is 0.00544884.
After 14699 training step(s), loss on training batch is 0.00490658.
After 14700 training step(s), loss on training batch is 0.00508007.
After 14701 training step(s), loss on training batch is 0.00482724.
After 14702 training step(s), loss on training batch is 0.00467079.
After 14703 training step(s), loss on training batch is 0.00496143.
After 14704 training step(s), loss on training batch is 0.00488607.
After 14705 training step(s), loss on training batch is 0.00497191.
After 14706 training step(s), loss on training batch is 0.00512575.
After 14707 training step(s), loss on training batch is 0.00531345.
After 14708 training step(s), loss on training batch is 0.00459804.
After 14709 training step(s), loss on training batch is 0.00498836.
After 14710 training step(s), loss on training batch is 0.00510486.
After 14711 training step(s), loss on training batch is 0.00503863.
After 14712 training step(s), loss on training batch is 0.00626047.
After 14713 training step(s), loss on training batch is 0.00503763.
After 14714 training step(s), loss on training batch is 0.00588825.
After 14715 training step(s), loss on training batch is 0.00587537.
After 14716 training step(s), loss on training batch is 0.00502574.
After 14717 training step(s), loss on training batch is 0.00530639.
After 14718 training step(s), loss on training batch is 0.00504782.
After 14719 training step(s), loss on training batch is 0.00505024.
After 14720 training step(s), loss on training batch is 0.00522279.
After 14721 training step(s), loss on training batch is 0.00737785.
After 14722 training step(s), loss on training batch is 0.00529121.
After 14723 training step(s), loss on training batch is 0.00520203.
After 14724 training step(s), loss on training batch is 0.00490072.
After 14725 training step(s), loss on training batch is 0.00481753.
After 14726 training step(s), loss on training batch is 0.00515232.
After 14727 training step(s), loss on training batch is 0.00528748.
After 14728 training step(s), loss on training batch is 0.00500741.
After 14729 training step(s), loss on training batch is 0.00568572.
After 14730 training step(s), loss on training batch is 0.00499859.
After 14731 training step(s), loss on training batch is 0.00574136.
After 14732 training step(s), loss on training batch is 0.00620829.
After 14733 training step(s), loss on training batch is 0.00520988.
After 14734 training step(s), loss on training batch is 0.00496408.
After 14735 training step(s), loss on training batch is 0.00496509.
After 14736 training step(s), loss on training batch is 0.00514953.
After 14737 training step(s), loss on training batch is 0.00543736.
After 14738 training step(s), loss on training batch is 0.00553901.
After 14739 training step(s), loss on training batch is 0.00515635.
After 14740 training step(s), loss on training batch is 0.00458333.
After 14741 training step(s), loss on training batch is 0.0046119.
After 14742 training step(s), loss on training batch is 0.00463868.
After 14743 training step(s), loss on training batch is 0.00498242.
After 14744 training step(s), loss on training batch is 0.00539151.
After 14745 training step(s), loss on training batch is 0.0046034.
After 14746 training step(s), loss on training batch is 0.00657815.
After 14747 training step(s), loss on training batch is 0.00486844.
After 14748 training step(s), loss on training batch is 0.00483998.
After 14749 training step(s), loss on training batch is 0.00530958.
After 14750 training step(s), loss on training batch is 0.00543865.
After 14751 training step(s), loss on training batch is 0.00518703.
After 14752 training step(s), loss on training batch is 0.00548931.
After 14753 training step(s), loss on training batch is 0.00628991.
After 14754 training step(s), loss on training batch is 0.00486195.
After 14755 training step(s), loss on training batch is 0.00554857.
After 14756 training step(s), loss on training batch is 0.00451174.
After 14757 training step(s), loss on training batch is 0.00564406.
After 14758 training step(s), loss on training batch is 0.00458568.
After 14759 training step(s), loss on training batch is 0.00468736.
After 14760 training step(s), loss on training batch is 0.00515368.
After 14761 training step(s), loss on training batch is 0.00557704.
After 14762 training step(s), loss on training batch is 0.00574849.
After 14763 training step(s), loss on training batch is 0.00525003.
After 14764 training step(s), loss on training batch is 0.00639248.
After 14765 training step(s), loss on training batch is 0.00500828.
After 14766 training step(s), loss on training batch is 0.00517134.
After 14767 training step(s), loss on training batch is 0.00540127.
After 14768 training step(s), loss on training batch is 0.00471078.
After 14769 training step(s), loss on training batch is 0.00513028.
After 14770 training step(s), loss on training batch is 0.00485549.
After 14771 training step(s), loss on training batch is 0.00498458.
After 14772 training step(s), loss on training batch is 0.00526278.
After 14773 training step(s), loss on training batch is 0.0058789.
After 14774 training step(s), loss on training batch is 0.00467516.
After 14775 training step(s), loss on training batch is 0.00487413.
After 14776 training step(s), loss on training batch is 0.00540927.
After 14777 training step(s), loss on training batch is 0.0049691.
After 14778 training step(s), loss on training batch is 0.00481765.
After 14779 training step(s), loss on training batch is 0.0047504.
After 14780 training step(s), loss on training batch is 0.00455248.
After 14781 training step(s), loss on training batch is 0.00616937.
After 14782 training step(s), loss on training batch is 0.00492794.
After 14783 training step(s), loss on training batch is 0.0054264.
After 14784 training step(s), loss on training batch is 0.00582789.
After 14785 training step(s), loss on training batch is 0.00568578.
After 14786 training step(s), loss on training batch is 0.00521741.
After 14787 training step(s), loss on training batch is 0.00493834.
After 14788 training step(s), loss on training batch is 0.00449449.
After 14789 training step(s), loss on training batch is 0.0052685.
After 14790 training step(s), loss on training batch is 0.00484905.
After 14791 training step(s), loss on training batch is 0.00467062.
After 14792 training step(s), loss on training batch is 0.00480974.
After 14793 training step(s), loss on training batch is 0.00554729.
After 14794 training step(s), loss on training batch is 0.00515542.
After 14795 training step(s), loss on training batch is 0.00488341.
After 14796 training step(s), loss on training batch is 0.00532584.
After 14797 training step(s), loss on training batch is 0.00530784.
After 14798 training step(s), loss on training batch is 0.00467562.
After 14799 training step(s), loss on training batch is 0.00631334.
After 14800 training step(s), loss on training batch is 0.00540582.
After 14801 training step(s), loss on training batch is 0.00587787.
After 14802 training step(s), loss on training batch is 0.00541933.
After 14803 training step(s), loss on training batch is 0.00494958.
After 14804 training step(s), loss on training batch is 0.00489406.
After 14805 training step(s), loss on training batch is 0.0047206.
After 14806 training step(s), loss on training batch is 0.00567284.
After 14807 training step(s), loss on training batch is 0.00550373.
After 14808 training step(s), loss on training batch is 0.00493667.
After 14809 training step(s), loss on training batch is 0.00508554.
After 14810 training step(s), loss on training batch is 0.0047025.
After 14811 training step(s), loss on training batch is 0.00503262.
After 14812 training step(s), loss on training batch is 0.00477342.
After 14813 training step(s), loss on training batch is 0.00530583.
After 14814 training step(s), loss on training batch is 0.00537877.
After 14815 training step(s), loss on training batch is 0.00554711.
After 14816 training step(s), loss on training batch is 0.00487314.
After 14817 training step(s), loss on training batch is 0.00514211.
After 14818 training step(s), loss on training batch is 0.00569117.
After 14819 training step(s), loss on training batch is 0.00452277.
After 14820 training step(s), loss on training batch is 0.0046501.
After 14821 training step(s), loss on training batch is 0.00530182.
After 14822 training step(s), loss on training batch is 0.00626738.
After 14823 training step(s), loss on training batch is 0.00500306.
After 14824 training step(s), loss on training batch is 0.00519678.
After 14825 training step(s), loss on training batch is 0.00537819.
After 14826 training step(s), loss on training batch is 0.00545574.
After 14827 training step(s), loss on training batch is 0.00483162.
After 14828 training step(s), loss on training batch is 0.00533001.
After 14829 training step(s), loss on training batch is 0.00555977.
After 14830 training step(s), loss on training batch is 0.00446336.
After 14831 training step(s), loss on training batch is 0.0061019.
After 14832 training step(s), loss on training batch is 0.00526643.
After 14833 training step(s), loss on training batch is 0.00503094.
After 14834 training step(s), loss on training batch is 0.0048777.
After 14835 training step(s), loss on training batch is 0.00534158.
After 14836 training step(s), loss on training batch is 0.00573374.
After 14837 training step(s), loss on training batch is 0.00543305.
After 14838 training step(s), loss on training batch is 0.00714871.
After 14839 training step(s), loss on training batch is 0.00503791.
After 14840 training step(s), loss on training batch is 0.00482122.
After 14841 training step(s), loss on training batch is 0.00486178.
After 14842 training step(s), loss on training batch is 0.00533064.
After 14843 training step(s), loss on training batch is 0.004779.
After 14844 training step(s), loss on training batch is 0.00560946.
After 14845 training step(s), loss on training batch is 0.00488631.
After 14846 training step(s), loss on training batch is 0.00495921.
After 14847 training step(s), loss on training batch is 0.00575698.
After 14848 training step(s), loss on training batch is 0.00469804.
After 14849 training step(s), loss on training batch is 0.00502379.
After 14850 training step(s), loss on training batch is 0.00488362.
After 14851 training step(s), loss on training batch is 0.00519377.
After 14852 training step(s), loss on training batch is 0.00549405.
After 14853 training step(s), loss on training batch is 0.00510241.
After 14854 training step(s), loss on training batch is 0.00477742.
After 14855 training step(s), loss on training batch is 0.00510144.
After 14856 training step(s), loss on training batch is 0.00510121.
After 14857 training step(s), loss on training batch is 0.00514557.
After 14858 training step(s), loss on training batch is 0.0045968.
After 14859 training step(s), loss on training batch is 0.00496802.
After 14860 training step(s), loss on training batch is 0.0047803.
After 14861 training step(s), loss on training batch is 0.00494648.
After 14862 training step(s), loss on training batch is 0.00511403.
After 14863 training step(s), loss on training batch is 0.00529549.
After 14864 training step(s), loss on training batch is 0.00490357.
After 14865 training step(s), loss on training batch is 0.0049367.
After 14866 training step(s), loss on training batch is 0.00473639.
After 14867 training step(s), loss on training batch is 0.00488234.
After 14868 training step(s), loss on training batch is 0.0049812.
After 14869 training step(s), loss on training batch is 0.00583445.
After 14870 training step(s), loss on training batch is 0.00476895.
After 14871 training step(s), loss on training batch is 0.00528897.
After 14872 training step(s), loss on training batch is 0.00444123.
After 14873 training step(s), loss on training batch is 0.00503876.
After 14874 training step(s), loss on training batch is 0.005499.
After 14875 training step(s), loss on training batch is 0.00499787.
After 14876 training step(s), loss on training batch is 0.00476665.
After 14877 training step(s), loss on training batch is 0.00518918.
After 14878 training step(s), loss on training batch is 0.00546064.
After 14879 training step(s), loss on training batch is 0.00606025.
After 14880 training step(s), loss on training batch is 0.0047641.
After 14881 training step(s), loss on training batch is 0.00488323.
After 14882 training step(s), loss on training batch is 0.00470471.
After 14883 training step(s), loss on training batch is 0.00491587.
After 14884 training step(s), loss on training batch is 0.00513894.
After 14885 training step(s), loss on training batch is 0.00450434.
After 14886 training step(s), loss on training batch is 0.00492298.
After 14887 training step(s), loss on training batch is 0.00497904.
After 14888 training step(s), loss on training batch is 0.00463919.
After 14889 training step(s), loss on training batch is 0.00487824.
After 14890 training step(s), loss on training batch is 0.00499733.
After 14891 training step(s), loss on training batch is 0.00465576.
After 14892 training step(s), loss on training batch is 0.00495084.
After 14893 training step(s), loss on training batch is 0.00479874.
After 14894 training step(s), loss on training batch is 0.00474873.
After 14895 training step(s), loss on training batch is 0.00486333.
After 14896 training step(s), loss on training batch is 0.00676971.
After 14897 training step(s), loss on training batch is 0.00470263.
After 14898 training step(s), loss on training batch is 0.00481004.
After 14899 training step(s), loss on training batch is 0.00616248.
After 14900 training step(s), loss on training batch is 0.00471239.
After 14901 training step(s), loss on training batch is 0.00502398.
After 14902 training step(s), loss on training batch is 0.00503759.
After 14903 training step(s), loss on training batch is 0.00461569.
After 14904 training step(s), loss on training batch is 0.00494177.
After 14905 training step(s), loss on training batch is 0.00486859.
After 14906 training step(s), loss on training batch is 0.00482101.
After 14907 training step(s), loss on training batch is 0.00475454.
After 14908 training step(s), loss on training batch is 0.00532326.
After 14909 training step(s), loss on training batch is 0.00461215.
After 14910 training step(s), loss on training batch is 0.00494997.
After 14911 training step(s), loss on training batch is 0.0048954.
After 14912 training step(s), loss on training batch is 0.0047757.
After 14913 training step(s), loss on training batch is 0.00542425.
After 14914 training step(s), loss on training batch is 0.00493802.
After 14915 training step(s), loss on training batch is 0.00535291.
After 14916 training step(s), loss on training batch is 0.0048845.
After 14917 training step(s), loss on training batch is 0.00483215.
After 14918 training step(s), loss on training batch is 0.00517978.
After 14919 training step(s), loss on training batch is 0.00528955.
After 14920 training step(s), loss on training batch is 0.0058196.
After 14921 training step(s), loss on training batch is 0.00480161.
After 14922 training step(s), loss on training batch is 0.00454111.
After 14923 training step(s), loss on training batch is 0.00556245.
After 14924 training step(s), loss on training batch is 0.00488303.
After 14925 training step(s), loss on training batch is 0.00473507.
After 14926 training step(s), loss on training batch is 0.00529205.
After 14927 training step(s), loss on training batch is 0.00476346.
After 14928 training step(s), loss on training batch is 0.00546852.
After 14929 training step(s), loss on training batch is 0.00550023.
After 14930 training step(s), loss on training batch is 0.00575922.
After 14931 training step(s), loss on training batch is 0.00497453.
After 14932 training step(s), loss on training batch is 0.00502917.
After 14933 training step(s), loss on training batch is 0.00498838.
After 14934 training step(s), loss on training batch is 0.00534372.
After 14935 training step(s), loss on training batch is 0.00599151.
After 14936 training step(s), loss on training batch is 0.00473355.
After 14937 training step(s), loss on training batch is 0.0046519.
After 14938 training step(s), loss on training batch is 0.00521406.
After 14939 training step(s), loss on training batch is 0.00479972.
After 14940 training step(s), loss on training batch is 0.00515136.
After 14941 training step(s), loss on training batch is 0.00519875.
After 14942 training step(s), loss on training batch is 0.00540606.
After 14943 training step(s), loss on training batch is 0.0056913.
After 14944 training step(s), loss on training batch is 0.00474535.
After 14945 training step(s), loss on training batch is 0.00453626.
After 14946 training step(s), loss on training batch is 0.00466659.
After 14947 training step(s), loss on training batch is 0.00506661.
After 14948 training step(s), loss on training batch is 0.00491639.
After 14949 training step(s), loss on training batch is 0.00576998.
After 14950 training step(s), loss on training batch is 0.00450713.
After 14951 training step(s), loss on training batch is 0.0048194.
After 14952 training step(s), loss on training batch is 0.00516757.
After 14953 training step(s), loss on training batch is 0.00504997.
After 14954 training step(s), loss on training batch is 0.00539369.
After 14955 training step(s), loss on training batch is 0.00481917.
After 14956 training step(s), loss on training batch is 0.00542036.
After 14957 training step(s), loss on training batch is 0.00449184.
After 14958 training step(s), loss on training batch is 0.00543235.
After 14959 training step(s), loss on training batch is 0.00479868.
After 14960 training step(s), loss on training batch is 0.00542597.
After 14961 training step(s), loss on training batch is 0.00462767.
After 14962 training step(s), loss on training batch is 0.00575599.
After 14963 training step(s), loss on training batch is 0.00473203.
After 14964 training step(s), loss on training batch is 0.00462813.
After 14965 training step(s), loss on training batch is 0.00602191.
After 14966 training step(s), loss on training batch is 0.00489953.
After 14967 training step(s), loss on training batch is 0.00492475.
After 14968 training step(s), loss on training batch is 0.00458487.
After 14969 training step(s), loss on training batch is 0.00477777.
After 14970 training step(s), loss on training batch is 0.00482772.
After 14971 training step(s), loss on training batch is 0.0051811.
After 14972 training step(s), loss on training batch is 0.00560213.
After 14973 training step(s), loss on training batch is 0.00518601.
After 14974 training step(s), loss on training batch is 0.00532656.
After 14975 training step(s), loss on training batch is 0.00616269.
After 14976 training step(s), loss on training batch is 0.00479397.
After 14977 training step(s), loss on training batch is 0.00507912.
After 14978 training step(s), loss on training batch is 0.00506629.
After 14979 training step(s), loss on training batch is 0.00528324.
After 14980 training step(s), loss on training batch is 0.00522029.
After 14981 training step(s), loss on training batch is 0.0050442.
After 14982 training step(s), loss on training batch is 0.00485999.
After 14983 training step(s), loss on training batch is 0.00478031.
After 14984 training step(s), loss on training batch is 0.0046737.
After 14985 training step(s), loss on training batch is 0.00680595.
After 14986 training step(s), loss on training batch is 0.00574697.
After 14987 training step(s), loss on training batch is 0.00510534.
After 14988 training step(s), loss on training batch is 0.00473588.
After 14989 training step(s), loss on training batch is 0.00520175.
After 14990 training step(s), loss on training batch is 0.00607203.
After 14991 training step(s), loss on training batch is 0.00438756.
After 14992 training step(s), loss on training batch is 0.00563591.
After 14993 training step(s), loss on training batch is 0.00540904.
After 14994 training step(s), loss on training batch is 0.00464872.
After 14995 training step(s), loss on training batch is 0.00487727.
After 14996 training step(s), loss on training batch is 0.00543755.
After 14997 training step(s), loss on training batch is 0.00494068.
After 14998 training step(s), loss on training batch is 0.00506294.
After 14999 training step(s), loss on training batch is 0.00436432.
After 15000 training step(s), loss on training batch is 0.00555027.
After 15001 training step(s), loss on training batch is 0.00541103.
After 15002 training step(s), loss on training batch is 0.00529852.
After 15003 training step(s), loss on training batch is 0.0048522.
After 15004 training step(s), loss on training batch is 0.00548104.
After 15005 training step(s), loss on training batch is 0.0053611.
After 15006 training step(s), loss on training batch is 0.00480335.
After 15007 training step(s), loss on training batch is 0.00551348.
After 15008 training step(s), loss on training batch is 0.00492834.
After 15009 training step(s), loss on training batch is 0.00644618.
After 15010 training step(s), loss on training batch is 0.00910217.
After 15011 training step(s), loss on training batch is 0.00551266.
After 15012 training step(s), loss on training batch is 0.00509784.
After 15013 training step(s), loss on training batch is 0.00550772.
After 15014 training step(s), loss on training batch is 0.00456493.
After 15015 training step(s), loss on training batch is 0.00465547.
After 15016 training step(s), loss on training batch is 0.00534897.
After 15017 training step(s), loss on training batch is 0.00447828.
After 15018 training step(s), loss on training batch is 0.00519666.
After 15019 training step(s), loss on training batch is 0.00499111.
After 15020 training step(s), loss on training batch is 0.00467101.
After 15021 training step(s), loss on training batch is 0.00483484.
After 15022 training step(s), loss on training batch is 0.00544674.
After 15023 training step(s), loss on training batch is 0.00511592.
After 15024 training step(s), loss on training batch is 0.00504754.
After 15025 training step(s), loss on training batch is 0.00599957.
After 15026 training step(s), loss on training batch is 0.00485067.
After 15027 training step(s), loss on training batch is 0.00526883.
After 15028 training step(s), loss on training batch is 0.00503964.
After 15029 training step(s), loss on training batch is 0.00509708.
After 15030 training step(s), loss on training batch is 0.00479731.
After 15031 training step(s), loss on training batch is 0.00555567.
After 15032 training step(s), loss on training batch is 0.00499792.
After 15033 training step(s), loss on training batch is 0.00446818.
After 15034 training step(s), loss on training batch is 0.00466696.
After 15035 training step(s), loss on training batch is 0.00460564.
After 15036 training step(s), loss on training batch is 0.00490753.
After 15037 training step(s), loss on training batch is 0.00463269.
After 15038 training step(s), loss on training batch is 0.00515326.
After 15039 training step(s), loss on training batch is 0.00478627.
After 15040 training step(s), loss on training batch is 0.00442082.
After 15041 training step(s), loss on training batch is 0.00522766.
After 15042 training step(s), loss on training batch is 0.00475137.
After 15043 training step(s), loss on training batch is 0.00471733.
After 15044 training step(s), loss on training batch is 0.0050812.
After 15045 training step(s), loss on training batch is 0.00453.
After 15046 training step(s), loss on training batch is 0.00546703.
After 15047 training step(s), loss on training batch is 0.00504403.
After 15048 training step(s), loss on training batch is 0.00517194.
After 15049 training step(s), loss on training batch is 0.00444474.
After 15050 training step(s), loss on training batch is 0.0051028.
After 15051 training step(s), loss on training batch is 0.00486015.
After 15052 training step(s), loss on training batch is 0.0049129.
After 15053 training step(s), loss on training batch is 0.00518493.
After 15054 training step(s), loss on training batch is 0.0044158.
After 15055 training step(s), loss on training batch is 0.00444835.
After 15056 training step(s), loss on training batch is 0.00479914.
After 15057 training step(s), loss on training batch is 0.00445524.
After 15058 training step(s), loss on training batch is 0.00477604.
After 15059 training step(s), loss on training batch is 0.00563205.
After 15060 training step(s), loss on training batch is 0.00501019.
After 15061 training step(s), loss on training batch is 0.00512015.
After 15062 training step(s), loss on training batch is 0.00542041.
After 15063 training step(s), loss on training batch is 0.00503498.
After 15064 training step(s), loss on training batch is 0.00495816.
After 15065 training step(s), loss on training batch is 0.0052423.
After 15066 training step(s), loss on training batch is 0.00462767.
After 15067 training step(s), loss on training batch is 0.00513009.
After 15068 training step(s), loss on training batch is 0.00592299.
After 15069 training step(s), loss on training batch is 0.00522495.
After 15070 training step(s), loss on training batch is 0.00577079.
After 15071 training step(s), loss on training batch is 0.00480235.
After 15072 training step(s), loss on training batch is 0.0045202.
After 15073 training step(s), loss on training batch is 0.00459013.
After 15074 training step(s), loss on training batch is 0.00465992.
After 15075 training step(s), loss on training batch is 0.00444416.
After 15076 training step(s), loss on training batch is 0.00515694.
After 15077 training step(s), loss on training batch is 0.00503796.
After 15078 training step(s), loss on training batch is 0.00847742.
After 15079 training step(s), loss on training batch is 0.0049263.
After 15080 training step(s), loss on training batch is 0.00464263.
After 15081 training step(s), loss on training batch is 0.00533234.
After 15082 training step(s), loss on training batch is 0.00483349.
After 15083 training step(s), loss on training batch is 0.005683.
After 15084 training step(s), loss on training batch is 0.00440791.
After 15085 training step(s), loss on training batch is 0.00533409.
After 15086 training step(s), loss on training batch is 0.00498094.
After 15087 training step(s), loss on training batch is 0.00504853.
After 15088 training step(s), loss on training batch is 0.00489287.
After 15089 training step(s), loss on training batch is 0.0048488.
After 15090 training step(s), loss on training batch is 0.00461448.
After 15091 training step(s), loss on training batch is 0.00446822.
After 15092 training step(s), loss on training batch is 0.00519852.
After 15093 training step(s), loss on training batch is 0.00463366.
After 15094 training step(s), loss on training batch is 0.00524697.
After 15095 training step(s), loss on training batch is 0.00480267.
After 15096 training step(s), loss on training batch is 0.00487871.
After 15097 training step(s), loss on training batch is 0.0046815.
After 15098 training step(s), loss on training batch is 0.00652103.
After 15099 training step(s), loss on training batch is 0.00465469.
After 15100 training step(s), loss on training batch is 0.00476529.
After 15101 training step(s), loss on training batch is 0.0052755.
After 15102 training step(s), loss on training batch is 0.004893.
After 15103 training step(s), loss on training batch is 0.00472488.
After 15104 training step(s), loss on training batch is 0.00459199.
After 15105 training step(s), loss on training batch is 0.00481289.
After 15106 training step(s), loss on training batch is 0.00446824.
After 15107 training step(s), loss on training batch is 0.00435497.
After 15108 training step(s), loss on training batch is 0.0061107.
After 15109 training step(s), loss on training batch is 0.00534042.
After 15110 training step(s), loss on training batch is 0.00468978.
After 15111 training step(s), loss on training batch is 0.00503089.
After 15112 training step(s), loss on training batch is 0.00449078.
After 15113 training step(s), loss on training batch is 0.00491405.
After 15114 training step(s), loss on training batch is 0.0052685.
After 15115 training step(s), loss on training batch is 0.00489264.
After 15116 training step(s), loss on training batch is 0.00499036.
After 15117 training step(s), loss on training batch is 0.00532373.
After 15118 training step(s), loss on training batch is 0.00453711.
After 15119 training step(s), loss on training batch is 0.00511019.
After 15120 training step(s), loss on training batch is 0.00511545.
After 15121 training step(s), loss on training batch is 0.00480637.
After 15122 training step(s), loss on training batch is 0.0047527.
After 15123 training step(s), loss on training batch is 0.00499824.
After 15124 training step(s), loss on training batch is 0.00499988.
After 15125 training step(s), loss on training batch is 0.00494474.
After 15126 training step(s), loss on training batch is 0.00472921.
After 15127 training step(s), loss on training batch is 0.00444972.
After 15128 training step(s), loss on training batch is 0.0056766.
After 15129 training step(s), loss on training batch is 0.00576244.
After 15130 training step(s), loss on training batch is 0.00511558.
After 15131 training step(s), loss on training batch is 0.00461018.
After 15132 training step(s), loss on training batch is 0.00449255.
After 15133 training step(s), loss on training batch is 0.00574951.
After 15134 training step(s), loss on training batch is 0.00490268.
After 15135 training step(s), loss on training batch is 0.0045859.
After 15136 training step(s), loss on training batch is 0.00498632.
After 15137 training step(s), loss on training batch is 0.0047262.
After 15138 training step(s), loss on training batch is 0.00473879.
After 15139 training step(s), loss on training batch is 0.00507839.
After 15140 training step(s), loss on training batch is 0.00468493.
After 15141 training step(s), loss on training batch is 0.00478825.
After 15142 training step(s), loss on training batch is 0.00486899.
After 15143 training step(s), loss on training batch is 0.00519418.
After 15144 training step(s), loss on training batch is 0.00492985.
After 15145 training step(s), loss on training batch is 0.00494616.
After 15146 training step(s), loss on training batch is 0.00474047.
After 15147 training step(s), loss on training batch is 0.00461673.
After 15148 training step(s), loss on training batch is 0.00509577.
After 15149 training step(s), loss on training batch is 0.00472328.
After 15150 training step(s), loss on training batch is 0.00468206.
After 15151 training step(s), loss on training batch is 0.00503587.
After 15152 training step(s), loss on training batch is 0.00508611.
After 15153 training step(s), loss on training batch is 0.00530252.
After 15154 training step(s), loss on training batch is 0.00490923.
After 15155 training step(s), loss on training batch is 0.00463479.
After 15156 training step(s), loss on training batch is 0.00562578.
After 15157 training step(s), loss on training batch is 0.00459399.
After 15158 training step(s), loss on training batch is 0.00463448.
After 15159 training step(s), loss on training batch is 0.00474198.
After 15160 training step(s), loss on training batch is 0.00549393.
After 15161 training step(s), loss on training batch is 0.00493854.
After 15162 training step(s), loss on training batch is 0.0045886.
After 15163 training step(s), loss on training batch is 0.00431378.
After 15164 training step(s), loss on training batch is 0.00484535.
After 15165 training step(s), loss on training batch is 0.00456798.
After 15166 training step(s), loss on training batch is 0.00501778.
After 15167 training step(s), loss on training batch is 0.0050513.
After 15168 training step(s), loss on training batch is 0.00438141.
After 15169 training step(s), loss on training batch is 0.00463309.
After 15170 training step(s), loss on training batch is 0.00492707.
After 15171 training step(s), loss on training batch is 0.00592379.
After 15172 training step(s), loss on training batch is 0.00552899.
After 15173 training step(s), loss on training batch is 0.00495546.
After 15174 training step(s), loss on training batch is 0.00601586.
After 15175 training step(s), loss on training batch is 0.00464848.
After 15176 training step(s), loss on training batch is 0.00460375.
After 15177 training step(s), loss on training batch is 0.00465544.
After 15178 training step(s), loss on training batch is 0.00524582.
After 15179 training step(s), loss on training batch is 0.00466839.
After 15180 training step(s), loss on training batch is 0.00503086.
After 15181 training step(s), loss on training batch is 0.00479301.
After 15182 training step(s), loss on training batch is 0.00482758.
After 15183 training step(s), loss on training batch is 0.00560364.
After 15184 training step(s), loss on training batch is 0.0051311.
After 15185 training step(s), loss on training batch is 0.00547066.
After 15186 training step(s), loss on training batch is 0.00466868.
After 15187 training step(s), loss on training batch is 0.0050213.
After 15188 training step(s), loss on training batch is 0.0051821.
After 15189 training step(s), loss on training batch is 0.00546189.
After 15190 training step(s), loss on training batch is 0.00565178.
After 15191 training step(s), loss on training batch is 0.00510486.
After 15192 training step(s), loss on training batch is 0.0046943.
After 15193 training step(s), loss on training batch is 0.00533053.
After 15194 training step(s), loss on training batch is 0.00791038.
After 15195 training step(s), loss on training batch is 0.00452612.
After 15196 training step(s), loss on training batch is 0.00473323.
After 15197 training step(s), loss on training batch is 0.00550296.
After 15198 training step(s), loss on training batch is 0.00468905.
After 15199 training step(s), loss on training batch is 0.00519858.
After 15200 training step(s), loss on training batch is 0.00465494.
After 15201 training step(s), loss on training batch is 0.00601359.
After 15202 training step(s), loss on training batch is 0.00452423.
After 15203 training step(s), loss on training batch is 0.00528858.
After 15204 training step(s), loss on training batch is 0.00450585.
After 15205 training step(s), loss on training batch is 0.00482161.
After 15206 training step(s), loss on training batch is 0.00457606.
After 15207 training step(s), loss on training batch is 0.00479166.
After 15208 training step(s), loss on training batch is 0.00481005.
After 15209 training step(s), loss on training batch is 0.00500586.
After 15210 training step(s), loss on training batch is 0.00442807.
After 15211 training step(s), loss on training batch is 0.00465798.
After 15212 training step(s), loss on training batch is 0.00442548.
After 15213 training step(s), loss on training batch is 0.0047382.
After 15214 training step(s), loss on training batch is 0.00464834.
After 15215 training step(s), loss on training batch is 0.00515117.
After 15216 training step(s), loss on training batch is 0.00458077.
After 15217 training step(s), loss on training batch is 0.0051056.
After 15218 training step(s), loss on training batch is 0.00438776.
After 15219 training step(s), loss on training batch is 0.0048142.
After 15220 training step(s), loss on training batch is 0.00481575.
After 15221 training step(s), loss on training batch is 0.0045977.
After 15222 training step(s), loss on training batch is 0.00451578.
After 15223 training step(s), loss on training batch is 0.00470868.
After 15224 training step(s), loss on training batch is 0.00451384.
After 15225 training step(s), loss on training batch is 0.00538837.
After 15226 training step(s), loss on training batch is 0.00493427.
After 15227 training step(s), loss on training batch is 0.00452385.
After 15228 training step(s), loss on training batch is 0.00469891.
After 15229 training step(s), loss on training batch is 0.00456866.
After 15230 training step(s), loss on training batch is 0.00467699.
After 15231 training step(s), loss on training batch is 0.00549713.
After 15232 training step(s), loss on training batch is 0.00605834.
After 15233 training step(s), loss on training batch is 0.00486387.
After 15234 training step(s), loss on training batch is 0.00480532.
After 15235 training step(s), loss on training batch is 0.00458373.
After 15236 training step(s), loss on training batch is 0.00639503.
After 15237 training step(s), loss on training batch is 0.00477685.
After 15238 training step(s), loss on training batch is 0.00585584.
After 15239 training step(s), loss on training batch is 0.00505087.
After 15240 training step(s), loss on training batch is 0.00471221.
After 15241 training step(s), loss on training batch is 0.00490378.
After 15242 training step(s), loss on training batch is 0.00522349.
After 15243 training step(s), loss on training batch is 0.00444779.
After 15244 training step(s), loss on training batch is 0.00505687.
After 15245 training step(s), loss on training batch is 0.00486239.
After 15246 training step(s), loss on training batch is 0.0052245.
After 15247 training step(s), loss on training batch is 0.00453098.
After 15248 training step(s), loss on training batch is 0.00446193.
After 15249 training step(s), loss on training batch is 0.00538964.
After 15250 training step(s), loss on training batch is 0.00451659.
After 15251 training step(s), loss on training batch is 0.00486578.
After 15252 training step(s), loss on training batch is 0.0044729.
After 15253 training step(s), loss on training batch is 0.00488193.
After 15254 training step(s), loss on training batch is 0.00472654.
After 15255 training step(s), loss on training batch is 0.00462314.
After 15256 training step(s), loss on training batch is 0.00570757.
After 15257 training step(s), loss on training batch is 0.00513151.
After 15258 training step(s), loss on training batch is 0.00539706.
After 15259 training step(s), loss on training batch is 0.00518074.
After 15260 training step(s), loss on training batch is 0.00482061.
After 15261 training step(s), loss on training batch is 0.00506124.
After 15262 training step(s), loss on training batch is 0.00473816.
After 15263 training step(s), loss on training batch is 0.00467183.
After 15264 training step(s), loss on training batch is 0.00503545.
After 15265 training step(s), loss on training batch is 0.00466487.
After 15266 training step(s), loss on training batch is 0.00552803.
After 15267 training step(s), loss on training batch is 0.00480934.
After 15268 training step(s), loss on training batch is 0.00491595.
After 15269 training step(s), loss on training batch is 0.0046194.
After 15270 training step(s), loss on training batch is 0.00556382.
After 15271 training step(s), loss on training batch is 0.0049011.
After 15272 training step(s), loss on training batch is 0.00477149.
After 15273 training step(s), loss on training batch is 0.00490905.
After 15274 training step(s), loss on training batch is 0.00454115.
After 15275 training step(s), loss on training batch is 0.00522338.
After 15276 training step(s), loss on training batch is 0.00539187.
After 15277 training step(s), loss on training batch is 0.00496801.
After 15278 training step(s), loss on training batch is 0.00524234.
After 15279 training step(s), loss on training batch is 0.00490836.
After 15280 training step(s), loss on training batch is 0.00480678.
After 15281 training step(s), loss on training batch is 0.00529519.
After 15282 training step(s), loss on training batch is 0.0043943.
After 15283 training step(s), loss on training batch is 0.00466871.
After 15284 training step(s), loss on training batch is 0.00459216.
After 15285 training step(s), loss on training batch is 0.0049575.
After 15286 training step(s), loss on training batch is 0.00457695.
After 15287 training step(s), loss on training batch is 0.0046931.
After 15288 training step(s), loss on training batch is 0.00595103.
After 15289 training step(s), loss on training batch is 0.00530747.
After 15290 training step(s), loss on training batch is 0.00461543.
After 15291 training step(s), loss on training batch is 0.00476226.
After 15292 training step(s), loss on training batch is 0.00690827.
After 15293 training step(s), loss on training batch is 0.00482483.
After 15294 training step(s), loss on training batch is 0.00580285.
After 15295 training step(s), loss on training batch is 0.00436482.
After 15296 training step(s), loss on training batch is 0.005548.
After 15297 training step(s), loss on training batch is 0.00474608.
After 15298 training step(s), loss on training batch is 0.00493202.
After 15299 training step(s), loss on training batch is 0.00497383.
After 15300 training step(s), loss on training batch is 0.00468427.
After 15301 training step(s), loss on training batch is 0.0047774.
After 15302 training step(s), loss on training batch is 0.00469472.
After 15303 training step(s), loss on training batch is 0.00469701.
After 15304 training step(s), loss on training batch is 0.00514832.
After 15305 training step(s), loss on training batch is 0.00509316.
After 15306 training step(s), loss on training batch is 0.00432236.
After 15307 training step(s), loss on training batch is 0.00529604.
After 15308 training step(s), loss on training batch is 0.00506259.
After 15309 training step(s), loss on training batch is 0.00545962.
After 15310 training step(s), loss on training batch is 0.00557132.
After 15311 training step(s), loss on training batch is 0.00465658.
After 15312 training step(s), loss on training batch is 0.00581102.
After 15313 training step(s), loss on training batch is 0.00584517.
After 15314 training step(s), loss on training batch is 0.00490214.
After 15315 training step(s), loss on training batch is 0.00586521.
After 15316 training step(s), loss on training batch is 0.00527188.
After 15317 training step(s), loss on training batch is 0.00537719.
After 15318 training step(s), loss on training batch is 0.00445901.
After 15319 training step(s), loss on training batch is 0.00464848.
After 15320 training step(s), loss on training batch is 0.00468373.
After 15321 training step(s), loss on training batch is 0.00528875.
After 15322 training step(s), loss on training batch is 0.00480306.
After 15323 training step(s), loss on training batch is 0.00558937.
After 15324 training step(s), loss on training batch is 0.0053366.
After 15325 training step(s), loss on training batch is 0.00539862.
After 15326 training step(s), loss on training batch is 0.00486962.
After 15327 training step(s), loss on training batch is 0.00493898.
After 15328 training step(s), loss on training batch is 0.00437651.
After 15329 training step(s), loss on training batch is 0.00459803.
After 15330 training step(s), loss on training batch is 0.0050322.
After 15331 training step(s), loss on training batch is 0.00469764.
After 15332 training step(s), loss on training batch is 0.00506157.
After 15333 training step(s), loss on training batch is 0.0052723.
After 15334 training step(s), loss on training batch is 0.00538446.
After 15335 training step(s), loss on training batch is 0.00480845.
After 15336 training step(s), loss on training batch is 0.00535276.
After 15337 training step(s), loss on training batch is 0.00494694.
After 15338 training step(s), loss on training batch is 0.00481824.
After 15339 training step(s), loss on training batch is 0.00517913.
After 15340 training step(s), loss on training batch is 0.00617344.
After 15341 training step(s), loss on training batch is 0.00463543.
After 15342 training step(s), loss on training batch is 0.00492471.
After 15343 training step(s), loss on training batch is 0.00614468.
After 15344 training step(s), loss on training batch is 0.00479284.
After 15345 training step(s), loss on training batch is 0.00439883.
After 15346 training step(s), loss on training batch is 0.00484366.
After 15347 training step(s), loss on training batch is 0.00515216.
After 15348 training step(s), loss on training batch is 0.00460317.
After 15349 training step(s), loss on training batch is 0.00522405.
After 15350 training step(s), loss on training batch is 0.00475946.
After 15351 training step(s), loss on training batch is 0.0059928.
After 15352 training step(s), loss on training batch is 0.00477722.
After 15353 training step(s), loss on training batch is 0.00502841.
After 15354 training step(s), loss on training batch is 0.00519403.
After 15355 training step(s), loss on training batch is 0.00466763.
After 15356 training step(s), loss on training batch is 0.00486243.
After 15357 training step(s), loss on training batch is 0.00565261.
After 15358 training step(s), loss on training batch is 0.00495525.
After 15359 training step(s), loss on training batch is 0.00443638.
After 15360 training step(s), loss on training batch is 0.00487584.
After 15361 training step(s), loss on training batch is 0.00518952.
After 15362 training step(s), loss on training batch is 0.00513009.
After 15363 training step(s), loss on training batch is 0.00475033.
After 15364 training step(s), loss on training batch is 0.00493756.
After 15365 training step(s), loss on training batch is 0.00462396.
After 15366 training step(s), loss on training batch is 0.00504936.
After 15367 training step(s), loss on training batch is 0.00532405.
After 15368 training step(s), loss on training batch is 0.00442768.
After 15369 training step(s), loss on training batch is 0.00520581.
After 15370 training step(s), loss on training batch is 0.00448569.
After 15371 training step(s), loss on training batch is 0.00545439.
After 15372 training step(s), loss on training batch is 0.00440732.
After 15373 training step(s), loss on training batch is 0.00532558.
After 15374 training step(s), loss on training batch is 0.00435637.
After 15375 training step(s), loss on training batch is 0.00499166.
After 15376 training step(s), loss on training batch is 0.0047623.
After 15377 training step(s), loss on training batch is 0.00488774.
After 15378 training step(s), loss on training batch is 0.00490676.
After 15379 training step(s), loss on training batch is 0.00586273.
After 15380 training step(s), loss on training batch is 0.00490002.
After 15381 training step(s), loss on training batch is 0.00498023.
After 15382 training step(s), loss on training batch is 0.00518687.
After 15383 training step(s), loss on training batch is 0.00442554.
After 15384 training step(s), loss on training batch is 0.00434694.
After 15385 training step(s), loss on training batch is 0.00440492.
After 15386 training step(s), loss on training batch is 0.00513546.
After 15387 training step(s), loss on training batch is 0.00501069.
After 15388 training step(s), loss on training batch is 0.00630565.
After 15389 training step(s), loss on training batch is 0.00470502.
After 15390 training step(s), loss on training batch is 0.00453564.
After 15391 training step(s), loss on training batch is 0.00458723.
After 15392 training step(s), loss on training batch is 0.00615157.
After 15393 training step(s), loss on training batch is 0.00479554.
After 15394 training step(s), loss on training batch is 0.00477023.
After 15395 training step(s), loss on training batch is 0.00672797.
After 15396 training step(s), loss on training batch is 0.0048898.
After 15397 training step(s), loss on training batch is 0.00500782.
After 15398 training step(s), loss on training batch is 0.00454052.
After 15399 training step(s), loss on training batch is 0.00443547.
After 15400 training step(s), loss on training batch is 0.00447315.
After 15401 training step(s), loss on training batch is 0.0046659.
After 15402 training step(s), loss on training batch is 0.00564718.
After 15403 training step(s), loss on training batch is 0.00467939.
After 15404 training step(s), loss on training batch is 0.00469387.
After 15405 training step(s), loss on training batch is 0.0043428.
After 15406 training step(s), loss on training batch is 0.00466852.
After 15407 training step(s), loss on training batch is 0.00450857.
After 15408 training step(s), loss on training batch is 0.00460903.
After 15409 training step(s), loss on training batch is 0.00499718.
After 15410 training step(s), loss on training batch is 0.00472385.
After 15411 training step(s), loss on training batch is 0.00531439.
After 15412 training step(s), loss on training batch is 0.0045175.
After 15413 training step(s), loss on training batch is 0.0047658.
After 15414 training step(s), loss on training batch is 0.00468157.
After 15415 training step(s), loss on training batch is 0.00432488.
After 15416 training step(s), loss on training batch is 0.00486559.
After 15417 training step(s), loss on training batch is 0.00454532.
After 15418 training step(s), loss on training batch is 0.00496246.
After 15419 training step(s), loss on training batch is 0.00457781.
After 15420 training step(s), loss on training batch is 0.00485963.
After 15421 training step(s), loss on training batch is 0.0043074.
After 15422 training step(s), loss on training batch is 0.0045667.
After 15423 training step(s), loss on training batch is 0.00466894.
After 15424 training step(s), loss on training batch is 0.00456558.
After 15425 training step(s), loss on training batch is 0.00461417.
After 15426 training step(s), loss on training batch is 0.00433377.
After 15427 training step(s), loss on training batch is 0.00497154.
After 15428 training step(s), loss on training batch is 0.00510026.
After 15429 training step(s), loss on training batch is 0.00449615.
After 15430 training step(s), loss on training batch is 0.00438937.
After 15431 training step(s), loss on training batch is 0.00490278.
After 15432 training step(s), loss on training batch is 0.00487886.
After 15433 training step(s), loss on training batch is 0.00430211.
After 15434 training step(s), loss on training batch is 0.00428899.
After 15435 training step(s), loss on training batch is 0.00491231.
After 15436 training step(s), loss on training batch is 0.00465065.
After 15437 training step(s), loss on training batch is 0.00501254.
After 15438 training step(s), loss on training batch is 0.00464943.
After 15439 training step(s), loss on training batch is 0.00466568.
After 15440 training step(s), loss on training batch is 0.00463929.
After 15441 training step(s), loss on training batch is 0.0046761.
After 15442 training step(s), loss on training batch is 0.00498469.
After 15443 training step(s), loss on training batch is 0.00484621.
After 15444 training step(s), loss on training batch is 0.00498896.
After 15445 training step(s), loss on training batch is 0.00524168.
After 15446 training step(s), loss on training batch is 0.00500118.
After 15447 training step(s), loss on training batch is 0.00508063.
After 15448 training step(s), loss on training batch is 0.00469597.
After 15449 training step(s), loss on training batch is 0.00538674.
After 15450 training step(s), loss on training batch is 0.0049413.
After 15451 training step(s), loss on training batch is 0.00459339.
After 15452 training step(s), loss on training batch is 0.00464003.
After 15453 training step(s), loss on training batch is 0.00464392.
After 15454 training step(s), loss on training batch is 0.00467115.
After 15455 training step(s), loss on training batch is 0.00457462.
After 15456 training step(s), loss on training batch is 0.0044553.
After 15457 training step(s), loss on training batch is 0.00491801.
After 15458 training step(s), loss on training batch is 0.00498836.
After 15459 training step(s), loss on training batch is 0.00493673.
After 15460 training step(s), loss on training batch is 0.00470667.
After 15461 training step(s), loss on training batch is 0.00491214.
After 15462 training step(s), loss on training batch is 0.00450458.
After 15463 training step(s), loss on training batch is 0.00448322.
After 15464 training step(s), loss on training batch is 0.00505207.
After 15465 training step(s), loss on training batch is 0.00516388.
After 15466 training step(s), loss on training batch is 0.00453294.
After 15467 training step(s), loss on training batch is 0.00442251.
After 15468 training step(s), loss on training batch is 0.00486956.
After 15469 training step(s), loss on training batch is 0.00498487.
After 15470 training step(s), loss on training batch is 0.00465549.
After 15471 training step(s), loss on training batch is 0.0049659.
After 15472 training step(s), loss on training batch is 0.0045036.
After 15473 training step(s), loss on training batch is 0.00445467.
After 15474 training step(s), loss on training batch is 0.004541.
After 15475 training step(s), loss on training batch is 0.00450256.
After 15476 training step(s), loss on training batch is 0.00523612.
After 15477 training step(s), loss on training batch is 0.00465534.
After 15478 training step(s), loss on training batch is 0.00439221.
After 15479 training step(s), loss on training batch is 0.00478092.
After 15480 training step(s), loss on training batch is 0.00435308.
After 15481 training step(s), loss on training batch is 0.00488399.
After 15482 training step(s), loss on training batch is 0.00534014.
After 15483 training step(s), loss on training batch is 0.00467422.
After 15484 training step(s), loss on training batch is 0.00463552.
After 15485 training step(s), loss on training batch is 0.00450564.
After 15486 training step(s), loss on training batch is 0.00456637.
After 15487 training step(s), loss on training batch is 0.00482944.
After 15488 training step(s), loss on training batch is 0.00492124.
After 15489 training step(s), loss on training batch is 0.00531596.
After 15490 training step(s), loss on training batch is 0.00437163.
After 15491 training step(s), loss on training batch is 0.0043846.
After 15492 training step(s), loss on training batch is 0.00430437.
After 15493 training step(s), loss on training batch is 0.00545618.
After 15494 training step(s), loss on training batch is 0.00487549.
After 15495 training step(s), loss on training batch is 0.0048288.
After 15496 training step(s), loss on training batch is 0.00551385.
After 15497 training step(s), loss on training batch is 0.00477045.
After 15498 training step(s), loss on training batch is 0.00484534.
After 15499 training step(s), loss on training batch is 0.00449056.
After 15500 training step(s), loss on training batch is 0.00507508.
After 15501 training step(s), loss on training batch is 0.00521643.
After 15502 training step(s), loss on training batch is 0.00563672.
After 15503 training step(s), loss on training batch is 0.0044425.
After 15504 training step(s), loss on training batch is 0.0054815.
After 15505 training step(s), loss on training batch is 0.00498313.
After 15506 training step(s), loss on training batch is 0.00652709.
After 15507 training step(s), loss on training batch is 0.00592981.
After 15508 training step(s), loss on training batch is 0.00810043.
After 15509 training step(s), loss on training batch is 0.00475109.
After 15510 training step(s), loss on training batch is 0.00549239.
After 15511 training step(s), loss on training batch is 0.00588366.
After 15512 training step(s), loss on training batch is 0.00552857.
After 15513 training step(s), loss on training batch is 0.00524834.
After 15514 training step(s), loss on training batch is 0.00469771.
After 15515 training step(s), loss on training batch is 0.0051014.
After 15516 training step(s), loss on training batch is 0.00446718.
After 15517 training step(s), loss on training batch is 0.00585802.
After 15518 training step(s), loss on training batch is 0.00505011.
After 15519 training step(s), loss on training batch is 0.00448739.
After 15520 training step(s), loss on training batch is 0.00456841.
After 15521 training step(s), loss on training batch is 0.00535257.
After 15522 training step(s), loss on training batch is 0.00483642.
After 15523 training step(s), loss on training batch is 0.00466195.
After 15524 training step(s), loss on training batch is 0.00429893.
After 15525 training step(s), loss on training batch is 0.00530556.
After 15526 training step(s), loss on training batch is 0.00445793.
After 15527 training step(s), loss on training batch is 0.00666493.
After 15528 training step(s), loss on training batch is 0.00462944.
After 15529 training step(s), loss on training batch is 0.00602107.
After 15530 training step(s), loss on training batch is 0.00439372.
After 15531 training step(s), loss on training batch is 0.00496939.
After 15532 training step(s), loss on training batch is 0.00526153.
After 15533 training step(s), loss on training batch is 0.00541472.
After 15534 training step(s), loss on training batch is 0.00463259.
After 15535 training step(s), loss on training batch is 0.00449482.
After 15536 training step(s), loss on training batch is 0.00449784.
After 15537 training step(s), loss on training batch is 0.00457067.
After 15538 training step(s), loss on training batch is 0.00506778.
After 15539 training step(s), loss on training batch is 0.00581027.
After 15540 training step(s), loss on training batch is 0.00450787.
After 15541 training step(s), loss on training batch is 0.00507503.
After 15542 training step(s), loss on training batch is 0.00482296.
After 15543 training step(s), loss on training batch is 0.00459522.
After 15544 training step(s), loss on training batch is 0.00515938.
After 15545 training step(s), loss on training batch is 0.00432782.
After 15546 training step(s), loss on training batch is 0.00467541.
After 15547 training step(s), loss on training batch is 0.00449865.
After 15548 training step(s), loss on training batch is 0.00619432.
After 15549 training step(s), loss on training batch is 0.00465317.
After 15550 training step(s), loss on training batch is 0.00477518.
After 15551 training step(s), loss on training batch is 0.00507203.
After 15552 training step(s), loss on training batch is 0.0043466.
After 15553 training step(s), loss on training batch is 0.00453645.
After 15554 training step(s), loss on training batch is 0.00456923.
After 15555 training step(s), loss on training batch is 0.00477342.
After 15556 training step(s), loss on training batch is 0.00538848.
After 15557 training step(s), loss on training batch is 0.00560176.
After 15558 training step(s), loss on training batch is 0.00498466.
After 15559 training step(s), loss on training batch is 0.0057823.
After 15560 training step(s), loss on training batch is 0.00465426.
After 15561 training step(s), loss on training batch is 0.00434266.
After 15562 training step(s), loss on training batch is 0.00439334.
After 15563 training step(s), loss on training batch is 0.00473459.
After 15564 training step(s), loss on training batch is 0.00465963.
After 15565 training step(s), loss on training batch is 0.00483393.
After 15566 training step(s), loss on training batch is 0.00477.
After 15567 training step(s), loss on training batch is 0.00482764.
After 15568 training step(s), loss on training batch is 0.00465521.
After 15569 training step(s), loss on training batch is 0.00437768.
After 15570 training step(s), loss on training batch is 0.0043144.
After 15571 training step(s), loss on training batch is 0.00496686.
After 15572 training step(s), loss on training batch is 0.00452499.
After 15573 training step(s), loss on training batch is 0.00501949.
After 15574 training step(s), loss on training batch is 0.00476812.
After 15575 training step(s), loss on training batch is 0.00480074.
After 15576 training step(s), loss on training batch is 0.00432748.
After 15577 training step(s), loss on training batch is 0.00449607.
After 15578 training step(s), loss on training batch is 0.00470864.
After 15579 training step(s), loss on training batch is 0.00526182.
After 15580 training step(s), loss on training batch is 0.00461361.
After 15581 training step(s), loss on training batch is 0.00488937.
After 15582 training step(s), loss on training batch is 0.00506317.
After 15583 training step(s), loss on training batch is 0.00492981.
After 15584 training step(s), loss on training batch is 0.00757265.
After 15585 training step(s), loss on training batch is 0.00462229.
After 15586 training step(s), loss on training batch is 0.00554121.
After 15587 training step(s), loss on training batch is 0.00468124.
After 15588 training step(s), loss on training batch is 0.00555402.
After 15589 training step(s), loss on training batch is 0.00521177.
After 15590 training step(s), loss on training batch is 0.00428282.
After 15591 training step(s), loss on training batch is 0.00617256.
After 15592 training step(s), loss on training batch is 0.00445726.
After 15593 training step(s), loss on training batch is 0.00496625.
After 15594 training step(s), loss on training batch is 0.00510245.
After 15595 training step(s), loss on training batch is 0.00450565.
After 15596 training step(s), loss on training batch is 0.00579418.
After 15597 training step(s), loss on training batch is 0.00480262.
After 15598 training step(s), loss on training batch is 0.00435874.
After 15599 training step(s), loss on training batch is 0.00451537.
After 15600 training step(s), loss on training batch is 0.00501661.
After 15601 training step(s), loss on training batch is 0.00472277.
After 15602 training step(s), loss on training batch is 0.00991923.
After 15603 training step(s), loss on training batch is 0.00588631.
After 15604 training step(s), loss on training batch is 0.00498725.
After 15605 training step(s), loss on training batch is 0.00462134.
After 15606 training step(s), loss on training batch is 0.00463543.
After 15607 training step(s), loss on training batch is 0.00488841.
After 15608 training step(s), loss on training batch is 0.00493219.
After 15609 training step(s), loss on training batch is 0.00610899.
After 15610 training step(s), loss on training batch is 0.00478456.
After 15611 training step(s), loss on training batch is 0.00441355.
After 15612 training step(s), loss on training batch is 0.00505809.
After 15613 training step(s), loss on training batch is 0.00472212.
After 15614 training step(s), loss on training batch is 0.00443319.
After 15615 training step(s), loss on training batch is 0.00591914.
After 15616 training step(s), loss on training batch is 0.00541745.
After 15617 training step(s), loss on training batch is 0.00449629.
After 15618 training step(s), loss on training batch is 0.00494761.
After 15619 training step(s), loss on training batch is 0.00452346.
After 15620 training step(s), loss on training batch is 0.00609103.
After 15621 training step(s), loss on training batch is 0.00468229.
After 15622 training step(s), loss on training batch is 0.00462689.
After 15623 training step(s), loss on training batch is 0.00540037.
After 15624 training step(s), loss on training batch is 0.00473898.
After 15625 training step(s), loss on training batch is 0.00646186.
After 15626 training step(s), loss on training batch is 0.00494382.
After 15627 training step(s), loss on training batch is 0.0052329.
After 15628 training step(s), loss on training batch is 0.00489015.
After 15629 training step(s), loss on training batch is 0.00536783.
After 15630 training step(s), loss on training batch is 0.00487349.
After 15631 training step(s), loss on training batch is 0.0048247.
After 15632 training step(s), loss on training batch is 0.00445996.
After 15633 training step(s), loss on training batch is 0.00449777.
After 15634 training step(s), loss on training batch is 0.00479103.
After 15635 training step(s), loss on training batch is 0.00493579.
After 15636 training step(s), loss on training batch is 0.00535961.
After 15637 training step(s), loss on training batch is 0.00444479.
After 15638 training step(s), loss on training batch is 0.00467265.
After 15639 training step(s), loss on training batch is 0.00425173.
After 15640 training step(s), loss on training batch is 0.00447944.
After 15641 training step(s), loss on training batch is 0.00503882.
After 15642 training step(s), loss on training batch is 0.00485556.
After 15643 training step(s), loss on training batch is 0.00492688.
After 15644 training step(s), loss on training batch is 0.00444197.
After 15645 training step(s), loss on training batch is 0.00479718.
After 15646 training step(s), loss on training batch is 0.00480219.
After 15647 training step(s), loss on training batch is 0.00477846.
After 15648 training step(s), loss on training batch is 0.00477787.
After 15649 training step(s), loss on training batch is 0.00473842.
After 15650 training step(s), loss on training batch is 0.00455974.
After 15651 training step(s), loss on training batch is 0.00440159.
After 15652 training step(s), loss on training batch is 0.00543741.
After 15653 training step(s), loss on training batch is 0.00466276.
After 15654 training step(s), loss on training batch is 0.00444476.
After 15655 training step(s), loss on training batch is 0.00515521.
After 15656 training step(s), loss on training batch is 0.00499363.
After 15657 training step(s), loss on training batch is 0.00469845.
After 15658 training step(s), loss on training batch is 0.00454219.
After 15659 training step(s), loss on training batch is 0.00572073.
After 15660 training step(s), loss on training batch is 0.00452774.
After 15661 training step(s), loss on training batch is 0.00467356.
After 15662 training step(s), loss on training batch is 0.00459297.
After 15663 training step(s), loss on training batch is 0.00522351.
After 15664 training step(s), loss on training batch is 0.00470858.
After 15665 training step(s), loss on training batch is 0.0048214.
After 15666 training step(s), loss on training batch is 0.00494636.
After 15667 training step(s), loss on training batch is 0.00471382.
After 15668 training step(s), loss on training batch is 0.00557649.
After 15669 training step(s), loss on training batch is 0.00466257.
After 15670 training step(s), loss on training batch is 0.00510813.
After 15671 training step(s), loss on training batch is 0.00733037.
After 15672 training step(s), loss on training batch is 0.00547084.
After 15673 training step(s), loss on training batch is 0.00556814.
After 15674 training step(s), loss on training batch is 0.00463883.
After 15675 training step(s), loss on training batch is 0.00515196.
After 15676 training step(s), loss on training batch is 0.00441734.
After 15677 training step(s), loss on training batch is 0.00617229.
After 15678 training step(s), loss on training batch is 0.00723098.
After 15679 training step(s), loss on training batch is 0.00493851.
After 15680 training step(s), loss on training batch is 0.00538912.
After 15681 training step(s), loss on training batch is 0.00504917.
After 15682 training step(s), loss on training batch is 0.0046455.
After 15683 training step(s), loss on training batch is 0.0047187.
After 15684 training step(s), loss on training batch is 0.00473134.
After 15685 training step(s), loss on training batch is 0.0059996.
After 15686 training step(s), loss on training batch is 0.00491691.
After 15687 training step(s), loss on training batch is 0.00448858.
After 15688 training step(s), loss on training batch is 0.00466761.
After 15689 training step(s), loss on training batch is 0.0052614.
After 15690 training step(s), loss on training batch is 0.00432355.
After 15691 training step(s), loss on training batch is 0.00492793.
After 15692 training step(s), loss on training batch is 0.00496619.
After 15693 training step(s), loss on training batch is 0.00594434.
After 15694 training step(s), loss on training batch is 0.00446507.
After 15695 training step(s), loss on training batch is 0.00527819.
After 15696 training step(s), loss on training batch is 0.00463829.
After 15697 training step(s), loss on training batch is 0.00458835.
After 15698 training step(s), loss on training batch is 0.00453245.
After 15699 training step(s), loss on training batch is 0.00497427.
After 15700 training step(s), loss on training batch is 0.0053463.
After 15701 training step(s), loss on training batch is 0.00620224.
After 15702 training step(s), loss on training batch is 0.00456665.
After 15703 training step(s), loss on training batch is 0.00537889.
After 15704 training step(s), loss on training batch is 0.00539764.
After 15705 training step(s), loss on training batch is 0.0047529.
After 15706 training step(s), loss on training batch is 0.0049291.
After 15707 training step(s), loss on training batch is 0.00456804.
After 15708 training step(s), loss on training batch is 0.00521899.
After 15709 training step(s), loss on training batch is 0.00433639.
After 15710 training step(s), loss on training batch is 0.00503004.
After 15711 training step(s), loss on training batch is 0.00479748.
After 15712 training step(s), loss on training batch is 0.00524765.
After 15713 training step(s), loss on training batch is 0.00475345.
After 15714 training step(s), loss on training batch is 0.00489869.
After 15715 training step(s), loss on training batch is 0.00468786.
After 15716 training step(s), loss on training batch is 0.00467505.
After 15717 training step(s), loss on training batch is 0.00466437.
After 15718 training step(s), loss on training batch is 0.00475233.
After 15719 training step(s), loss on training batch is 0.00465459.
After 15720 training step(s), loss on training batch is 0.00668746.
After 15721 training step(s), loss on training batch is 0.00506409.
After 15722 training step(s), loss on training batch is 0.00472909.
After 15723 training step(s), loss on training batch is 0.00545069.
After 15724 training step(s), loss on training batch is 0.00438135.
After 15725 training step(s), loss on training batch is 0.00465675.
After 15726 training step(s), loss on training batch is 0.0047304.
After 15727 training step(s), loss on training batch is 0.00489233.
After 15728 training step(s), loss on training batch is 0.00484374.
After 15729 training step(s), loss on training batch is 0.00471683.
After 15730 training step(s), loss on training batch is 0.00552974.
After 15731 training step(s), loss on training batch is 0.00573632.
After 15732 training step(s), loss on training batch is 0.00456061.
After 15733 training step(s), loss on training batch is 0.00516097.
After 15734 training step(s), loss on training batch is 0.00473007.
After 15735 training step(s), loss on training batch is 0.00420239.
After 15736 training step(s), loss on training batch is 0.00452608.
After 15737 training step(s), loss on training batch is 0.00451839.
After 15738 training step(s), loss on training batch is 0.00437316.
After 15739 training step(s), loss on training batch is 0.00512291.
After 15740 training step(s), loss on training batch is 0.0045382.
After 15741 training step(s), loss on training batch is 0.00520583.
After 15742 training step(s), loss on training batch is 0.00469082.
After 15743 training step(s), loss on training batch is 0.00521167.
After 15744 training step(s), loss on training batch is 0.00442484.
After 15745 training step(s), loss on training batch is 0.00476114.
After 15746 training step(s), loss on training batch is 0.0049024.
After 15747 training step(s), loss on training batch is 0.00463274.
After 15748 training step(s), loss on training batch is 0.00429245.
After 15749 training step(s), loss on training batch is 0.00444838.
After 15750 training step(s), loss on training batch is 0.00433355.
After 15751 training step(s), loss on training batch is 0.00477241.
After 15752 training step(s), loss on training batch is 0.00525398.
After 15753 training step(s), loss on training batch is 0.00464109.
After 15754 training step(s), loss on training batch is 0.00502692.
After 15755 training step(s), loss on training batch is 0.00415741.
After 15756 training step(s), loss on training batch is 0.00409703.
After 15757 training step(s), loss on training batch is 0.00486481.
After 15758 training step(s), loss on training batch is 0.00504689.
After 15759 training step(s), loss on training batch is 0.00516138.
After 15760 training step(s), loss on training batch is 0.00440475.
After 15761 training step(s), loss on training batch is 0.00521364.
After 15762 training step(s), loss on training batch is 0.00435512.
After 15763 training step(s), loss on training batch is 0.00492157.
After 15764 training step(s), loss on training batch is 0.00714697.
After 15765 training step(s), loss on training batch is 0.00430263.
After 15766 training step(s), loss on training batch is 0.00511194.
After 15767 training step(s), loss on training batch is 0.00418036.
After 15768 training step(s), loss on training batch is 0.0045823.
After 15769 training step(s), loss on training batch is 0.00448076.
After 15770 training step(s), loss on training batch is 0.00454598.
After 15771 training step(s), loss on training batch is 0.00455942.
After 15772 training step(s), loss on training batch is 0.00491728.
After 15773 training step(s), loss on training batch is 0.00468055.
After 15774 training step(s), loss on training batch is 0.00451443.
After 15775 training step(s), loss on training batch is 0.00436671.
After 15776 training step(s), loss on training batch is 0.0044341.
After 15777 training step(s), loss on training batch is 0.0049436.
After 15778 training step(s), loss on training batch is 0.00505402.
After 15779 training step(s), loss on training batch is 0.00507539.
After 15780 training step(s), loss on training batch is 0.00457172.
After 15781 training step(s), loss on training batch is 0.00498753.
After 15782 training step(s), loss on training batch is 0.00463153.
After 15783 training step(s), loss on training batch is 0.00427366.
After 15784 training step(s), loss on training batch is 0.005987.
After 15785 training step(s), loss on training batch is 0.00433503.
After 15786 training step(s), loss on training batch is 0.004388.
After 15787 training step(s), loss on training batch is 0.00454058.
After 15788 training step(s), loss on training batch is 0.00541929.
After 15789 training step(s), loss on training batch is 0.00488922.
After 15790 training step(s), loss on training batch is 0.00452763.
After 15791 training step(s), loss on training batch is 0.00476912.
After 15792 training step(s), loss on training batch is 0.00448443.
After 15793 training step(s), loss on training batch is 0.00725554.
After 15794 training step(s), loss on training batch is 0.00423923.
After 15795 training step(s), loss on training batch is 0.00704045.
After 15796 training step(s), loss on training batch is 0.00454553.
After 15797 training step(s), loss on training batch is 0.00442887.
After 15798 training step(s), loss on training batch is 0.00455123.
After 15799 training step(s), loss on training batch is 0.0042946.
After 15800 training step(s), loss on training batch is 0.00481708.
After 15801 training step(s), loss on training batch is 0.00472623.
After 15802 training step(s), loss on training batch is 0.00441527.
After 15803 training step(s), loss on training batch is 0.00500483.
After 15804 training step(s), loss on training batch is 0.00440975.
After 15805 training step(s), loss on training batch is 0.00503785.
After 15806 training step(s), loss on training batch is 0.00440558.
After 15807 training step(s), loss on training batch is 0.00502021.
After 15808 training step(s), loss on training batch is 0.0046913.
After 15809 training step(s), loss on training batch is 0.00546586.
After 15810 training step(s), loss on training batch is 0.00441018.
After 15811 training step(s), loss on training batch is 0.00428151.
After 15812 training step(s), loss on training batch is 0.00445419.
After 15813 training step(s), loss on training batch is 0.00447755.
After 15814 training step(s), loss on training batch is 0.00484341.
After 15815 training step(s), loss on training batch is 0.00500117.
After 15816 training step(s), loss on training batch is 0.00515072.
After 15817 training step(s), loss on training batch is 0.00486848.
After 15818 training step(s), loss on training batch is 0.00526977.
After 15819 training step(s), loss on training batch is 0.00464375.
After 15820 training step(s), loss on training batch is 0.00425134.
After 15821 training step(s), loss on training batch is 0.00476798.
After 15822 training step(s), loss on training batch is 0.00477018.
After 15823 training step(s), loss on training batch is 0.00479537.
After 15824 training step(s), loss on training batch is 0.00572746.
After 15825 training step(s), loss on training batch is 0.00455685.
After 15826 training step(s), loss on training batch is 0.00462084.
After 15827 training step(s), loss on training batch is 0.00449954.
After 15828 training step(s), loss on training batch is 0.00464054.
After 15829 training step(s), loss on training batch is 0.00460079.
After 15830 training step(s), loss on training batch is 0.00457734.
After 15831 training step(s), loss on training batch is 0.00465633.
After 15832 training step(s), loss on training batch is 0.00503197.
After 15833 training step(s), loss on training batch is 0.00513433.
After 15834 training step(s), loss on training batch is 0.00434141.
After 15835 training step(s), loss on training batch is 0.00453709.
After 15836 training step(s), loss on training batch is 0.00465576.
After 15837 training step(s), loss on training batch is 0.00715907.
After 15838 training step(s), loss on training batch is 0.00475351.
After 15839 training step(s), loss on training batch is 0.00562475.
After 15840 training step(s), loss on training batch is 0.00445148.
After 15841 training step(s), loss on training batch is 0.0048244.
After 15842 training step(s), loss on training batch is 0.00466475.
After 15843 training step(s), loss on training batch is 0.00450637.
After 15844 training step(s), loss on training batch is 0.00460516.
After 15845 training step(s), loss on training batch is 0.00447604.
After 15846 training step(s), loss on training batch is 0.00475051.
After 15847 training step(s), loss on training batch is 0.00452553.
After 15848 training step(s), loss on training batch is 0.00482488.
After 15849 training step(s), loss on training batch is 0.0048165.
After 15850 training step(s), loss on training batch is 0.00433963.
After 15851 training step(s), loss on training batch is 0.00553456.
After 15852 training step(s), loss on training batch is 0.00476755.
After 15853 training step(s), loss on training batch is 0.00457184.
After 15854 training step(s), loss on training batch is 0.00600147.
After 15855 training step(s), loss on training batch is 0.00439793.
After 15856 training step(s), loss on training batch is 0.00429637.
After 15857 training step(s), loss on training batch is 0.00517395.
After 15858 training step(s), loss on training batch is 0.0053719.
After 15859 training step(s), loss on training batch is 0.00462267.
After 15860 training step(s), loss on training batch is 0.00463365.
After 15861 training step(s), loss on training batch is 0.00492255.
After 15862 training step(s), loss on training batch is 0.00413761.
After 15863 training step(s), loss on training batch is 0.00429723.
After 15864 training step(s), loss on training batch is 0.00460639.
After 15865 training step(s), loss on training batch is 0.00482315.
After 15866 training step(s), loss on training batch is 0.00525074.
After 15867 training step(s), loss on training batch is 0.00429212.
After 15868 training step(s), loss on training batch is 0.00542881.
After 15869 training step(s), loss on training batch is 0.00517539.
After 15870 training step(s), loss on training batch is 0.00464606.
After 15871 training step(s), loss on training batch is 0.00462354.
After 15872 training step(s), loss on training batch is 0.00529686.
After 15873 training step(s), loss on training batch is 0.00434409.
After 15874 training step(s), loss on training batch is 0.00455039.
After 15875 training step(s), loss on training batch is 0.00455801.
After 15876 training step(s), loss on training batch is 0.0052678.
After 15877 training step(s), loss on training batch is 0.00464025.
After 15878 training step(s), loss on training batch is 0.00465437.
After 15879 training step(s), loss on training batch is 0.00510633.
After 15880 training step(s), loss on training batch is 0.00528701.
After 15881 training step(s), loss on training batch is 0.00498086.
After 15882 training step(s), loss on training batch is 0.00420032.
After 15883 training step(s), loss on training batch is 0.00471509.
After 15884 training step(s), loss on training batch is 0.00458683.
After 15885 training step(s), loss on training batch is 0.00515316.
After 15886 training step(s), loss on training batch is 0.00437217.
After 15887 training step(s), loss on training batch is 0.00428136.
After 15888 training step(s), loss on training batch is 0.00480071.
After 15889 training step(s), loss on training batch is 0.00569268.
After 15890 training step(s), loss on training batch is 0.0044174.
After 15891 training step(s), loss on training batch is 0.00445743.
After 15892 training step(s), loss on training batch is 0.00429691.
After 15893 training step(s), loss on training batch is 0.00479118.
After 15894 training step(s), loss on training batch is 0.00512045.
After 15895 training step(s), loss on training batch is 0.00492098.
After 15896 training step(s), loss on training batch is 0.00447763.
After 15897 training step(s), loss on training batch is 0.00461489.
After 15898 training step(s), loss on training batch is 0.00460351.
After 15899 training step(s), loss on training batch is 0.00445591.
After 15900 training step(s), loss on training batch is 0.00470012.
After 15901 training step(s), loss on training batch is 0.00457015.
After 15902 training step(s), loss on training batch is 0.00482999.
After 15903 training step(s), loss on training batch is 0.00446477.
After 15904 training step(s), loss on training batch is 0.00498138.
After 15905 training step(s), loss on training batch is 0.00504923.
After 15906 training step(s), loss on training batch is 0.00494572.
After 15907 training step(s), loss on training batch is 0.0043402.
After 15908 training step(s), loss on training batch is 0.00516353.
After 15909 training step(s), loss on training batch is 0.00487918.
After 15910 training step(s), loss on training batch is 0.00447676.
After 15911 training step(s), loss on training batch is 0.00429884.
After 15912 training step(s), loss on training batch is 0.00494047.
After 15913 training step(s), loss on training batch is 0.00465449.
After 15914 training step(s), loss on training batch is 0.00478455.
After 15915 training step(s), loss on training batch is 0.00491933.
After 15916 training step(s), loss on training batch is 0.00472376.
After 15917 training step(s), loss on training batch is 0.004593.
After 15918 training step(s), loss on training batch is 0.00548502.
After 15919 training step(s), loss on training batch is 0.00477654.
After 15920 training step(s), loss on training batch is 0.00446831.
After 15921 training step(s), loss on training batch is 0.00428979.
After 15922 training step(s), loss on training batch is 0.00468738.
After 15923 training step(s), loss on training batch is 0.00445854.
After 15924 training step(s), loss on training batch is 0.00419491.
After 15925 training step(s), loss on training batch is 0.00691636.
After 15926 training step(s), loss on training batch is 0.00444793.
After 15927 training step(s), loss on training batch is 0.00456883.
After 15928 training step(s), loss on training batch is 0.00424842.
After 15929 training step(s), loss on training batch is 0.00485795.
After 15930 training step(s), loss on training batch is 0.00771287.
After 15931 training step(s), loss on training batch is 0.00523451.
After 15932 training step(s), loss on training batch is 0.00459692.
After 15933 training step(s), loss on training batch is 0.00476565.
After 15934 training step(s), loss on training batch is 0.0044154.
After 15935 training step(s), loss on training batch is 0.00479181.
After 15936 training step(s), loss on training batch is 0.0060073.
After 15937 training step(s), loss on training batch is 0.00488541.
After 15938 training step(s), loss on training batch is 0.00451877.
After 15939 training step(s), loss on training batch is 0.00529036.
After 15940 training step(s), loss on training batch is 0.00461138.
After 15941 training step(s), loss on training batch is 0.00505971.
After 15942 training step(s), loss on training batch is 0.00457639.
After 15943 training step(s), loss on training batch is 0.00419857.
After 15944 training step(s), loss on training batch is 0.00419802.
After 15945 training step(s), loss on training batch is 0.00482854.
After 15946 training step(s), loss on training batch is 0.00558776.
After 15947 training step(s), loss on training batch is 0.00479722.
After 15948 training step(s), loss on training batch is 0.00497954.
After 15949 training step(s), loss on training batch is 0.00506773.
After 15950 training step(s), loss on training batch is 0.00441168.
After 15951 training step(s), loss on training batch is 0.00453761.
After 15952 training step(s), loss on training batch is 0.00487968.
After 15953 training step(s), loss on training batch is 0.00508074.
After 15954 training step(s), loss on training batch is 0.00606493.
After 15955 training step(s), loss on training batch is 0.00509725.
After 15956 training step(s), loss on training batch is 0.0052414.
After 15957 training step(s), loss on training batch is 0.00444301.
After 15958 training step(s), loss on training batch is 0.00476858.
After 15959 training step(s), loss on training batch is 0.00450767.
After 15960 training step(s), loss on training batch is 0.00440983.
After 15961 training step(s), loss on training batch is 0.00450584.
After 15962 training step(s), loss on training batch is 0.00444312.
After 15963 training step(s), loss on training batch is 0.00414447.
After 15964 training step(s), loss on training batch is 0.00475041.
After 15965 training step(s), loss on training batch is 0.00508041.
After 15966 training step(s), loss on training batch is 0.00444702.
After 15967 training step(s), loss on training batch is 0.00478135.
After 15968 training step(s), loss on training batch is 0.00424263.
After 15969 training step(s), loss on training batch is 0.00504107.
After 15970 training step(s), loss on training batch is 0.00473649.
After 15971 training step(s), loss on training batch is 0.00421301.
After 15972 training step(s), loss on training batch is 0.00516659.
After 15973 training step(s), loss on training batch is 0.00458134.
After 15974 training step(s), loss on training batch is 0.00442576.
After 15975 training step(s), loss on training batch is 0.00477265.
After 15976 training step(s), loss on training batch is 0.00473693.
After 15977 training step(s), loss on training batch is 0.00441946.
After 15978 training step(s), loss on training batch is 0.00446178.
After 15979 training step(s), loss on training batch is 0.0041504.
After 15980 training step(s), loss on training batch is 0.00413524.
After 15981 training step(s), loss on training batch is 0.00503892.
After 15982 training step(s), loss on training batch is 0.00420313.
After 15983 training step(s), loss on training batch is 0.00458323.
After 15984 training step(s), loss on training batch is 0.00476191.
After 15985 training step(s), loss on training batch is 0.00564357.
After 15986 training step(s), loss on training batch is 0.00489661.
After 15987 training step(s), loss on training batch is 0.00447683.
After 15988 training step(s), loss on training batch is 0.00467784.
After 15989 training step(s), loss on training batch is 0.00443142.
After 15990 training step(s), loss on training batch is 0.00526888.
After 15991 training step(s), loss on training batch is 0.00458013.
After 15992 training step(s), loss on training batch is 0.0046843.
After 15993 training step(s), loss on training batch is 0.00448894.
After 15994 training step(s), loss on training batch is 0.0047396.
After 15995 training step(s), loss on training batch is 0.00524867.
After 15996 training step(s), loss on training batch is 0.00424014.
After 15997 training step(s), loss on training batch is 0.004338.
After 15998 training step(s), loss on training batch is 0.00480267.
After 15999 training step(s), loss on training batch is 0.00432249.
After 16000 training step(s), loss on training batch is 0.00548397.
After 16001 training step(s), loss on training batch is 0.00432619.
After 16002 training step(s), loss on training batch is 0.00460732.
After 16003 training step(s), loss on training batch is 0.00445563.
After 16004 training step(s), loss on training batch is 0.00556831.
After 16005 training step(s), loss on training batch is 0.0049781.
After 16006 training step(s), loss on training batch is 0.0046706.
After 16007 training step(s), loss on training batch is 0.00466342.
After 16008 training step(s), loss on training batch is 0.0046219.
After 16009 training step(s), loss on training batch is 0.00474769.
After 16010 training step(s), loss on training batch is 0.00469569.
After 16011 training step(s), loss on training batch is 0.00457819.
After 16012 training step(s), loss on training batch is 0.0046904.
After 16013 training step(s), loss on training batch is 0.00474132.
After 16014 training step(s), loss on training batch is 0.00455059.
After 16015 training step(s), loss on training batch is 0.00426149.
After 16016 training step(s), loss on training batch is 0.0042629.
After 16017 training step(s), loss on training batch is 0.00500493.
After 16018 training step(s), loss on training batch is 0.00448781.
After 16019 training step(s), loss on training batch is 0.00449624.
After 16020 training step(s), loss on training batch is 0.00506321.
After 16021 training step(s), loss on training batch is 0.0047184.
After 16022 training step(s), loss on training batch is 0.00427695.
After 16023 training step(s), loss on training batch is 0.00431452.
After 16024 training step(s), loss on training batch is 0.00439601.
After 16025 training step(s), loss on training batch is 0.00460043.
After 16026 training step(s), loss on training batch is 0.00473181.
After 16027 training step(s), loss on training batch is 0.00425491.
After 16028 training step(s), loss on training batch is 0.00439863.
After 16029 training step(s), loss on training batch is 0.00459179.
After 16030 training step(s), loss on training batch is 0.00698197.
After 16031 training step(s), loss on training batch is 0.00432343.
After 16032 training step(s), loss on training batch is 0.0050475.
After 16033 training step(s), loss on training batch is 0.00467985.
After 16034 training step(s), loss on training batch is 0.00455961.
After 16035 training step(s), loss on training batch is 0.00419155.
After 16036 training step(s), loss on training batch is 0.00484186.
After 16037 training step(s), loss on training batch is 0.00420682.
After 16038 training step(s), loss on training batch is 0.00450241.
After 16039 training step(s), loss on training batch is 0.00462614.
After 16040 training step(s), loss on training batch is 0.00431035.
After 16041 training step(s), loss on training batch is 0.0043549.
After 16042 training step(s), loss on training batch is 0.00445421.
After 16043 training step(s), loss on training batch is 0.00449714.
After 16044 training step(s), loss on training batch is 0.00515683.
After 16045 training step(s), loss on training batch is 0.00462305.
After 16046 training step(s), loss on training batch is 0.00455713.
After 16047 training step(s), loss on training batch is 0.00508038.
After 16048 training step(s), loss on training batch is 0.00458885.
After 16049 training step(s), loss on training batch is 0.00449784.
After 16050 training step(s), loss on training batch is 0.00414845.
After 16051 training step(s), loss on training batch is 0.0043154.
After 16052 training step(s), loss on training batch is 0.00441386.
After 16053 training step(s), loss on training batch is 0.00432436.
After 16054 training step(s), loss on training batch is 0.00553771.
After 16055 training step(s), loss on training batch is 0.00620567.
After 16056 training step(s), loss on training batch is 0.0046181.
After 16057 training step(s), loss on training batch is 0.00468599.
After 16058 training step(s), loss on training batch is 0.00859286.
After 16059 training step(s), loss on training batch is 0.00473087.
After 16060 training step(s), loss on training batch is 0.00548718.
After 16061 training step(s), loss on training batch is 0.00521027.
After 16062 training step(s), loss on training batch is 0.00467724.
After 16063 training step(s), loss on training batch is 0.00589401.
After 16064 training step(s), loss on training batch is 0.00488628.
After 16065 training step(s), loss on training batch is 0.00427434.
After 16066 training step(s), loss on training batch is 0.00512226.
After 16067 training step(s), loss on training batch is 0.00421775.
After 16068 training step(s), loss on training batch is 0.00492457.
After 16069 training step(s), loss on training batch is 0.00435718.
After 16070 training step(s), loss on training batch is 0.00434129.
After 16071 training step(s), loss on training batch is 0.00466881.
After 16072 training step(s), loss on training batch is 0.00455559.
After 16073 training step(s), loss on training batch is 0.00446281.
After 16074 training step(s), loss on training batch is 0.00475085.
After 16075 training step(s), loss on training batch is 0.00533993.
After 16076 training step(s), loss on training batch is 0.00473841.
After 16077 training step(s), loss on training batch is 0.00430809.
After 16078 training step(s), loss on training batch is 0.00439582.
After 16079 training step(s), loss on training batch is 0.0045232.
After 16080 training step(s), loss on training batch is 0.00438141.
After 16081 training step(s), loss on training batch is 0.00548669.
After 16082 training step(s), loss on training batch is 0.00444663.
After 16083 training step(s), loss on training batch is 0.00424759.
After 16084 training step(s), loss on training batch is 0.00484631.
After 16085 training step(s), loss on training batch is 0.00492943.
After 16086 training step(s), loss on training batch is 0.00468605.
After 16087 training step(s), loss on training batch is 0.00432179.
After 16088 training step(s), loss on training batch is 0.00473107.
After 16089 training step(s), loss on training batch is 0.00454689.
After 16090 training step(s), loss on training batch is 0.00540836.
After 16091 training step(s), loss on training batch is 0.0049979.
After 16092 training step(s), loss on training batch is 0.00449481.
After 16093 training step(s), loss on training batch is 0.00433077.
After 16094 training step(s), loss on training batch is 0.00505705.
After 16095 training step(s), loss on training batch is 0.00484334.
After 16096 training step(s), loss on training batch is 0.00439896.
After 16097 training step(s), loss on training batch is 0.0044245.
After 16098 training step(s), loss on training batch is 0.00436265.
After 16099 training step(s), loss on training batch is 0.00503394.
After 16100 training step(s), loss on training batch is 0.00523737.
After 16101 training step(s), loss on training batch is 0.00501087.
After 16102 training step(s), loss on training batch is 0.00458481.
After 16103 training step(s), loss on training batch is 0.00414676.
After 16104 training step(s), loss on training batch is 0.00489833.
After 16105 training step(s), loss on training batch is 0.00424264.
After 16106 training step(s), loss on training batch is 0.00469594.
After 16107 training step(s), loss on training batch is 0.00469223.
After 16108 training step(s), loss on training batch is 0.00462576.
After 16109 training step(s), loss on training batch is 0.00492491.
After 16110 training step(s), loss on training batch is 0.00511796.
After 16111 training step(s), loss on training batch is 0.00423495.
After 16112 training step(s), loss on training batch is 0.0050637.
After 16113 training step(s), loss on training batch is 0.00463431.
After 16114 training step(s), loss on training batch is 0.00499749.
After 16115 training step(s), loss on training batch is 0.00437547.
After 16116 training step(s), loss on training batch is 0.00479314.
After 16117 training step(s), loss on training batch is 0.00536279.
After 16118 training step(s), loss on training batch is 0.00405901.
After 16119 training step(s), loss on training batch is 0.00459296.
After 16120 training step(s), loss on training batch is 0.00470465.
After 16121 training step(s), loss on training batch is 0.00437012.
After 16122 training step(s), loss on training batch is 0.00455958.
After 16123 training step(s), loss on training batch is 0.00457002.
After 16124 training step(s), loss on training batch is 0.00442152.
After 16125 training step(s), loss on training batch is 0.00461562.
After 16126 training step(s), loss on training batch is 0.00492703.
After 16127 training step(s), loss on training batch is 0.00421613.
After 16128 training step(s), loss on training batch is 0.0049089.
After 16129 training step(s), loss on training batch is 0.0052538.
After 16130 training step(s), loss on training batch is 0.00473974.
After 16131 training step(s), loss on training batch is 0.00453124.
After 16132 training step(s), loss on training batch is 0.00449893.
After 16133 training step(s), loss on training batch is 0.00459141.
After 16134 training step(s), loss on training batch is 0.00452739.
After 16135 training step(s), loss on training batch is 0.00453623.
After 16136 training step(s), loss on training batch is 0.0043581.
After 16137 training step(s), loss on training batch is 0.00419237.
After 16138 training step(s), loss on training batch is 0.0048191.
After 16139 training step(s), loss on training batch is 0.00472789.
After 16140 training step(s), loss on training batch is 0.0056.
After 16141 training step(s), loss on training batch is 0.00474752.
After 16142 training step(s), loss on training batch is 0.00473429.
After 16143 training step(s), loss on training batch is 0.00485061.
After 16144 training step(s), loss on training batch is 0.00431217.
After 16145 training step(s), loss on training batch is 0.00471982.
After 16146 training step(s), loss on training batch is 0.00486424.
After 16147 training step(s), loss on training batch is 0.00512555.
After 16148 training step(s), loss on training batch is 0.00453218.
After 16149 training step(s), loss on training batch is 0.00430003.
After 16150 training step(s), loss on training batch is 0.00550067.
After 16151 training step(s), loss on training batch is 0.00484026.
After 16152 training step(s), loss on training batch is 0.00434735.
After 16153 training step(s), loss on training batch is 0.00487786.
After 16154 training step(s), loss on training batch is 0.00429627.
After 16155 training step(s), loss on training batch is 0.00414599.
After 16156 training step(s), loss on training batch is 0.00506627.
After 16157 training step(s), loss on training batch is 0.00481096.
After 16158 training step(s), loss on training batch is 0.00458851.
After 16159 training step(s), loss on training batch is 0.00472823.
After 16160 training step(s), loss on training batch is 0.00463355.
After 16161 training step(s), loss on training batch is 0.00441968.
After 16162 training step(s), loss on training batch is 0.00412444.
After 16163 training step(s), loss on training batch is 0.0046873.
After 16164 training step(s), loss on training batch is 0.00432918.
After 16165 training step(s), loss on training batch is 0.00461494.
After 16166 training step(s), loss on training batch is 0.00475741.
After 16167 training step(s), loss on training batch is 0.00630371.
After 16168 training step(s), loss on training batch is 0.00447144.
After 16169 training step(s), loss on training batch is 0.00458236.
After 16170 training step(s), loss on training batch is 0.00437676.
After 16171 training step(s), loss on training batch is 0.00453121.
After 16172 training step(s), loss on training batch is 0.00506126.
After 16173 training step(s), loss on training batch is 0.00483869.
After 16174 training step(s), loss on training batch is 0.00456999.
After 16175 training step(s), loss on training batch is 0.00434595.
After 16176 training step(s), loss on training batch is 0.00567691.
After 16177 training step(s), loss on training batch is 0.00605335.
After 16178 training step(s), loss on training batch is 0.00495145.
After 16179 training step(s), loss on training batch is 0.00492603.
After 16180 training step(s), loss on training batch is 0.00426476.
After 16181 training step(s), loss on training batch is 0.00438617.
After 16182 training step(s), loss on training batch is 0.00482308.
After 16183 training step(s), loss on training batch is 0.00485993.
After 16184 training step(s), loss on training batch is 0.0048007.
After 16185 training step(s), loss on training batch is 0.00482485.
After 16186 training step(s), loss on training batch is 0.00443516.
After 16187 training step(s), loss on training batch is 0.00408711.
After 16188 training step(s), loss on training batch is 0.0054031.
After 16189 training step(s), loss on training batch is 0.00479258.
After 16190 training step(s), loss on training batch is 0.00459505.
After 16191 training step(s), loss on training batch is 0.00618136.
After 16192 training step(s), loss on training batch is 0.00421312.
After 16193 training step(s), loss on training batch is 0.00458445.
After 16194 training step(s), loss on training batch is 0.00461005.
After 16195 training step(s), loss on training batch is 0.00465906.
After 16196 training step(s), loss on training batch is 0.00501198.
After 16197 training step(s), loss on training batch is 0.00518699.
After 16198 training step(s), loss on training batch is 0.00453416.
After 16199 training step(s), loss on training batch is 0.00434418.
After 16200 training step(s), loss on training batch is 0.00441539.
After 16201 training step(s), loss on training batch is 0.0048579.
After 16202 training step(s), loss on training batch is 0.00524524.
After 16203 training step(s), loss on training batch is 0.0045589.
After 16204 training step(s), loss on training batch is 0.00584464.
After 16205 training step(s), loss on training batch is 0.00483516.
After 16206 training step(s), loss on training batch is 0.00481088.
After 16207 training step(s), loss on training batch is 0.00456859.
After 16208 training step(s), loss on training batch is 0.00505892.
After 16209 training step(s), loss on training batch is 0.0047984.
After 16210 training step(s), loss on training batch is 0.00471809.
After 16211 training step(s), loss on training batch is 0.00433288.
After 16212 training step(s), loss on training batch is 0.00447206.
After 16213 training step(s), loss on training batch is 0.00548489.
After 16214 training step(s), loss on training batch is 0.00463239.
After 16215 training step(s), loss on training batch is 0.00413041.
After 16216 training step(s), loss on training batch is 0.00422676.
After 16217 training step(s), loss on training batch is 0.00472732.
After 16218 training step(s), loss on training batch is 0.00601777.
After 16219 training step(s), loss on training batch is 0.00435458.
After 16220 training step(s), loss on training batch is 0.00477723.
After 16221 training step(s), loss on training batch is 0.0043484.
After 16222 training step(s), loss on training batch is 0.00516934.
After 16223 training step(s), loss on training batch is 0.00451803.
After 16224 training step(s), loss on training batch is 0.00474851.
After 16225 training step(s), loss on training batch is 0.00453685.
After 16226 training step(s), loss on training batch is 0.00458847.
After 16227 training step(s), loss on training batch is 0.00558646.
After 16228 training step(s), loss on training batch is 0.00630342.
After 16229 training step(s), loss on training batch is 0.00420128.
After 16230 training step(s), loss on training batch is 0.00622719.
After 16231 training step(s), loss on training batch is 0.00472825.
After 16232 training step(s), loss on training batch is 0.0048703.
After 16233 training step(s), loss on training batch is 0.00426885.
After 16234 training step(s), loss on training batch is 0.00443969.
After 16235 training step(s), loss on training batch is 0.0043321.
After 16236 training step(s), loss on training batch is 0.00434388.
After 16237 training step(s), loss on training batch is 0.00515107.
After 16238 training step(s), loss on training batch is 0.00430775.
After 16239 training step(s), loss on training batch is 0.00486552.
After 16240 training step(s), loss on training batch is 0.00464241.
After 16241 training step(s), loss on training batch is 0.00470819.
After 16242 training step(s), loss on training batch is 0.00449106.
After 16243 training step(s), loss on training batch is 0.00443252.
After 16244 training step(s), loss on training batch is 0.00485018.
After 16245 training step(s), loss on training batch is 0.00454347.
After 16246 training step(s), loss on training batch is 0.00439286.
After 16247 training step(s), loss on training batch is 0.00424339.
After 16248 training step(s), loss on training batch is 0.00488142.
After 16249 training step(s), loss on training batch is 0.00436297.
After 16250 training step(s), loss on training batch is 0.00451614.
After 16251 training step(s), loss on training batch is 0.0059365.
After 16252 training step(s), loss on training batch is 0.00456297.
After 16253 training step(s), loss on training batch is 0.00484515.
After 16254 training step(s), loss on training batch is 0.00444746.
After 16255 training step(s), loss on training batch is 0.00565845.
After 16256 training step(s), loss on training batch is 0.00429383.
After 16257 training step(s), loss on training batch is 0.00467247.
After 16258 training step(s), loss on training batch is 0.00420367.
After 16259 training step(s), loss on training batch is 0.00468261.
After 16260 training step(s), loss on training batch is 0.00439095.
After 16261 training step(s), loss on training batch is 0.0046125.
After 16262 training step(s), loss on training batch is 0.00465752.
After 16263 training step(s), loss on training batch is 0.00463854.
After 16264 training step(s), loss on training batch is 0.00493675.
After 16265 training step(s), loss on training batch is 0.0045827.
After 16266 training step(s), loss on training batch is 0.00439566.
After 16267 training step(s), loss on training batch is 0.00487209.
After 16268 training step(s), loss on training batch is 0.00407213.
After 16269 training step(s), loss on training batch is 0.00483512.
After 16270 training step(s), loss on training batch is 0.00722645.
After 16271 training step(s), loss on training batch is 0.00520242.
After 16272 training step(s), loss on training batch is 0.00428562.
After 16273 training step(s), loss on training batch is 0.00586775.
After 16274 training step(s), loss on training batch is 0.00416905.
After 16275 training step(s), loss on training batch is 0.00522988.
After 16276 training step(s), loss on training batch is 0.00518299.
After 16277 training step(s), loss on training batch is 0.00425441.
After 16278 training step(s), loss on training batch is 0.0049534.
After 16279 training step(s), loss on training batch is 0.00459495.
After 16280 training step(s), loss on training batch is 0.00569768.
After 16281 training step(s), loss on training batch is 0.0049115.
After 16282 training step(s), loss on training batch is 0.00418826.
After 16283 training step(s), loss on training batch is 0.00463319.
After 16284 training step(s), loss on training batch is 0.00463687.
After 16285 training step(s), loss on training batch is 0.00549847.
After 16286 training step(s), loss on training batch is 0.00476958.
After 16287 training step(s), loss on training batch is 0.00445759.
After 16288 training step(s), loss on training batch is 0.00473955.
After 16289 training step(s), loss on training batch is 0.00407153.
After 16290 training step(s), loss on training batch is 0.00431357.
After 16291 training step(s), loss on training batch is 0.00462494.
After 16292 training step(s), loss on training batch is 0.00441256.
After 16293 training step(s), loss on training batch is 0.00437683.
After 16294 training step(s), loss on training batch is 0.00458978.
After 16295 training step(s), loss on training batch is 0.00458581.
After 16296 training step(s), loss on training batch is 0.00490339.
After 16297 training step(s), loss on training batch is 0.00431372.
After 16298 training step(s), loss on training batch is 0.005342.
After 16299 training step(s), loss on training batch is 0.00447785.
After 16300 training step(s), loss on training batch is 0.00472553.
After 16301 training step(s), loss on training batch is 0.0041491.
After 16302 training step(s), loss on training batch is 0.00462827.
After 16303 training step(s), loss on training batch is 0.00484404.
After 16304 training step(s), loss on training batch is 0.00509724.
After 16305 training step(s), loss on training batch is 0.00475599.
After 16306 training step(s), loss on training batch is 0.00427042.
After 16307 training step(s), loss on training batch is 0.00425751.
After 16308 training step(s), loss on training batch is 0.0045636.
After 16309 training step(s), loss on training batch is 0.00422367.
After 16310 training step(s), loss on training batch is 0.0050892.
After 16311 training step(s), loss on training batch is 0.00506701.
After 16312 training step(s), loss on training batch is 0.00430387.
After 16313 training step(s), loss on training batch is 0.00511277.
After 16314 training step(s), loss on training batch is 0.00437568.
After 16315 training step(s), loss on training batch is 0.0052211.
After 16316 training step(s), loss on training batch is 0.00446335.
After 16317 training step(s), loss on training batch is 0.00545873.
After 16318 training step(s), loss on training batch is 0.00440993.
After 16319 training step(s), loss on training batch is 0.00578765.
After 16320 training step(s), loss on training batch is 0.00466954.
After 16321 training step(s), loss on training batch is 0.00446736.
After 16322 training step(s), loss on training batch is 0.00470811.
After 16323 training step(s), loss on training batch is 0.00444268.
After 16324 training step(s), loss on training batch is 0.00427336.
After 16325 training step(s), loss on training batch is 0.0047414.
After 16326 training step(s), loss on training batch is 0.0046117.
After 16327 training step(s), loss on training batch is 0.00435042.
After 16328 training step(s), loss on training batch is 0.00487889.
After 16329 training step(s), loss on training batch is 0.00427369.
After 16330 training step(s), loss on training batch is 0.00429858.
After 16331 training step(s), loss on training batch is 0.00446296.
After 16332 training step(s), loss on training batch is 0.0041462.
After 16333 training step(s), loss on training batch is 0.00454391.
After 16334 training step(s), loss on training batch is 0.00477272.
After 16335 training step(s), loss on training batch is 0.0051502.
After 16336 training step(s), loss on training batch is 0.00418642.
After 16337 training step(s), loss on training batch is 0.00480607.
After 16338 training step(s), loss on training batch is 0.00423869.
After 16339 training step(s), loss on training batch is 0.00473043.
After 16340 training step(s), loss on training batch is 0.00450643.
After 16341 training step(s), loss on training batch is 0.00463536.
After 16342 training step(s), loss on training batch is 0.00457476.
After 16343 training step(s), loss on training batch is 0.00453752.
After 16344 training step(s), loss on training batch is 0.00470637.
After 16345 training step(s), loss on training batch is 0.0045214.
After 16346 training step(s), loss on training batch is 0.00464198.
After 16347 training step(s), loss on training batch is 0.0042879.
After 16348 training step(s), loss on training batch is 0.0046419.
After 16349 training step(s), loss on training batch is 0.00472173.
After 16350 training step(s), loss on training batch is 0.00592432.
After 16351 training step(s), loss on training batch is 0.00508857.
After 16352 training step(s), loss on training batch is 0.0043036.
After 16353 training step(s), loss on training batch is 0.00465086.
After 16354 training step(s), loss on training batch is 0.00417993.
After 16355 training step(s), loss on training batch is 0.00464019.
After 16356 training step(s), loss on training batch is 0.00414654.
After 16357 training step(s), loss on training batch is 0.0051561.
After 16358 training step(s), loss on training batch is 0.00478079.
After 16359 training step(s), loss on training batch is 0.00436259.
After 16360 training step(s), loss on training batch is 0.00466535.
After 16361 training step(s), loss on training batch is 0.00517783.
After 16362 training step(s), loss on training batch is 0.0048891.
After 16363 training step(s), loss on training batch is 0.00413238.
After 16364 training step(s), loss on training batch is 0.00410584.
After 16365 training step(s), loss on training batch is 0.00426259.
After 16366 training step(s), loss on training batch is 0.00653737.
After 16367 training step(s), loss on training batch is 0.00468832.
After 16368 training step(s), loss on training batch is 0.00459928.
After 16369 training step(s), loss on training batch is 0.00427123.
After 16370 training step(s), loss on training batch is 0.00492392.
After 16371 training step(s), loss on training batch is 0.00417697.
After 16372 training step(s), loss on training batch is 0.00428838.
After 16373 training step(s), loss on training batch is 0.00540466.
After 16374 training step(s), loss on training batch is 0.00416692.
After 16375 training step(s), loss on training batch is 0.00462842.
After 16376 training step(s), loss on training batch is 0.00444726.
After 16377 training step(s), loss on training batch is 0.00481852.
After 16378 training step(s), loss on training batch is 0.00430566.
After 16379 training step(s), loss on training batch is 0.00489273.
After 16380 training step(s), loss on training batch is 0.00465308.
After 16381 training step(s), loss on training batch is 0.00411867.
After 16382 training step(s), loss on training batch is 0.00473544.
After 16383 training step(s), loss on training batch is 0.00488992.
After 16384 training step(s), loss on training batch is 0.00431852.
After 16385 training step(s), loss on training batch is 0.00423987.
After 16386 training step(s), loss on training batch is 0.00505111.
After 16387 training step(s), loss on training batch is 0.00434025.
After 16388 training step(s), loss on training batch is 0.00473099.
After 16389 training step(s), loss on training batch is 0.0045082.
After 16390 training step(s), loss on training batch is 0.00433989.
After 16391 training step(s), loss on training batch is 0.00427284.
After 16392 training step(s), loss on training batch is 0.00467998.
After 16393 training step(s), loss on training batch is 0.00474916.
After 16394 training step(s), loss on training batch is 0.00439696.
After 16395 training step(s), loss on training batch is 0.0041316.
After 16396 training step(s), loss on training batch is 0.0046712.
After 16397 training step(s), loss on training batch is 0.00453888.
After 16398 training step(s), loss on training batch is 0.00437707.
After 16399 training step(s), loss on training batch is 0.00450263.
After 16400 training step(s), loss on training batch is 0.00472694.
After 16401 training step(s), loss on training batch is 0.00431551.
After 16402 training step(s), loss on training batch is 0.00455201.
After 16403 training step(s), loss on training batch is 0.0046181.
After 16404 training step(s), loss on training batch is 0.00445945.
After 16405 training step(s), loss on training batch is 0.00418843.
After 16406 training step(s), loss on training batch is 0.00436302.
After 16407 training step(s), loss on training batch is 0.00432217.
After 16408 training step(s), loss on training batch is 0.00490291.
After 16409 training step(s), loss on training batch is 0.00436896.
After 16410 training step(s), loss on training batch is 0.00419429.
After 16411 training step(s), loss on training batch is 0.00455644.
After 16412 training step(s), loss on training batch is 0.0047781.
After 16413 training step(s), loss on training batch is 0.00436566.
After 16414 training step(s), loss on training batch is 0.00427422.
After 16415 training step(s), loss on training batch is 0.00452846.
After 16416 training step(s), loss on training batch is 0.00503657.
After 16417 training step(s), loss on training batch is 0.00555944.
After 16418 training step(s), loss on training batch is 0.00429374.
After 16419 training step(s), loss on training batch is 0.00425297.
After 16420 training step(s), loss on training batch is 0.00465027.
After 16421 training step(s), loss on training batch is 0.00400741.
After 16422 training step(s), loss on training batch is 0.00447874.
After 16423 training step(s), loss on training batch is 0.00450128.
After 16424 training step(s), loss on training batch is 0.00418984.
After 16425 training step(s), loss on training batch is 0.00452057.
After 16426 training step(s), loss on training batch is 0.00443005.
After 16427 training step(s), loss on training batch is 0.00472168.
After 16428 training step(s), loss on training batch is 0.00481867.
After 16429 training step(s), loss on training batch is 0.004906.
After 16430 training step(s), loss on training batch is 0.00430942.
After 16431 training step(s), loss on training batch is 0.00434767.
After 16432 training step(s), loss on training batch is 0.00482295.
After 16433 training step(s), loss on training batch is 0.00431379.
After 16434 training step(s), loss on training batch is 0.0053233.
After 16435 training step(s), loss on training batch is 0.00493498.
After 16436 training step(s), loss on training batch is 0.0046915.
After 16437 training step(s), loss on training batch is 0.00452919.
After 16438 training step(s), loss on training batch is 0.00409464.
After 16439 training step(s), loss on training batch is 0.00567415.
After 16440 training step(s), loss on training batch is 0.00406688.
After 16441 training step(s), loss on training batch is 0.00506369.
After 16442 training step(s), loss on training batch is 0.00447536.
After 16443 training step(s), loss on training batch is 0.00463639.
After 16444 training step(s), loss on training batch is 0.00448716.
After 16445 training step(s), loss on training batch is 0.0042558.
After 16446 training step(s), loss on training batch is 0.00450359.
After 16447 training step(s), loss on training batch is 0.00518249.
After 16448 training step(s), loss on training batch is 0.00454738.
After 16449 training step(s), loss on training batch is 0.00447793.
After 16450 training step(s), loss on training batch is 0.00442242.
After 16451 training step(s), loss on training batch is 0.00420145.
After 16452 training step(s), loss on training batch is 0.00424333.
After 16453 training step(s), loss on training batch is 0.00483374.
After 16454 training step(s), loss on training batch is 0.00451701.
After 16455 training step(s), loss on training batch is 0.00472743.
After 16456 training step(s), loss on training batch is 0.00462355.
After 16457 training step(s), loss on training batch is 0.00454279.
After 16458 training step(s), loss on training batch is 0.00465343.
After 16459 training step(s), loss on training batch is 0.0043444.
After 16460 training step(s), loss on training batch is 0.00473346.
After 16461 training step(s), loss on training batch is 0.00440784.
After 16462 training step(s), loss on training batch is 0.00399005.
After 16463 training step(s), loss on training batch is 0.00516372.
After 16464 training step(s), loss on training batch is 0.00440787.
After 16465 training step(s), loss on training batch is 0.00448148.
After 16466 training step(s), loss on training batch is 0.00444541.
After 16467 training step(s), loss on training batch is 0.00429832.
After 16468 training step(s), loss on training batch is 0.00453939.
After 16469 training step(s), loss on training batch is 0.00511416.
After 16470 training step(s), loss on training batch is 0.00521437.
After 16471 training step(s), loss on training batch is 0.00406128.
After 16472 training step(s), loss on training batch is 0.00445238.
After 16473 training step(s), loss on training batch is 0.00461948.
After 16474 training step(s), loss on training batch is 0.00514753.
After 16475 training step(s), loss on training batch is 0.00472165.
After 16476 training step(s), loss on training batch is 0.00463478.
After 16477 training step(s), loss on training batch is 0.00397116.
After 16478 training step(s), loss on training batch is 0.00458819.
After 16479 training step(s), loss on training batch is 0.00446337.
After 16480 training step(s), loss on training batch is 0.00431398.
After 16481 training step(s), loss on training batch is 0.00500363.
After 16482 training step(s), loss on training batch is 0.00406441.
After 16483 training step(s), loss on training batch is 0.0047164.
After 16484 training step(s), loss on training batch is 0.00585907.
After 16485 training step(s), loss on training batch is 0.00408638.
After 16486 training step(s), loss on training batch is 0.00475593.
After 16487 training step(s), loss on training batch is 0.00453229.
After 16488 training step(s), loss on training batch is 0.00463529.
After 16489 training step(s), loss on training batch is 0.00446352.
After 16490 training step(s), loss on training batch is 0.00503474.
After 16491 training step(s), loss on training batch is 0.00409393.
After 16492 training step(s), loss on training batch is 0.00656702.
After 16493 training step(s), loss on training batch is 0.00458939.
After 16494 training step(s), loss on training batch is 0.00539564.
After 16495 training step(s), loss on training batch is 0.00476736.
After 16496 training step(s), loss on training batch is 0.00439374.
After 16497 training step(s), loss on training batch is 0.00402592.
After 16498 training step(s), loss on training batch is 0.00468233.
After 16499 training step(s), loss on training batch is 0.00441505.
After 16500 training step(s), loss on training batch is 0.00470685.
After 16501 training step(s), loss on training batch is 0.00408427.
After 16502 training step(s), loss on training batch is 0.00440334.
After 16503 training step(s), loss on training batch is 0.00421296.
After 16504 training step(s), loss on training batch is 0.00446568.
After 16505 training step(s), loss on training batch is 0.0044419.
After 16506 training step(s), loss on training batch is 0.00422061.
After 16507 training step(s), loss on training batch is 0.00483066.
After 16508 training step(s), loss on training batch is 0.00430314.
After 16509 training step(s), loss on training batch is 0.00467224.
After 16510 training step(s), loss on training batch is 0.00610232.
After 16511 training step(s), loss on training batch is 0.00503996.
After 16512 training step(s), loss on training batch is 0.00501599.
After 16513 training step(s), loss on training batch is 0.00448008.
After 16514 training step(s), loss on training batch is 0.00454871.
After 16515 training step(s), loss on training batch is 0.00405236.
After 16516 training step(s), loss on training batch is 0.00481036.
After 16517 training step(s), loss on training batch is 0.00447901.
After 16518 training step(s), loss on training batch is 0.00464521.
After 16519 training step(s), loss on training batch is 0.00437457.
After 16520 training step(s), loss on training batch is 0.00586766.
After 16521 training step(s), loss on training batch is 0.0043711.
After 16522 training step(s), loss on training batch is 0.00412698.
After 16523 training step(s), loss on training batch is 0.00488396.
After 16524 training step(s), loss on training batch is 0.00448158.
After 16525 training step(s), loss on training batch is 0.00413983.
After 16526 training step(s), loss on training batch is 0.00440625.
After 16527 training step(s), loss on training batch is 0.00445156.
After 16528 training step(s), loss on training batch is 0.00465104.
After 16529 training step(s), loss on training batch is 0.00414761.
After 16530 training step(s), loss on training batch is 0.0045688.
After 16531 training step(s), loss on training batch is 0.0041997.
After 16532 training step(s), loss on training batch is 0.00453989.
After 16533 training step(s), loss on training batch is 0.0042448.
After 16534 training step(s), loss on training batch is 0.00476443.
After 16535 training step(s), loss on training batch is 0.00464291.
After 16536 training step(s), loss on training batch is 0.00454446.
After 16537 training step(s), loss on training batch is 0.00409077.
After 16538 training step(s), loss on training batch is 0.0045649.
After 16539 training step(s), loss on training batch is 0.00409037.
After 16540 training step(s), loss on training batch is 0.00410276.
After 16541 training step(s), loss on training batch is 0.00480808.
After 16542 training step(s), loss on training batch is 0.00424165.
After 16543 training step(s), loss on training batch is 0.00418917.
After 16544 training step(s), loss on training batch is 0.00466963.
After 16545 training step(s), loss on training batch is 0.00434829.
After 16546 training step(s), loss on training batch is 0.00465747.
After 16547 training step(s), loss on training batch is 0.00448985.
After 16548 training step(s), loss on training batch is 0.00450536.
After 16549 training step(s), loss on training batch is 0.00516182.
After 16550 training step(s), loss on training batch is 0.00436449.
After 16551 training step(s), loss on training batch is 0.00441533.
After 16552 training step(s), loss on training batch is 0.00475765.
After 16553 training step(s), loss on training batch is 0.00470654.
After 16554 training step(s), loss on training batch is 0.00442295.
After 16555 training step(s), loss on training batch is 0.00438406.
After 16556 training step(s), loss on training batch is 0.00436771.
After 16557 training step(s), loss on training batch is 0.00491918.
After 16558 training step(s), loss on training batch is 0.00473762.
After 16559 training step(s), loss on training batch is 0.00446217.
After 16560 training step(s), loss on training batch is 0.00516913.
After 16561 training step(s), loss on training batch is 0.00456851.
After 16562 training step(s), loss on training batch is 0.00413238.
After 16563 training step(s), loss on training batch is 0.00435949.
After 16564 training step(s), loss on training batch is 0.00462911.
After 16565 training step(s), loss on training batch is 0.00474793.
After 16566 training step(s), loss on training batch is 0.00460876.
After 16567 training step(s), loss on training batch is 0.00532065.
After 16568 training step(s), loss on training batch is 0.00411112.
After 16569 training step(s), loss on training batch is 0.00411929.
After 16570 training step(s), loss on training batch is 0.00449594.
After 16571 training step(s), loss on training batch is 0.00456156.
After 16572 training step(s), loss on training batch is 0.00461796.
After 16573 training step(s), loss on training batch is 0.00495641.
After 16574 training step(s), loss on training batch is 0.00472034.
After 16575 training step(s), loss on training batch is 0.00468252.
After 16576 training step(s), loss on training batch is 0.00410173.
After 16577 training step(s), loss on training batch is 0.00425167.
After 16578 training step(s), loss on training batch is 0.0046692.
After 16579 training step(s), loss on training batch is 0.00459989.
After 16580 training step(s), loss on training batch is 0.00396595.
After 16581 training step(s), loss on training batch is 0.00414884.
After 16582 training step(s), loss on training batch is 0.00411028.
After 16583 training step(s), loss on training batch is 0.0040296.
After 16584 training step(s), loss on training batch is 0.00414391.
After 16585 training step(s), loss on training batch is 0.00426266.
After 16586 training step(s), loss on training batch is 0.00416981.
After 16587 training step(s), loss on training batch is 0.00456832.
After 16588 training step(s), loss on training batch is 0.00461198.
After 16589 training step(s), loss on training batch is 0.00437195.
After 16590 training step(s), loss on training batch is 0.0042726.
After 16591 training step(s), loss on training batch is 0.00441485.
After 16592 training step(s), loss on training batch is 0.00531292.
After 16593 training step(s), loss on training batch is 0.00421685.
After 16594 training step(s), loss on training batch is 0.00428528.
After 16595 training step(s), loss on training batch is 0.00482187.
After 16596 training step(s), loss on training batch is 0.00455452.
After 16597 training step(s), loss on training batch is 0.00423403.
After 16598 training step(s), loss on training batch is 0.00438368.
After 16599 training step(s), loss on training batch is 0.004223.
After 16600 training step(s), loss on training batch is 0.00427785.
After 16601 training step(s), loss on training batch is 0.00543949.
After 16602 training step(s), loss on training batch is 0.00425963.
After 16603 training step(s), loss on training batch is 0.00551725.
After 16604 training step(s), loss on training batch is 0.00488832.
After 16605 training step(s), loss on training batch is 0.00436172.
After 16606 training step(s), loss on training batch is 0.00418985.
After 16607 training step(s), loss on training batch is 0.00425016.
After 16608 training step(s), loss on training batch is 0.00428906.
After 16609 training step(s), loss on training batch is 0.00540376.
After 16610 training step(s), loss on training batch is 0.00507443.
After 16611 training step(s), loss on training batch is 0.00419538.
After 16612 training step(s), loss on training batch is 0.00417576.
After 16613 training step(s), loss on training batch is 0.00439882.
After 16614 training step(s), loss on training batch is 0.00440185.
After 16615 training step(s), loss on training batch is 0.00424233.
After 16616 training step(s), loss on training batch is 0.00414736.
After 16617 training step(s), loss on training batch is 0.00448575.
After 16618 training step(s), loss on training batch is 0.00395875.
After 16619 training step(s), loss on training batch is 0.00438844.
After 16620 training step(s), loss on training batch is 0.00447902.
After 16621 training step(s), loss on training batch is 0.00438078.
After 16622 training step(s), loss on training batch is 0.00457843.
After 16623 training step(s), loss on training batch is 0.00456641.
After 16624 training step(s), loss on training batch is 0.00437028.
After 16625 training step(s), loss on training batch is 0.00462151.
After 16626 training step(s), loss on training batch is 0.00418564.
After 16627 training step(s), loss on training batch is 0.0043758.
After 16628 training step(s), loss on training batch is 0.00488203.
After 16629 training step(s), loss on training batch is 0.0041546.
After 16630 training step(s), loss on training batch is 0.0052786.
After 16631 training step(s), loss on training batch is 0.00440976.
After 16632 training step(s), loss on training batch is 0.00451404.
After 16633 training step(s), loss on training batch is 0.00527993.
After 16634 training step(s), loss on training batch is 0.00464698.
After 16635 training step(s), loss on training batch is 0.0042724.
After 16636 training step(s), loss on training batch is 0.00433902.
After 16637 training step(s), loss on training batch is 0.00462534.
After 16638 training step(s), loss on training batch is 0.00410832.
After 16639 training step(s), loss on training batch is 0.0048401.
After 16640 training step(s), loss on training batch is 0.00455724.
After 16641 training step(s), loss on training batch is 0.00470289.
After 16642 training step(s), loss on training batch is 0.00438947.
After 16643 training step(s), loss on training batch is 0.0052822.
After 16644 training step(s), loss on training batch is 0.00417379.
After 16645 training step(s), loss on training batch is 0.00408013.
After 16646 training step(s), loss on training batch is 0.00407868.
After 16647 training step(s), loss on training batch is 0.00528703.
After 16648 training step(s), loss on training batch is 0.00446339.
After 16649 training step(s), loss on training batch is 0.0057646.
After 16650 training step(s), loss on training batch is 0.00461377.
After 16651 training step(s), loss on training batch is 0.00403876.
After 16652 training step(s), loss on training batch is 0.00453735.
After 16653 training step(s), loss on training batch is 0.00439323.
After 16654 training step(s), loss on training batch is 0.00423998.
After 16655 training step(s), loss on training batch is 0.00419737.
After 16656 training step(s), loss on training batch is 0.00430699.
After 16657 training step(s), loss on training batch is 0.00448104.
After 16658 training step(s), loss on training batch is 0.00410339.
After 16659 training step(s), loss on training batch is 0.0048288.
After 16660 training step(s), loss on training batch is 0.00489496.
After 16661 training step(s), loss on training batch is 0.0045807.
After 16662 training step(s), loss on training batch is 0.00464096.
After 16663 training step(s), loss on training batch is 0.0041989.
After 16664 training step(s), loss on training batch is 0.00412407.
After 16665 training step(s), loss on training batch is 0.00444606.
After 16666 training step(s), loss on training batch is 0.00416275.
After 16667 training step(s), loss on training batch is 0.00473138.
After 16668 training step(s), loss on training batch is 0.0048468.
After 16669 training step(s), loss on training batch is 0.00393594.
After 16670 training step(s), loss on training batch is 0.00450289.
After 16671 training step(s), loss on training batch is 0.00481199.
After 16672 training step(s), loss on training batch is 0.00466431.
After 16673 training step(s), loss on training batch is 0.00465238.
After 16674 training step(s), loss on training batch is 0.00465384.
After 16675 training step(s), loss on training batch is 0.00474708.
After 16676 training step(s), loss on training batch is 0.00436662.
After 16677 training step(s), loss on training batch is 0.00415417.
After 16678 training step(s), loss on training batch is 0.00413302.
After 16679 training step(s), loss on training batch is 0.00486381.
After 16680 training step(s), loss on training batch is 0.00483206.
After 16681 training step(s), loss on training batch is 0.00464671.
After 16682 training step(s), loss on training batch is 0.00447182.
After 16683 training step(s), loss on training batch is 0.00433401.
After 16684 training step(s), loss on training batch is 0.00422006.
After 16685 training step(s), loss on training batch is 0.00496158.
After 16686 training step(s), loss on training batch is 0.00448791.
After 16687 training step(s), loss on training batch is 0.00433609.
After 16688 training step(s), loss on training batch is 0.00510856.
After 16689 training step(s), loss on training batch is 0.0045235.
After 16690 training step(s), loss on training batch is 0.00594402.
After 16691 training step(s), loss on training batch is 0.00686269.
After 16692 training step(s), loss on training batch is 0.00455123.
After 16693 training step(s), loss on training batch is 0.00494446.
After 16694 training step(s), loss on training batch is 0.00558382.
After 16695 training step(s), loss on training batch is 0.00392189.
After 16696 training step(s), loss on training batch is 0.00427324.
After 16697 training step(s), loss on training batch is 0.0047303.
After 16698 training step(s), loss on training batch is 0.00422513.
After 16699 training step(s), loss on training batch is 0.00443885.
After 16700 training step(s), loss on training batch is 0.00499986.
After 16701 training step(s), loss on training batch is 0.00420869.
After 16702 training step(s), loss on training batch is 0.00474368.
After 16703 training step(s), loss on training batch is 0.00478176.
After 16704 training step(s), loss on training batch is 0.00453868.
After 16705 training step(s), loss on training batch is 0.00698062.
After 16706 training step(s), loss on training batch is 0.00458546.
After 16707 training step(s), loss on training batch is 0.00415531.
After 16708 training step(s), loss on training batch is 0.00523529.
After 16709 training step(s), loss on training batch is 0.00457454.
After 16710 training step(s), loss on training batch is 0.00420979.
After 16711 training step(s), loss on training batch is 0.00478053.
After 16712 training step(s), loss on training batch is 0.0042661.
After 16713 training step(s), loss on training batch is 0.00444578.
After 16714 training step(s), loss on training batch is 0.00444745.
After 16715 training step(s), loss on training batch is 0.00486008.
After 16716 training step(s), loss on training batch is 0.00467369.
After 16717 training step(s), loss on training batch is 0.00415477.
After 16718 training step(s), loss on training batch is 0.00493261.
After 16719 training step(s), loss on training batch is 0.00419844.
After 16720 training step(s), loss on training batch is 0.00426914.
After 16721 training step(s), loss on training batch is 0.00545008.
After 16722 training step(s), loss on training batch is 0.00492862.
After 16723 training step(s), loss on training batch is 0.00446738.
After 16724 training step(s), loss on training batch is 0.00450034.
After 16725 training step(s), loss on training batch is 0.00421418.
After 16726 training step(s), loss on training batch is 0.00410438.
After 16727 training step(s), loss on training batch is 0.00466743.
After 16728 training step(s), loss on training batch is 0.00506025.
After 16729 training step(s), loss on training batch is 0.00447689.
After 16730 training step(s), loss on training batch is 0.00450803.
After 16731 training step(s), loss on training batch is 0.00420126.
After 16732 training step(s), loss on training batch is 0.00443817.
After 16733 training step(s), loss on training batch is 0.00458958.
After 16734 training step(s), loss on training batch is 0.0055563.
After 16735 training step(s), loss on training batch is 0.00456601.
After 16736 training step(s), loss on training batch is 0.00487865.
After 16737 training step(s), loss on training batch is 0.00658685.
After 16738 training step(s), loss on training batch is 0.00550804.
After 16739 training step(s), loss on training batch is 0.00490585.
After 16740 training step(s), loss on training batch is 0.00487131.
After 16741 training step(s), loss on training batch is 0.00423722.
After 16742 training step(s), loss on training batch is 0.00488604.
After 16743 training step(s), loss on training batch is 0.00421125.
After 16744 training step(s), loss on training batch is 0.00447599.
After 16745 training step(s), loss on training batch is 0.00509976.
After 16746 training step(s), loss on training batch is 0.0061417.
After 16747 training step(s), loss on training batch is 0.00469531.
After 16748 training step(s), loss on training batch is 0.0051612.
After 16749 training step(s), loss on training batch is 0.00404241.
After 16750 training step(s), loss on training batch is 0.0047891.
After 16751 training step(s), loss on training batch is 0.00469109.
After 16752 training step(s), loss on training batch is 0.00474984.
After 16753 training step(s), loss on training batch is 0.00443861.
After 16754 training step(s), loss on training batch is 0.00489058.
After 16755 training step(s), loss on training batch is 0.00472841.
After 16756 training step(s), loss on training batch is 0.00446947.
After 16757 training step(s), loss on training batch is 0.00457881.
After 16758 training step(s), loss on training batch is 0.00472808.
After 16759 training step(s), loss on training batch is 0.00514786.
After 16760 training step(s), loss on training batch is 0.00437274.
After 16761 training step(s), loss on training batch is 0.00467156.
After 16762 training step(s), loss on training batch is 0.00433914.
After 16763 training step(s), loss on training batch is 0.00455151.
After 16764 training step(s), loss on training batch is 0.00439506.
After 16765 training step(s), loss on training batch is 0.00597711.
After 16766 training step(s), loss on training batch is 0.00447697.
After 16767 training step(s), loss on training batch is 0.00430368.
After 16768 training step(s), loss on training batch is 0.00439927.
After 16769 training step(s), loss on training batch is 0.00451856.
After 16770 training step(s), loss on training batch is 0.00426399.
After 16771 training step(s), loss on training batch is 0.00440854.
After 16772 training step(s), loss on training batch is 0.00417748.
After 16773 training step(s), loss on training batch is 0.00467456.
After 16774 training step(s), loss on training batch is 0.0041023.
After 16775 training step(s), loss on training batch is 0.00467436.
After 16776 training step(s), loss on training batch is 0.00475972.
After 16777 training step(s), loss on training batch is 0.00429828.
After 16778 training step(s), loss on training batch is 0.00495437.
After 16779 training step(s), loss on training batch is 0.00403389.
After 16780 training step(s), loss on training batch is 0.00445529.
After 16781 training step(s), loss on training batch is 0.0044735.
After 16782 training step(s), loss on training batch is 0.00464581.
After 16783 training step(s), loss on training batch is 0.00427621.
After 16784 training step(s), loss on training batch is 0.00445519.
After 16785 training step(s), loss on training batch is 0.00459113.
After 16786 training step(s), loss on training batch is 0.00437694.
After 16787 training step(s), loss on training batch is 0.00427869.
After 16788 training step(s), loss on training batch is 0.00645019.
After 16789 training step(s), loss on training batch is 0.00448997.
After 16790 training step(s), loss on training batch is 0.00450798.
After 16791 training step(s), loss on training batch is 0.00484812.
After 16792 training step(s), loss on training batch is 0.0044996.
After 16793 training step(s), loss on training batch is 0.00451602.
After 16794 training step(s), loss on training batch is 0.00466836.
After 16795 training step(s), loss on training batch is 0.00461184.
After 16796 training step(s), loss on training batch is 0.00472492.
After 16797 training step(s), loss on training batch is 0.00532673.
After 16798 training step(s), loss on training batch is 0.00428929.
After 16799 training step(s), loss on training batch is 0.00448314.
After 16800 training step(s), loss on training batch is 0.00398779.
After 16801 training step(s), loss on training batch is 0.00434756.
After 16802 training step(s), loss on training batch is 0.00525068.
After 16803 training step(s), loss on training batch is 0.00420531.
After 16804 training step(s), loss on training batch is 0.00495701.
After 16805 training step(s), loss on training batch is 0.00402871.
After 16806 training step(s), loss on training batch is 0.0039706.
After 16807 training step(s), loss on training batch is 0.00431959.
After 16808 training step(s), loss on training batch is 0.00403076.
After 16809 training step(s), loss on training batch is 0.00525242.
After 16810 training step(s), loss on training batch is 0.00452438.
After 16811 training step(s), loss on training batch is 0.00416665.
After 16812 training step(s), loss on training batch is 0.00406716.
After 16813 training step(s), loss on training batch is 0.00449075.
After 16814 training step(s), loss on training batch is 0.00457676.
After 16815 training step(s), loss on training batch is 0.00429442.
After 16816 training step(s), loss on training batch is 0.00447753.
After 16817 training step(s), loss on training batch is 0.00402721.
After 16818 training step(s), loss on training batch is 0.00427801.
After 16819 training step(s), loss on training batch is 0.0045481.
After 16820 training step(s), loss on training batch is 0.00461554.
After 16821 training step(s), loss on training batch is 0.00583361.
After 16822 training step(s), loss on training batch is 0.00463461.
After 16823 training step(s), loss on training batch is 0.0043416.
After 16824 training step(s), loss on training batch is 0.00443212.
After 16825 training step(s), loss on training batch is 0.00406409.
After 16826 training step(s), loss on training batch is 0.00409311.
After 16827 training step(s), loss on training batch is 0.00429225.
After 16828 training step(s), loss on training batch is 0.00490191.
After 16829 training step(s), loss on training batch is 0.00446259.
After 16830 training step(s), loss on training batch is 0.00430162.
After 16831 training step(s), loss on training batch is 0.00399085.
After 16832 training step(s), loss on training batch is 0.0046173.
After 16833 training step(s), loss on training batch is 0.00440774.
After 16834 training step(s), loss on training batch is 0.00449448.
After 16835 training step(s), loss on training batch is 0.00430814.
After 16836 training step(s), loss on training batch is 0.00436438.
After 16837 training step(s), loss on training batch is 0.00493705.
After 16838 training step(s), loss on training batch is 0.00455901.
After 16839 training step(s), loss on training batch is 0.00406585.
After 16840 training step(s), loss on training batch is 0.00429173.
After 16841 training step(s), loss on training batch is 0.00445965.
After 16842 training step(s), loss on training batch is 0.00419684.
After 16843 training step(s), loss on training batch is 0.00430042.
After 16844 training step(s), loss on training batch is 0.00452104.
After 16845 training step(s), loss on training batch is 0.00429129.
After 16846 training step(s), loss on training batch is 0.00480533.
After 16847 training step(s), loss on training batch is 0.00497748.
After 16848 training step(s), loss on training batch is 0.00463825.
After 16849 training step(s), loss on training batch is 0.00471441.
After 16850 training step(s), loss on training batch is 0.00399791.
After 16851 training step(s), loss on training batch is 0.00470262.
After 16852 training step(s), loss on training batch is 0.0043167.
After 16853 training step(s), loss on training batch is 0.00430571.
After 16854 training step(s), loss on training batch is 0.00406466.
After 16855 training step(s), loss on training batch is 0.00456703.
After 16856 training step(s), loss on training batch is 0.00437011.
After 16857 training step(s), loss on training batch is 0.00427871.
After 16858 training step(s), loss on training batch is 0.00479982.
After 16859 training step(s), loss on training batch is 0.00435874.
After 16860 training step(s), loss on training batch is 0.00415065.
After 16861 training step(s), loss on training batch is 0.00492505.
After 16862 training step(s), loss on training batch is 0.00430118.
After 16863 training step(s), loss on training batch is 0.00417502.
After 16864 training step(s), loss on training batch is 0.00486138.
After 16865 training step(s), loss on training batch is 0.00463499.
After 16866 training step(s), loss on training batch is 0.00448521.
After 16867 training step(s), loss on training batch is 0.00395113.
After 16868 training step(s), loss on training batch is 0.0041727.
After 16869 training step(s), loss on training batch is 0.00485333.
After 16870 training step(s), loss on training batch is 0.00412424.
After 16871 training step(s), loss on training batch is 0.00454248.
After 16872 training step(s), loss on training batch is 0.00473938.
After 16873 training step(s), loss on training batch is 0.00431506.
After 16874 training step(s), loss on training batch is 0.00408454.
After 16875 training step(s), loss on training batch is 0.00504679.
After 16876 training step(s), loss on training batch is 0.00458283.
After 16877 training step(s), loss on training batch is 0.00415522.
After 16878 training step(s), loss on training batch is 0.0043127.
After 16879 training step(s), loss on training batch is 0.00466095.
After 16880 training step(s), loss on training batch is 0.00428299.
After 16881 training step(s), loss on training batch is 0.00421325.
After 16882 training step(s), loss on training batch is 0.0045457.
After 16883 training step(s), loss on training batch is 0.00475343.
After 16884 training step(s), loss on training batch is 0.00478139.
After 16885 training step(s), loss on training batch is 0.00443491.
After 16886 training step(s), loss on training batch is 0.00448387.
After 16887 training step(s), loss on training batch is 0.00446786.
After 16888 training step(s), loss on training batch is 0.00591171.
After 16889 training step(s), loss on training batch is 0.00476996.
After 16890 training step(s), loss on training batch is 0.00431783.
After 16891 training step(s), loss on training batch is 0.00402161.
After 16892 training step(s), loss on training batch is 0.0043957.
After 16893 training step(s), loss on training batch is 0.00418572.
After 16894 training step(s), loss on training batch is 0.00423853.
After 16895 training step(s), loss on training batch is 0.00425026.
After 16896 training step(s), loss on training batch is 0.00455132.
After 16897 training step(s), loss on training batch is 0.00422547.
After 16898 training step(s), loss on training batch is 0.00565333.
After 16899 training step(s), loss on training batch is 0.00433932.
After 16900 training step(s), loss on training batch is 0.00415757.
After 16901 training step(s), loss on training batch is 0.00513278.
After 16902 training step(s), loss on training batch is 0.00475265.
After 16903 training step(s), loss on training batch is 0.00474201.
After 16904 training step(s), loss on training batch is 0.00430667.
After 16905 training step(s), loss on training batch is 0.00431668.
After 16906 training step(s), loss on training batch is 0.00573321.
After 16907 training step(s), loss on training batch is 0.0052568.
After 16908 training step(s), loss on training batch is 0.00451517.
After 16909 training step(s), loss on training batch is 0.0064013.
After 16910 training step(s), loss on training batch is 0.00477315.
After 16911 training step(s), loss on training batch is 0.00434582.
After 16912 training step(s), loss on training batch is 0.00464244.
After 16913 training step(s), loss on training batch is 0.00416116.
After 16914 training step(s), loss on training batch is 0.00552847.
After 16915 training step(s), loss on training batch is 0.0041768.
After 16916 training step(s), loss on training batch is 0.00487323.
After 16917 training step(s), loss on training batch is 0.00411587.
After 16918 training step(s), loss on training batch is 0.00434048.
After 16919 training step(s), loss on training batch is 0.00431154.
After 16920 training step(s), loss on training batch is 0.00418825.
After 16921 training step(s), loss on training batch is 0.00431744.
After 16922 training step(s), loss on training batch is 0.00396044.
After 16923 training step(s), loss on training batch is 0.00511169.
After 16924 training step(s), loss on training batch is 0.0059997.
After 16925 training step(s), loss on training batch is 0.0046843.
After 16926 training step(s), loss on training batch is 0.00478011.
After 16927 training step(s), loss on training batch is 0.00432138.
After 16928 training step(s), loss on training batch is 0.00546032.
After 16929 training step(s), loss on training batch is 0.00437207.
After 16930 training step(s), loss on training batch is 0.00489166.
After 16931 training step(s), loss on training batch is 0.00413089.
After 16932 training step(s), loss on training batch is 0.00431546.
After 16933 training step(s), loss on training batch is 0.00446596.
After 16934 training step(s), loss on training batch is 0.00477787.
After 16935 training step(s), loss on training batch is 0.00419249.
After 16936 training step(s), loss on training batch is 0.00427181.
After 16937 training step(s), loss on training batch is 0.00440115.
After 16938 training step(s), loss on training batch is 0.00461468.
After 16939 training step(s), loss on training batch is 0.00461082.
After 16940 training step(s), loss on training batch is 0.00470517.
After 16941 training step(s), loss on training batch is 0.00470815.
After 16942 training step(s), loss on training batch is 0.00517373.
After 16943 training step(s), loss on training batch is 0.00460187.
After 16944 training step(s), loss on training batch is 0.00525868.
After 16945 training step(s), loss on training batch is 0.00500543.
After 16946 training step(s), loss on training batch is 0.00430252.
After 16947 training step(s), loss on training batch is 0.00412436.
After 16948 training step(s), loss on training batch is 0.00435008.
After 16949 training step(s), loss on training batch is 0.00406897.
After 16950 training step(s), loss on training batch is 0.00435713.
After 16951 training step(s), loss on training batch is 0.00491511.
After 16952 training step(s), loss on training batch is 0.0043416.
After 16953 training step(s), loss on training batch is 0.00387979.
After 16954 training step(s), loss on training batch is 0.00414707.
After 16955 training step(s), loss on training batch is 0.00412105.
After 16956 training step(s), loss on training batch is 0.00446046.
After 16957 training step(s), loss on training batch is 0.00413228.
After 16958 training step(s), loss on training batch is 0.00455292.
After 16959 training step(s), loss on training batch is 0.00452018.
After 16960 training step(s), loss on training batch is 0.00403856.
After 16961 training step(s), loss on training batch is 0.00433314.
After 16962 training step(s), loss on training batch is 0.00455721.
After 16963 training step(s), loss on training batch is 0.00433667.
After 16964 training step(s), loss on training batch is 0.00492999.
After 16965 training step(s), loss on training batch is 0.00471325.
After 16966 training step(s), loss on training batch is 0.00466719.
After 16967 training step(s), loss on training batch is 0.00473325.
After 16968 training step(s), loss on training batch is 0.00501775.
After 16969 training step(s), loss on training batch is 0.00457237.
After 16970 training step(s), loss on training batch is 0.00475479.
After 16971 training step(s), loss on training batch is 0.00458587.
After 16972 training step(s), loss on training batch is 0.00406288.
After 16973 training step(s), loss on training batch is 0.00438461.
After 16974 training step(s), loss on training batch is 0.00395957.
After 16975 training step(s), loss on training batch is 0.00440724.
After 16976 training step(s), loss on training batch is 0.00443203.
After 16977 training step(s), loss on training batch is 0.00445631.
After 16978 training step(s), loss on training batch is 0.0042436.
After 16979 training step(s), loss on training batch is 0.00485984.
After 16980 training step(s), loss on training batch is 0.00471743.
After 16981 training step(s), loss on training batch is 0.00429346.
After 16982 training step(s), loss on training batch is 0.0046642.
After 16983 training step(s), loss on training batch is 0.00388306.
After 16984 training step(s), loss on training batch is 0.00410374.
After 16985 training step(s), loss on training batch is 0.0044555.
After 16986 training step(s), loss on training batch is 0.00443789.
After 16987 training step(s), loss on training batch is 0.00480786.
After 16988 training step(s), loss on training batch is 0.00430318.
After 16989 training step(s), loss on training batch is 0.00411506.
After 16990 training step(s), loss on training batch is 0.0044535.
After 16991 training step(s), loss on training batch is 0.00479453.
After 16992 training step(s), loss on training batch is 0.00415559.
After 16993 training step(s), loss on training batch is 0.00469624.
After 16994 training step(s), loss on training batch is 0.004008.
After 16995 training step(s), loss on training batch is 0.00487741.
After 16996 training step(s), loss on training batch is 0.00402617.
After 16997 training step(s), loss on training batch is 0.00442918.
After 16998 training step(s), loss on training batch is 0.00425055.
After 16999 training step(s), loss on training batch is 0.00470668.
After 17000 training step(s), loss on training batch is 0.00403071.
After 17001 training step(s), loss on training batch is 0.00411054.
After 17002 training step(s), loss on training batch is 0.00445547.
After 17003 training step(s), loss on training batch is 0.00419358.
After 17004 training step(s), loss on training batch is 0.00420838.
After 17005 training step(s), loss on training batch is 0.00459766.
After 17006 training step(s), loss on training batch is 0.00497983.
After 17007 training step(s), loss on training batch is 0.00480775.
After 17008 training step(s), loss on training batch is 0.0045549.
After 17009 training step(s), loss on training batch is 0.00456546.
After 17010 training step(s), loss on training batch is 0.00447728.
After 17011 training step(s), loss on training batch is 0.00418841.
After 17012 training step(s), loss on training batch is 0.00486508.
After 17013 training step(s), loss on training batch is 0.0041511.
After 17014 training step(s), loss on training batch is 0.00471504.
After 17015 training step(s), loss on training batch is 0.00455031.
After 17016 training step(s), loss on training batch is 0.00415077.
After 17017 training step(s), loss on training batch is 0.00472864.
After 17018 training step(s), loss on training batch is 0.00458462.
After 17019 training step(s), loss on training batch is 0.00428795.
After 17020 training step(s), loss on training batch is 0.00434555.
After 17021 training step(s), loss on training batch is 0.00418384.
After 17022 training step(s), loss on training batch is 0.00480123.
After 17023 training step(s), loss on training batch is 0.00420423.
After 17024 training step(s), loss on training batch is 0.00581741.
After 17025 training step(s), loss on training batch is 0.00429845.
After 17026 training step(s), loss on training batch is 0.00414561.
After 17027 training step(s), loss on training batch is 0.00437277.
After 17028 training step(s), loss on training batch is 0.0040177.
After 17029 training step(s), loss on training batch is 0.00541516.
After 17030 training step(s), loss on training batch is 0.00482342.
After 17031 training step(s), loss on training batch is 0.00441412.
After 17032 training step(s), loss on training batch is 0.00439985.
After 17033 training step(s), loss on training batch is 0.00559327.
After 17034 training step(s), loss on training batch is 0.00396066.
After 17035 training step(s), loss on training batch is 0.00496743.
After 17036 training step(s), loss on training batch is 0.00427041.
After 17037 training step(s), loss on training batch is 0.0042327.
After 17038 training step(s), loss on training batch is 0.0039623.
After 17039 training step(s), loss on training batch is 0.00771201.
After 17040 training step(s), loss on training batch is 0.00516556.
After 17041 training step(s), loss on training batch is 0.00407445.
After 17042 training step(s), loss on training batch is 0.00427232.
After 17043 training step(s), loss on training batch is 0.00454189.
After 17044 training step(s), loss on training batch is 0.00406682.
After 17045 training step(s), loss on training batch is 0.00530974.
After 17046 training step(s), loss on training batch is 0.00486866.
After 17047 training step(s), loss on training batch is 0.00492621.
After 17048 training step(s), loss on training batch is 0.00537069.
After 17049 training step(s), loss on training batch is 0.00405276.
After 17050 training step(s), loss on training batch is 0.00467259.
After 17051 training step(s), loss on training batch is 0.00412982.
After 17052 training step(s), loss on training batch is 0.00465211.
After 17053 training step(s), loss on training batch is 0.00469691.
After 17054 training step(s), loss on training batch is 0.00415013.
After 17055 training step(s), loss on training batch is 0.00407528.
After 17056 training step(s), loss on training batch is 0.00397019.
After 17057 training step(s), loss on training batch is 0.00429957.
After 17058 training step(s), loss on training batch is 0.00399297.
After 17059 training step(s), loss on training batch is 0.00478058.
After 17060 training step(s), loss on training batch is 0.0041718.
After 17061 training step(s), loss on training batch is 0.00466468.
After 17062 training step(s), loss on training batch is 0.00438333.
After 17063 training step(s), loss on training batch is 0.00458804.
After 17064 training step(s), loss on training batch is 0.00447256.
After 17065 training step(s), loss on training batch is 0.00423091.
After 17066 training step(s), loss on training batch is 0.00422302.
After 17067 training step(s), loss on training batch is 0.00425293.
After 17068 training step(s), loss on training batch is 0.00412813.
After 17069 training step(s), loss on training batch is 0.00447948.
After 17070 training step(s), loss on training batch is 0.00423952.
After 17071 training step(s), loss on training batch is 0.00520149.
After 17072 training step(s), loss on training batch is 0.00455204.
After 17073 training step(s), loss on training batch is 0.00440404.
After 17074 training step(s), loss on training batch is 0.00508633.
After 17075 training step(s), loss on training batch is 0.00446321.
After 17076 training step(s), loss on training batch is 0.00582205.
After 17077 training step(s), loss on training batch is 0.00422268.
After 17078 training step(s), loss on training batch is 0.00461974.
After 17079 training step(s), loss on training batch is 0.00499331.
After 17080 training step(s), loss on training batch is 0.00407455.
After 17081 training step(s), loss on training batch is 0.00443481.
After 17082 training step(s), loss on training batch is 0.00418635.
After 17083 training step(s), loss on training batch is 0.00428285.
After 17084 training step(s), loss on training batch is 0.00532338.
After 17085 training step(s), loss on training batch is 0.00440602.
After 17086 training step(s), loss on training batch is 0.00513984.
After 17087 training step(s), loss on training batch is 0.00463482.
After 17088 training step(s), loss on training batch is 0.00430221.
After 17089 training step(s), loss on training batch is 0.00476436.
After 17090 training step(s), loss on training batch is 0.00479496.
After 17091 training step(s), loss on training batch is 0.00502375.
After 17092 training step(s), loss on training batch is 0.00438449.
After 17093 training step(s), loss on training batch is 0.00417498.
After 17094 training step(s), loss on training batch is 0.00448667.
After 17095 training step(s), loss on training batch is 0.00405332.
After 17096 training step(s), loss on training batch is 0.00460752.
After 17097 training step(s), loss on training batch is 0.00476391.
After 17098 training step(s), loss on training batch is 0.00403409.
After 17099 training step(s), loss on training batch is 0.00423654.
After 17100 training step(s), loss on training batch is 0.0040749.
After 17101 training step(s), loss on training batch is 0.00443729.
After 17102 training step(s), loss on training batch is 0.00400605.
After 17103 training step(s), loss on training batch is 0.00427001.
After 17104 training step(s), loss on training batch is 0.00422359.
After 17105 training step(s), loss on training batch is 0.00483603.
After 17106 training step(s), loss on training batch is 0.00393031.
After 17107 training step(s), loss on training batch is 0.00418439.
After 17108 training step(s), loss on training batch is 0.00405383.
After 17109 training step(s), loss on training batch is 0.00444351.
After 17110 training step(s), loss on training batch is 0.00403964.
After 17111 training step(s), loss on training batch is 0.00480194.
After 17112 training step(s), loss on training batch is 0.00483127.
After 17113 training step(s), loss on training batch is 0.00401842.
After 17114 training step(s), loss on training batch is 0.00419186.
After 17115 training step(s), loss on training batch is 0.00434215.
After 17116 training step(s), loss on training batch is 0.00385523.
After 17117 training step(s), loss on training batch is 0.00411718.
After 17118 training step(s), loss on training batch is 0.00392613.
After 17119 training step(s), loss on training batch is 0.00405098.
After 17120 training step(s), loss on training batch is 0.00533306.
After 17121 training step(s), loss on training batch is 0.00434748.
After 17122 training step(s), loss on training batch is 0.00512526.
After 17123 training step(s), loss on training batch is 0.00434298.
After 17124 training step(s), loss on training batch is 0.005108.
After 17125 training step(s), loss on training batch is 0.00462134.
After 17126 training step(s), loss on training batch is 0.00510652.
After 17127 training step(s), loss on training batch is 0.00431447.
After 17128 training step(s), loss on training batch is 0.00448135.
After 17129 training step(s), loss on training batch is 0.00397951.
After 17130 training step(s), loss on training batch is 0.00405867.
After 17131 training step(s), loss on training batch is 0.00413224.
After 17132 training step(s), loss on training batch is 0.00440353.
After 17133 training step(s), loss on training batch is 0.00410869.
After 17134 training step(s), loss on training batch is 0.00455152.
After 17135 training step(s), loss on training batch is 0.00495871.
After 17136 training step(s), loss on training batch is 0.00451588.
After 17137 training step(s), loss on training batch is 0.00412483.
After 17138 training step(s), loss on training batch is 0.00387906.
After 17139 training step(s), loss on training batch is 0.00417477.
After 17140 training step(s), loss on training batch is 0.00410832.
After 17141 training step(s), loss on training batch is 0.00413903.
After 17142 training step(s), loss on training batch is 0.00505207.
After 17143 training step(s), loss on training batch is 0.00418871.
After 17144 training step(s), loss on training batch is 0.00430001.
After 17145 training step(s), loss on training batch is 0.00448951.
After 17146 training step(s), loss on training batch is 0.0046878.
After 17147 training step(s), loss on training batch is 0.00428725.
After 17148 training step(s), loss on training batch is 0.00434449.
After 17149 training step(s), loss on training batch is 0.00423612.
After 17150 training step(s), loss on training batch is 0.00422024.
After 17151 training step(s), loss on training batch is 0.00418.
After 17152 training step(s), loss on training batch is 0.0042601.
After 17153 training step(s), loss on training batch is 0.00421139.
After 17154 training step(s), loss on training batch is 0.00412443.
After 17155 training step(s), loss on training batch is 0.00430098.
After 17156 training step(s), loss on training batch is 0.00418626.
After 17157 training step(s), loss on training batch is 0.00436569.
After 17158 training step(s), loss on training batch is 0.00410527.
After 17159 training step(s), loss on training batch is 0.00465045.
After 17160 training step(s), loss on training batch is 0.00384924.
After 17161 training step(s), loss on training batch is 0.0039627.
After 17162 training step(s), loss on training batch is 0.00473429.
After 17163 training step(s), loss on training batch is 0.00496514.
After 17164 training step(s), loss on training batch is 0.00422688.
After 17165 training step(s), loss on training batch is 0.00393671.
After 17166 training step(s), loss on training batch is 0.00486525.
After 17167 training step(s), loss on training batch is 0.00433795.
After 17168 training step(s), loss on training batch is 0.00441951.
After 17169 training step(s), loss on training batch is 0.00424036.
After 17170 training step(s), loss on training batch is 0.00449653.
After 17171 training step(s), loss on training batch is 0.00402391.
After 17172 training step(s), loss on training batch is 0.00463352.
After 17173 training step(s), loss on training batch is 0.00386358.
After 17174 training step(s), loss on training batch is 0.00392101.
After 17175 training step(s), loss on training batch is 0.0041222.
After 17176 training step(s), loss on training batch is 0.00510843.
After 17177 training step(s), loss on training batch is 0.00448074.
After 17178 training step(s), loss on training batch is 0.00466858.
After 17179 training step(s), loss on training batch is 0.00430388.
After 17180 training step(s), loss on training batch is 0.0044005.
After 17181 training step(s), loss on training batch is 0.00401841.
After 17182 training step(s), loss on training batch is 0.00454092.
After 17183 training step(s), loss on training batch is 0.00456907.
After 17184 training step(s), loss on training batch is 0.00545497.
After 17185 training step(s), loss on training batch is 0.00519624.
After 17186 training step(s), loss on training batch is 0.00414759.
After 17187 training step(s), loss on training batch is 0.00401097.
After 17188 training step(s), loss on training batch is 0.00412094.
After 17189 training step(s), loss on training batch is 0.0049708.
After 17190 training step(s), loss on training batch is 0.00427793.
After 17191 training step(s), loss on training batch is 0.00393441.
After 17192 training step(s), loss on training batch is 0.00409577.
After 17193 training step(s), loss on training batch is 0.00475083.
After 17194 training step(s), loss on training batch is 0.00485521.
After 17195 training step(s), loss on training batch is 0.00409232.
After 17196 training step(s), loss on training batch is 0.00387742.
After 17197 training step(s), loss on training batch is 0.00420218.
After 17198 training step(s), loss on training batch is 0.00443344.
After 17199 training step(s), loss on training batch is 0.0044581.
After 17200 training step(s), loss on training batch is 0.00410243.
After 17201 training step(s), loss on training batch is 0.00479125.
After 17202 training step(s), loss on training batch is 0.00436872.
After 17203 training step(s), loss on training batch is 0.00459296.
After 17204 training step(s), loss on training batch is 0.00429668.
After 17205 training step(s), loss on training batch is 0.00493644.
After 17206 training step(s), loss on training batch is 0.00416786.
After 17207 training step(s), loss on training batch is 0.00422498.
After 17208 training step(s), loss on training batch is 0.00461496.
After 17209 training step(s), loss on training batch is 0.0043363.
After 17210 training step(s), loss on training batch is 0.00432077.
After 17211 training step(s), loss on training batch is 0.0039497.
After 17212 training step(s), loss on training batch is 0.00480946.
After 17213 training step(s), loss on training batch is 0.00467151.
After 17214 training step(s), loss on training batch is 0.00436722.
After 17215 training step(s), loss on training batch is 0.00438905.
After 17216 training step(s), loss on training batch is 0.0047792.
After 17217 training step(s), loss on training batch is 0.00461189.
After 17218 training step(s), loss on training batch is 0.0042085.
After 17219 training step(s), loss on training batch is 0.0047492.
After 17220 training step(s), loss on training batch is 0.0046743.
After 17221 training step(s), loss on training batch is 0.00532991.
After 17222 training step(s), loss on training batch is 0.00428175.
After 17223 training step(s), loss on training batch is 0.00391372.
After 17224 training step(s), loss on training batch is 0.00436497.
After 17225 training step(s), loss on training batch is 0.00484367.
After 17226 training step(s), loss on training batch is 0.00447714.
After 17227 training step(s), loss on training batch is 0.0048333.
After 17228 training step(s), loss on training batch is 0.00458977.
After 17229 training step(s), loss on training batch is 0.00424907.
After 17230 training step(s), loss on training batch is 0.00406291.
After 17231 training step(s), loss on training batch is 0.00549347.
After 17232 training step(s), loss on training batch is 0.00427132.
After 17233 training step(s), loss on training batch is 0.00463914.
After 17234 training step(s), loss on training batch is 0.00582142.
After 17235 training step(s), loss on training batch is 0.0043156.
After 17236 training step(s), loss on training batch is 0.00452501.
After 17237 training step(s), loss on training batch is 0.00710407.
After 17238 training step(s), loss on training batch is 0.00427801.
After 17239 training step(s), loss on training batch is 0.00441221.
After 17240 training step(s), loss on training batch is 0.00397494.
After 17241 training step(s), loss on training batch is 0.00427598.
After 17242 training step(s), loss on training batch is 0.0043669.
After 17243 training step(s), loss on training batch is 0.00428071.
After 17244 training step(s), loss on training batch is 0.00509731.
After 17245 training step(s), loss on training batch is 0.00475222.
After 17246 training step(s), loss on training batch is 0.00864235.
After 17247 training step(s), loss on training batch is 0.00398561.
After 17248 training step(s), loss on training batch is 0.00643008.
After 17249 training step(s), loss on training batch is 0.00384879.
After 17250 training step(s), loss on training batch is 0.00407411.
After 17251 training step(s), loss on training batch is 0.00433451.
After 17252 training step(s), loss on training batch is 0.00430689.
After 17253 training step(s), loss on training batch is 0.00417119.
After 17254 training step(s), loss on training batch is 0.00509689.
After 17255 training step(s), loss on training batch is 0.00439748.
After 17256 training step(s), loss on training batch is 0.0042532.
After 17257 training step(s), loss on training batch is 0.00460365.
After 17258 training step(s), loss on training batch is 0.0104376.
After 17259 training step(s), loss on training batch is 0.00448779.
After 17260 training step(s), loss on training batch is 0.00397744.
After 17261 training step(s), loss on training batch is 0.00462491.
After 17262 training step(s), loss on training batch is 0.00441368.
After 17263 training step(s), loss on training batch is 0.00427106.
After 17264 training step(s), loss on training batch is 0.00437131.
After 17265 training step(s), loss on training batch is 0.00530959.
After 17266 training step(s), loss on training batch is 0.00504886.
After 17267 training step(s), loss on training batch is 0.00427693.
After 17268 training step(s), loss on training batch is 0.00422901.
After 17269 training step(s), loss on training batch is 0.00499421.
After 17270 training step(s), loss on training batch is 0.00426262.
After 17271 training step(s), loss on training batch is 0.00588017.
After 17272 training step(s), loss on training batch is 0.00398662.
After 17273 training step(s), loss on training batch is 0.00616598.
After 17274 training step(s), loss on training batch is 0.00426234.
After 17275 training step(s), loss on training batch is 0.00433133.
After 17276 training step(s), loss on training batch is 0.00430368.
After 17277 training step(s), loss on training batch is 0.00447473.
After 17278 training step(s), loss on training batch is 0.00460884.
After 17279 training step(s), loss on training batch is 0.0043512.
After 17280 training step(s), loss on training batch is 0.00416062.
After 17281 training step(s), loss on training batch is 0.00421583.
After 17282 training step(s), loss on training batch is 0.0045758.
After 17283 training step(s), loss on training batch is 0.0042208.
After 17284 training step(s), loss on training batch is 0.00469327.
After 17285 training step(s), loss on training batch is 0.00476598.
After 17286 training step(s), loss on training batch is 0.0049507.
After 17287 training step(s), loss on training batch is 0.00436638.
After 17288 training step(s), loss on training batch is 0.00496382.
After 17289 training step(s), loss on training batch is 0.00431567.
After 17290 training step(s), loss on training batch is 0.00383814.
After 17291 training step(s), loss on training batch is 0.00404325.
After 17292 training step(s), loss on training batch is 0.00444892.
After 17293 training step(s), loss on training batch is 0.00463939.
After 17294 training step(s), loss on training batch is 0.00507699.
After 17295 training step(s), loss on training batch is 0.00389231.
After 17296 training step(s), loss on training batch is 0.00455519.
After 17297 training step(s), loss on training batch is 0.00493688.
After 17298 training step(s), loss on training batch is 0.00425034.
After 17299 training step(s), loss on training batch is 0.00439854.
After 17300 training step(s), loss on training batch is 0.00416792.
After 17301 training step(s), loss on training batch is 0.00515943.
After 17302 training step(s), loss on training batch is 0.00412792.
After 17303 training step(s), loss on training batch is 0.00535128.
After 17304 training step(s), loss on training batch is 0.00452481.
After 17305 training step(s), loss on training batch is 0.0041728.
After 17306 training step(s), loss on training batch is 0.00431588.
After 17307 training step(s), loss on training batch is 0.00471551.
After 17308 training step(s), loss on training batch is 0.00599253.
After 17309 training step(s), loss on training batch is 0.00411405.
After 17310 training step(s), loss on training batch is 0.00628107.
After 17311 training step(s), loss on training batch is 0.00490876.
After 17312 training step(s), loss on training batch is 0.00473694.
After 17313 training step(s), loss on training batch is 0.0044681.
After 17314 training step(s), loss on training batch is 0.00470287.
After 17315 training step(s), loss on training batch is 0.00424794.
After 17316 training step(s), loss on training batch is 0.00429254.
After 17317 training step(s), loss on training batch is 0.00409819.
After 17318 training step(s), loss on training batch is 0.00464426.
After 17319 training step(s), loss on training batch is 0.00436304.
After 17320 training step(s), loss on training batch is 0.0042622.
After 17321 training step(s), loss on training batch is 0.00478895.
After 17322 training step(s), loss on training batch is 0.00519795.
After 17323 training step(s), loss on training batch is 0.00402893.
After 17324 training step(s), loss on training batch is 0.00415279.
After 17325 training step(s), loss on training batch is 0.00575375.
After 17326 training step(s), loss on training batch is 0.00387363.
After 17327 training step(s), loss on training batch is 0.00418343.
After 17328 training step(s), loss on training batch is 0.0042684.
After 17329 training step(s), loss on training batch is 0.00432008.
After 17330 training step(s), loss on training batch is 0.00434101.
After 17331 training step(s), loss on training batch is 0.00489233.
After 17332 training step(s), loss on training batch is 0.00421553.
After 17333 training step(s), loss on training batch is 0.00450542.
After 17334 training step(s), loss on training batch is 0.00420786.
After 17335 training step(s), loss on training batch is 0.00565695.
After 17336 training step(s), loss on training batch is 0.00420524.
After 17337 training step(s), loss on training batch is 0.0044568.
After 17338 training step(s), loss on training batch is 0.00391775.
After 17339 training step(s), loss on training batch is 0.00446754.
After 17340 training step(s), loss on training batch is 0.00424732.
After 17341 training step(s), loss on training batch is 0.0038611.
After 17342 training step(s), loss on training batch is 0.0048243.
After 17343 training step(s), loss on training batch is 0.00424913.
After 17344 training step(s), loss on training batch is 0.00418363.
After 17345 training step(s), loss on training batch is 0.00407649.
After 17346 training step(s), loss on training batch is 0.0048121.
After 17347 training step(s), loss on training batch is 0.00423824.
After 17348 training step(s), loss on training batch is 0.00392042.
After 17349 training step(s), loss on training batch is 0.00448756.
After 17350 training step(s), loss on training batch is 0.00422368.
After 17351 training step(s), loss on training batch is 0.00396325.
After 17352 training step(s), loss on training batch is 0.00470346.
After 17353 training step(s), loss on training batch is 0.00417223.
After 17354 training step(s), loss on training batch is 0.00408425.
After 17355 training step(s), loss on training batch is 0.0048524.
After 17356 training step(s), loss on training batch is 0.00389346.
After 17357 training step(s), loss on training batch is 0.00440694.
After 17358 training step(s), loss on training batch is 0.00439947.
After 17359 training step(s), loss on training batch is 0.0052369.
After 17360 training step(s), loss on training batch is 0.00426987.
After 17361 training step(s), loss on training batch is 0.00434739.
After 17362 training step(s), loss on training batch is 0.00448519.
After 17363 training step(s), loss on training batch is 0.00439662.
After 17364 training step(s), loss on training batch is 0.00450748.
After 17365 training step(s), loss on training batch is 0.00415133.
After 17366 training step(s), loss on training batch is 0.00420766.
After 17367 training step(s), loss on training batch is 0.00439863.
After 17368 training step(s), loss on training batch is 0.00464139.
After 17369 training step(s), loss on training batch is 0.0041361.
After 17370 training step(s), loss on training batch is 0.0038313.
After 17371 training step(s), loss on training batch is 0.00485239.
After 17372 training step(s), loss on training batch is 0.00455786.
After 17373 training step(s), loss on training batch is 0.00409729.
After 17374 training step(s), loss on training batch is 0.00486767.
After 17375 training step(s), loss on training batch is 0.00406384.
After 17376 training step(s), loss on training batch is 0.00442167.
After 17377 training step(s), loss on training batch is 0.00430474.
After 17378 training step(s), loss on training batch is 0.00380914.
After 17379 training step(s), loss on training batch is 0.00428705.
After 17380 training step(s), loss on training batch is 0.00420951.
After 17381 training step(s), loss on training batch is 0.00407182.
After 17382 training step(s), loss on training batch is 0.00477714.
After 17383 training step(s), loss on training batch is 0.00410442.
After 17384 training step(s), loss on training batch is 0.0040188.
After 17385 training step(s), loss on training batch is 0.00457455.
After 17386 training step(s), loss on training batch is 0.00395904.
After 17387 training step(s), loss on training batch is 0.00404994.
After 17388 training step(s), loss on training batch is 0.00603496.
After 17389 training step(s), loss on training batch is 0.00477624.
After 17390 training step(s), loss on training batch is 0.00397099.
After 17391 training step(s), loss on training batch is 0.00409224.
After 17392 training step(s), loss on training batch is 0.00466232.
After 17393 training step(s), loss on training batch is 0.00467324.
After 17394 training step(s), loss on training batch is 0.00536969.
After 17395 training step(s), loss on training batch is 0.00396083.
After 17396 training step(s), loss on training batch is 0.00462022.
After 17397 training step(s), loss on training batch is 0.00455247.
After 17398 training step(s), loss on training batch is 0.0045231.
After 17399 training step(s), loss on training batch is 0.00469855.
After 17400 training step(s), loss on training batch is 0.00474195.
After 17401 training step(s), loss on training batch is 0.00407031.
After 17402 training step(s), loss on training batch is 0.00450689.
After 17403 training step(s), loss on training batch is 0.00413507.
After 17404 training step(s), loss on training batch is 0.00427319.
After 17405 training step(s), loss on training batch is 0.00412394.
After 17406 training step(s), loss on training batch is 0.00455336.
After 17407 training step(s), loss on training batch is 0.00602201.
After 17408 training step(s), loss on training batch is 0.00450482.
After 17409 training step(s), loss on training batch is 0.00431699.
After 17410 training step(s), loss on training batch is 0.00423295.
After 17411 training step(s), loss on training batch is 0.00460615.
After 17412 training step(s), loss on training batch is 0.00457224.
After 17413 training step(s), loss on training batch is 0.00431392.
After 17414 training step(s), loss on training batch is 0.00413554.
After 17415 training step(s), loss on training batch is 0.00414665.
After 17416 training step(s), loss on training batch is 0.00428748.
After 17417 training step(s), loss on training batch is 0.00419058.
After 17418 training step(s), loss on training batch is 0.00393728.
After 17419 training step(s), loss on training batch is 0.00411651.
After 17420 training step(s), loss on training batch is 0.00415309.
After 17421 training step(s), loss on training batch is 0.00411789.
After 17422 training step(s), loss on training batch is 0.00462895.
After 17423 training step(s), loss on training batch is 0.00426094.
After 17424 training step(s), loss on training batch is 0.0042138.
After 17425 training step(s), loss on training batch is 0.00469917.
After 17426 training step(s), loss on training batch is 0.00405561.
After 17427 training step(s), loss on training batch is 0.00463273.
After 17428 training step(s), loss on training batch is 0.0040293.
After 17429 training step(s), loss on training batch is 0.00419431.
After 17430 training step(s), loss on training batch is 0.00450127.
After 17431 training step(s), loss on training batch is 0.00453697.
After 17432 training step(s), loss on training batch is 0.00445335.
After 17433 training step(s), loss on training batch is 0.0047325.
After 17434 training step(s), loss on training batch is 0.00402117.
After 17435 training step(s), loss on training batch is 0.00412491.
After 17436 training step(s), loss on training batch is 0.00450425.
After 17437 training step(s), loss on training batch is 0.00417988.
After 17438 training step(s), loss on training batch is 0.00432833.
After 17439 training step(s), loss on training batch is 0.00401783.
After 17440 training step(s), loss on training batch is 0.00402769.
After 17441 training step(s), loss on training batch is 0.00485489.
After 17442 training step(s), loss on training batch is 0.0041045.
After 17443 training step(s), loss on training batch is 0.00443895.
After 17444 training step(s), loss on training batch is 0.00445503.
After 17445 training step(s), loss on training batch is 0.00420514.
After 17446 training step(s), loss on training batch is 0.00435302.
After 17447 training step(s), loss on training batch is 0.00421188.
After 17448 training step(s), loss on training batch is 0.00433231.
After 17449 training step(s), loss on training batch is 0.00455405.
After 17450 training step(s), loss on training batch is 0.00417307.
After 17451 training step(s), loss on training batch is 0.00431024.
After 17452 training step(s), loss on training batch is 0.00439176.
After 17453 training step(s), loss on training batch is 0.00389033.
After 17454 training step(s), loss on training batch is 0.00423549.
After 17455 training step(s), loss on training batch is 0.00431969.
After 17456 training step(s), loss on training batch is 0.00421608.
After 17457 training step(s), loss on training batch is 0.00452737.
After 17458 training step(s), loss on training batch is 0.00439444.
After 17459 training step(s), loss on training batch is 0.00434229.
After 17460 training step(s), loss on training batch is 0.00420734.
After 17461 training step(s), loss on training batch is 0.00411324.
After 17462 training step(s), loss on training batch is 0.00411845.
After 17463 training step(s), loss on training batch is 0.0049193.
After 17464 training step(s), loss on training batch is 0.00385677.
After 17465 training step(s), loss on training batch is 0.00396138.
After 17466 training step(s), loss on training batch is 0.00464062.
After 17467 training step(s), loss on training batch is 0.00406658.
After 17468 training step(s), loss on training batch is 0.00420084.
After 17469 training step(s), loss on training batch is 0.00407497.
After 17470 training step(s), loss on training batch is 0.00500093.
After 17471 training step(s), loss on training batch is 0.00411881.
After 17472 training step(s), loss on training batch is 0.00442352.
After 17473 training step(s), loss on training batch is 0.00626684.
After 17474 training step(s), loss on training batch is 0.00394314.
After 17475 training step(s), loss on training batch is 0.00392685.
After 17476 training step(s), loss on training batch is 0.0039426.
After 17477 training step(s), loss on training batch is 0.00444815.
After 17478 training step(s), loss on training batch is 0.00551252.
After 17479 training step(s), loss on training batch is 0.00431073.
After 17480 training step(s), loss on training batch is 0.00510103.
After 17481 training step(s), loss on training batch is 0.00417818.
After 17482 training step(s), loss on training batch is 0.00422259.
After 17483 training step(s), loss on training batch is 0.00400236.
After 17484 training step(s), loss on training batch is 0.00417579.
After 17485 training step(s), loss on training batch is 0.00385547.
After 17486 training step(s), loss on training batch is 0.00434266.
After 17487 training step(s), loss on training batch is 0.00402856.
After 17488 training step(s), loss on training batch is 0.00380834.
After 17489 training step(s), loss on training batch is 0.00506757.
After 17490 training step(s), loss on training batch is 0.00461026.
After 17491 training step(s), loss on training batch is 0.00385611.
After 17492 training step(s), loss on training batch is 0.00581105.
After 17493 training step(s), loss on training batch is 0.00467066.
After 17494 training step(s), loss on training batch is 0.00445717.
After 17495 training step(s), loss on training batch is 0.00420582.
After 17496 training step(s), loss on training batch is 0.00411979.
After 17497 training step(s), loss on training batch is 0.0063473.
After 17498 training step(s), loss on training batch is 0.00435156.
After 17499 training step(s), loss on training batch is 0.00428995.
After 17500 training step(s), loss on training batch is 0.00436831.
After 17501 training step(s), loss on training batch is 0.00417492.
After 17502 training step(s), loss on training batch is 0.00412207.
After 17503 training step(s), loss on training batch is 0.00491614.
After 17504 training step(s), loss on training batch is 0.00418437.
After 17505 training step(s), loss on training batch is 0.00437442.
After 17506 training step(s), loss on training batch is 0.00409021.
After 17507 training step(s), loss on training batch is 0.00471832.
After 17508 training step(s), loss on training batch is 0.00461254.
After 17509 training step(s), loss on training batch is 0.00525811.
After 17510 training step(s), loss on training batch is 0.0040074.
After 17511 training step(s), loss on training batch is 0.00594957.
After 17512 training step(s), loss on training batch is 0.00438235.
After 17513 training step(s), loss on training batch is 0.00385855.
After 17514 training step(s), loss on training batch is 0.00411646.
After 17515 training step(s), loss on training batch is 0.0041778.
After 17516 training step(s), loss on training batch is 0.00446051.
After 17517 training step(s), loss on training batch is 0.00399278.
After 17518 training step(s), loss on training batch is 0.00417716.
After 17519 training step(s), loss on training batch is 0.00515234.
After 17520 training step(s), loss on training batch is 0.00419747.
After 17521 training step(s), loss on training batch is 0.00438124.
After 17522 training step(s), loss on training batch is 0.00451158.
After 17523 training step(s), loss on training batch is 0.00392414.
After 17524 training step(s), loss on training batch is 0.00432725.
After 17525 training step(s), loss on training batch is 0.00397725.
After 17526 training step(s), loss on training batch is 0.00418195.
After 17527 training step(s), loss on training batch is 0.00479446.
After 17528 training step(s), loss on training batch is 0.00461048.
After 17529 training step(s), loss on training batch is 0.00458933.
After 17530 training step(s), loss on training batch is 0.00456823.
After 17531 training step(s), loss on training batch is 0.00414724.
After 17532 training step(s), loss on training batch is 0.00415688.
After 17533 training step(s), loss on training batch is 0.0044223.
After 17534 training step(s), loss on training batch is 0.00440885.
After 17535 training step(s), loss on training batch is 0.00477551.
After 17536 training step(s), loss on training batch is 0.00509783.
After 17537 training step(s), loss on training batch is 0.0039598.
After 17538 training step(s), loss on training batch is 0.00443358.
After 17539 training step(s), loss on training batch is 0.00538074.
After 17540 training step(s), loss on training batch is 0.00415588.
After 17541 training step(s), loss on training batch is 0.00520323.
After 17542 training step(s), loss on training batch is 0.00425192.
After 17543 training step(s), loss on training batch is 0.00425497.
After 17544 training step(s), loss on training batch is 0.00445126.
After 17545 training step(s), loss on training batch is 0.00450408.
After 17546 training step(s), loss on training batch is 0.00426813.
After 17547 training step(s), loss on training batch is 0.00432633.
After 17548 training step(s), loss on training batch is 0.00533598.
After 17549 training step(s), loss on training batch is 0.0057193.
After 17550 training step(s), loss on training batch is 0.00412374.
After 17551 training step(s), loss on training batch is 0.00473018.
After 17552 training step(s), loss on training batch is 0.00442383.
After 17553 training step(s), loss on training batch is 0.00410346.
After 17554 training step(s), loss on training batch is 0.00394664.
After 17555 training step(s), loss on training batch is 0.00386988.
After 17556 training step(s), loss on training batch is 0.00383391.
After 17557 training step(s), loss on training batch is 0.00447424.
After 17558 training step(s), loss on training batch is 0.00407303.
After 17559 training step(s), loss on training batch is 0.00443905.
After 17560 training step(s), loss on training batch is 0.00404647.
After 17561 training step(s), loss on training batch is 0.00450811.
After 17562 training step(s), loss on training batch is 0.0045271.
After 17563 training step(s), loss on training batch is 0.00431047.
After 17564 training step(s), loss on training batch is 0.00397302.
After 17565 training step(s), loss on training batch is 0.00503476.
After 17566 training step(s), loss on training batch is 0.00421686.
After 17567 training step(s), loss on training batch is 0.00403845.
After 17568 training step(s), loss on training batch is 0.00439601.
After 17569 training step(s), loss on training batch is 0.00403164.
After 17570 training step(s), loss on training batch is 0.0047555.
After 17571 training step(s), loss on training batch is 0.00433204.
After 17572 training step(s), loss on training batch is 0.00417179.
After 17573 training step(s), loss on training batch is 0.00396443.
After 17574 training step(s), loss on training batch is 0.00443628.
After 17575 training step(s), loss on training batch is 0.0037609.
After 17576 training step(s), loss on training batch is 0.00404289.
After 17577 training step(s), loss on training batch is 0.00472795.
After 17578 training step(s), loss on training batch is 0.00603076.
After 17579 training step(s), loss on training batch is 0.00404807.
After 17580 training step(s), loss on training batch is 0.00382584.
After 17581 training step(s), loss on training batch is 0.00449167.
After 17582 training step(s), loss on training batch is 0.00415021.
After 17583 training step(s), loss on training batch is 0.00509615.
After 17584 training step(s), loss on training batch is 0.00437126.
After 17585 training step(s), loss on training batch is 0.0038904.
After 17586 training step(s), loss on training batch is 0.00470213.
After 17587 training step(s), loss on training batch is 0.00392546.
After 17588 training step(s), loss on training batch is 0.00444415.
After 17589 training step(s), loss on training batch is 0.00406357.
After 17590 training step(s), loss on training batch is 0.00426715.
After 17591 training step(s), loss on training batch is 0.0042797.
After 17592 training step(s), loss on training batch is 0.00448033.
After 17593 training step(s), loss on training batch is 0.00444908.
After 17594 training step(s), loss on training batch is 0.00418258.
After 17595 training step(s), loss on training batch is 0.00465661.
After 17596 training step(s), loss on training batch is 0.0046177.
After 17597 training step(s), loss on training batch is 0.00552013.
After 17598 training step(s), loss on training batch is 0.00430214.
After 17599 training step(s), loss on training batch is 0.00418918.
After 17600 training step(s), loss on training batch is 0.00441826.
After 17601 training step(s), loss on training batch is 0.00386946.
After 17602 training step(s), loss on training batch is 0.00438254.
After 17603 training step(s), loss on training batch is 0.00520261.
After 17604 training step(s), loss on training batch is 0.00408207.
After 17605 training step(s), loss on training batch is 0.00437028.
After 17606 training step(s), loss on training batch is 0.00429619.
After 17607 training step(s), loss on training batch is 0.00402389.
After 17608 training step(s), loss on training batch is 0.00442703.
After 17609 training step(s), loss on training batch is 0.00440471.
After 17610 training step(s), loss on training batch is 0.00436755.
After 17611 training step(s), loss on training batch is 0.00436638.
After 17612 training step(s), loss on training batch is 0.00412781.
After 17613 training step(s), loss on training batch is 0.00442679.
After 17614 training step(s), loss on training batch is 0.00505825.
After 17615 training step(s), loss on training batch is 0.00433587.
After 17616 training step(s), loss on training batch is 0.00440231.
After 17617 training step(s), loss on training batch is 0.00399708.
After 17618 training step(s), loss on training batch is 0.00448626.
After 17619 training step(s), loss on training batch is 0.0041197.
After 17620 training step(s), loss on training batch is 0.00453898.
After 17621 training step(s), loss on training batch is 0.00482704.
After 17622 training step(s), loss on training batch is 0.00419982.
After 17623 training step(s), loss on training batch is 0.00405128.
After 17624 training step(s), loss on training batch is 0.0044911.
After 17625 training step(s), loss on training batch is 0.00392364.
After 17626 training step(s), loss on training batch is 0.00402214.
After 17627 training step(s), loss on training batch is 0.00436309.
After 17628 training step(s), loss on training batch is 0.00423159.
After 17629 training step(s), loss on training batch is 0.00413749.
After 17630 training step(s), loss on training batch is 0.00401578.
After 17631 training step(s), loss on training batch is 0.00432876.
After 17632 training step(s), loss on training batch is 0.00379066.
After 17633 training step(s), loss on training batch is 0.00442958.
After 17634 training step(s), loss on training batch is 0.00394067.
After 17635 training step(s), loss on training batch is 0.00485707.
After 17636 training step(s), loss on training batch is 0.00388613.
After 17637 training step(s), loss on training batch is 0.00417796.
After 17638 training step(s), loss on training batch is 0.00477084.
After 17639 training step(s), loss on training batch is 0.00394737.
After 17640 training step(s), loss on training batch is 0.00404261.
After 17641 training step(s), loss on training batch is 0.00428781.
After 17642 training step(s), loss on training batch is 0.00395058.
After 17643 training step(s), loss on training batch is 0.00409456.
After 17644 training step(s), loss on training batch is 0.00603289.
After 17645 training step(s), loss on training batch is 0.00432194.
After 17646 training step(s), loss on training batch is 0.00409393.
After 17647 training step(s), loss on training batch is 0.00499168.
After 17648 training step(s), loss on training batch is 0.0040866.
After 17649 training step(s), loss on training batch is 0.00509769.
After 17650 training step(s), loss on training batch is 0.00397425.
After 17651 training step(s), loss on training batch is 0.00416764.
After 17652 training step(s), loss on training batch is 0.00390962.
After 17653 training step(s), loss on training batch is 0.00390067.
After 17654 training step(s), loss on training batch is 0.00415001.
After 17655 training step(s), loss on training batch is 0.00561865.
After 17656 training step(s), loss on training batch is 0.00387967.
After 17657 training step(s), loss on training batch is 0.00428175.
After 17658 training step(s), loss on training batch is 0.00404913.
After 17659 training step(s), loss on training batch is 0.00434031.
After 17660 training step(s), loss on training batch is 0.00403853.
After 17661 training step(s), loss on training batch is 0.0041136.
After 17662 training step(s), loss on training batch is 0.00422483.
After 17663 training step(s), loss on training batch is 0.00417438.
After 17664 training step(s), loss on training batch is 0.00407815.
After 17665 training step(s), loss on training batch is 0.00404419.
After 17666 training step(s), loss on training batch is 0.00431799.
After 17667 training step(s), loss on training batch is 0.004499.
After 17668 training step(s), loss on training batch is 0.0047509.
After 17669 training step(s), loss on training batch is 0.00658992.
After 17670 training step(s), loss on training batch is 0.00396405.
After 17671 training step(s), loss on training batch is 0.00404884.
After 17672 training step(s), loss on training batch is 0.00446859.
After 17673 training step(s), loss on training batch is 0.00409955.
After 17674 training step(s), loss on training batch is 0.00435596.
After 17675 training step(s), loss on training batch is 0.00430096.
After 17676 training step(s), loss on training batch is 0.00409393.
After 17677 training step(s), loss on training batch is 0.00407778.
After 17678 training step(s), loss on training batch is 0.00390367.
After 17679 training step(s), loss on training batch is 0.00443624.
After 17680 training step(s), loss on training batch is 0.00418061.
After 17681 training step(s), loss on training batch is 0.00434917.
After 17682 training step(s), loss on training batch is 0.00441918.
After 17683 training step(s), loss on training batch is 0.00491278.
After 17684 training step(s), loss on training batch is 0.00402317.
After 17685 training step(s), loss on training batch is 0.00400339.
After 17686 training step(s), loss on training batch is 0.00456363.
After 17687 training step(s), loss on training batch is 0.00435092.
After 17688 training step(s), loss on training batch is 0.00381793.
After 17689 training step(s), loss on training batch is 0.00385446.
After 17690 training step(s), loss on training batch is 0.00441704.
After 17691 training step(s), loss on training batch is 0.0042106.
After 17692 training step(s), loss on training batch is 0.00430196.
After 17693 training step(s), loss on training batch is 0.004143.
After 17694 training step(s), loss on training batch is 0.00496284.
After 17695 training step(s), loss on training batch is 0.00407118.
After 17696 training step(s), loss on training batch is 0.00394109.
After 17697 training step(s), loss on training batch is 0.00402087.
After 17698 training step(s), loss on training batch is 0.0044263.
After 17699 training step(s), loss on training batch is 0.00389814.
After 17700 training step(s), loss on training batch is 0.00434664.
After 17701 training step(s), loss on training batch is 0.0039382.
After 17702 training step(s), loss on training batch is 0.00461922.
After 17703 training step(s), loss on training batch is 0.00469292.
After 17704 training step(s), loss on training batch is 0.00588074.
After 17705 training step(s), loss on training batch is 0.00377561.
After 17706 training step(s), loss on training batch is 0.00463424.
After 17707 training step(s), loss on training batch is 0.00432238.
After 17708 training step(s), loss on training batch is 0.00409184.
After 17709 training step(s), loss on training batch is 0.00389988.
After 17710 training step(s), loss on training batch is 0.00388152.
After 17711 training step(s), loss on training batch is 0.00469679.
After 17712 training step(s), loss on training batch is 0.00512516.
After 17713 training step(s), loss on training batch is 0.00465468.
After 17714 training step(s), loss on training batch is 0.00399701.
After 17715 training step(s), loss on training batch is 0.00393024.
After 17716 training step(s), loss on training batch is 0.00477936.
After 17717 training step(s), loss on training batch is 0.00466653.
After 17718 training step(s), loss on training batch is 0.0039721.
After 17719 training step(s), loss on training batch is 0.00391126.
After 17720 training step(s), loss on training batch is 0.00430753.
After 17721 training step(s), loss on training batch is 0.00421439.
After 17722 training step(s), loss on training batch is 0.00454409.
After 17723 training step(s), loss on training batch is 0.00412154.
After 17724 training step(s), loss on training batch is 0.0041515.
After 17725 training step(s), loss on training batch is 0.00381588.
After 17726 training step(s), loss on training batch is 0.00451161.
After 17727 training step(s), loss on training batch is 0.00415668.
After 17728 training step(s), loss on training batch is 0.00415864.
After 17729 training step(s), loss on training batch is 0.00403262.
After 17730 training step(s), loss on training batch is 0.00448572.
After 17731 training step(s), loss on training batch is 0.00436826.
After 17732 training step(s), loss on training batch is 0.00425208.
After 17733 training step(s), loss on training batch is 0.00404217.
After 17734 training step(s), loss on training batch is 0.00398382.
After 17735 training step(s), loss on training batch is 0.00429889.
After 17736 training step(s), loss on training batch is 0.00394325.
After 17737 training step(s), loss on training batch is 0.00403162.
After 17738 training step(s), loss on training batch is 0.00403997.
After 17739 training step(s), loss on training batch is 0.00406637.
After 17740 training step(s), loss on training batch is 0.00502818.
After 17741 training step(s), loss on training batch is 0.00416222.
After 17742 training step(s), loss on training batch is 0.00430097.
After 17743 training step(s), loss on training batch is 0.00437513.
After 17744 training step(s), loss on training batch is 0.00398926.
After 17745 training step(s), loss on training batch is 0.00449307.
After 17746 training step(s), loss on training batch is 0.00483347.
After 17747 training step(s), loss on training batch is 0.004596.
After 17748 training step(s), loss on training batch is 0.00412587.
After 17749 training step(s), loss on training batch is 0.0039425.
After 17750 training step(s), loss on training batch is 0.00446384.
After 17751 training step(s), loss on training batch is 0.00410165.
After 17752 training step(s), loss on training batch is 0.00436346.
After 17753 training step(s), loss on training batch is 0.0038115.
After 17754 training step(s), loss on training batch is 0.00422405.
After 17755 training step(s), loss on training batch is 0.00403007.
After 17756 training step(s), loss on training batch is 0.00416543.
After 17757 training step(s), loss on training batch is 0.00426408.
After 17758 training step(s), loss on training batch is 0.0040395.
After 17759 training step(s), loss on training batch is 0.00376569.
After 17760 training step(s), loss on training batch is 0.0044718.
After 17761 training step(s), loss on training batch is 0.0039626.
After 17762 training step(s), loss on training batch is 0.00401707.
After 17763 training step(s), loss on training batch is 0.00395032.
After 17764 training step(s), loss on training batch is 0.00392978.
After 17765 training step(s), loss on training batch is 0.00424927.
After 17766 training step(s), loss on training batch is 0.00460078.
After 17767 training step(s), loss on training batch is 0.00432956.
After 17768 training step(s), loss on training batch is 0.00456709.
After 17769 training step(s), loss on training batch is 0.00448163.
After 17770 training step(s), loss on training batch is 0.00486333.
After 17771 training step(s), loss on training batch is 0.0042969.
After 17772 training step(s), loss on training batch is 0.0042118.
After 17773 training step(s), loss on training batch is 0.00414143.
After 17774 training step(s), loss on training batch is 0.00504911.
After 17775 training step(s), loss on training batch is 0.00596973.
After 17776 training step(s), loss on training batch is 0.00425227.
After 17777 training step(s), loss on training batch is 0.00430771.
After 17778 training step(s), loss on training batch is 0.00427808.
After 17779 training step(s), loss on training batch is 0.00436817.
After 17780 training step(s), loss on training batch is 0.00402191.
After 17781 training step(s), loss on training batch is 0.00404861.
After 17782 training step(s), loss on training batch is 0.00446385.
After 17783 training step(s), loss on training batch is 0.00517078.
After 17784 training step(s), loss on training batch is 0.00418067.
After 17785 training step(s), loss on training batch is 0.00406854.
After 17786 training step(s), loss on training batch is 0.00486383.
After 17787 training step(s), loss on training batch is 0.00443682.
After 17788 training step(s), loss on training batch is 0.00407768.
After 17789 training step(s), loss on training batch is 0.00516029.
After 17790 training step(s), loss on training batch is 0.00418903.
After 17791 training step(s), loss on training batch is 0.00437382.
After 17792 training step(s), loss on training batch is 0.00400797.
After 17793 training step(s), loss on training batch is 0.00471074.
After 17794 training step(s), loss on training batch is 0.00413957.
After 17795 training step(s), loss on training batch is 0.00469448.
After 17796 training step(s), loss on training batch is 0.00421411.
After 17797 training step(s), loss on training batch is 0.00386436.
After 17798 training step(s), loss on training batch is 0.00393192.
After 17799 training step(s), loss on training batch is 0.00413235.
After 17800 training step(s), loss on training batch is 0.00403281.
After 17801 training step(s), loss on training batch is 0.00407127.
After 17802 training step(s), loss on training batch is 0.00409944.
After 17803 training step(s), loss on training batch is 0.0042735.
After 17804 training step(s), loss on training batch is 0.00469832.
After 17805 training step(s), loss on training batch is 0.00432246.
After 17806 training step(s), loss on training batch is 0.00376181.
After 17807 training step(s), loss on training batch is 0.00687433.
After 17808 training step(s), loss on training batch is 0.00463679.
After 17809 training step(s), loss on training batch is 0.00399196.
After 17810 training step(s), loss on training batch is 0.00428398.
After 17811 training step(s), loss on training batch is 0.00428349.
After 17812 training step(s), loss on training batch is 0.00417263.
After 17813 training step(s), loss on training batch is 0.00379422.
After 17814 training step(s), loss on training batch is 0.00455417.
After 17815 training step(s), loss on training batch is 0.00387559.
After 17816 training step(s), loss on training batch is 0.00419467.
After 17817 training step(s), loss on training batch is 0.00444526.
After 17818 training step(s), loss on training batch is 0.00431941.
After 17819 training step(s), loss on training batch is 0.00391537.
After 17820 training step(s), loss on training batch is 0.00472426.
After 17821 training step(s), loss on training batch is 0.00427717.
After 17822 training step(s), loss on training batch is 0.00482677.
After 17823 training step(s), loss on training batch is 0.00409715.
After 17824 training step(s), loss on training batch is 0.00384741.
After 17825 training step(s), loss on training batch is 0.00487037.
After 17826 training step(s), loss on training batch is 0.00406499.
After 17827 training step(s), loss on training batch is 0.00452026.
After 17828 training step(s), loss on training batch is 0.00385804.
After 17829 training step(s), loss on training batch is 0.00428002.
After 17830 training step(s), loss on training batch is 0.00419452.
After 17831 training step(s), loss on training batch is 0.00450707.
After 17832 training step(s), loss on training batch is 0.00425367.
After 17833 training step(s), loss on training batch is 0.00420658.
After 17834 training step(s), loss on training batch is 0.00447647.
After 17835 training step(s), loss on training batch is 0.00441233.
After 17836 training step(s), loss on training batch is 0.00401406.
After 17837 training step(s), loss on training batch is 0.00432016.
After 17838 training step(s), loss on training batch is 0.00450954.
After 17839 training step(s), loss on training batch is 0.00383322.
After 17840 training step(s), loss on training batch is 0.00393502.
After 17841 training step(s), loss on training batch is 0.0041362.
After 17842 training step(s), loss on training batch is 0.00389034.
After 17843 training step(s), loss on training batch is 0.00441459.
After 17844 training step(s), loss on training batch is 0.00540369.
After 17845 training step(s), loss on training batch is 0.00907769.
After 17846 training step(s), loss on training batch is 0.00453209.
After 17847 training step(s), loss on training batch is 0.00466553.
After 17848 training step(s), loss on training batch is 0.00475298.
After 17849 training step(s), loss on training batch is 0.0044692.
After 17850 training step(s), loss on training batch is 0.00590319.
After 17851 training step(s), loss on training batch is 0.00416661.
After 17852 training step(s), loss on training batch is 0.00393773.
After 17853 training step(s), loss on training batch is 0.00392677.
After 17854 training step(s), loss on training batch is 0.00470196.
After 17855 training step(s), loss on training batch is 0.00427144.
After 17856 training step(s), loss on training batch is 0.00406794.
After 17857 training step(s), loss on training batch is 0.00516577.
After 17858 training step(s), loss on training batch is 0.0037886.
After 17859 training step(s), loss on training batch is 0.00443474.
After 17860 training step(s), loss on training batch is 0.00436502.
After 17861 training step(s), loss on training batch is 0.00432602.
After 17862 training step(s), loss on training batch is 0.00431807.
After 17863 training step(s), loss on training batch is 0.00469019.
After 17864 training step(s), loss on training batch is 0.00412148.
After 17865 training step(s), loss on training batch is 0.00485463.
After 17866 training step(s), loss on training batch is 0.00429461.
After 17867 training step(s), loss on training batch is 0.00458923.
After 17868 training step(s), loss on training batch is 0.00511874.
After 17869 training step(s), loss on training batch is 0.00573286.
After 17870 training step(s), loss on training batch is 0.00414475.
After 17871 training step(s), loss on training batch is 0.00452645.
After 17872 training step(s), loss on training batch is 0.00449111.
After 17873 training step(s), loss on training batch is 0.00429554.
After 17874 training step(s), loss on training batch is 0.00453253.
After 17875 training step(s), loss on training batch is 0.00432648.
After 17876 training step(s), loss on training batch is 0.00382145.
After 17877 training step(s), loss on training batch is 0.00428707.
After 17878 training step(s), loss on training batch is 0.00426933.
After 17879 training step(s), loss on training batch is 0.0040645.
After 17880 training step(s), loss on training batch is 0.00453947.
After 17881 training step(s), loss on training batch is 0.0039736.
After 17882 training step(s), loss on training batch is 0.00389415.
After 17883 training step(s), loss on training batch is 0.00394349.
After 17884 training step(s), loss on training batch is 0.00413616.
After 17885 training step(s), loss on training batch is 0.00396777.
After 17886 training step(s), loss on training batch is 0.00418535.
After 17887 training step(s), loss on training batch is 0.00415401.
After 17888 training step(s), loss on training batch is 0.00405879.
After 17889 training step(s), loss on training batch is 0.00404691.
After 17890 training step(s), loss on training batch is 0.00405858.
After 17891 training step(s), loss on training batch is 0.00406908.
After 17892 training step(s), loss on training batch is 0.00447303.
After 17893 training step(s), loss on training batch is 0.00413339.
After 17894 training step(s), loss on training batch is 0.00439688.
After 17895 training step(s), loss on training batch is 0.00398223.
After 17896 training step(s), loss on training batch is 0.00398583.
After 17897 training step(s), loss on training batch is 0.00425577.
After 17898 training step(s), loss on training batch is 0.00419462.
After 17899 training step(s), loss on training batch is 0.00452203.
After 17900 training step(s), loss on training batch is 0.0039682.
After 17901 training step(s), loss on training batch is 0.00416327.
After 17902 training step(s), loss on training batch is 0.00430183.
After 17903 training step(s), loss on training batch is 0.00448395.
After 17904 training step(s), loss on training batch is 0.00399054.
After 17905 training step(s), loss on training batch is 0.00401123.
After 17906 training step(s), loss on training batch is 0.00418696.
After 17907 training step(s), loss on training batch is 0.00377787.
After 17908 training step(s), loss on training batch is 0.00472353.
After 17909 training step(s), loss on training batch is 0.00401109.
After 17910 training step(s), loss on training batch is 0.00408468.
After 17911 training step(s), loss on training batch is 0.0038238.
After 17912 training step(s), loss on training batch is 0.00400762.
After 17913 training step(s), loss on training batch is 0.00409407.
After 17914 training step(s), loss on training batch is 0.00441053.
After 17915 training step(s), loss on training batch is 0.0042982.
After 17916 training step(s), loss on training batch is 0.00397238.
After 17917 training step(s), loss on training batch is 0.00383782.
After 17918 training step(s), loss on training batch is 0.00475606.
After 17919 training step(s), loss on training batch is 0.00413043.
After 17920 training step(s), loss on training batch is 0.00426554.
After 17921 training step(s), loss on training batch is 0.00489849.
After 17922 training step(s), loss on training batch is 0.00414608.
After 17923 training step(s), loss on training batch is 0.00400782.
After 17924 training step(s), loss on training batch is 0.00431198.
After 17925 training step(s), loss on training batch is 0.00425955.
After 17926 training step(s), loss on training batch is 0.00416284.
After 17927 training step(s), loss on training batch is 0.00440696.
After 17928 training step(s), loss on training batch is 0.00416962.
After 17929 training step(s), loss on training batch is 0.00389449.
After 17930 training step(s), loss on training batch is 0.00402421.
After 17931 training step(s), loss on training batch is 0.00424603.
After 17932 training step(s), loss on training batch is 0.00420008.
After 17933 training step(s), loss on training batch is 0.00435745.
After 17934 training step(s), loss on training batch is 0.00710298.
After 17935 training step(s), loss on training batch is 0.00444503.
After 17936 training step(s), loss on training batch is 0.00399831.
After 17937 training step(s), loss on training batch is 0.00394727.
After 17938 training step(s), loss on training batch is 0.0042067.
After 17939 training step(s), loss on training batch is 0.00385562.
After 17940 training step(s), loss on training batch is 0.00438631.
After 17941 training step(s), loss on training batch is 0.00435061.
After 17942 training step(s), loss on training batch is 0.00402874.
After 17943 training step(s), loss on training batch is 0.00392073.
After 17944 training step(s), loss on training batch is 0.00492859.
After 17945 training step(s), loss on training batch is 0.00399524.
After 17946 training step(s), loss on training batch is 0.00420143.
After 17947 training step(s), loss on training batch is 0.00652879.
After 17948 training step(s), loss on training batch is 0.00480532.
After 17949 training step(s), loss on training batch is 0.00416355.
After 17950 training step(s), loss on training batch is 0.00377367.
After 17951 training step(s), loss on training batch is 0.00401627.
After 17952 training step(s), loss on training batch is 0.00402582.
After 17953 training step(s), loss on training batch is 0.00525351.
After 17954 training step(s), loss on training batch is 0.00427258.
After 17955 training step(s), loss on training batch is 0.00399653.
After 17956 training step(s), loss on training batch is 0.00466636.
After 17957 training step(s), loss on training batch is 0.00408943.
After 17958 training step(s), loss on training batch is 0.00425044.
After 17959 training step(s), loss on training batch is 0.00400015.
After 17960 training step(s), loss on training batch is 0.00407722.
After 17961 training step(s), loss on training batch is 0.00448468.
After 17962 training step(s), loss on training batch is 0.00425137.
After 17963 training step(s), loss on training batch is 0.00417758.
After 17964 training step(s), loss on training batch is 0.00381462.
After 17965 training step(s), loss on training batch is 0.00436902.
After 17966 training step(s), loss on training batch is 0.00484907.
After 17967 training step(s), loss on training batch is 0.00456558.
After 17968 training step(s), loss on training batch is 0.00412502.
After 17969 training step(s), loss on training batch is 0.00473558.
After 17970 training step(s), loss on training batch is 0.00493693.
After 17971 training step(s), loss on training batch is 0.00420261.
After 17972 training step(s), loss on training batch is 0.00396319.
After 17973 training step(s), loss on training batch is 0.00433873.
After 17974 training step(s), loss on training batch is 0.00462808.
After 17975 training step(s), loss on training batch is 0.00478034.
After 17976 training step(s), loss on training batch is 0.00410445.
After 17977 training step(s), loss on training batch is 0.00430368.
After 17978 training step(s), loss on training batch is 0.00425175.
After 17979 training step(s), loss on training batch is 0.00457347.
After 17980 training step(s), loss on training batch is 0.00415687.
After 17981 training step(s), loss on training batch is 0.00383989.
After 17982 training step(s), loss on training batch is 0.00383581.
After 17983 training step(s), loss on training batch is 0.00431713.
After 17984 training step(s), loss on training batch is 0.00428379.
After 17985 training step(s), loss on training batch is 0.00404395.
After 17986 training step(s), loss on training batch is 0.00413179.
After 17987 training step(s), loss on training batch is 0.00418616.
After 17988 training step(s), loss on training batch is 0.00399701.
After 17989 training step(s), loss on training batch is 0.00455976.
After 17990 training step(s), loss on training batch is 0.00492584.
After 17991 training step(s), loss on training batch is 0.00473078.
After 17992 training step(s), loss on training batch is 0.0039925.
After 17993 training step(s), loss on training batch is 0.00449778.
After 17994 training step(s), loss on training batch is 0.00411653.
After 17995 training step(s), loss on training batch is 0.003903.
After 17996 training step(s), loss on training batch is 0.0044032.
After 17997 training step(s), loss on training batch is 0.00464082.
After 17998 training step(s), loss on training batch is 0.00449259.
After 17999 training step(s), loss on training batch is 0.00404096.
After 18000 training step(s), loss on training batch is 0.00456212.
After 18001 training step(s), loss on training batch is 0.00489082.
After 18002 training step(s), loss on training batch is 0.00445023.
After 18003 training step(s), loss on training batch is 0.00413882.
After 18004 training step(s), loss on training batch is 0.00381997.
After 18005 training step(s), loss on training batch is 0.00433241.
After 18006 training step(s), loss on training batch is 0.00385886.
After 18007 training step(s), loss on training batch is 0.00485491.
After 18008 training step(s), loss on training batch is 0.00404041.
After 18009 training step(s), loss on training batch is 0.00534184.
After 18010 training step(s), loss on training batch is 0.00431456.
After 18011 training step(s), loss on training batch is 0.00484241.
After 18012 training step(s), loss on training batch is 0.00401274.
After 18013 training step(s), loss on training batch is 0.00431615.
After 18014 training step(s), loss on training batch is 0.00372565.
After 18015 training step(s), loss on training batch is 0.00424256.
After 18016 training step(s), loss on training batch is 0.00407416.
After 18017 training step(s), loss on training batch is 0.00467126.
After 18018 training step(s), loss on training batch is 0.00383457.
After 18019 training step(s), loss on training batch is 0.00461833.
After 18020 training step(s), loss on training batch is 0.00426592.
After 18021 training step(s), loss on training batch is 0.00395419.
After 18022 training step(s), loss on training batch is 0.00450145.
After 18023 training step(s), loss on training batch is 0.0043716.
After 18024 training step(s), loss on training batch is 0.00410241.
After 18025 training step(s), loss on training batch is 0.00566592.
After 18026 training step(s), loss on training batch is 0.00431844.
After 18027 training step(s), loss on training batch is 0.00437473.
After 18028 training step(s), loss on training batch is 0.0038042.
After 18029 training step(s), loss on training batch is 0.00380184.
After 18030 training step(s), loss on training batch is 0.00430883.
After 18031 training step(s), loss on training batch is 0.00407863.
After 18032 training step(s), loss on training batch is 0.00476847.
After 18033 training step(s), loss on training batch is 0.00413938.
After 18034 training step(s), loss on training batch is 0.00392641.
After 18035 training step(s), loss on training batch is 0.00413476.
After 18036 training step(s), loss on training batch is 0.00411063.
After 18037 training step(s), loss on training batch is 0.00390268.
After 18038 training step(s), loss on training batch is 0.00469018.
After 18039 training step(s), loss on training batch is 0.00369376.
After 18040 training step(s), loss on training batch is 0.00539823.
After 18041 training step(s), loss on training batch is 0.00406765.
After 18042 training step(s), loss on training batch is 0.00384166.
After 18043 training step(s), loss on training batch is 0.00455512.
After 18044 training step(s), loss on training batch is 0.00399981.
After 18045 training step(s), loss on training batch is 0.00440261.
After 18046 training step(s), loss on training batch is 0.0041108.
After 18047 training step(s), loss on training batch is 0.00459663.
After 18048 training step(s), loss on training batch is 0.00414377.
After 18049 training step(s), loss on training batch is 0.00425901.
After 18050 training step(s), loss on training batch is 0.00456642.
After 18051 training step(s), loss on training batch is 0.00406564.
After 18052 training step(s), loss on training batch is 0.00382967.
After 18053 training step(s), loss on training batch is 0.00441319.
After 18054 training step(s), loss on training batch is 0.00388687.
After 18055 training step(s), loss on training batch is 0.00372483.
After 18056 training step(s), loss on training batch is 0.00394362.
After 18057 training step(s), loss on training batch is 0.00427418.
After 18058 training step(s), loss on training batch is 0.00429347.
After 18059 training step(s), loss on training batch is 0.00441774.
After 18060 training step(s), loss on training batch is 0.00501089.
After 18061 training step(s), loss on training batch is 0.00496851.
After 18062 training step(s), loss on training batch is 0.00401118.
After 18063 training step(s), loss on training batch is 0.00390152.
After 18064 training step(s), loss on training batch is 0.00416393.
After 18065 training step(s), loss on training batch is 0.00392092.
After 18066 training step(s), loss on training batch is 0.00433643.
After 18067 training step(s), loss on training batch is 0.00482987.
After 18068 training step(s), loss on training batch is 0.00383567.
After 18069 training step(s), loss on training batch is 0.00446488.
After 18070 training step(s), loss on training batch is 0.00435049.
After 18071 training step(s), loss on training batch is 0.00461991.
After 18072 training step(s), loss on training batch is 0.004049.
After 18073 training step(s), loss on training batch is 0.00428529.
After 18074 training step(s), loss on training batch is 0.00407033.
After 18075 training step(s), loss on training batch is 0.00396546.
After 18076 training step(s), loss on training batch is 0.00422938.
After 18077 training step(s), loss on training batch is 0.00505008.
After 18078 training step(s), loss on training batch is 0.00410534.
After 18079 training step(s), loss on training batch is 0.00445794.
After 18080 training step(s), loss on training batch is 0.00424546.
After 18081 training step(s), loss on training batch is 0.00464341.
After 18082 training step(s), loss on training batch is 0.0048284.
After 18083 training step(s), loss on training batch is 0.00448701.
After 18084 training step(s), loss on training batch is 0.00516598.
After 18085 training step(s), loss on training batch is 0.0038474.
After 18086 training step(s), loss on training batch is 0.0047409.
After 18087 training step(s), loss on training batch is 0.00397483.
After 18088 training step(s), loss on training batch is 0.00473796.
After 18089 training step(s), loss on training batch is 0.00445056.
After 18090 training step(s), loss on training batch is 0.00418719.
After 18091 training step(s), loss on training batch is 0.00437568.
After 18092 training step(s), loss on training batch is 0.00393047.
After 18093 training step(s), loss on training batch is 0.00432552.
After 18094 training step(s), loss on training batch is 0.00391698.
After 18095 training step(s), loss on training batch is 0.00421751.
After 18096 training step(s), loss on training batch is 0.00414057.
After 18097 training step(s), loss on training batch is 0.0039935.
After 18098 training step(s), loss on training batch is 0.00490739.
After 18099 training step(s), loss on training batch is 0.00403449.
After 18100 training step(s), loss on training batch is 0.00473039.
After 18101 training step(s), loss on training batch is 0.0041253.
After 18102 training step(s), loss on training batch is 0.00400033.
After 18103 training step(s), loss on training batch is 0.003904.
After 18104 training step(s), loss on training batch is 0.00420053.
After 18105 training step(s), loss on training batch is 0.00377172.
After 18106 training step(s), loss on training batch is 0.00402023.
After 18107 training step(s), loss on training batch is 0.00426851.
After 18108 training step(s), loss on training batch is 0.00437178.
After 18109 training step(s), loss on training batch is 0.0037885.
After 18110 training step(s), loss on training batch is 0.00437243.
After 18111 training step(s), loss on training batch is 0.00475851.
After 18112 training step(s), loss on training batch is 0.00449106.
After 18113 training step(s), loss on training batch is 0.00457271.
After 18114 training step(s), loss on training batch is 0.00448788.
After 18115 training step(s), loss on training batch is 0.00453276.
After 18116 training step(s), loss on training batch is 0.00419587.
After 18117 training step(s), loss on training batch is 0.00426086.
After 18118 training step(s), loss on training batch is 0.00482198.
After 18119 training step(s), loss on training batch is 0.00390045.
After 18120 training step(s), loss on training batch is 0.00403129.
After 18121 training step(s), loss on training batch is 0.00461675.
After 18122 training step(s), loss on training batch is 0.00403031.
After 18123 training step(s), loss on training batch is 0.00625899.
After 18124 training step(s), loss on training batch is 0.00420616.
After 18125 training step(s), loss on training batch is 0.00483085.
After 18126 training step(s), loss on training batch is 0.00407389.
After 18127 training step(s), loss on training batch is 0.00406088.
After 18128 training step(s), loss on training batch is 0.00378061.
After 18129 training step(s), loss on training batch is 0.00406734.
After 18130 training step(s), loss on training batch is 0.00394609.
After 18131 training step(s), loss on training batch is 0.00514509.
After 18132 training step(s), loss on training batch is 0.00404811.
After 18133 training step(s), loss on training batch is 0.00395086.
After 18134 training step(s), loss on training batch is 0.00394904.
After 18135 training step(s), loss on training batch is 0.00421662.
After 18136 training step(s), loss on training batch is 0.00384749.
After 18137 training step(s), loss on training batch is 0.00460446.
After 18138 training step(s), loss on training batch is 0.00399476.
After 18139 training step(s), loss on training batch is 0.00402021.
After 18140 training step(s), loss on training batch is 0.0041746.
After 18141 training step(s), loss on training batch is 0.00418958.
After 18142 training step(s), loss on training batch is 0.00530302.
After 18143 training step(s), loss on training batch is 0.00393058.
After 18144 training step(s), loss on training batch is 0.00421128.
After 18145 training step(s), loss on training batch is 0.00432379.
After 18146 training step(s), loss on training batch is 0.00400845.
After 18147 training step(s), loss on training batch is 0.00490614.
After 18148 training step(s), loss on training batch is 0.00448627.
After 18149 training step(s), loss on training batch is 0.00435839.
After 18150 training step(s), loss on training batch is 0.00482568.
After 18151 training step(s), loss on training batch is 0.00404227.
After 18152 training step(s), loss on training batch is 0.00405917.
After 18153 training step(s), loss on training batch is 0.00432611.
After 18154 training step(s), loss on training batch is 0.00408688.
After 18155 training step(s), loss on training batch is 0.00480118.
After 18156 training step(s), loss on training batch is 0.00412473.
After 18157 training step(s), loss on training batch is 0.00455907.
After 18158 training step(s), loss on training batch is 0.00387012.
After 18159 training step(s), loss on training batch is 0.00482189.
After 18160 training step(s), loss on training batch is 0.00441717.
After 18161 training step(s), loss on training batch is 0.00404648.
After 18162 training step(s), loss on training batch is 0.00364215.
After 18163 training step(s), loss on training batch is 0.00430163.
After 18164 training step(s), loss on training batch is 0.00387201.
After 18165 training step(s), loss on training batch is 0.0039537.
After 18166 training step(s), loss on training batch is 0.00392614.
After 18167 training step(s), loss on training batch is 0.00408114.
After 18168 training step(s), loss on training batch is 0.00451322.
After 18169 training step(s), loss on training batch is 0.00396078.
After 18170 training step(s), loss on training batch is 0.00421533.
After 18171 training step(s), loss on training batch is 0.00418472.
After 18172 training step(s), loss on training batch is 0.00480814.
After 18173 training step(s), loss on training batch is 0.00433251.
After 18174 training step(s), loss on training batch is 0.00375407.
After 18175 training step(s), loss on training batch is 0.00437518.
After 18176 training step(s), loss on training batch is 0.00395147.
After 18177 training step(s), loss on training batch is 0.00440163.
After 18178 training step(s), loss on training batch is 0.00447962.
After 18179 training step(s), loss on training batch is 0.00448428.
After 18180 training step(s), loss on training batch is 0.00415941.
After 18181 training step(s), loss on training batch is 0.00375196.
After 18182 training step(s), loss on training batch is 0.00412438.
After 18183 training step(s), loss on training batch is 0.00421473.
After 18184 training step(s), loss on training batch is 0.00397112.
After 18185 training step(s), loss on training batch is 0.00378877.
After 18186 training step(s), loss on training batch is 0.00451268.
After 18187 training step(s), loss on training batch is 0.00402572.
After 18188 training step(s), loss on training batch is 0.00401259.
After 18189 training step(s), loss on training batch is 0.00499637.
After 18190 training step(s), loss on training batch is 0.00399445.
After 18191 training step(s), loss on training batch is 0.0042871.
After 18192 training step(s), loss on training batch is 0.00435389.
After 18193 training step(s), loss on training batch is 0.00391516.
After 18194 training step(s), loss on training batch is 0.00398926.
After 18195 training step(s), loss on training batch is 0.00406192.
After 18196 training step(s), loss on training batch is 0.00429631.
After 18197 training step(s), loss on training batch is 0.00461617.
After 18198 training step(s), loss on training batch is 0.00408501.
After 18199 training step(s), loss on training batch is 0.00400462.
After 18200 training step(s), loss on training batch is 0.00411254.
After 18201 training step(s), loss on training batch is 0.0044379.
After 18202 training step(s), loss on training batch is 0.00400137.
After 18203 training step(s), loss on training batch is 0.00429707.
After 18204 training step(s), loss on training batch is 0.00403287.
After 18205 training step(s), loss on training batch is 0.00404324.
After 18206 training step(s), loss on training batch is 0.00391835.
After 18207 training step(s), loss on training batch is 0.00419186.
After 18208 training step(s), loss on training batch is 0.00503899.
After 18209 training step(s), loss on training batch is 0.00401735.
After 18210 training step(s), loss on training batch is 0.0038125.
After 18211 training step(s), loss on training batch is 0.00427019.
After 18212 training step(s), loss on training batch is 0.00393962.
After 18213 training step(s), loss on training batch is 0.00406682.
After 18214 training step(s), loss on training batch is 0.00547671.
After 18215 training step(s), loss on training batch is 0.00399748.
After 18216 training step(s), loss on training batch is 0.00415235.
After 18217 training step(s), loss on training batch is 0.00419248.
After 18218 training step(s), loss on training batch is 0.00388772.
After 18219 training step(s), loss on training batch is 0.0042073.
After 18220 training step(s), loss on training batch is 0.0040044.
After 18221 training step(s), loss on training batch is 0.00383051.
After 18222 training step(s), loss on training batch is 0.00402762.
After 18223 training step(s), loss on training batch is 0.00373118.
After 18224 training step(s), loss on training batch is 0.00388193.
After 18225 training step(s), loss on training batch is 0.00397971.
After 18226 training step(s), loss on training batch is 0.00482531.
After 18227 training step(s), loss on training batch is 0.00497829.
After 18228 training step(s), loss on training batch is 0.00401413.
After 18229 training step(s), loss on training batch is 0.00405026.
After 18230 training step(s), loss on training batch is 0.00433816.
After 18231 training step(s), loss on training batch is 0.0048113.
After 18232 training step(s), loss on training batch is 0.00417492.
After 18233 training step(s), loss on training batch is 0.00419476.
After 18234 training step(s), loss on training batch is 0.00393333.
After 18235 training step(s), loss on training batch is 0.00480425.
After 18236 training step(s), loss on training batch is 0.00446538.
After 18237 training step(s), loss on training batch is 0.00376326.
After 18238 training step(s), loss on training batch is 0.00460157.
After 18239 training step(s), loss on training batch is 0.00449813.
After 18240 training step(s), loss on training batch is 0.00421915.
After 18241 training step(s), loss on training batch is 0.00482889.
After 18242 training step(s), loss on training batch is 0.00398399.
After 18243 training step(s), loss on training batch is 0.00390046.
After 18244 training step(s), loss on training batch is 0.00474072.
After 18245 training step(s), loss on training batch is 0.00429967.
After 18246 training step(s), loss on training batch is 0.00435075.
After 18247 training step(s), loss on training batch is 0.00388133.
After 18248 training step(s), loss on training batch is 0.00468549.
After 18249 training step(s), loss on training batch is 0.00429087.
After 18250 training step(s), loss on training batch is 0.00409036.
After 18251 training step(s), loss on training batch is 0.00388005.
After 18252 training step(s), loss on training batch is 0.00421608.
After 18253 training step(s), loss on training batch is 0.00379111.
After 18254 training step(s), loss on training batch is 0.00406312.
After 18255 training step(s), loss on training batch is 0.00428518.
After 18256 training step(s), loss on training batch is 0.00467516.
After 18257 training step(s), loss on training batch is 0.00450791.
After 18258 training step(s), loss on training batch is 0.00455308.
After 18259 training step(s), loss on training batch is 0.00385293.
After 18260 training step(s), loss on training batch is 0.00460246.
After 18261 training step(s), loss on training batch is 0.00385247.
After 18262 training step(s), loss on training batch is 0.00455788.
After 18263 training step(s), loss on training batch is 0.00415039.
After 18264 training step(s), loss on training batch is 0.00368812.
After 18265 training step(s), loss on training batch is 0.00438018.
After 18266 training step(s), loss on training batch is 0.00432545.
After 18267 training step(s), loss on training batch is 0.00594602.
After 18268 training step(s), loss on training batch is 0.00434714.
After 18269 training step(s), loss on training batch is 0.00490591.
After 18270 training step(s), loss on training batch is 0.00393529.
After 18271 training step(s), loss on training batch is 0.00428901.
After 18272 training step(s), loss on training batch is 0.00390405.
After 18273 training step(s), loss on training batch is 0.00395879.
After 18274 training step(s), loss on training batch is 0.00404182.
After 18275 training step(s), loss on training batch is 0.00396693.
After 18276 training step(s), loss on training batch is 0.00385348.
After 18277 training step(s), loss on training batch is 0.00419377.
After 18278 training step(s), loss on training batch is 0.00433861.
After 18279 training step(s), loss on training batch is 0.00447302.
After 18280 training step(s), loss on training batch is 0.00413635.
After 18281 training step(s), loss on training batch is 0.00451652.
After 18282 training step(s), loss on training batch is 0.00379518.
After 18283 training step(s), loss on training batch is 0.00398009.
After 18284 training step(s), loss on training batch is 0.00402863.
After 18285 training step(s), loss on training batch is 0.00388339.
After 18286 training step(s), loss on training batch is 0.00386844.
After 18287 training step(s), loss on training batch is 0.00440092.
After 18288 training step(s), loss on training batch is 0.0037313.
After 18289 training step(s), loss on training batch is 0.00366521.
After 18290 training step(s), loss on training batch is 0.00432931.
After 18291 training step(s), loss on training batch is 0.0044002.
After 18292 training step(s), loss on training batch is 0.00430513.
After 18293 training step(s), loss on training batch is 0.00501068.
After 18294 training step(s), loss on training batch is 0.00401233.
After 18295 training step(s), loss on training batch is 0.00379693.
After 18296 training step(s), loss on training batch is 0.00429977.
After 18297 training step(s), loss on training batch is 0.00459295.
After 18298 training step(s), loss on training batch is 0.00414267.
After 18299 training step(s), loss on training batch is 0.003888.
After 18300 training step(s), loss on training batch is 0.00392415.
After 18301 training step(s), loss on training batch is 0.0053569.
After 18302 training step(s), loss on training batch is 0.00416944.
After 18303 training step(s), loss on training batch is 0.00522214.
After 18304 training step(s), loss on training batch is 0.00441468.
After 18305 training step(s), loss on training batch is 0.00419527.
After 18306 training step(s), loss on training batch is 0.0040463.
After 18307 training step(s), loss on training batch is 0.00383008.
After 18308 training step(s), loss on training batch is 0.00373577.
After 18309 training step(s), loss on training batch is 0.00392047.
After 18310 training step(s), loss on training batch is 0.00436935.
After 18311 training step(s), loss on training batch is 0.00386725.
After 18312 training step(s), loss on training batch is 0.00384648.
After 18313 training step(s), loss on training batch is 0.00452383.
After 18314 training step(s), loss on training batch is 0.00434486.
After 18315 training step(s), loss on training batch is 0.00452833.
After 18316 training step(s), loss on training batch is 0.00401002.
After 18317 training step(s), loss on training batch is 0.00418378.
After 18318 training step(s), loss on training batch is 0.00398833.
After 18319 training step(s), loss on training batch is 0.0039024.
After 18320 training step(s), loss on training batch is 0.00481813.
After 18321 training step(s), loss on training batch is 0.00409212.
After 18322 training step(s), loss on training batch is 0.00447228.
After 18323 training step(s), loss on training batch is 0.00435263.
After 18324 training step(s), loss on training batch is 0.00441716.
After 18325 training step(s), loss on training batch is 0.00483892.
After 18326 training step(s), loss on training batch is 0.0043561.
After 18327 training step(s), loss on training batch is 0.00429616.
After 18328 training step(s), loss on training batch is 0.00381938.
After 18329 training step(s), loss on training batch is 0.00400788.
After 18330 training step(s), loss on training batch is 0.00451988.
After 18331 training step(s), loss on training batch is 0.00391615.
After 18332 training step(s), loss on training batch is 0.00372359.
After 18333 training step(s), loss on training batch is 0.00426348.
After 18334 training step(s), loss on training batch is 0.0056148.
After 18335 training step(s), loss on training batch is 0.00366077.
After 18336 training step(s), loss on training batch is 0.00411599.
After 18337 training step(s), loss on training batch is 0.00484295.
After 18338 training step(s), loss on training batch is 0.00405141.
After 18339 training step(s), loss on training batch is 0.00378903.
After 18340 training step(s), loss on training batch is 0.00407795.
After 18341 training step(s), loss on training batch is 0.00398067.
After 18342 training step(s), loss on training batch is 0.00515173.
After 18343 training step(s), loss on training batch is 0.00411933.
After 18344 training step(s), loss on training batch is 0.0038696.
After 18345 training step(s), loss on training batch is 0.00426702.
After 18346 training step(s), loss on training batch is 0.00398021.
After 18347 training step(s), loss on training batch is 0.00426245.
After 18348 training step(s), loss on training batch is 0.00498202.
After 18349 training step(s), loss on training batch is 0.00427882.
After 18350 training step(s), loss on training batch is 0.00429029.
After 18351 training step(s), loss on training batch is 0.00416761.
After 18352 training step(s), loss on training batch is 0.00418457.
After 18353 training step(s), loss on training batch is 0.00459287.
After 18354 training step(s), loss on training batch is 0.00405444.
After 18355 training step(s), loss on training batch is 0.00400454.
After 18356 training step(s), loss on training batch is 0.00467937.
After 18357 training step(s), loss on training batch is 0.00389829.
After 18358 training step(s), loss on training batch is 0.00475521.
After 18359 training step(s), loss on training batch is 0.00390196.
After 18360 training step(s), loss on training batch is 0.00453634.
After 18361 training step(s), loss on training batch is 0.0045631.
After 18362 training step(s), loss on training batch is 0.00519627.
After 18363 training step(s), loss on training batch is 0.00497233.
After 18364 training step(s), loss on training batch is 0.00407776.
After 18365 training step(s), loss on training batch is 0.00464372.
After 18366 training step(s), loss on training batch is 0.00461797.
After 18367 training step(s), loss on training batch is 0.00397156.
After 18368 training step(s), loss on training batch is 0.00385966.
After 18369 training step(s), loss on training batch is 0.00389915.
After 18370 training step(s), loss on training batch is 0.00387334.
After 18371 training step(s), loss on training batch is 0.00407183.
After 18372 training step(s), loss on training batch is 0.00371169.
After 18373 training step(s), loss on training batch is 0.00470143.
After 18374 training step(s), loss on training batch is 0.00393253.
After 18375 training step(s), loss on training batch is 0.00380136.
After 18376 training step(s), loss on training batch is 0.00406652.
After 18377 training step(s), loss on training batch is 0.00409074.
After 18378 training step(s), loss on training batch is 0.00373075.
After 18379 training step(s), loss on training batch is 0.00429842.
After 18380 training step(s), loss on training batch is 0.0039163.
After 18381 training step(s), loss on training batch is 0.00405762.
After 18382 training step(s), loss on training batch is 0.0048451.
After 18383 training step(s), loss on training batch is 0.00407813.
After 18384 training step(s), loss on training batch is 0.00453046.
After 18385 training step(s), loss on training batch is 0.00373432.
After 18386 training step(s), loss on training batch is 0.00449809.
After 18387 training step(s), loss on training batch is 0.00434143.
After 18388 training step(s), loss on training batch is 0.00391966.
After 18389 training step(s), loss on training batch is 0.00405229.
After 18390 training step(s), loss on training batch is 0.0039454.
After 18391 training step(s), loss on training batch is 0.00384717.
After 18392 training step(s), loss on training batch is 0.00431094.
After 18393 training step(s), loss on training batch is 0.00381975.
After 18394 training step(s), loss on training batch is 0.0042883.
After 18395 training step(s), loss on training batch is 0.00484214.
After 18396 training step(s), loss on training batch is 0.00424613.
After 18397 training step(s), loss on training batch is 0.00391465.
After 18398 training step(s), loss on training batch is 0.00394755.
After 18399 training step(s), loss on training batch is 0.00434845.
After 18400 training step(s), loss on training batch is 0.00422734.
After 18401 training step(s), loss on training batch is 0.00403392.
After 18402 training step(s), loss on training batch is 0.00409293.
After 18403 training step(s), loss on training batch is 0.00394575.
After 18404 training step(s), loss on training batch is 0.00417338.
After 18405 training step(s), loss on training batch is 0.00532084.
After 18406 training step(s), loss on training batch is 0.00424215.
After 18407 training step(s), loss on training batch is 0.00387409.
After 18408 training step(s), loss on training batch is 0.00401462.
After 18409 training step(s), loss on training batch is 0.00467894.
After 18410 training step(s), loss on training batch is 0.00377768.
After 18411 training step(s), loss on training batch is 0.00387249.
After 18412 training step(s), loss on training batch is 0.00391785.
After 18413 training step(s), loss on training batch is 0.004393.
After 18414 training step(s), loss on training batch is 0.00512468.
After 18415 training step(s), loss on training batch is 0.00397069.
After 18416 training step(s), loss on training batch is 0.00401119.
After 18417 training step(s), loss on training batch is 0.00447677.
After 18418 training step(s), loss on training batch is 0.00425651.
After 18419 training step(s), loss on training batch is 0.00391904.
After 18420 training step(s), loss on training batch is 0.00389531.
After 18421 training step(s), loss on training batch is 0.00451155.
After 18422 training step(s), loss on training batch is 0.00409123.
After 18423 training step(s), loss on training batch is 0.00462215.
After 18424 training step(s), loss on training batch is 0.00406675.
After 18425 training step(s), loss on training batch is 0.0040104.
After 18426 training step(s), loss on training batch is 0.00463607.
After 18427 training step(s), loss on training batch is 0.00428439.
After 18428 training step(s), loss on training batch is 0.00416216.
After 18429 training step(s), loss on training batch is 0.00387033.
After 18430 training step(s), loss on training batch is 0.00409789.
After 18431 training step(s), loss on training batch is 0.00424717.
After 18432 training step(s), loss on training batch is 0.0038397.
After 18433 training step(s), loss on training batch is 0.0045341.
After 18434 training step(s), loss on training batch is 0.00401735.
After 18435 training step(s), loss on training batch is 0.00421735.
After 18436 training step(s), loss on training batch is 0.00404007.
After 18437 training step(s), loss on training batch is 0.00467081.
After 18438 training step(s), loss on training batch is 0.00380502.
After 18439 training step(s), loss on training batch is 0.00449973.
After 18440 training step(s), loss on training batch is 0.00427259.
After 18441 training step(s), loss on training batch is 0.00376376.
After 18442 training step(s), loss on training batch is 0.00382064.
After 18443 training step(s), loss on training batch is 0.00411776.
After 18444 training step(s), loss on training batch is 0.00425288.
After 18445 training step(s), loss on training batch is 0.00416183.
After 18446 training step(s), loss on training batch is 0.00408548.
After 18447 training step(s), loss on training batch is 0.00426506.
After 18448 training step(s), loss on training batch is 0.00405216.
After 18449 training step(s), loss on training batch is 0.00396314.
After 18450 training step(s), loss on training batch is 0.00463479.
After 18451 training step(s), loss on training batch is 0.00438872.
After 18452 training step(s), loss on training batch is 0.00393265.
After 18453 training step(s), loss on training batch is 0.00396484.
After 18454 training step(s), loss on training batch is 0.00361081.
After 18455 training step(s), loss on training batch is 0.00381896.
After 18456 training step(s), loss on training batch is 0.00396592.
After 18457 training step(s), loss on training batch is 0.00390844.
After 18458 training step(s), loss on training batch is 0.00456047.
After 18459 training step(s), loss on training batch is 0.00380306.
After 18460 training step(s), loss on training batch is 0.00370831.
After 18461 training step(s), loss on training batch is 0.00377912.
After 18462 training step(s), loss on training batch is 0.00433863.
After 18463 training step(s), loss on training batch is 0.00375652.
After 18464 training step(s), loss on training batch is 0.00408535.
After 18465 training step(s), loss on training batch is 0.00367448.
After 18466 training step(s), loss on training batch is 0.00396038.
After 18467 training step(s), loss on training batch is 0.0047765.
After 18468 training step(s), loss on training batch is 0.00373357.
After 18469 training step(s), loss on training batch is 0.00413402.
After 18470 training step(s), loss on training batch is 0.00463872.
After 18471 training step(s), loss on training batch is 0.00368561.
After 18472 training step(s), loss on training batch is 0.00394076.
After 18473 training step(s), loss on training batch is 0.00409614.
After 18474 training step(s), loss on training batch is 0.0044238.
After 18475 training step(s), loss on training batch is 0.00469077.
After 18476 training step(s), loss on training batch is 0.00393275.
After 18477 training step(s), loss on training batch is 0.00411364.
After 18478 training step(s), loss on training batch is 0.00463499.
After 18479 training step(s), loss on training batch is 0.00417128.
After 18480 training step(s), loss on training batch is 0.00431748.
After 18481 training step(s), loss on training batch is 0.00511381.
After 18482 training step(s), loss on training batch is 0.0043668.
After 18483 training step(s), loss on training batch is 0.00435584.
After 18484 training step(s), loss on training batch is 0.0044773.
After 18485 training step(s), loss on training batch is 0.00436955.
After 18486 training step(s), loss on training batch is 0.00378887.
After 18487 training step(s), loss on training batch is 0.00370413.
After 18488 training step(s), loss on training batch is 0.00364652.
After 18489 training step(s), loss on training batch is 0.00599632.
After 18490 training step(s), loss on training batch is 0.00410412.
After 18491 training step(s), loss on training batch is 0.00403076.
After 18492 training step(s), loss on training batch is 0.00484339.
After 18493 training step(s), loss on training batch is 0.00429309.
After 18494 training step(s), loss on training batch is 0.00423633.
After 18495 training step(s), loss on training batch is 0.00360531.
After 18496 training step(s), loss on training batch is 0.00400106.
After 18497 training step(s), loss on training batch is 0.00394049.
After 18498 training step(s), loss on training batch is 0.00383664.
After 18499 training step(s), loss on training batch is 0.00488776.
After 18500 training step(s), loss on training batch is 0.00454368.
After 18501 training step(s), loss on training batch is 0.00358078.
After 18502 training step(s), loss on training batch is 0.00423157.
After 18503 training step(s), loss on training batch is 0.00384219.
After 18504 training step(s), loss on training batch is 0.00453963.
After 18505 training step(s), loss on training batch is 0.00427403.
After 18506 training step(s), loss on training batch is 0.00403851.
After 18507 training step(s), loss on training batch is 0.00406132.
After 18508 training step(s), loss on training batch is 0.00391343.
After 18509 training step(s), loss on training batch is 0.00422558.
After 18510 training step(s), loss on training batch is 0.00393118.
After 18511 training step(s), loss on training batch is 0.00404212.
After 18512 training step(s), loss on training batch is 0.00411532.
After 18513 training step(s), loss on training batch is 0.00415757.
After 18514 training step(s), loss on training batch is 0.00395791.
After 18515 training step(s), loss on training batch is 0.00438933.
After 18516 training step(s), loss on training batch is 0.00378455.
After 18517 training step(s), loss on training batch is 0.00473607.
After 18518 training step(s), loss on training batch is 0.00441455.
After 18519 training step(s), loss on training batch is 0.00407269.
After 18520 training step(s), loss on training batch is 0.00414796.
After 18521 training step(s), loss on training batch is 0.0039585.
After 18522 training step(s), loss on training batch is 0.00480255.
After 18523 training step(s), loss on training batch is 0.00443096.
After 18524 training step(s), loss on training batch is 0.0043397.
After 18525 training step(s), loss on training batch is 0.00380219.
After 18526 training step(s), loss on training batch is 0.00397115.
After 18527 training step(s), loss on training batch is 0.00392747.
After 18528 training step(s), loss on training batch is 0.0041196.
After 18529 training step(s), loss on training batch is 0.00393752.
After 18530 training step(s), loss on training batch is 0.00396618.
After 18531 training step(s), loss on training batch is 0.00485802.
After 18532 training step(s), loss on training batch is 0.0037945.
After 18533 training step(s), loss on training batch is 0.00373252.
After 18534 training step(s), loss on training batch is 0.00413809.
After 18535 training step(s), loss on training batch is 0.00420895.
After 18536 training step(s), loss on training batch is 0.00414984.
After 18537 training step(s), loss on training batch is 0.0039636.
After 18538 training step(s), loss on training batch is 0.00435228.
After 18539 training step(s), loss on training batch is 0.00491254.
After 18540 training step(s), loss on training batch is 0.00366043.
After 18541 training step(s), loss on training batch is 0.0040352.
After 18542 training step(s), loss on training batch is 0.00393842.
After 18543 training step(s), loss on training batch is 0.00399657.
After 18544 training step(s), loss on training batch is 0.00378946.
After 18545 training step(s), loss on training batch is 0.00389209.
After 18546 training step(s), loss on training batch is 0.00447731.
After 18547 training step(s), loss on training batch is 0.00413664.
After 18548 training step(s), loss on training batch is 0.0039724.
After 18549 training step(s), loss on training batch is 0.00426043.
After 18550 training step(s), loss on training batch is 0.00441728.
After 18551 training step(s), loss on training batch is 0.00462928.
After 18552 training step(s), loss on training batch is 0.00390761.
After 18553 training step(s), loss on training batch is 0.00432758.
After 18554 training step(s), loss on training batch is 0.00414079.
After 18555 training step(s), loss on training batch is 0.00377711.
After 18556 training step(s), loss on training batch is 0.00455936.
After 18557 training step(s), loss on training batch is 0.00431067.
After 18558 training step(s), loss on training batch is 0.00413184.
After 18559 training step(s), loss on training batch is 0.00450644.
After 18560 training step(s), loss on training batch is 0.00407584.
After 18561 training step(s), loss on training batch is 0.00495245.
After 18562 training step(s), loss on training batch is 0.00408578.
After 18563 training step(s), loss on training batch is 0.00435028.
After 18564 training step(s), loss on training batch is 0.00408085.
After 18565 training step(s), loss on training batch is 0.00435506.
After 18566 training step(s), loss on training batch is 0.0038569.
After 18567 training step(s), loss on training batch is 0.00449104.
After 18568 training step(s), loss on training batch is 0.00419334.
After 18569 training step(s), loss on training batch is 0.00381123.
After 18570 training step(s), loss on training batch is 0.00420346.
After 18571 training step(s), loss on training batch is 0.00420395.
After 18572 training step(s), loss on training batch is 0.00376593.
After 18573 training step(s), loss on training batch is 0.00447307.
After 18574 training step(s), loss on training batch is 0.00426981.
After 18575 training step(s), loss on training batch is 0.00389606.
After 18576 training step(s), loss on training batch is 0.00375032.
After 18577 training step(s), loss on training batch is 0.00379024.
After 18578 training step(s), loss on training batch is 0.00435112.
After 18579 training step(s), loss on training batch is 0.00402537.
After 18580 training step(s), loss on training batch is 0.00378699.
After 18581 training step(s), loss on training batch is 0.00383671.
After 18582 training step(s), loss on training batch is 0.00375331.
After 18583 training step(s), loss on training batch is 0.00427549.
After 18584 training step(s), loss on training batch is 0.00412674.
After 18585 training step(s), loss on training batch is 0.00397244.
After 18586 training step(s), loss on training batch is 0.00422151.
After 18587 training step(s), loss on training batch is 0.00468404.
After 18588 training step(s), loss on training batch is 0.00485079.
After 18589 training step(s), loss on training batch is 0.00449409.
After 18590 training step(s), loss on training batch is 0.00458997.
After 18591 training step(s), loss on training batch is 0.0037712.
After 18592 training step(s), loss on training batch is 0.00422231.
After 18593 training step(s), loss on training batch is 0.0040969.
After 18594 training step(s), loss on training batch is 0.00438536.
After 18595 training step(s), loss on training batch is 0.004044.
After 18596 training step(s), loss on training batch is 0.00380419.
After 18597 training step(s), loss on training batch is 0.00387666.
After 18598 training step(s), loss on training batch is 0.00374526.
After 18599 training step(s), loss on training batch is 0.00371269.
After 18600 training step(s), loss on training batch is 0.00441244.
After 18601 training step(s), loss on training batch is 0.00379272.
After 18602 training step(s), loss on training batch is 0.00431274.
After 18603 training step(s), loss on training batch is 0.00391173.
After 18604 training step(s), loss on training batch is 0.00399902.
After 18605 training step(s), loss on training batch is 0.00421434.
After 18606 training step(s), loss on training batch is 0.0039008.
After 18607 training step(s), loss on training batch is 0.0048379.
After 18608 training step(s), loss on training batch is 0.0039799.
After 18609 training step(s), loss on training batch is 0.00395118.
After 18610 training step(s), loss on training batch is 0.00382013.
After 18611 training step(s), loss on training batch is 0.00438368.
After 18612 training step(s), loss on training batch is 0.00440789.
After 18613 training step(s), loss on training batch is 0.00391281.
After 18614 training step(s), loss on training batch is 0.00409868.
After 18615 training step(s), loss on training batch is 0.00387267.
After 18616 training step(s), loss on training batch is 0.0037493.
After 18617 training step(s), loss on training batch is 0.00481586.
After 18618 training step(s), loss on training batch is 0.00417737.
After 18619 training step(s), loss on training batch is 0.00411439.
After 18620 training step(s), loss on training batch is 0.00397723.
After 18621 training step(s), loss on training batch is 0.00459323.
After 18622 training step(s), loss on training batch is 0.00417106.
After 18623 training step(s), loss on training batch is 0.00379396.
After 18624 training step(s), loss on training batch is 0.00462088.
After 18625 training step(s), loss on training batch is 0.00390655.
After 18626 training step(s), loss on training batch is 0.00359015.
After 18627 training step(s), loss on training batch is 0.0042792.
After 18628 training step(s), loss on training batch is 0.00464837.
After 18629 training step(s), loss on training batch is 0.00501526.
After 18630 training step(s), loss on training batch is 0.00428827.
After 18631 training step(s), loss on training batch is 0.00427307.
After 18632 training step(s), loss on training batch is 0.00396568.
After 18633 training step(s), loss on training batch is 0.00456132.
After 18634 training step(s), loss on training batch is 0.00399333.
After 18635 training step(s), loss on training batch is 0.00418311.
After 18636 training step(s), loss on training batch is 0.00374444.
After 18637 training step(s), loss on training batch is 0.00415435.
After 18638 training step(s), loss on training batch is 0.004391.
After 18639 training step(s), loss on training batch is 0.00411046.
After 18640 training step(s), loss on training batch is 0.00437364.
After 18641 training step(s), loss on training batch is 0.00403618.
After 18642 training step(s), loss on training batch is 0.00401384.
After 18643 training step(s), loss on training batch is 0.00367756.
After 18644 training step(s), loss on training batch is 0.00417777.
After 18645 training step(s), loss on training batch is 0.0047672.
After 18646 training step(s), loss on training batch is 0.0038037.
After 18647 training step(s), loss on training batch is 0.00679729.
After 18648 training step(s), loss on training batch is 0.00517438.
After 18649 training step(s), loss on training batch is 0.00364549.
After 18650 training step(s), loss on training batch is 0.00508393.
After 18651 training step(s), loss on training batch is 0.00426827.
After 18652 training step(s), loss on training batch is 0.0040407.
After 18653 training step(s), loss on training batch is 0.00431053.
After 18654 training step(s), loss on training batch is 0.00395636.
After 18655 training step(s), loss on training batch is 0.00365567.
After 18656 training step(s), loss on training batch is 0.00382213.
After 18657 training step(s), loss on training batch is 0.00405629.
After 18658 training step(s), loss on training batch is 0.00392008.
After 18659 training step(s), loss on training batch is 0.00395668.
After 18660 training step(s), loss on training batch is 0.00412537.
After 18661 training step(s), loss on training batch is 0.00415598.
After 18662 training step(s), loss on training batch is 0.00440494.
After 18663 training step(s), loss on training batch is 0.00453487.
After 18664 training step(s), loss on training batch is 0.00510646.
After 18665 training step(s), loss on training batch is 0.00468294.
After 18666 training step(s), loss on training batch is 0.00410857.
After 18667 training step(s), loss on training batch is 0.00499148.
After 18668 training step(s), loss on training batch is 0.004146.
After 18669 training step(s), loss on training batch is 0.00399826.
After 18670 training step(s), loss on training batch is 0.00387216.
After 18671 training step(s), loss on training batch is 0.00397158.
After 18672 training step(s), loss on training batch is 0.00414552.
After 18673 training step(s), loss on training batch is 0.00408949.
After 18674 training step(s), loss on training batch is 0.0049387.
After 18675 training step(s), loss on training batch is 0.00440732.
After 18676 training step(s), loss on training batch is 0.00448528.
After 18677 training step(s), loss on training batch is 0.00423002.
After 18678 training step(s), loss on training batch is 0.0049946.
After 18679 training step(s), loss on training batch is 0.00395974.
After 18680 training step(s), loss on training batch is 0.00402463.
After 18681 training step(s), loss on training batch is 0.00402928.
After 18682 training step(s), loss on training batch is 0.00401605.
After 18683 training step(s), loss on training batch is 0.00378295.
After 18684 training step(s), loss on training batch is 0.00371838.
After 18685 training step(s), loss on training batch is 0.00410006.
After 18686 training step(s), loss on training batch is 0.00476012.
After 18687 training step(s), loss on training batch is 0.0040171.
After 18688 training step(s), loss on training batch is 0.00466809.
After 18689 training step(s), loss on training batch is 0.00423391.
After 18690 training step(s), loss on training batch is 0.00396039.
After 18691 training step(s), loss on training batch is 0.00397414.
After 18692 training step(s), loss on training batch is 0.00371749.
After 18693 training step(s), loss on training batch is 0.00440989.
After 18694 training step(s), loss on training batch is 0.00377971.
After 18695 training step(s), loss on training batch is 0.00400886.
After 18696 training step(s), loss on training batch is 0.00427305.
After 18697 training step(s), loss on training batch is 0.00439889.
After 18698 training step(s), loss on training batch is 0.00424629.
After 18699 training step(s), loss on training batch is 0.00428636.
After 18700 training step(s), loss on training batch is 0.00371862.
After 18701 training step(s), loss on training batch is 0.00400687.
After 18702 training step(s), loss on training batch is 0.00413246.
After 18703 training step(s), loss on training batch is 0.0037339.
After 18704 training step(s), loss on training batch is 0.00384222.
After 18705 training step(s), loss on training batch is 0.00423997.
After 18706 training step(s), loss on training batch is 0.00461191.
After 18707 training step(s), loss on training batch is 0.0048035.
After 18708 training step(s), loss on training batch is 0.00393736.
After 18709 training step(s), loss on training batch is 0.00404096.
After 18710 training step(s), loss on training batch is 0.00384979.
After 18711 training step(s), loss on training batch is 0.00381873.
After 18712 training step(s), loss on training batch is 0.00417403.
After 18713 training step(s), loss on training batch is 0.00373586.
After 18714 training step(s), loss on training batch is 0.00398683.
After 18715 training step(s), loss on training batch is 0.00423891.
After 18716 training step(s), loss on training batch is 0.0037956.
After 18717 training step(s), loss on training batch is 0.00451809.
After 18718 training step(s), loss on training batch is 0.00386581.
After 18719 training step(s), loss on training batch is 0.00431653.
After 18720 training step(s), loss on training batch is 0.00419992.
After 18721 training step(s), loss on training batch is 0.00392673.
After 18722 training step(s), loss on training batch is 0.00433629.
After 18723 training step(s), loss on training batch is 0.00396437.
After 18724 training step(s), loss on training batch is 0.00412568.
After 18725 training step(s), loss on training batch is 0.00373265.
After 18726 training step(s), loss on training batch is 0.003899.
After 18727 training step(s), loss on training batch is 0.00464965.
After 18728 training step(s), loss on training batch is 0.00397539.
After 18729 training step(s), loss on training batch is 0.00403804.
After 18730 training step(s), loss on training batch is 0.00406514.
After 18731 training step(s), loss on training batch is 0.00454178.
After 18732 training step(s), loss on training batch is 0.00419973.
After 18733 training step(s), loss on training batch is 0.00414684.
After 18734 training step(s), loss on training batch is 0.00400167.
After 18735 training step(s), loss on training batch is 0.00437417.
After 18736 training step(s), loss on training batch is 0.00372138.
After 18737 training step(s), loss on training batch is 0.00439361.
After 18738 training step(s), loss on training batch is 0.00403832.
After 18739 training step(s), loss on training batch is 0.00532127.
After 18740 training step(s), loss on training batch is 0.00423229.
After 18741 training step(s), loss on training batch is 0.00396244.
After 18742 training step(s), loss on training batch is 0.00430961.
After 18743 training step(s), loss on training batch is 0.00393609.
After 18744 training step(s), loss on training batch is 0.00449695.
After 18745 training step(s), loss on training batch is 0.00394427.
After 18746 training step(s), loss on training batch is 0.00446139.
After 18747 training step(s), loss on training batch is 0.00417314.
After 18748 training step(s), loss on training batch is 0.00432962.
After 18749 training step(s), loss on training batch is 0.00410098.
After 18750 training step(s), loss on training batch is 0.00375248.
After 18751 training step(s), loss on training batch is 0.00374449.
After 18752 training step(s), loss on training batch is 0.00480772.
After 18753 training step(s), loss on training batch is 0.00367955.
After 18754 training step(s), loss on training batch is 0.00423657.
After 18755 training step(s), loss on training batch is 0.00418194.
After 18756 training step(s), loss on training batch is 0.00409411.
After 18757 training step(s), loss on training batch is 0.0040249.
After 18758 training step(s), loss on training batch is 0.00457118.
After 18759 training step(s), loss on training batch is 0.00432434.
After 18760 training step(s), loss on training batch is 0.0038818.
After 18761 training step(s), loss on training batch is 0.00376402.
After 18762 training step(s), loss on training batch is 0.00375939.
After 18763 training step(s), loss on training batch is 0.00397813.
After 18764 training step(s), loss on training batch is 0.00423352.
After 18765 training step(s), loss on training batch is 0.00387456.
After 18766 training step(s), loss on training batch is 0.00427177.
After 18767 training step(s), loss on training batch is 0.00432875.
After 18768 training step(s), loss on training batch is 0.00378031.
After 18769 training step(s), loss on training batch is 0.00374236.
After 18770 training step(s), loss on training batch is 0.00372988.
After 18771 training step(s), loss on training batch is 0.00413382.
After 18772 training step(s), loss on training batch is 0.00420403.
After 18773 training step(s), loss on training batch is 0.00403955.
After 18774 training step(s), loss on training batch is 0.00417382.
After 18775 training step(s), loss on training batch is 0.0037599.
After 18776 training step(s), loss on training batch is 0.00414547.
After 18777 training step(s), loss on training batch is 0.00497401.
After 18778 training step(s), loss on training batch is 0.00453539.
After 18779 training step(s), loss on training batch is 0.00392625.
After 18780 training step(s), loss on training batch is 0.00386168.
After 18781 training step(s), loss on training batch is 0.00382535.
After 18782 training step(s), loss on training batch is 0.00381861.
After 18783 training step(s), loss on training batch is 0.00372857.
After 18784 training step(s), loss on training batch is 0.004177.
After 18785 training step(s), loss on training batch is 0.00393599.
After 18786 training step(s), loss on training batch is 0.00435012.
After 18787 training step(s), loss on training batch is 0.00410651.
After 18788 training step(s), loss on training batch is 0.00392958.
After 18789 training step(s), loss on training batch is 0.00407214.
After 18790 training step(s), loss on training batch is 0.00410087.
After 18791 training step(s), loss on training batch is 0.00397198.
After 18792 training step(s), loss on training batch is 0.00381518.
After 18793 training step(s), loss on training batch is 0.0039547.
After 18794 training step(s), loss on training batch is 0.00424744.
After 18795 training step(s), loss on training batch is 0.00397962.
After 18796 training step(s), loss on training batch is 0.00411044.
After 18797 training step(s), loss on training batch is 0.00388645.
After 18798 training step(s), loss on training batch is 0.0039995.
After 18799 training step(s), loss on training batch is 0.003841.
After 18800 training step(s), loss on training batch is 0.00452209.
After 18801 training step(s), loss on training batch is 0.00410866.
After 18802 training step(s), loss on training batch is 0.00471409.
After 18803 training step(s), loss on training batch is 0.00398691.
After 18804 training step(s), loss on training batch is 0.00391669.
After 18805 training step(s), loss on training batch is 0.00393839.
After 18806 training step(s), loss on training batch is 0.00467098.
After 18807 training step(s), loss on training batch is 0.00373765.
After 18808 training step(s), loss on training batch is 0.00434709.
After 18809 training step(s), loss on training batch is 0.00422178.
After 18810 training step(s), loss on training batch is 0.00358561.
After 18811 training step(s), loss on training batch is 0.00424395.
After 18812 training step(s), loss on training batch is 0.00549599.
After 18813 training step(s), loss on training batch is 0.00414236.
After 18814 training step(s), loss on training batch is 0.00405601.
After 18815 training step(s), loss on training batch is 0.00370571.
After 18816 training step(s), loss on training batch is 0.00369141.
After 18817 training step(s), loss on training batch is 0.00421298.
After 18818 training step(s), loss on training batch is 0.0044705.
After 18819 training step(s), loss on training batch is 0.00396705.
After 18820 training step(s), loss on training batch is 0.00385358.
After 18821 training step(s), loss on training batch is 0.00357531.
After 18822 training step(s), loss on training batch is 0.00397084.
After 18823 training step(s), loss on training batch is 0.00433393.
After 18824 training step(s), loss on training batch is 0.00405081.
After 18825 training step(s), loss on training batch is 0.00429716.
After 18826 training step(s), loss on training batch is 0.00454936.
After 18827 training step(s), loss on training batch is 0.00427077.
After 18828 training step(s), loss on training batch is 0.00389589.
After 18829 training step(s), loss on training batch is 0.00370043.
After 18830 training step(s), loss on training batch is 0.00396656.
After 18831 training step(s), loss on training batch is 0.00429399.
After 18832 training step(s), loss on training batch is 0.00404901.
After 18833 training step(s), loss on training batch is 0.00421827.
After 18834 training step(s), loss on training batch is 0.00437731.
After 18835 training step(s), loss on training batch is 0.00580724.
After 18836 training step(s), loss on training batch is 0.00496138.
After 18837 training step(s), loss on training batch is 0.00379956.
After 18838 training step(s), loss on training batch is 0.00412305.
After 18839 training step(s), loss on training batch is 0.0036055.
After 18840 training step(s), loss on training batch is 0.0042932.
After 18841 training step(s), loss on training batch is 0.004075.
After 18842 training step(s), loss on training batch is 0.00395217.
After 18843 training step(s), loss on training batch is 0.00378763.
After 18844 training step(s), loss on training batch is 0.00378206.
After 18845 training step(s), loss on training batch is 0.00398284.
After 18846 training step(s), loss on training batch is 0.00444861.
After 18847 training step(s), loss on training batch is 0.00459958.
After 18848 training step(s), loss on training batch is 0.0044006.
After 18849 training step(s), loss on training batch is 0.00384987.
After 18850 training step(s), loss on training batch is 0.00414414.
After 18851 training step(s), loss on training batch is 0.00392069.
After 18852 training step(s), loss on training batch is 0.00398088.
After 18853 training step(s), loss on training batch is 0.00377766.
After 18854 training step(s), loss on training batch is 0.00382135.
After 18855 training step(s), loss on training batch is 0.00458935.
After 18856 training step(s), loss on training batch is 0.00378269.
After 18857 training step(s), loss on training batch is 0.00369197.
After 18858 training step(s), loss on training batch is 0.00417581.
After 18859 training step(s), loss on training batch is 0.00393287.
After 18860 training step(s), loss on training batch is 0.00423247.
After 18861 training step(s), loss on training batch is 0.00433427.
After 18862 training step(s), loss on training batch is 0.00378024.
After 18863 training step(s), loss on training batch is 0.00424299.
After 18864 training step(s), loss on training batch is 0.00402295.
After 18865 training step(s), loss on training batch is 0.00423066.
After 18866 training step(s), loss on training batch is 0.00477801.
After 18867 training step(s), loss on training batch is 0.00374146.
After 18868 training step(s), loss on training batch is 0.00383182.
After 18869 training step(s), loss on training batch is 0.00395745.
After 18870 training step(s), loss on training batch is 0.00377716.
After 18871 training step(s), loss on training batch is 0.00396111.
After 18872 training step(s), loss on training batch is 0.003912.
After 18873 training step(s), loss on training batch is 0.00420237.
After 18874 training step(s), loss on training batch is 0.00394829.
After 18875 training step(s), loss on training batch is 0.00404348.
After 18876 training step(s), loss on training batch is 0.00460903.
After 18877 training step(s), loss on training batch is 0.00376275.
After 18878 training step(s), loss on training batch is 0.00524318.
After 18879 training step(s), loss on training batch is 0.00381209.
After 18880 training step(s), loss on training batch is 0.00400813.
After 18881 training step(s), loss on training batch is 0.00464284.
After 18882 training step(s), loss on training batch is 0.00398727.
After 18883 training step(s), loss on training batch is 0.0046559.
After 18884 training step(s), loss on training batch is 0.00466063.
After 18885 training step(s), loss on training batch is 0.00388418.
After 18886 training step(s), loss on training batch is 0.00390182.
After 18887 training step(s), loss on training batch is 0.00376091.
After 18888 training step(s), loss on training batch is 0.00433031.
After 18889 training step(s), loss on training batch is 0.00397804.
After 18890 training step(s), loss on training batch is 0.00362656.
After 18891 training step(s), loss on training batch is 0.00365848.
After 18892 training step(s), loss on training batch is 0.00395871.
After 18893 training step(s), loss on training batch is 0.00399918.
After 18894 training step(s), loss on training batch is 0.00381961.
After 18895 training step(s), loss on training batch is 0.00426675.
After 18896 training step(s), loss on training batch is 0.00407849.
After 18897 training step(s), loss on training batch is 0.00412.
After 18898 training step(s), loss on training batch is 0.00417313.
After 18899 training step(s), loss on training batch is 0.00399388.
After 18900 training step(s), loss on training batch is 0.00435008.
After 18901 training step(s), loss on training batch is 0.00471905.
After 18902 training step(s), loss on training batch is 0.00368181.
After 18903 training step(s), loss on training batch is 0.00541954.
After 18904 training step(s), loss on training batch is 0.00445375.
After 18905 training step(s), loss on training batch is 0.00371223.
After 18906 training step(s), loss on training batch is 0.00479877.
After 18907 training step(s), loss on training batch is 0.00386784.
After 18908 training step(s), loss on training batch is 0.00381919.
After 18909 training step(s), loss on training batch is 0.00463451.
After 18910 training step(s), loss on training batch is 0.00415201.
After 18911 training step(s), loss on training batch is 0.00370026.
After 18912 training step(s), loss on training batch is 0.00372978.
After 18913 training step(s), loss on training batch is 0.00428259.
After 18914 training step(s), loss on training batch is 0.00412916.
After 18915 training step(s), loss on training batch is 0.0037885.
After 18916 training step(s), loss on training batch is 0.00388525.
After 18917 training step(s), loss on training batch is 0.00407748.
After 18918 training step(s), loss on training batch is 0.00380694.
After 18919 training step(s), loss on training batch is 0.00393375.
After 18920 training step(s), loss on training batch is 0.00436693.
After 18921 training step(s), loss on training batch is 0.0045193.
After 18922 training step(s), loss on training batch is 0.00361866.
After 18923 training step(s), loss on training batch is 0.00365835.
After 18924 training step(s), loss on training batch is 0.00409908.
After 18925 training step(s), loss on training batch is 0.00371975.
After 18926 training step(s), loss on training batch is 0.00414472.
After 18927 training step(s), loss on training batch is 0.0040399.
After 18928 training step(s), loss on training batch is 0.00388734.
After 18929 training step(s), loss on training batch is 0.003982.
After 18930 training step(s), loss on training batch is 0.00453423.
After 18931 training step(s), loss on training batch is 0.00395411.
After 18932 training step(s), loss on training batch is 0.00474816.
After 18933 training step(s), loss on training batch is 0.00384397.
After 18934 training step(s), loss on training batch is 0.00435548.
After 18935 training step(s), loss on training batch is 0.00409343.
After 18936 training step(s), loss on training batch is 0.00370665.
After 18937 training step(s), loss on training batch is 0.00370634.
After 18938 training step(s), loss on training batch is 0.00366513.
After 18939 training step(s), loss on training batch is 0.00371836.
After 18940 training step(s), loss on training batch is 0.00424903.
After 18941 training step(s), loss on training batch is 0.00413088.
After 18942 training step(s), loss on training batch is 0.003844.
After 18943 training step(s), loss on training batch is 0.005965.
After 18944 training step(s), loss on training batch is 0.00407019.
After 18945 training step(s), loss on training batch is 0.00428912.
After 18946 training step(s), loss on training batch is 0.0038039.
After 18947 training step(s), loss on training batch is 0.00405185.
After 18948 training step(s), loss on training batch is 0.00392709.
After 18949 training step(s), loss on training batch is 0.00588172.
After 18950 training step(s), loss on training batch is 0.00534506.
After 18951 training step(s), loss on training batch is 0.00410179.
After 18952 training step(s), loss on training batch is 0.00403437.
After 18953 training step(s), loss on training batch is 0.00412257.
After 18954 training step(s), loss on training batch is 0.00396112.
After 18955 training step(s), loss on training batch is 0.00417498.
After 18956 training step(s), loss on training batch is 0.00375562.
After 18957 training step(s), loss on training batch is 0.00408694.
After 18958 training step(s), loss on training batch is 0.0039268.
After 18959 training step(s), loss on training batch is 0.00462283.
After 18960 training step(s), loss on training batch is 0.00449386.
After 18961 training step(s), loss on training batch is 0.00453001.
After 18962 training step(s), loss on training batch is 0.00387888.
After 18963 training step(s), loss on training batch is 0.00541609.
After 18964 training step(s), loss on training batch is 0.00431988.
After 18965 training step(s), loss on training batch is 0.00385245.
After 18966 training step(s), loss on training batch is 0.00378992.
After 18967 training step(s), loss on training batch is 0.0037684.
After 18968 training step(s), loss on training batch is 0.00481066.
After 18969 training step(s), loss on training batch is 0.00398497.
After 18970 training step(s), loss on training batch is 0.00421149.
After 18971 training step(s), loss on training batch is 0.00417017.
After 18972 training step(s), loss on training batch is 0.00406233.
After 18973 training step(s), loss on training batch is 0.00375185.
After 18974 training step(s), loss on training batch is 0.00467704.
After 18975 training step(s), loss on training batch is 0.00387165.
After 18976 training step(s), loss on training batch is 0.00402238.
After 18977 training step(s), loss on training batch is 0.00378832.
After 18978 training step(s), loss on training batch is 0.00445578.
After 18979 training step(s), loss on training batch is 0.00365456.
After 18980 training step(s), loss on training batch is 0.00394909.
After 18981 training step(s), loss on training batch is 0.00434487.
After 18982 training step(s), loss on training batch is 0.00387853.
After 18983 training step(s), loss on training batch is 0.00439388.
After 18984 training step(s), loss on training batch is 0.0037526.
After 18985 training step(s), loss on training batch is 0.00367828.
After 18986 training step(s), loss on training batch is 0.00398452.
After 18987 training step(s), loss on training batch is 0.00450307.
After 18988 training step(s), loss on training batch is 0.00444817.
After 18989 training step(s), loss on training batch is 0.00427857.
After 18990 training step(s), loss on training batch is 0.00395194.
After 18991 training step(s), loss on training batch is 0.00432024.
After 18992 training step(s), loss on training batch is 0.00379686.
After 18993 training step(s), loss on training batch is 0.00390879.
After 18994 training step(s), loss on training batch is 0.00422596.
After 18995 training step(s), loss on training batch is 0.004274.
After 18996 training step(s), loss on training batch is 0.00381759.
After 18997 training step(s), loss on training batch is 0.00394716.
After 18998 training step(s), loss on training batch is 0.00380692.
After 18999 training step(s), loss on training batch is 0.00370309.
After 19000 training step(s), loss on training batch is 0.00404646.
After 19001 training step(s), loss on training batch is 0.0043183.
After 19002 training step(s), loss on training batch is 0.00372628.
After 19003 training step(s), loss on training batch is 0.00451812.
After 19004 training step(s), loss on training batch is 0.00455092.
After 19005 training step(s), loss on training batch is 0.00376938.
After 19006 training step(s), loss on training batch is 0.00366428.
After 19007 training step(s), loss on training batch is 0.00371021.
After 19008 training step(s), loss on training batch is 0.00450442.
After 19009 training step(s), loss on training batch is 0.00388989.
After 19010 training step(s), loss on training batch is 0.00556877.
After 19011 training step(s), loss on training batch is 0.00391784.
After 19012 training step(s), loss on training batch is 0.00450251.
After 19013 training step(s), loss on training batch is 0.00388445.
After 19014 training step(s), loss on training batch is 0.00380744.
After 19015 training step(s), loss on training batch is 0.00367291.
After 19016 training step(s), loss on training batch is 0.00377548.
After 19017 training step(s), loss on training batch is 0.00438748.
After 19018 training step(s), loss on training batch is 0.00385127.
After 19019 training step(s), loss on training batch is 0.00438893.
After 19020 training step(s), loss on training batch is 0.00384869.
After 19021 training step(s), loss on training batch is 0.00410612.
After 19022 training step(s), loss on training batch is 0.00435315.
After 19023 training step(s), loss on training batch is 0.00398225.
After 19024 training step(s), loss on training batch is 0.00369969.
After 19025 training step(s), loss on training batch is 0.0038657.
After 19026 training step(s), loss on training batch is 0.00404164.
After 19027 training step(s), loss on training batch is 0.00366832.
After 19028 training step(s), loss on training batch is 0.00376429.
After 19029 training step(s), loss on training batch is 0.0044478.
After 19030 training step(s), loss on training batch is 0.0037615.
After 19031 training step(s), loss on training batch is 0.00377775.
After 19032 training step(s), loss on training batch is 0.00387035.
After 19033 training step(s), loss on training batch is 0.00383623.
After 19034 training step(s), loss on training batch is 0.00399894.
After 19035 training step(s), loss on training batch is 0.00424229.
After 19036 training step(s), loss on training batch is 0.00418514.
After 19037 training step(s), loss on training batch is 0.0040314.
After 19038 training step(s), loss on training batch is 0.0042284.
After 19039 training step(s), loss on training batch is 0.00399214.
After 19040 training step(s), loss on training batch is 0.00397246.
After 19041 training step(s), loss on training batch is 0.00373368.
After 19042 training step(s), loss on training batch is 0.00397239.
After 19043 training step(s), loss on training batch is 0.00417134.
After 19044 training step(s), loss on training batch is 0.00373123.
After 19045 training step(s), loss on training batch is 0.00413069.
After 19046 training step(s), loss on training batch is 0.00519324.
After 19047 training step(s), loss on training batch is 0.00421236.
After 19048 training step(s), loss on training batch is 0.00455289.
After 19049 training step(s), loss on training batch is 0.00420872.
After 19050 training step(s), loss on training batch is 0.0045322.
After 19051 training step(s), loss on training batch is 0.00364392.
After 19052 training step(s), loss on training batch is 0.00405865.
After 19053 training step(s), loss on training batch is 0.00387152.
After 19054 training step(s), loss on training batch is 0.00357129.
After 19055 training step(s), loss on training batch is 0.00393579.
After 19056 training step(s), loss on training batch is 0.00398126.
After 19057 training step(s), loss on training batch is 0.00434328.
After 19058 training step(s), loss on training batch is 0.00374736.
After 19059 training step(s), loss on training batch is 0.00358797.
After 19060 training step(s), loss on training batch is 0.00396108.
After 19061 training step(s), loss on training batch is 0.00374516.
After 19062 training step(s), loss on training batch is 0.00411608.
After 19063 training step(s), loss on training batch is 0.00410408.
After 19064 training step(s), loss on training batch is 0.00396504.
After 19065 training step(s), loss on training batch is 0.00386697.
After 19066 training step(s), loss on training batch is 0.00452134.
After 19067 training step(s), loss on training batch is 0.00397714.
After 19068 training step(s), loss on training batch is 0.00403651.
After 19069 training step(s), loss on training batch is 0.00384178.
After 19070 training step(s), loss on training batch is 0.0038351.
After 19071 training step(s), loss on training batch is 0.00451339.
After 19072 training step(s), loss on training batch is 0.00383603.
After 19073 training step(s), loss on training batch is 0.00374084.
After 19074 training step(s), loss on training batch is 0.00410502.
After 19075 training step(s), loss on training batch is 0.00409752.
After 19076 training step(s), loss on training batch is 0.00388872.
After 19077 training step(s), loss on training batch is 0.00432132.
After 19078 training step(s), loss on training batch is 0.00358829.
After 19079 training step(s), loss on training batch is 0.00418466.
After 19080 training step(s), loss on training batch is 0.0039038.
After 19081 training step(s), loss on training batch is 0.00451066.
After 19082 training step(s), loss on training batch is 0.00508758.
After 19083 training step(s), loss on training batch is 0.00483318.
After 19084 training step(s), loss on training batch is 0.00367584.
After 19085 training step(s), loss on training batch is 0.00409813.
After 19086 training step(s), loss on training batch is 0.00419237.
After 19087 training step(s), loss on training batch is 0.00407451.
After 19088 training step(s), loss on training batch is 0.00361891.
After 19089 training step(s), loss on training batch is 0.00380205.
After 19090 training step(s), loss on training batch is 0.00404161.
After 19091 training step(s), loss on training batch is 0.0037951.
After 19092 training step(s), loss on training batch is 0.00409191.
After 19093 training step(s), loss on training batch is 0.00405438.
After 19094 training step(s), loss on training batch is 0.00398196.
After 19095 training step(s), loss on training batch is 0.00423518.
After 19096 training step(s), loss on training batch is 0.00394935.
After 19097 training step(s), loss on training batch is 0.0041675.
After 19098 training step(s), loss on training batch is 0.00384061.
After 19099 training step(s), loss on training batch is 0.00365238.
After 19100 training step(s), loss on training batch is 0.00362038.
After 19101 training step(s), loss on training batch is 0.00412937.
After 19102 training step(s), loss on training batch is 0.00366494.
After 19103 training step(s), loss on training batch is 0.00450888.
After 19104 training step(s), loss on training batch is 0.00383369.
After 19105 training step(s), loss on training batch is 0.00395787.
After 19106 training step(s), loss on training batch is 0.00474837.
After 19107 training step(s), loss on training batch is 0.00438191.
After 19108 training step(s), loss on training batch is 0.003663.
After 19109 training step(s), loss on training batch is 0.00410091.
After 19110 training step(s), loss on training batch is 0.00433002.
After 19111 training step(s), loss on training batch is 0.00382533.
After 19112 training step(s), loss on training batch is 0.00366096.
After 19113 training step(s), loss on training batch is 0.00380847.
After 19114 training step(s), loss on training batch is 0.00410956.
After 19115 training step(s), loss on training batch is 0.00402329.
After 19116 training step(s), loss on training batch is 0.00374724.
After 19117 training step(s), loss on training batch is 0.00415466.
After 19118 training step(s), loss on training batch is 0.00392322.
After 19119 training step(s), loss on training batch is 0.00357415.
After 19120 training step(s), loss on training batch is 0.00395889.
After 19121 training step(s), loss on training batch is 0.00522227.
After 19122 training step(s), loss on training batch is 0.00435235.
After 19123 training step(s), loss on training batch is 0.00402588.
After 19124 training step(s), loss on training batch is 0.00356193.
After 19125 training step(s), loss on training batch is 0.00404711.
After 19126 training step(s), loss on training batch is 0.00426752.
After 19127 training step(s), loss on training batch is 0.00405473.
After 19128 training step(s), loss on training batch is 0.00412665.
After 19129 training step(s), loss on training batch is 0.00392426.
After 19130 training step(s), loss on training batch is 0.00377976.
After 19131 training step(s), loss on training batch is 0.00365412.
After 19132 training step(s), loss on training batch is 0.00368223.
After 19133 training step(s), loss on training batch is 0.00416917.
After 19134 training step(s), loss on training batch is 0.00384483.
After 19135 training step(s), loss on training batch is 0.0038044.
After 19136 training step(s), loss on training batch is 0.00367826.
After 19137 training step(s), loss on training batch is 0.00445165.
After 19138 training step(s), loss on training batch is 0.00438659.
After 19139 training step(s), loss on training batch is 0.00444647.
After 19140 training step(s), loss on training batch is 0.00384447.
After 19141 training step(s), loss on training batch is 0.00415499.
After 19142 training step(s), loss on training batch is 0.00374753.
After 19143 training step(s), loss on training batch is 0.00405897.
After 19144 training step(s), loss on training batch is 0.0042486.
After 19145 training step(s), loss on training batch is 0.00402679.
After 19146 training step(s), loss on training batch is 0.00370765.
After 19147 training step(s), loss on training batch is 0.00477396.
After 19148 training step(s), loss on training batch is 0.00386778.
After 19149 training step(s), loss on training batch is 0.00458628.
After 19150 training step(s), loss on training batch is 0.00367353.
After 19151 training step(s), loss on training batch is 0.00502473.
After 19152 training step(s), loss on training batch is 0.00373818.
After 19153 training step(s), loss on training batch is 0.0036409.
After 19154 training step(s), loss on training batch is 0.00400872.
After 19155 training step(s), loss on training batch is 0.00393312.
After 19156 training step(s), loss on training batch is 0.00362008.
After 19157 training step(s), loss on training batch is 0.0041167.
After 19158 training step(s), loss on training batch is 0.0035862.
After 19159 training step(s), loss on training batch is 0.00472155.
After 19160 training step(s), loss on training batch is 0.00411471.
After 19161 training step(s), loss on training batch is 0.00515777.
After 19162 training step(s), loss on training batch is 0.0036215.
After 19163 training step(s), loss on training batch is 0.00394883.
After 19164 training step(s), loss on training batch is 0.00448854.
After 19165 training step(s), loss on training batch is 0.00398081.
After 19166 training step(s), loss on training batch is 0.00415692.
After 19167 training step(s), loss on training batch is 0.00393817.
After 19168 training step(s), loss on training batch is 0.00385594.
After 19169 training step(s), loss on training batch is 0.00382432.
After 19170 training step(s), loss on training batch is 0.00400948.
After 19171 training step(s), loss on training batch is 0.00422782.
After 19172 training step(s), loss on training batch is 0.00425473.
After 19173 training step(s), loss on training batch is 0.00424517.
After 19174 training step(s), loss on training batch is 0.00390456.
After 19175 training step(s), loss on training batch is 0.00386512.
After 19176 training step(s), loss on training batch is 0.00362159.
After 19177 training step(s), loss on training batch is 0.00383786.
After 19178 training step(s), loss on training batch is 0.00418057.
After 19179 training step(s), loss on training batch is 0.0042081.
After 19180 training step(s), loss on training batch is 0.00444347.
After 19181 training step(s), loss on training batch is 0.00367918.
After 19182 training step(s), loss on training batch is 0.00386831.
After 19183 training step(s), loss on training batch is 0.00433318.
After 19184 training step(s), loss on training batch is 0.00377507.
After 19185 training step(s), loss on training batch is 0.00517999.
After 19186 training step(s), loss on training batch is 0.00418194.
After 19187 training step(s), loss on training batch is 0.00379452.
After 19188 training step(s), loss on training batch is 0.00382726.
After 19189 training step(s), loss on training batch is 0.00392659.
After 19190 training step(s), loss on training batch is 0.00410936.
After 19191 training step(s), loss on training batch is 0.00432685.
After 19192 training step(s), loss on training batch is 0.00397339.
After 19193 training step(s), loss on training batch is 0.00386894.
After 19194 training step(s), loss on training batch is 0.00411321.
After 19195 training step(s), loss on training batch is 0.00656361.
After 19196 training step(s), loss on training batch is 0.00362254.
After 19197 training step(s), loss on training batch is 0.0046526.
After 19198 training step(s), loss on training batch is 0.0043993.
After 19199 training step(s), loss on training batch is 0.00416518.
After 19200 training step(s), loss on training batch is 0.00414407.
After 19201 training step(s), loss on training batch is 0.00393952.
After 19202 training step(s), loss on training batch is 0.00370675.
After 19203 training step(s), loss on training batch is 0.00373245.
After 19204 training step(s), loss on training batch is 0.004235.
After 19205 training step(s), loss on training batch is 0.00396395.
After 19206 training step(s), loss on training batch is 0.0042419.
After 19207 training step(s), loss on training batch is 0.00389564.
After 19208 training step(s), loss on training batch is 0.00487881.
After 19209 training step(s), loss on training batch is 0.00438417.
After 19210 training step(s), loss on training batch is 0.00442288.
After 19211 training step(s), loss on training batch is 0.00403768.
After 19212 training step(s), loss on training batch is 0.00425324.
After 19213 training step(s), loss on training batch is 0.00398253.
After 19214 training step(s), loss on training batch is 0.00491293.
After 19215 training step(s), loss on training batch is 0.00424242.
After 19216 training step(s), loss on training batch is 0.00403755.
After 19217 training step(s), loss on training batch is 0.00388971.
After 19218 training step(s), loss on training batch is 0.00370675.
After 19219 training step(s), loss on training batch is 0.00400485.
After 19220 training step(s), loss on training batch is 0.00449791.
After 19221 training step(s), loss on training batch is 0.00412052.
After 19222 training step(s), loss on training batch is 0.00399027.
After 19223 training step(s), loss on training batch is 0.00381959.
After 19224 training step(s), loss on training batch is 0.00555005.
After 19225 training step(s), loss on training batch is 0.00386818.
After 19226 training step(s), loss on training batch is 0.00382222.
After 19227 training step(s), loss on training batch is 0.00431958.
After 19228 training step(s), loss on training batch is 0.00440901.
After 19229 training step(s), loss on training batch is 0.00410498.
After 19230 training step(s), loss on training batch is 0.00395433.
After 19231 training step(s), loss on training batch is 0.00439144.
After 19232 training step(s), loss on training batch is 0.00379243.
After 19233 training step(s), loss on training batch is 0.00395314.
After 19234 training step(s), loss on training batch is 0.00374424.
After 19235 training step(s), loss on training batch is 0.003839.
After 19236 training step(s), loss on training batch is 0.00388405.
After 19237 training step(s), loss on training batch is 0.0048054.
After 19238 training step(s), loss on training batch is 0.00447933.
After 19239 training step(s), loss on training batch is 0.00363183.
After 19240 training step(s), loss on training batch is 0.00393109.
After 19241 training step(s), loss on training batch is 0.0038362.
After 19242 training step(s), loss on training batch is 0.00400918.
After 19243 training step(s), loss on training batch is 0.00377564.
After 19244 training step(s), loss on training batch is 0.00653377.
After 19245 training step(s), loss on training batch is 0.00378713.
After 19246 training step(s), loss on training batch is 0.00430701.
After 19247 training step(s), loss on training batch is 0.00478984.
After 19248 training step(s), loss on training batch is 0.00459013.
After 19249 training step(s), loss on training batch is 0.00662409.
After 19250 training step(s), loss on training batch is 0.0039673.
After 19251 training step(s), loss on training batch is 0.00369989.
After 19252 training step(s), loss on training batch is 0.00375164.
After 19253 training step(s), loss on training batch is 0.00393271.
After 19254 training step(s), loss on training batch is 0.00395383.
After 19255 training step(s), loss on training batch is 0.0037698.
After 19256 training step(s), loss on training batch is 0.00352813.
After 19257 training step(s), loss on training batch is 0.00423525.
After 19258 training step(s), loss on training batch is 0.00399279.
After 19259 training step(s), loss on training batch is 0.00413624.
After 19260 training step(s), loss on training batch is 0.00409054.
After 19261 training step(s), loss on training batch is 0.00395789.
After 19262 training step(s), loss on training batch is 0.00413255.
After 19263 training step(s), loss on training batch is 0.00370648.
After 19264 training step(s), loss on training batch is 0.00432729.
After 19265 training step(s), loss on training batch is 0.00399113.
After 19266 training step(s), loss on training batch is 0.00387621.
After 19267 training step(s), loss on training batch is 0.00392434.
After 19268 training step(s), loss on training batch is 0.00404153.
After 19269 training step(s), loss on training batch is 0.00446712.
After 19270 training step(s), loss on training batch is 0.00369683.
After 19271 training step(s), loss on training batch is 0.00391312.
After 19272 training step(s), loss on training batch is 0.00380252.
After 19273 training step(s), loss on training batch is 0.00416588.
After 19274 training step(s), loss on training batch is 0.00432765.
After 19275 training step(s), loss on training batch is 0.00387766.
After 19276 training step(s), loss on training batch is 0.00354604.
After 19277 training step(s), loss on training batch is 0.0037535.
After 19278 training step(s), loss on training batch is 0.00421934.
After 19279 training step(s), loss on training batch is 0.00411182.
After 19280 training step(s), loss on training batch is 0.00382411.
After 19281 training step(s), loss on training batch is 0.00480786.
After 19282 training step(s), loss on training batch is 0.00380073.
After 19283 training step(s), loss on training batch is 0.00433404.
After 19284 training step(s), loss on training batch is 0.00360624.
After 19285 training step(s), loss on training batch is 0.00362917.
After 19286 training step(s), loss on training batch is 0.00370708.
After 19287 training step(s), loss on training batch is 0.00450645.
After 19288 training step(s), loss on training batch is 0.00376704.
After 19289 training step(s), loss on training batch is 0.00437139.
After 19290 training step(s), loss on training batch is 0.00436246.
After 19291 training step(s), loss on training batch is 0.00463365.
After 19292 training step(s), loss on training batch is 0.0040042.
After 19293 training step(s), loss on training batch is 0.00388944.
After 19294 training step(s), loss on training batch is 0.00408967.
After 19295 training step(s), loss on training batch is 0.00412664.
After 19296 training step(s), loss on training batch is 0.00366344.
After 19297 training step(s), loss on training batch is 0.00446509.
After 19298 training step(s), loss on training batch is 0.00382634.
After 19299 training step(s), loss on training batch is 0.00400711.
After 19300 training step(s), loss on training batch is 0.00377455.
After 19301 training step(s), loss on training batch is 0.00352116.
After 19302 training step(s), loss on training batch is 0.00454314.
After 19303 training step(s), loss on training batch is 0.00398718.
After 19304 training step(s), loss on training batch is 0.00372874.
After 19305 training step(s), loss on training batch is 0.0041404.
After 19306 training step(s), loss on training batch is 0.003869.
After 19307 training step(s), loss on training batch is 0.00403966.
After 19308 training step(s), loss on training batch is 0.00424796.
After 19309 training step(s), loss on training batch is 0.00372082.
After 19310 training step(s), loss on training batch is 0.00421075.
After 19311 training step(s), loss on training batch is 0.00439277.
After 19312 training step(s), loss on training batch is 0.00419169.
After 19313 training step(s), loss on training batch is 0.00540726.
After 19314 training step(s), loss on training batch is 0.00363106.
After 19315 training step(s), loss on training batch is 0.00419031.
After 19316 training step(s), loss on training batch is 0.00397744.
After 19317 training step(s), loss on training batch is 0.00364228.
After 19318 training step(s), loss on training batch is 0.00386929.
After 19319 training step(s), loss on training batch is 0.00419021.
After 19320 training step(s), loss on training batch is 0.00412791.
After 19321 training step(s), loss on training batch is 0.00444966.
After 19322 training step(s), loss on training batch is 0.004373.
After 19323 training step(s), loss on training batch is 0.00398623.
After 19324 training step(s), loss on training batch is 0.00406144.
After 19325 training step(s), loss on training batch is 0.00357791.
After 19326 training step(s), loss on training batch is 0.00368259.
After 19327 training step(s), loss on training batch is 0.00358341.
After 19328 training step(s), loss on training batch is 0.00373777.
After 19329 training step(s), loss on training batch is 0.00364054.
After 19330 training step(s), loss on training batch is 0.00371286.
After 19331 training step(s), loss on training batch is 0.00384077.
After 19332 training step(s), loss on training batch is 0.00422674.
After 19333 training step(s), loss on training batch is 0.00389306.
After 19334 training step(s), loss on training batch is 0.00416979.
After 19335 training step(s), loss on training batch is 0.00493094.
After 19336 training step(s), loss on training batch is 0.004695.
After 19337 training step(s), loss on training batch is 0.00371224.
After 19338 training step(s), loss on training batch is 0.0041738.
After 19339 training step(s), loss on training batch is 0.00390645.
After 19340 training step(s), loss on training batch is 0.00382019.
After 19341 training step(s), loss on training batch is 0.00395854.
After 19342 training step(s), loss on training batch is 0.00370902.
After 19343 training step(s), loss on training batch is 0.00355634.
After 19344 training step(s), loss on training batch is 0.0037722.
After 19345 training step(s), loss on training batch is 0.00393131.
After 19346 training step(s), loss on training batch is 0.00403993.
After 19347 training step(s), loss on training batch is 0.00406967.
After 19348 training step(s), loss on training batch is 0.00389604.
After 19349 training step(s), loss on training batch is 0.00405987.
After 19350 training step(s), loss on training batch is 0.00459119.
After 19351 training step(s), loss on training batch is 0.00383104.
After 19352 training step(s), loss on training batch is 0.00357317.
After 19353 training step(s), loss on training batch is 0.00375126.
After 19354 training step(s), loss on training batch is 0.00377916.
After 19355 training step(s), loss on training batch is 0.0038113.
After 19356 training step(s), loss on training batch is 0.00402888.
After 19357 training step(s), loss on training batch is 0.00367932.
After 19358 training step(s), loss on training batch is 0.00366548.
After 19359 training step(s), loss on training batch is 0.00390113.
After 19360 training step(s), loss on training batch is 0.00439806.
After 19361 training step(s), loss on training batch is 0.00377841.
After 19362 training step(s), loss on training batch is 0.00384028.
After 19363 training step(s), loss on training batch is 0.00450324.
After 19364 training step(s), loss on training batch is 0.0037866.
After 19365 training step(s), loss on training batch is 0.00367199.
After 19366 training step(s), loss on training batch is 0.00444207.
After 19367 training step(s), loss on training batch is 0.00383852.
After 19368 training step(s), loss on training batch is 0.00379723.
After 19369 training step(s), loss on training batch is 0.00381388.
After 19370 training step(s), loss on training batch is 0.00468259.
After 19371 training step(s), loss on training batch is 0.00408954.
After 19372 training step(s), loss on training batch is 0.00385485.
After 19373 training step(s), loss on training batch is 0.00421538.
After 19374 training step(s), loss on training batch is 0.00409613.
After 19375 training step(s), loss on training batch is 0.00396615.
After 19376 training step(s), loss on training batch is 0.00419391.
After 19377 training step(s), loss on training batch is 0.00382115.
After 19378 training step(s), loss on training batch is 0.00381934.
After 19379 training step(s), loss on training batch is 0.00394571.
After 19380 training step(s), loss on training batch is 0.00406087.
After 19381 training step(s), loss on training batch is 0.00418092.
After 19382 training step(s), loss on training batch is 0.00396145.
After 19383 training step(s), loss on training batch is 0.00389094.
After 19384 training step(s), loss on training batch is 0.0045419.
After 19385 training step(s), loss on training batch is 0.00425721.
After 19386 training step(s), loss on training batch is 0.00584918.
After 19387 training step(s), loss on training batch is 0.00366041.
After 19388 training step(s), loss on training batch is 0.0036852.
After 19389 training step(s), loss on training batch is 0.0039135.
After 19390 training step(s), loss on training batch is 0.00391256.
After 19391 training step(s), loss on training batch is 0.00419315.
After 19392 training step(s), loss on training batch is 0.00517263.
After 19393 training step(s), loss on training batch is 0.00348183.
After 19394 training step(s), loss on training batch is 0.00396157.
After 19395 training step(s), loss on training batch is 0.00364285.
After 19396 training step(s), loss on training batch is 0.00409551.
After 19397 training step(s), loss on training batch is 0.00428926.
After 19398 training step(s), loss on training batch is 0.00379358.
After 19399 training step(s), loss on training batch is 0.00415993.
After 19400 training step(s), loss on training batch is 0.00385187.
After 19401 training step(s), loss on training batch is 0.00392997.
After 19402 training step(s), loss on training batch is 0.00403499.
After 19403 training step(s), loss on training batch is 0.00396144.
After 19404 training step(s), loss on training batch is 0.0044519.
After 19405 training step(s), loss on training batch is 0.0036529.
After 19406 training step(s), loss on training batch is 0.00399988.
After 19407 training step(s), loss on training batch is 0.0039663.
After 19408 training step(s), loss on training batch is 0.00444156.
After 19409 training step(s), loss on training batch is 0.00409723.
After 19410 training step(s), loss on training batch is 0.00502459.
After 19411 training step(s), loss on training batch is 0.00376247.
After 19412 training step(s), loss on training batch is 0.00397073.
After 19413 training step(s), loss on training batch is 0.0042748.
After 19414 training step(s), loss on training batch is 0.00377865.
After 19415 training step(s), loss on training batch is 0.00378862.
After 19416 training step(s), loss on training batch is 0.00424237.
After 19417 training step(s), loss on training batch is 0.00418288.
After 19418 training step(s), loss on training batch is 0.00431466.
After 19419 training step(s), loss on training batch is 0.00402361.
After 19420 training step(s), loss on training batch is 0.00381854.
After 19421 training step(s), loss on training batch is 0.00656542.
After 19422 training step(s), loss on training batch is 0.00778919.
After 19423 training step(s), loss on training batch is 0.00440038.
After 19424 training step(s), loss on training batch is 0.00437033.
After 19425 training step(s), loss on training batch is 0.00380717.
After 19426 training step(s), loss on training batch is 0.00437647.
After 19427 training step(s), loss on training batch is 0.00364948.
After 19428 training step(s), loss on training batch is 0.00438745.
After 19429 training step(s), loss on training batch is 0.00443551.
After 19430 training step(s), loss on training batch is 0.00413706.
After 19431 training step(s), loss on training batch is 0.00401882.
After 19432 training step(s), loss on training batch is 0.00376305.
After 19433 training step(s), loss on training batch is 0.0040973.
After 19434 training step(s), loss on training batch is 0.00367921.
After 19435 training step(s), loss on training batch is 0.00435133.
After 19436 training step(s), loss on training batch is 0.00390999.
After 19437 training step(s), loss on training batch is 0.00393689.
After 19438 training step(s), loss on training batch is 0.00407489.
After 19439 training step(s), loss on training batch is 0.00419442.
After 19440 training step(s), loss on training batch is 0.00408385.
After 19441 training step(s), loss on training batch is 0.00444373.
After 19442 training step(s), loss on training batch is 0.00357879.
After 19443 training step(s), loss on training batch is 0.00373109.
After 19444 training step(s), loss on training batch is 0.00364107.
After 19445 training step(s), loss on training batch is 0.0040333.
After 19446 training step(s), loss on training batch is 0.0036665.
After 19447 training step(s), loss on training batch is 0.00403035.
After 19448 training step(s), loss on training batch is 0.00501697.
After 19449 training step(s), loss on training batch is 0.00419944.
After 19450 training step(s), loss on training batch is 0.00492873.
After 19451 training step(s), loss on training batch is 0.00417323.
After 19452 training step(s), loss on training batch is 0.00377009.
After 19453 training step(s), loss on training batch is 0.00408177.
After 19454 training step(s), loss on training batch is 0.00406303.
After 19455 training step(s), loss on training batch is 0.00400748.
After 19456 training step(s), loss on training batch is 0.00395263.
After 19457 training step(s), loss on training batch is 0.00392207.
After 19458 training step(s), loss on training batch is 0.00479837.
After 19459 training step(s), loss on training batch is 0.00381474.
After 19460 training step(s), loss on training batch is 0.00386386.
After 19461 training step(s), loss on training batch is 0.00534627.
After 19462 training step(s), loss on training batch is 0.00390409.
After 19463 training step(s), loss on training batch is 0.00442184.
After 19464 training step(s), loss on training batch is 0.00375971.
After 19465 training step(s), loss on training batch is 0.00410766.
After 19466 training step(s), loss on training batch is 0.00506526.
After 19467 training step(s), loss on training batch is 0.0040064.
After 19468 training step(s), loss on training batch is 0.00440871.
After 19469 training step(s), loss on training batch is 0.00408289.
After 19470 training step(s), loss on training batch is 0.0040758.
After 19471 training step(s), loss on training batch is 0.00404205.
After 19472 training step(s), loss on training batch is 0.00395986.
After 19473 training step(s), loss on training batch is 0.00430346.
After 19474 training step(s), loss on training batch is 0.00447007.
After 19475 training step(s), loss on training batch is 0.00354639.
After 19476 training step(s), loss on training batch is 0.00388115.
After 19477 training step(s), loss on training batch is 0.00455581.
After 19478 training step(s), loss on training batch is 0.00401539.
After 19479 training step(s), loss on training batch is 0.00428069.
After 19480 training step(s), loss on training batch is 0.0040809.
After 19481 training step(s), loss on training batch is 0.00616935.
After 19482 training step(s), loss on training batch is 0.00407423.
After 19483 training step(s), loss on training batch is 0.00382943.
After 19484 training step(s), loss on training batch is 0.00376441.
After 19485 training step(s), loss on training batch is 0.00398446.
After 19486 training step(s), loss on training batch is 0.00369495.
After 19487 training step(s), loss on training batch is 0.00405584.
After 19488 training step(s), loss on training batch is 0.00377468.
After 19489 training step(s), loss on training batch is 0.00403484.
After 19490 training step(s), loss on training batch is 0.00400111.
After 19491 training step(s), loss on training batch is 0.00433303.
After 19492 training step(s), loss on training batch is 0.00371533.
After 19493 training step(s), loss on training batch is 0.00402924.
After 19494 training step(s), loss on training batch is 0.00376266.
After 19495 training step(s), loss on training batch is 0.00390299.
After 19496 training step(s), loss on training batch is 0.00407364.
After 19497 training step(s), loss on training batch is 0.00370598.
After 19498 training step(s), loss on training batch is 0.0060278.
After 19499 training step(s), loss on training batch is 0.00371342.
After 19500 training step(s), loss on training batch is 0.0035715.
After 19501 training step(s), loss on training batch is 0.00410764.
After 19502 training step(s), loss on training batch is 0.0041178.
After 19503 training step(s), loss on training batch is 0.00365096.
After 19504 training step(s), loss on training batch is 0.00363404.
After 19505 training step(s), loss on training batch is 0.00379918.
After 19506 training step(s), loss on training batch is 0.00391754.
After 19507 training step(s), loss on training batch is 0.00377948.
After 19508 training step(s), loss on training batch is 0.00408335.
After 19509 training step(s), loss on training batch is 0.00438164.
After 19510 training step(s), loss on training batch is 0.00500235.
After 19511 training step(s), loss on training batch is 0.00450316.
After 19512 training step(s), loss on training batch is 0.00364539.
After 19513 training step(s), loss on training batch is 0.00409477.
After 19514 training step(s), loss on training batch is 0.00381231.
After 19515 training step(s), loss on training batch is 0.0042466.
After 19516 training step(s), loss on training batch is 0.00440347.
After 19517 training step(s), loss on training batch is 0.00356335.
After 19518 training step(s), loss on training batch is 0.0037844.
After 19519 training step(s), loss on training batch is 0.00417032.
After 19520 training step(s), loss on training batch is 0.00372554.
After 19521 training step(s), loss on training batch is 0.00395885.
After 19522 training step(s), loss on training batch is 0.0039508.
After 19523 training step(s), loss on training batch is 0.00387046.
After 19524 training step(s), loss on training batch is 0.00354877.
After 19525 training step(s), loss on training batch is 0.00448359.
After 19526 training step(s), loss on training batch is 0.00398735.
After 19527 training step(s), loss on training batch is 0.00455012.
After 19528 training step(s), loss on training batch is 0.00398998.
After 19529 training step(s), loss on training batch is 0.00420397.
After 19530 training step(s), loss on training batch is 0.00369869.
After 19531 training step(s), loss on training batch is 0.00355036.
After 19532 training step(s), loss on training batch is 0.00374898.
After 19533 training step(s), loss on training batch is 0.00402014.
After 19534 training step(s), loss on training batch is 0.00392493.
After 19535 training step(s), loss on training batch is 0.00419227.
After 19536 training step(s), loss on training batch is 0.0044337.
After 19537 training step(s), loss on training batch is 0.00377556.
After 19538 training step(s), loss on training batch is 0.00420586.
After 19539 training step(s), loss on training batch is 0.00381526.
After 19540 training step(s), loss on training batch is 0.00370742.
After 19541 training step(s), loss on training batch is 0.00418575.
After 19542 training step(s), loss on training batch is 0.00359926.
After 19543 training step(s), loss on training batch is 0.00380855.
After 19544 training step(s), loss on training batch is 0.00470498.
After 19545 training step(s), loss on training batch is 0.00377402.
After 19546 training step(s), loss on training batch is 0.00456422.
After 19547 training step(s), loss on training batch is 0.00411356.
After 19548 training step(s), loss on training batch is 0.00367512.
After 19549 training step(s), loss on training batch is 0.00437609.
After 19550 training step(s), loss on training batch is 0.00402968.
After 19551 training step(s), loss on training batch is 0.00381238.
After 19552 training step(s), loss on training batch is 0.00383806.
After 19553 training step(s), loss on training batch is 0.00363901.
After 19554 training step(s), loss on training batch is 0.00403849.
After 19555 training step(s), loss on training batch is 0.00465302.
After 19556 training step(s), loss on training batch is 0.00497111.
After 19557 training step(s), loss on training batch is 0.00400775.
After 19558 training step(s), loss on training batch is 0.00464855.
After 19559 training step(s), loss on training batch is 0.00366576.
After 19560 training step(s), loss on training batch is 0.003745.
After 19561 training step(s), loss on training batch is 0.00376064.
After 19562 training step(s), loss on training batch is 0.00398106.
After 19563 training step(s), loss on training batch is 0.00387531.
After 19564 training step(s), loss on training batch is 0.00356914.
After 19565 training step(s), loss on training batch is 0.00353014.
After 19566 training step(s), loss on training batch is 0.00405964.
After 19567 training step(s), loss on training batch is 0.00392661.
After 19568 training step(s), loss on training batch is 0.00384539.
After 19569 training step(s), loss on training batch is 0.00384471.
After 19570 training step(s), loss on training batch is 0.00409805.
After 19571 training step(s), loss on training batch is 0.00390954.
After 19572 training step(s), loss on training batch is 0.00396074.
After 19573 training step(s), loss on training batch is 0.00459136.
After 19574 training step(s), loss on training batch is 0.00384246.
After 19575 training step(s), loss on training batch is 0.00361221.
After 19576 training step(s), loss on training batch is 0.00378632.
After 19577 training step(s), loss on training batch is 0.00388918.
After 19578 training step(s), loss on training batch is 0.00390092.
After 19579 training step(s), loss on training batch is 0.00377323.
After 19580 training step(s), loss on training batch is 0.00425711.
After 19581 training step(s), loss on training batch is 0.00423019.
After 19582 training step(s), loss on training batch is 0.00423407.
After 19583 training step(s), loss on training batch is 0.00361395.
After 19584 training step(s), loss on training batch is 0.00421527.
After 19585 training step(s), loss on training batch is 0.00353083.
After 19586 training step(s), loss on training batch is 0.00449642.
After 19587 training step(s), loss on training batch is 0.00390287.
After 19588 training step(s), loss on training batch is 0.00380144.
After 19589 training step(s), loss on training batch is 0.00360284.
After 19590 training step(s), loss on training batch is 0.00403258.
After 19591 training step(s), loss on training batch is 0.00387107.
After 19592 training step(s), loss on training batch is 0.00415024.
After 19593 training step(s), loss on training batch is 0.00365432.
After 19594 training step(s), loss on training batch is 0.00381323.
After 19595 training step(s), loss on training batch is 0.00359412.
After 19596 training step(s), loss on training batch is 0.0055904.
After 19597 training step(s), loss on training batch is 0.00362431.
After 19598 training step(s), loss on training batch is 0.00474676.
After 19599 training step(s), loss on training batch is 0.00508263.
After 19600 training step(s), loss on training batch is 0.00374085.
After 19601 training step(s), loss on training batch is 0.00398588.
After 19602 training step(s), loss on training batch is 0.00410986.
After 19603 training step(s), loss on training batch is 0.00532622.
After 19604 training step(s), loss on training batch is 0.00368878.
After 19605 training step(s), loss on training batch is 0.00397238.
After 19606 training step(s), loss on training batch is 0.00386826.
After 19607 training step(s), loss on training batch is 0.003709.
After 19608 training step(s), loss on training batch is 0.0041785.
After 19609 training step(s), loss on training batch is 0.00425141.
After 19610 training step(s), loss on training batch is 0.00394012.
After 19611 training step(s), loss on training batch is 0.00376579.
After 19612 training step(s), loss on training batch is 0.00434015.
After 19613 training step(s), loss on training batch is 0.00425013.
After 19614 training step(s), loss on training batch is 0.00380926.
After 19615 training step(s), loss on training batch is 0.00384267.
After 19616 training step(s), loss on training batch is 0.00421193.
After 19617 training step(s), loss on training batch is 0.00358889.
After 19618 training step(s), loss on training batch is 0.00411233.
After 19619 training step(s), loss on training batch is 0.00412223.
After 19620 training step(s), loss on training batch is 0.00405642.
After 19621 training step(s), loss on training batch is 0.00418409.
After 19622 training step(s), loss on training batch is 0.00421135.
After 19623 training step(s), loss on training batch is 0.00385382.
After 19624 training step(s), loss on training batch is 0.00385466.
After 19625 training step(s), loss on training batch is 0.003872.
After 19626 training step(s), loss on training batch is 0.00368238.
After 19627 training step(s), loss on training batch is 0.00401668.
After 19628 training step(s), loss on training batch is 0.00347347.
After 19629 training step(s), loss on training batch is 0.00426541.
After 19630 training step(s), loss on training batch is 0.0038914.
After 19631 training step(s), loss on training batch is 0.00380666.
After 19632 training step(s), loss on training batch is 0.00391483.
After 19633 training step(s), loss on training batch is 0.00397001.
After 19634 training step(s), loss on training batch is 0.00363429.
After 19635 training step(s), loss on training batch is 0.00423156.
After 19636 training step(s), loss on training batch is 0.00547986.
After 19637 training step(s), loss on training batch is 0.003706.
After 19638 training step(s), loss on training batch is 0.0039123.
After 19639 training step(s), loss on training batch is 0.0039132.
After 19640 training step(s), loss on training batch is 0.00406183.
After 19641 training step(s), loss on training batch is 0.00386583.
After 19642 training step(s), loss on training batch is 0.00371622.
After 19643 training step(s), loss on training batch is 0.00357413.
After 19644 training step(s), loss on training batch is 0.00386996.
After 19645 training step(s), loss on training batch is 0.00379674.
After 19646 training step(s), loss on training batch is 0.00379051.
After 19647 training step(s), loss on training batch is 0.00363667.
After 19648 training step(s), loss on training batch is 0.00385818.
After 19649 training step(s), loss on training batch is 0.00363039.
After 19650 training step(s), loss on training batch is 0.00401658.
After 19651 training step(s), loss on training batch is 0.00409013.
After 19652 training step(s), loss on training batch is 0.00687112.
After 19653 training step(s), loss on training batch is 0.0054448.
After 19654 training step(s), loss on training batch is 0.00454502.
After 19655 training step(s), loss on training batch is 0.0043783.
After 19656 training step(s), loss on training batch is 0.00354089.
After 19657 training step(s), loss on training batch is 0.00353515.
After 19658 training step(s), loss on training batch is 0.00358028.
After 19659 training step(s), loss on training batch is 0.00409204.
After 19660 training step(s), loss on training batch is 0.00371918.
After 19661 training step(s), loss on training batch is 0.00369218.
After 19662 training step(s), loss on training batch is 0.00412507.
After 19663 training step(s), loss on training batch is 0.00390042.
After 19664 training step(s), loss on training batch is 0.00433164.
After 19665 training step(s), loss on training batch is 0.00390226.
After 19666 training step(s), loss on training batch is 0.00392598.
After 19667 training step(s), loss on training batch is 0.00379215.
After 19668 training step(s), loss on training batch is 0.00373787.
After 19669 training step(s), loss on training batch is 0.00391894.
After 19670 training step(s), loss on training batch is 0.00375208.
After 19671 training step(s), loss on training batch is 0.00520433.
After 19672 training step(s), loss on training batch is 0.00453332.
After 19673 training step(s), loss on training batch is 0.00380054.
After 19674 training step(s), loss on training batch is 0.00356947.
After 19675 training step(s), loss on training batch is 0.00369417.
After 19676 training step(s), loss on training batch is 0.00411007.
After 19677 training step(s), loss on training batch is 0.00495877.
After 19678 training step(s), loss on training batch is 0.00352323.
After 19679 training step(s), loss on training batch is 0.0041588.
After 19680 training step(s), loss on training batch is 0.0036905.
After 19681 training step(s), loss on training batch is 0.00435718.
After 19682 training step(s), loss on training batch is 0.00369699.
After 19683 training step(s), loss on training batch is 0.00375121.
After 19684 training step(s), loss on training batch is 0.00389443.
After 19685 training step(s), loss on training batch is 0.00402877.
After 19686 training step(s), loss on training batch is 0.00652905.
After 19687 training step(s), loss on training batch is 0.00385167.
After 19688 training step(s), loss on training batch is 0.00413175.
After 19689 training step(s), loss on training batch is 0.00431741.
After 19690 training step(s), loss on training batch is 0.00404308.
After 19691 training step(s), loss on training batch is 0.00419613.
After 19692 training step(s), loss on training batch is 0.00413106.
After 19693 training step(s), loss on training batch is 0.00349637.
After 19694 training step(s), loss on training batch is 0.00396155.
After 19695 training step(s), loss on training batch is 0.00442417.
After 19696 training step(s), loss on training batch is 0.00413746.
After 19697 training step(s), loss on training batch is 0.00396111.
After 19698 training step(s), loss on training batch is 0.00410546.
After 19699 training step(s), loss on training batch is 0.00395249.
After 19700 training step(s), loss on training batch is 0.00369853.
After 19701 training step(s), loss on training batch is 0.00355646.
After 19702 training step(s), loss on training batch is 0.00391172.
After 19703 training step(s), loss on training batch is 0.00392813.
After 19704 training step(s), loss on training batch is 0.00412884.
After 19705 training step(s), loss on training batch is 0.00376523.
After 19706 training step(s), loss on training batch is 0.00409116.
After 19707 training step(s), loss on training batch is 0.0041621.
After 19708 training step(s), loss on training batch is 0.00376917.
After 19709 training step(s), loss on training batch is 0.00404845.
After 19710 training step(s), loss on training batch is 0.00370399.
After 19711 training step(s), loss on training batch is 0.00372711.
After 19712 training step(s), loss on training batch is 0.00410132.
After 19713 training step(s), loss on training batch is 0.0037223.
After 19714 training step(s), loss on training batch is 0.00395877.
After 19715 training step(s), loss on training batch is 0.00404686.
After 19716 training step(s), loss on training batch is 0.00404787.
After 19717 training step(s), loss on training batch is 0.00419199.
After 19718 training step(s), loss on training batch is 0.00385463.
After 19719 training step(s), loss on training batch is 0.00366672.
After 19720 training step(s), loss on training batch is 0.00380864.
After 19721 training step(s), loss on training batch is 0.00376333.
After 19722 training step(s), loss on training batch is 0.00375207.
After 19723 training step(s), loss on training batch is 0.00417083.
After 19724 training step(s), loss on training batch is 0.00414569.
After 19725 training step(s), loss on training batch is 0.00370572.
After 19726 training step(s), loss on training batch is 0.00384368.
After 19727 training step(s), loss on training batch is 0.00374374.
After 19728 training step(s), loss on training batch is 0.0043636.
After 19729 training step(s), loss on training batch is 0.00373877.
After 19730 training step(s), loss on training batch is 0.00395286.
After 19731 training step(s), loss on training batch is 0.00385746.
After 19732 training step(s), loss on training batch is 0.00389558.
After 19733 training step(s), loss on training batch is 0.00402688.
After 19734 training step(s), loss on training batch is 0.00428446.
After 19735 training step(s), loss on training batch is 0.00390629.
After 19736 training step(s), loss on training batch is 0.00422981.
After 19737 training step(s), loss on training batch is 0.00376066.
After 19738 training step(s), loss on training batch is 0.00399612.
After 19739 training step(s), loss on training batch is 0.00375569.
After 19740 training step(s), loss on training batch is 0.00422061.
After 19741 training step(s), loss on training batch is 0.00385986.
After 19742 training step(s), loss on training batch is 0.00382223.
After 19743 training step(s), loss on training batch is 0.0042717.
After 19744 training step(s), loss on training batch is 0.00392603.
After 19745 training step(s), loss on training batch is 0.00388928.
After 19746 training step(s), loss on training batch is 0.0044524.
After 19747 training step(s), loss on training batch is 0.00423194.
After 19748 training step(s), loss on training batch is 0.00359305.
After 19749 training step(s), loss on training batch is 0.00407078.
After 19750 training step(s), loss on training batch is 0.00453254.
After 19751 training step(s), loss on training batch is 0.00507299.
After 19752 training step(s), loss on training batch is 0.00713256.
After 19753 training step(s), loss on training batch is 0.0041561.
After 19754 training step(s), loss on training batch is 0.00490692.
After 19755 training step(s), loss on training batch is 0.00377226.
After 19756 training step(s), loss on training batch is 0.00384735.
After 19757 training step(s), loss on training batch is 0.004129.
After 19758 training step(s), loss on training batch is 0.0042607.
After 19759 training step(s), loss on training batch is 0.00515913.
After 19760 training step(s), loss on training batch is 0.00404251.
After 19761 training step(s), loss on training batch is 0.00412775.
After 19762 training step(s), loss on training batch is 0.00407349.
After 19763 training step(s), loss on training batch is 0.00387523.
After 19764 training step(s), loss on training batch is 0.00394113.
After 19765 training step(s), loss on training batch is 0.0045704.
After 19766 training step(s), loss on training batch is 0.00382509.
After 19767 training step(s), loss on training batch is 0.00497875.
After 19768 training step(s), loss on training batch is 0.00374075.
After 19769 training step(s), loss on training batch is 0.00361555.
After 19770 training step(s), loss on training batch is 0.00450736.
After 19771 training step(s), loss on training batch is 0.00374636.
After 19772 training step(s), loss on training batch is 0.00354326.
After 19773 training step(s), loss on training batch is 0.0043166.
After 19774 training step(s), loss on training batch is 0.00408612.
After 19775 training step(s), loss on training batch is 0.00460678.
After 19776 training step(s), loss on training batch is 0.00377371.
After 19777 training step(s), loss on training batch is 0.00368937.
After 19778 training step(s), loss on training batch is 0.00425055.
After 19779 training step(s), loss on training batch is 0.0038089.
After 19780 training step(s), loss on training batch is 0.00377798.
After 19781 training step(s), loss on training batch is 0.00384197.
After 19782 training step(s), loss on training batch is 0.00418589.
After 19783 training step(s), loss on training batch is 0.00417646.
After 19784 training step(s), loss on training batch is 0.00508414.
After 19785 training step(s), loss on training batch is 0.00374302.
After 19786 training step(s), loss on training batch is 0.00441411.
After 19787 training step(s), loss on training batch is 0.00390878.
After 19788 training step(s), loss on training batch is 0.00364718.
After 19789 training step(s), loss on training batch is 0.00369746.
After 19790 training step(s), loss on training batch is 0.00398655.
After 19791 training step(s), loss on training batch is 0.00407298.
After 19792 training step(s), loss on training batch is 0.00479698.
After 19793 training step(s), loss on training batch is 0.00382587.
After 19794 training step(s), loss on training batch is 0.00352944.
After 19795 training step(s), loss on training batch is 0.00451804.
After 19796 training step(s), loss on training batch is 0.00406655.
After 19797 training step(s), loss on training batch is 0.00478568.
After 19798 training step(s), loss on training batch is 0.00405285.
After 19799 training step(s), loss on training batch is 0.00418749.
After 19800 training step(s), loss on training batch is 0.00400501.
After 19801 training step(s), loss on training batch is 0.00363317.
After 19802 training step(s), loss on training batch is 0.0036691.
After 19803 training step(s), loss on training batch is 0.0037376.
After 19804 training step(s), loss on training batch is 0.00413956.
After 19805 training step(s), loss on training batch is 0.00399164.
After 19806 training step(s), loss on training batch is 0.00421418.
After 19807 training step(s), loss on training batch is 0.00420887.
After 19808 training step(s), loss on training batch is 0.00362932.
After 19809 training step(s), loss on training batch is 0.00381837.
After 19810 training step(s), loss on training batch is 0.00364487.
After 19811 training step(s), loss on training batch is 0.00443557.
After 19812 training step(s), loss on training batch is 0.00368614.
After 19813 training step(s), loss on training batch is 0.00442929.
After 19814 training step(s), loss on training batch is 0.00371653.
After 19815 training step(s), loss on training batch is 0.00375628.
After 19816 training step(s), loss on training batch is 0.00365815.
After 19817 training step(s), loss on training batch is 0.00361163.
After 19818 training step(s), loss on training batch is 0.00483051.
After 19819 training step(s), loss on training batch is 0.00389539.
After 19820 training step(s), loss on training batch is 0.00412305.
After 19821 training step(s), loss on training batch is 0.00399389.
After 19822 training step(s), loss on training batch is 0.00432036.
After 19823 training step(s), loss on training batch is 0.00358971.
After 19824 training step(s), loss on training batch is 0.00392769.
After 19825 training step(s), loss on training batch is 0.00368273.
After 19826 training step(s), loss on training batch is 0.00420728.
After 19827 training step(s), loss on training batch is 0.00354823.
After 19828 training step(s), loss on training batch is 0.00378364.
After 19829 training step(s), loss on training batch is 0.00371148.
After 19830 training step(s), loss on training batch is 0.00372776.
After 19831 training step(s), loss on training batch is 0.00392858.
After 19832 training step(s), loss on training batch is 0.00403618.
After 19833 training step(s), loss on training batch is 0.00380387.
After 19834 training step(s), loss on training batch is 0.00364253.
After 19835 training step(s), loss on training batch is 0.00356344.
After 19836 training step(s), loss on training batch is 0.00465415.
After 19837 training step(s), loss on training batch is 0.0039283.
After 19838 training step(s), loss on training batch is 0.00392336.
After 19839 training step(s), loss on training batch is 0.00369715.
After 19840 training step(s), loss on training batch is 0.00395419.
After 19841 training step(s), loss on training batch is 0.00415194.
After 19842 training step(s), loss on training batch is 0.00390694.
After 19843 training step(s), loss on training batch is 0.00394421.
After 19844 training step(s), loss on training batch is 0.00435093.
After 19845 training step(s), loss on training batch is 0.00455998.
After 19846 training step(s), loss on training batch is 0.00367018.
After 19847 training step(s), loss on training batch is 0.00369184.
After 19848 training step(s), loss on training batch is 0.00379364.
After 19849 training step(s), loss on training batch is 0.00445746.
After 19850 training step(s), loss on training batch is 0.00400463.
After 19851 training step(s), loss on training batch is 0.00367423.
After 19852 training step(s), loss on training batch is 0.00391912.
After 19853 training step(s), loss on training batch is 0.00354879.
After 19854 training step(s), loss on training batch is 0.0037093.
After 19855 training step(s), loss on training batch is 0.00391872.
After 19856 training step(s), loss on training batch is 0.00421644.
After 19857 training step(s), loss on training batch is 0.00385776.
After 19858 training step(s), loss on training batch is 0.00414275.
After 19859 training step(s), loss on training batch is 0.00386048.
After 19860 training step(s), loss on training batch is 0.00384091.
After 19861 training step(s), loss on training batch is 0.00455878.
After 19862 training step(s), loss on training batch is 0.00394814.
After 19863 training step(s), loss on training batch is 0.00448318.
After 19864 training step(s), loss on training batch is 0.00358833.
After 19865 training step(s), loss on training batch is 0.00376194.
After 19866 training step(s), loss on training batch is 0.00374407.
After 19867 training step(s), loss on training batch is 0.00367948.
After 19868 training step(s), loss on training batch is 0.00375805.
After 19869 training step(s), loss on training batch is 0.0039754.
After 19870 training step(s), loss on training batch is 0.00364905.
After 19871 training step(s), loss on training batch is 0.0038531.
After 19872 training step(s), loss on training batch is 0.00570829.
After 19873 training step(s), loss on training batch is 0.00383688.
After 19874 training step(s), loss on training batch is 0.00414354.
After 19875 training step(s), loss on training batch is 0.00389331.
After 19876 training step(s), loss on training batch is 0.00405044.
After 19877 training step(s), loss on training batch is 0.00432657.
After 19878 training step(s), loss on training batch is 0.00355669.
After 19879 training step(s), loss on training batch is 0.00367688.
After 19880 training step(s), loss on training batch is 0.00375036.
After 19881 training step(s), loss on training batch is 0.00383758.
After 19882 training step(s), loss on training batch is 0.00372762.
After 19883 training step(s), loss on training batch is 0.00369035.
After 19884 training step(s), loss on training batch is 0.00398975.
After 19885 training step(s), loss on training batch is 0.00469852.
After 19886 training step(s), loss on training batch is 0.00397184.
After 19887 training step(s), loss on training batch is 0.004037.
After 19888 training step(s), loss on training batch is 0.00415485.
After 19889 training step(s), loss on training batch is 0.00457934.
After 19890 training step(s), loss on training batch is 0.00395371.
After 19891 training step(s), loss on training batch is 0.00382717.
After 19892 training step(s), loss on training batch is 0.00367659.
After 19893 training step(s), loss on training batch is 0.00399426.
After 19894 training step(s), loss on training batch is 0.00389707.
After 19895 training step(s), loss on training batch is 0.00376726.
After 19896 training step(s), loss on training batch is 0.00465081.
After 19897 training step(s), loss on training batch is 0.00365379.
After 19898 training step(s), loss on training batch is 0.00357716.
After 19899 training step(s), loss on training batch is 0.00397296.
After 19900 training step(s), loss on training batch is 0.00350689.
After 19901 training step(s), loss on training batch is 0.00393711.
After 19902 training step(s), loss on training batch is 0.00412907.
After 19903 training step(s), loss on training batch is 0.00387575.
After 19904 training step(s), loss on training batch is 0.00372935.
After 19905 training step(s), loss on training batch is 0.00363432.
After 19906 training step(s), loss on training batch is 0.00392385.
After 19907 training step(s), loss on training batch is 0.0038923.
After 19908 training step(s), loss on training batch is 0.00348541.
After 19909 training step(s), loss on training batch is 0.00408597.
After 19910 training step(s), loss on training batch is 0.00347828.
After 19911 training step(s), loss on training batch is 0.00367868.
After 19912 training step(s), loss on training batch is 0.0038264.
After 19913 training step(s), loss on training batch is 0.00399875.
After 19914 training step(s), loss on training batch is 0.00416443.
After 19915 training step(s), loss on training batch is 0.00438348.
After 19916 training step(s), loss on training batch is 0.00398129.
After 19917 training step(s), loss on training batch is 0.0040562.
After 19918 training step(s), loss on training batch is 0.00424482.
After 19919 training step(s), loss on training batch is 0.00360084.
After 19920 training step(s), loss on training batch is 0.00365137.
After 19921 training step(s), loss on training batch is 0.00387775.
After 19922 training step(s), loss on training batch is 0.00353289.
After 19923 training step(s), loss on training batch is 0.00389858.
After 19924 training step(s), loss on training batch is 0.00420114.
After 19925 training step(s), loss on training batch is 0.00426474.
After 19926 training step(s), loss on training batch is 0.00440002.
After 19927 training step(s), loss on training batch is 0.00400539.
After 19928 training step(s), loss on training batch is 0.00399968.
After 19929 training step(s), loss on training batch is 0.00425077.
After 19930 training step(s), loss on training batch is 0.00433878.
After 19931 training step(s), loss on training batch is 0.00401424.
After 19932 training step(s), loss on training batch is 0.00377697.
After 19933 training step(s), loss on training batch is 0.00374387.
After 19934 training step(s), loss on training batch is 0.00374872.
After 19935 training step(s), loss on training batch is 0.0040078.
After 19936 training step(s), loss on training batch is 0.0041968.
After 19937 training step(s), loss on training batch is 0.00397027.
After 19938 training step(s), loss on training batch is 0.00397336.
After 19939 training step(s), loss on training batch is 0.00353771.
After 19940 training step(s), loss on training batch is 0.00363803.
After 19941 training step(s), loss on training batch is 0.00430234.
After 19942 training step(s), loss on training batch is 0.00428058.
After 19943 training step(s), loss on training batch is 0.00432375.
After 19944 training step(s), loss on training batch is 0.00443619.
After 19945 training step(s), loss on training batch is 0.00345711.
After 19946 training step(s), loss on training batch is 0.00369488.
After 19947 training step(s), loss on training batch is 0.00363128.
After 19948 training step(s), loss on training batch is 0.00397578.
After 19949 training step(s), loss on training batch is 0.00349642.
After 19950 training step(s), loss on training batch is 0.00374513.
After 19951 training step(s), loss on training batch is 0.00398043.
After 19952 training step(s), loss on training batch is 0.00382885.
After 19953 training step(s), loss on training batch is 0.00366941.
After 19954 training step(s), loss on training batch is 0.00426087.
After 19955 training step(s), loss on training batch is 0.00377649.
After 19956 training step(s), loss on training batch is 0.00421037.
After 19957 training step(s), loss on training batch is 0.00359139.
After 19958 training step(s), loss on training batch is 0.00373945.
After 19959 training step(s), loss on training batch is 0.00386054.
After 19960 training step(s), loss on training batch is 0.00363203.
After 19961 training step(s), loss on training batch is 0.00429126.
After 19962 training step(s), loss on training batch is 0.00382444.
After 19963 training step(s), loss on training batch is 0.00370801.
After 19964 training step(s), loss on training batch is 0.00377495.
After 19965 training step(s), loss on training batch is 0.00389084.
After 19966 training step(s), loss on training batch is 0.00406994.
After 19967 training step(s), loss on training batch is 0.00362705.
After 19968 training step(s), loss on training batch is 0.00393424.
After 19969 training step(s), loss on training batch is 0.00398172.
After 19970 training step(s), loss on training batch is 0.00404813.
After 19971 training step(s), loss on training batch is 0.00361353.
After 19972 training step(s), loss on training batch is 0.00433143.
After 19973 training step(s), loss on training batch is 0.00410141.
After 19974 training step(s), loss on training batch is 0.00377729.
After 19975 training step(s), loss on training batch is 0.0042384.
After 19976 training step(s), loss on training batch is 0.00366504.
After 19977 training step(s), loss on training batch is 0.00401511.
After 19978 training step(s), loss on training batch is 0.00450775.
After 19979 training step(s), loss on training batch is 0.00452626.
After 19980 training step(s), loss on training batch is 0.00362548.
After 19981 training step(s), loss on training batch is 0.00425989.
After 19982 training step(s), loss on training batch is 0.00366197.
After 19983 training step(s), loss on training batch is 0.0042147.
After 19984 training step(s), loss on training batch is 0.00366426.
After 19985 training step(s), loss on training batch is 0.00459845.
After 19986 training step(s), loss on training batch is 0.00396604.
After 19987 training step(s), loss on training batch is 0.0038157.
After 19988 training step(s), loss on training batch is 0.00365515.
After 19989 training step(s), loss on training batch is 0.00374535.
After 19990 training step(s), loss on training batch is 0.00398396.
After 19991 training step(s), loss on training batch is 0.00455623.
After 19992 training step(s), loss on training batch is 0.0035166.
After 19993 training step(s), loss on training batch is 0.00403922.
After 19994 training step(s), loss on training batch is 0.00372652.
After 19995 training step(s), loss on training batch is 0.00390666.
After 19996 training step(s), loss on training batch is 0.00432385.
After 19997 training step(s), loss on training batch is 0.00419128.
After 19998 training step(s), loss on training batch is 0.00378351.
After 19999 training step(s), loss on training batch is 0.00349042.
After 20000 training step(s), loss on training batch is 0.00358135.
After 20001 training step(s), loss on training batch is 0.00394272.
After 20002 training step(s), loss on training batch is 0.00374923.
After 20003 training step(s), loss on training batch is 0.0041759.
After 20004 training step(s), loss on training batch is 0.00424917.
After 20005 training step(s), loss on training batch is 0.00400024.
After 20006 training step(s), loss on training batch is 0.00412591.
After 20007 training step(s), loss on training batch is 0.00358373.
After 20008 training step(s), loss on training batch is 0.00407213.
After 20009 training step(s), loss on training batch is 0.00442642.
After 20010 training step(s), loss on training batch is 0.00427903.
After 20011 training step(s), loss on training batch is 0.00385424.
After 20012 training step(s), loss on training batch is 0.004115.
After 20013 training step(s), loss on training batch is 0.00406231.
After 20014 training step(s), loss on training batch is 0.00399129.
After 20015 training step(s), loss on training batch is 0.00392726.
After 20016 training step(s), loss on training batch is 0.00420716.
After 20017 training step(s), loss on training batch is 0.00380596.
After 20018 training step(s), loss on training batch is 0.00429766.
After 20019 training step(s), loss on training batch is 0.00384747.
After 20020 training step(s), loss on training batch is 0.00465083.
After 20021 training step(s), loss on training batch is 0.00353929.
After 20022 training step(s), loss on training batch is 0.0036235.
After 20023 training step(s), loss on training batch is 0.00384647.
After 20024 training step(s), loss on training batch is 0.00433424.
After 20025 training step(s), loss on training batch is 0.00378947.
After 20026 training step(s), loss on training batch is 0.00394252.
After 20027 training step(s), loss on training batch is 0.00397648.
After 20028 training step(s), loss on training batch is 0.0039346.
After 20029 training step(s), loss on training batch is 0.00398192.
After 20030 training step(s), loss on training batch is 0.00372065.
After 20031 training step(s), loss on training batch is 0.00352495.
After 20032 training step(s), loss on training batch is 0.00391728.
After 20033 training step(s), loss on training batch is 0.00459115.
After 20034 training step(s), loss on training batch is 0.0040065.
After 20035 training step(s), loss on training batch is 0.00382226.
After 20036 training step(s), loss on training batch is 0.00357813.
After 20037 training step(s), loss on training batch is 0.00384705.
After 20038 training step(s), loss on training batch is 0.00359525.
After 20039 training step(s), loss on training batch is 0.00381084.
After 20040 training step(s), loss on training batch is 0.00395588.
After 20041 training step(s), loss on training batch is 0.00358396.
After 20042 training step(s), loss on training batch is 0.00413478.
After 20043 training step(s), loss on training batch is 0.00356275.
After 20044 training step(s), loss on training batch is 0.0042167.
After 20045 training step(s), loss on training batch is 0.00382886.
After 20046 training step(s), loss on training batch is 0.00427947.
After 20047 training step(s), loss on training batch is 0.00349571.
After 20048 training step(s), loss on training batch is 0.00394885.
After 20049 training step(s), loss on training batch is 0.00449463.
After 20050 training step(s), loss on training batch is 0.0039035.
After 20051 training step(s), loss on training batch is 0.00619332.
After 20052 training step(s), loss on training batch is 0.003592.
After 20053 training step(s), loss on training batch is 0.00482107.
After 20054 training step(s), loss on training batch is 0.00407438.
After 20055 training step(s), loss on training batch is 0.00434442.
After 20056 training step(s), loss on training batch is 0.00447328.
After 20057 training step(s), loss on training batch is 0.00410149.
After 20058 training step(s), loss on training batch is 0.00377931.
After 20059 training step(s), loss on training batch is 0.00355263.
After 20060 training step(s), loss on training batch is 0.00410503.
After 20061 training step(s), loss on training batch is 0.0043635.
After 20062 training step(s), loss on training batch is 0.0044632.
After 20063 training step(s), loss on training batch is 0.00356224.
After 20064 training step(s), loss on training batch is 0.00389264.
After 20065 training step(s), loss on training batch is 0.00408974.
After 20066 training step(s), loss on training batch is 0.00375464.
After 20067 training step(s), loss on training batch is 0.00406284.
After 20068 training step(s), loss on training batch is 0.00406445.
After 20069 training step(s), loss on training batch is 0.00376578.
After 20070 training step(s), loss on training batch is 0.00408159.
After 20071 training step(s), loss on training batch is 0.00416749.
After 20072 training step(s), loss on training batch is 0.00381952.
After 20073 training step(s), loss on training batch is 0.0044843.
After 20074 training step(s), loss on training batch is 0.00384898.
After 20075 training step(s), loss on training batch is 0.00372469.
After 20076 training step(s), loss on training batch is 0.0036749.
After 20077 training step(s), loss on training batch is 0.00361658.
After 20078 training step(s), loss on training batch is 0.00357714.
After 20079 training step(s), loss on training batch is 0.00379927.
After 20080 training step(s), loss on training batch is 0.0037459.
After 20081 training step(s), loss on training batch is 0.00368837.
After 20082 training step(s), loss on training batch is 0.00363899.
After 20083 training step(s), loss on training batch is 0.00379503.
After 20084 training step(s), loss on training batch is 0.0035776.
After 20085 training step(s), loss on training batch is 0.00416626.
After 20086 training step(s), loss on training batch is 0.00366582.
After 20087 training step(s), loss on training batch is 0.00372878.
After 20088 training step(s), loss on training batch is 0.00384151.
After 20089 training step(s), loss on training batch is 0.00354086.
After 20090 training step(s), loss on training batch is 0.00615412.
After 20091 training step(s), loss on training batch is 0.00354266.
After 20092 training step(s), loss on training batch is 0.00453406.
After 20093 training step(s), loss on training batch is 0.00377908.
After 20094 training step(s), loss on training batch is 0.00398301.
After 20095 training step(s), loss on training batch is 0.00415584.
After 20096 training step(s), loss on training batch is 0.00353572.
After 20097 training step(s), loss on training batch is 0.00385975.
After 20098 training step(s), loss on training batch is 0.00376728.
After 20099 training step(s), loss on training batch is 0.00432506.
After 20100 training step(s), loss on training batch is 0.00420063.
After 20101 training step(s), loss on training batch is 0.0050668.
After 20102 training step(s), loss on training batch is 0.00433383.
After 20103 training step(s), loss on training batch is 0.0040633.
After 20104 training step(s), loss on training batch is 0.00402161.
After 20105 training step(s), loss on training batch is 0.0043284.
After 20106 training step(s), loss on training batch is 0.00371521.
After 20107 training step(s), loss on training batch is 0.00377393.
After 20108 training step(s), loss on training batch is 0.00388482.
After 20109 training step(s), loss on training batch is 0.00471093.
After 20110 training step(s), loss on training batch is 0.00448865.
After 20111 training step(s), loss on training batch is 0.00401339.
After 20112 training step(s), loss on training batch is 0.00384073.
After 20113 training step(s), loss on training batch is 0.00370649.
After 20114 training step(s), loss on training batch is 0.00377484.
After 20115 training step(s), loss on training batch is 0.00395631.
After 20116 training step(s), loss on training batch is 0.00360693.
After 20117 training step(s), loss on training batch is 0.00411549.
After 20118 training step(s), loss on training batch is 0.00367244.
After 20119 training step(s), loss on training batch is 0.00383511.
After 20120 training step(s), loss on training batch is 0.00396636.
After 20121 training step(s), loss on training batch is 0.00407829.
After 20122 training step(s), loss on training batch is 0.00414708.
After 20123 training step(s), loss on training batch is 0.00399735.
After 20124 training step(s), loss on training batch is 0.00397476.
After 20125 training step(s), loss on training batch is 0.00371031.
After 20126 training step(s), loss on training batch is 0.00397814.
After 20127 training step(s), loss on training batch is 0.00361818.
After 20128 training step(s), loss on training batch is 0.0035622.
After 20129 training step(s), loss on training batch is 0.00388878.
After 20130 training step(s), loss on training batch is 0.00369156.
After 20131 training step(s), loss on training batch is 0.00425122.
After 20132 training step(s), loss on training batch is 0.00390938.
After 20133 training step(s), loss on training batch is 0.00366201.
After 20134 training step(s), loss on training batch is 0.00551409.
After 20135 training step(s), loss on training batch is 0.00418696.
After 20136 training step(s), loss on training batch is 0.00356796.
After 20137 training step(s), loss on training batch is 0.0043734.
After 20138 training step(s), loss on training batch is 0.00390327.
After 20139 training step(s), loss on training batch is 0.00351123.
After 20140 training step(s), loss on training batch is 0.00363079.
After 20141 training step(s), loss on training batch is 0.00400263.
After 20142 training step(s), loss on training batch is 0.00351422.
After 20143 training step(s), loss on training batch is 0.00512732.
After 20144 training step(s), loss on training batch is 0.00403614.
After 20145 training step(s), loss on training batch is 0.00363999.
After 20146 training step(s), loss on training batch is 0.003701.
After 20147 training step(s), loss on training batch is 0.00407084.
After 20148 training step(s), loss on training batch is 0.00351939.
After 20149 training step(s), loss on training batch is 0.00417684.
After 20150 training step(s), loss on training batch is 0.00378437.
After 20151 training step(s), loss on training batch is 0.00382891.
After 20152 training step(s), loss on training batch is 0.00387796.
After 20153 training step(s), loss on training batch is 0.00421274.
After 20154 training step(s), loss on training batch is 0.00370776.
After 20155 training step(s), loss on training batch is 0.00352162.
After 20156 training step(s), loss on training batch is 0.00484342.
After 20157 training step(s), loss on training batch is 0.00342971.
After 20158 training step(s), loss on training batch is 0.00385692.
After 20159 training step(s), loss on training batch is 0.00398795.
After 20160 training step(s), loss on training batch is 0.00387765.
After 20161 training step(s), loss on training batch is 0.00377584.
After 20162 training step(s), loss on training batch is 0.00360766.
After 20163 training step(s), loss on training batch is 0.00379864.
After 20164 training step(s), loss on training batch is 0.00409901.
After 20165 training step(s), loss on training batch is 0.00349012.
After 20166 training step(s), loss on training batch is 0.00438772.
After 20167 training step(s), loss on training batch is 0.00369554.
After 20168 training step(s), loss on training batch is 0.00386206.
After 20169 training step(s), loss on training batch is 0.00390515.
After 20170 training step(s), loss on training batch is 0.00381439.
After 20171 training step(s), loss on training batch is 0.00381002.
After 20172 training step(s), loss on training batch is 0.003562.
After 20173 training step(s), loss on training batch is 0.00358004.
After 20174 training step(s), loss on training batch is 0.00380596.
After 20175 training step(s), loss on training batch is 0.0041716.
After 20176 training step(s), loss on training batch is 0.00409598.
After 20177 training step(s), loss on training batch is 0.00367956.
After 20178 training step(s), loss on training batch is 0.00373632.
After 20179 training step(s), loss on training batch is 0.00375724.
After 20180 training step(s), loss on training batch is 0.0045311.
After 20181 training step(s), loss on training batch is 0.00414117.
After 20182 training step(s), loss on training batch is 0.00431554.
After 20183 training step(s), loss on training batch is 0.00406428.
After 20184 training step(s), loss on training batch is 0.00396965.
After 20185 training step(s), loss on training batch is 0.00444768.
After 20186 training step(s), loss on training batch is 0.00353198.
After 20187 training step(s), loss on training batch is 0.0036432.
After 20188 training step(s), loss on training batch is 0.00371197.
After 20189 training step(s), loss on training batch is 0.00359405.
After 20190 training step(s), loss on training batch is 0.00344951.
After 20191 training step(s), loss on training batch is 0.00361926.
After 20192 training step(s), loss on training batch is 0.00386553.
After 20193 training step(s), loss on training batch is 0.00375437.
After 20194 training step(s), loss on training batch is 0.00376177.
After 20195 training step(s), loss on training batch is 0.00400585.
After 20196 training step(s), loss on training batch is 0.0041247.
After 20197 training step(s), loss on training batch is 0.00377975.
After 20198 training step(s), loss on training batch is 0.00363483.
After 20199 training step(s), loss on training batch is 0.00370234.
After 20200 training step(s), loss on training batch is 0.00356791.
After 20201 training step(s), loss on training batch is 0.00374034.
After 20202 training step(s), loss on training batch is 0.00369235.
After 20203 training step(s), loss on training batch is 0.00375447.
After 20204 training step(s), loss on training batch is 0.00377786.
After 20205 training step(s), loss on training batch is 0.00370887.
After 20206 training step(s), loss on training batch is 0.00408534.
After 20207 training step(s), loss on training batch is 0.00378231.
After 20208 training step(s), loss on training batch is 0.00368766.
After 20209 training step(s), loss on training batch is 0.00428286.
After 20210 training step(s), loss on training batch is 0.00407833.
After 20211 training step(s), loss on training batch is 0.00365352.
After 20212 training step(s), loss on training batch is 0.003876.
After 20213 training step(s), loss on training batch is 0.00374941.
After 20214 training step(s), loss on training batch is 0.00369433.
After 20215 training step(s), loss on training batch is 0.00349658.
After 20216 training step(s), loss on training batch is 0.00386143.
After 20217 training step(s), loss on training batch is 0.00366054.
After 20218 training step(s), loss on training batch is 0.00393172.
After 20219 training step(s), loss on training batch is 0.00428642.
After 20220 training step(s), loss on training batch is 0.0039906.
After 20221 training step(s), loss on training batch is 0.00414352.
After 20222 training step(s), loss on training batch is 0.00353817.
After 20223 training step(s), loss on training batch is 0.00379466.
After 20224 training step(s), loss on training batch is 0.0041826.
After 20225 training step(s), loss on training batch is 0.00375631.
After 20226 training step(s), loss on training batch is 0.00361914.
After 20227 training step(s), loss on training batch is 0.00364603.
After 20228 training step(s), loss on training batch is 0.00576466.
After 20229 training step(s), loss on training batch is 0.00364321.
After 20230 training step(s), loss on training batch is 0.00430909.
After 20231 training step(s), loss on training batch is 0.00369088.
After 20232 training step(s), loss on training batch is 0.0041964.
After 20233 training step(s), loss on training batch is 0.00462422.
After 20234 training step(s), loss on training batch is 0.00440813.
After 20235 training step(s), loss on training batch is 0.0037466.
After 20236 training step(s), loss on training batch is 0.00601563.
After 20237 training step(s), loss on training batch is 0.00379322.
After 20238 training step(s), loss on training batch is 0.00398522.
After 20239 training step(s), loss on training batch is 0.00399561.
After 20240 training step(s), loss on training batch is 0.00370513.
After 20241 training step(s), loss on training batch is 0.00357013.
After 20242 training step(s), loss on training batch is 0.00439291.
After 20243 training step(s), loss on training batch is 0.00391452.
After 20244 training step(s), loss on training batch is 0.00391375.
After 20245 training step(s), loss on training batch is 0.00362971.
After 20246 training step(s), loss on training batch is 0.00446463.
After 20247 training step(s), loss on training batch is 0.00382849.
After 20248 training step(s), loss on training batch is 0.00387774.
After 20249 training step(s), loss on training batch is 0.0040649.
After 20250 training step(s), loss on training batch is 0.00368298.
After 20251 training step(s), loss on training batch is 0.00373661.
After 20252 training step(s), loss on training batch is 0.00417931.
After 20253 training step(s), loss on training batch is 0.00374276.
After 20254 training step(s), loss on training batch is 0.0036899.
After 20255 training step(s), loss on training batch is 0.00381652.
After 20256 training step(s), loss on training batch is 0.00452008.
After 20257 training step(s), loss on training batch is 0.00368203.
After 20258 training step(s), loss on training batch is 0.00408296.
After 20259 training step(s), loss on training batch is 0.0035504.
After 20260 training step(s), loss on training batch is 0.00412562.
After 20261 training step(s), loss on training batch is 0.00499028.
After 20262 training step(s), loss on training batch is 0.00369178.
After 20263 training step(s), loss on training batch is 0.00419716.
After 20264 training step(s), loss on training batch is 0.00450024.
After 20265 training step(s), loss on training batch is 0.00372842.
After 20266 training step(s), loss on training batch is 0.00400889.
After 20267 training step(s), loss on training batch is 0.00355322.
After 20268 training step(s), loss on training batch is 0.00351288.
After 20269 training step(s), loss on training batch is 0.0038196.
After 20270 training step(s), loss on training batch is 0.00388797.
After 20271 training step(s), loss on training batch is 0.00385411.
After 20272 training step(s), loss on training batch is 0.00361174.
After 20273 training step(s), loss on training batch is 0.0034953.
After 20274 training step(s), loss on training batch is 0.00473295.
After 20275 training step(s), loss on training batch is 0.00364962.
After 20276 training step(s), loss on training batch is 0.0054108.
After 20277 training step(s), loss on training batch is 0.00373157.
After 20278 training step(s), loss on training batch is 0.00412633.
After 20279 training step(s), loss on training batch is 0.00404658.
After 20280 training step(s), loss on training batch is 0.00385002.
After 20281 training step(s), loss on training batch is 0.00419517.
After 20282 training step(s), loss on training batch is 0.00346626.
After 20283 training step(s), loss on training batch is 0.00380231.
After 20284 training step(s), loss on training batch is 0.00412285.
After 20285 training step(s), loss on training batch is 0.00364401.
After 20286 training step(s), loss on training batch is 0.0038925.
After 20287 training step(s), loss on training batch is 0.00376177.
After 20288 training step(s), loss on training batch is 0.0035637.
After 20289 training step(s), loss on training batch is 0.00362392.
After 20290 training step(s), loss on training batch is 0.00352925.
After 20291 training step(s), loss on training batch is 0.00397218.
After 20292 training step(s), loss on training batch is 0.00479868.
After 20293 training step(s), loss on training batch is 0.00388857.
After 20294 training step(s), loss on training batch is 0.00399773.
After 20295 training step(s), loss on training batch is 0.00428404.
After 20296 training step(s), loss on training batch is 0.00406279.
After 20297 training step(s), loss on training batch is 0.00375684.
After 20298 training step(s), loss on training batch is 0.003966.
After 20299 training step(s), loss on training batch is 0.00407277.
After 20300 training step(s), loss on training batch is 0.00397142.
After 20301 training step(s), loss on training batch is 0.00368786.
After 20302 training step(s), loss on training batch is 0.00382575.
After 20303 training step(s), loss on training batch is 0.00463409.
After 20304 training step(s), loss on training batch is 0.00403595.
After 20305 training step(s), loss on training batch is 0.00366612.
After 20306 training step(s), loss on training batch is 0.00428619.
After 20307 training step(s), loss on training batch is 0.00367385.
After 20308 training step(s), loss on training batch is 0.00354059.
After 20309 training step(s), loss on training batch is 0.00394737.
After 20310 training step(s), loss on training batch is 0.00523198.
After 20311 training step(s), loss on training batch is 0.00357968.
After 20312 training step(s), loss on training batch is 0.00373383.
After 20313 training step(s), loss on training batch is 0.00362948.
After 20314 training step(s), loss on training batch is 0.00382081.
After 20315 training step(s), loss on training batch is 0.00401387.
After 20316 training step(s), loss on training batch is 0.00382924.
After 20317 training step(s), loss on training batch is 0.00396474.
After 20318 training step(s), loss on training batch is 0.00426553.
After 20319 training step(s), loss on training batch is 0.00399576.
After 20320 training step(s), loss on training batch is 0.00364462.
After 20321 training step(s), loss on training batch is 0.00368464.
After 20322 training step(s), loss on training batch is 0.00387455.
After 20323 training step(s), loss on training batch is 0.00363616.
After 20324 training step(s), loss on training batch is 0.00412954.
After 20325 training step(s), loss on training batch is 0.00344669.
After 20326 training step(s), loss on training batch is 0.00389905.
After 20327 training step(s), loss on training batch is 0.00407469.
After 20328 training step(s), loss on training batch is 0.00398801.
After 20329 training step(s), loss on training batch is 0.00392753.
After 20330 training step(s), loss on training batch is 0.00426654.
After 20331 training step(s), loss on training batch is 0.00416519.
After 20332 training step(s), loss on training batch is 0.00422518.
After 20333 training step(s), loss on training batch is 0.00385601.
After 20334 training step(s), loss on training batch is 0.00358692.
After 20335 training step(s), loss on training batch is 0.00389315.
After 20336 training step(s), loss on training batch is 0.00414572.
After 20337 training step(s), loss on training batch is 0.00380617.
After 20338 training step(s), loss on training batch is 0.00406208.
After 20339 training step(s), loss on training batch is 0.00351568.
After 20340 training step(s), loss on training batch is 0.00377059.
After 20341 training step(s), loss on training batch is 0.00431618.
After 20342 training step(s), loss on training batch is 0.00366653.
After 20343 training step(s), loss on training batch is 0.00372971.
After 20344 training step(s), loss on training batch is 0.00356862.
After 20345 training step(s), loss on training batch is 0.00405442.
After 20346 training step(s), loss on training batch is 0.00433789.
After 20347 training step(s), loss on training batch is 0.00369.
After 20348 training step(s), loss on training batch is 0.00445419.
After 20349 training step(s), loss on training batch is 0.0041741.
After 20350 training step(s), loss on training batch is 0.00475134.
After 20351 training step(s), loss on training batch is 0.00377371.
After 20352 training step(s), loss on training batch is 0.00375785.
After 20353 training step(s), loss on training batch is 0.00367004.
After 20354 training step(s), loss on training batch is 0.00379952.
After 20355 training step(s), loss on training batch is 0.00377825.
After 20356 training step(s), loss on training batch is 0.00363229.
After 20357 training step(s), loss on training batch is 0.00365086.
After 20358 training step(s), loss on training batch is 0.00356947.
After 20359 training step(s), loss on training batch is 0.00367469.
After 20360 training step(s), loss on training batch is 0.00374829.
After 20361 training step(s), loss on training batch is 0.00373629.
After 20362 training step(s), loss on training batch is 0.00363785.
After 20363 training step(s), loss on training batch is 0.00397622.
After 20364 training step(s), loss on training batch is 0.00365462.
After 20365 training step(s), loss on training batch is 0.00355252.
After 20366 training step(s), loss on training batch is 0.00363629.
After 20367 training step(s), loss on training batch is 0.00421056.
After 20368 training step(s), loss on training batch is 0.00426493.
After 20369 training step(s), loss on training batch is 0.00377411.
After 20370 training step(s), loss on training batch is 0.00359488.
After 20371 training step(s), loss on training batch is 0.00368568.
After 20372 training step(s), loss on training batch is 0.0038705.
After 20373 training step(s), loss on training batch is 0.00359739.
After 20374 training step(s), loss on training batch is 0.00355799.
After 20375 training step(s), loss on training batch is 0.00371672.
After 20376 training step(s), loss on training batch is 0.00411919.
After 20377 training step(s), loss on training batch is 0.00383285.
After 20378 training step(s), loss on training batch is 0.0039864.
After 20379 training step(s), loss on training batch is 0.00364864.
After 20380 training step(s), loss on training batch is 0.00367404.
After 20381 training step(s), loss on training batch is 0.00369042.
After 20382 training step(s), loss on training batch is 0.00372893.
After 20383 training step(s), loss on training batch is 0.00388885.
After 20384 training step(s), loss on training batch is 0.00363767.
After 20385 training step(s), loss on training batch is 0.00368381.
After 20386 training step(s), loss on training batch is 0.00374303.
After 20387 training step(s), loss on training batch is 0.00366806.
After 20388 training step(s), loss on training batch is 0.00393984.
After 20389 training step(s), loss on training batch is 0.00356663.
After 20390 training step(s), loss on training batch is 0.00428042.
After 20391 training step(s), loss on training batch is 0.00349601.
After 20392 training step(s), loss on training batch is 0.00447019.
After 20393 training step(s), loss on training batch is 0.0036025.
After 20394 training step(s), loss on training batch is 0.00370455.
After 20395 training step(s), loss on training batch is 0.00373439.
After 20396 training step(s), loss on training batch is 0.00383208.
After 20397 training step(s), loss on training batch is 0.00380828.
After 20398 training step(s), loss on training batch is 0.003813.
After 20399 training step(s), loss on training batch is 0.00385579.
After 20400 training step(s), loss on training batch is 0.0038214.
After 20401 training step(s), loss on training batch is 0.00347779.
After 20402 training step(s), loss on training batch is 0.00422907.
After 20403 training step(s), loss on training batch is 0.00357438.
After 20404 training step(s), loss on training batch is 0.00395011.
After 20405 training step(s), loss on training batch is 0.00402435.
After 20406 training step(s), loss on training batch is 0.00423608.
After 20407 training step(s), loss on training batch is 0.00346425.
After 20408 training step(s), loss on training batch is 0.00346393.
After 20409 training step(s), loss on training batch is 0.00396366.
After 20410 training step(s), loss on training batch is 0.00368789.
After 20411 training step(s), loss on training batch is 0.00373825.
After 20412 training step(s), loss on training batch is 0.00435126.
After 20413 training step(s), loss on training batch is 0.00399305.
After 20414 training step(s), loss on training batch is 0.00394551.
After 20415 training step(s), loss on training batch is 0.00372928.
After 20416 training step(s), loss on training batch is 0.00408128.
After 20417 training step(s), loss on training batch is 0.00389845.
After 20418 training step(s), loss on training batch is 0.00366839.
After 20419 training step(s), loss on training batch is 0.00497724.
After 20420 training step(s), loss on training batch is 0.00384556.
After 20421 training step(s), loss on training batch is 0.00368613.
After 20422 training step(s), loss on training batch is 0.00393623.
After 20423 training step(s), loss on training batch is 0.00361476.
After 20424 training step(s), loss on training batch is 0.00379506.
After 20425 training step(s), loss on training batch is 0.00430786.
After 20426 training step(s), loss on training batch is 0.00371987.
After 20427 training step(s), loss on training batch is 0.00376431.
After 20428 training step(s), loss on training batch is 0.00379966.
After 20429 training step(s), loss on training batch is 0.00351374.
After 20430 training step(s), loss on training batch is 0.00362318.
After 20431 training step(s), loss on training batch is 0.00361952.
After 20432 training step(s), loss on training batch is 0.00360563.
After 20433 training step(s), loss on training batch is 0.0038333.
After 20434 training step(s), loss on training batch is 0.00371066.
After 20435 training step(s), loss on training batch is 0.0037062.
After 20436 training step(s), loss on training batch is 0.00410415.
After 20437 training step(s), loss on training batch is 0.00378606.
After 20438 training step(s), loss on training batch is 0.00356723.
After 20439 training step(s), loss on training batch is 0.00372005.
After 20440 training step(s), loss on training batch is 0.00370792.
After 20441 training step(s), loss on training batch is 0.0054479.
After 20442 training step(s), loss on training batch is 0.00421985.
After 20443 training step(s), loss on training batch is 0.00399385.
After 20444 training step(s), loss on training batch is 0.00338613.
After 20445 training step(s), loss on training batch is 0.00421442.
After 20446 training step(s), loss on training batch is 0.00424179.
After 20447 training step(s), loss on training batch is 0.00353629.
After 20448 training step(s), loss on training batch is 0.00443533.
After 20449 training step(s), loss on training batch is 0.00412344.
After 20450 training step(s), loss on training batch is 0.00377201.
After 20451 training step(s), loss on training batch is 0.00416786.
After 20452 training step(s), loss on training batch is 0.00370762.
After 20453 training step(s), loss on training batch is 0.00416786.
After 20454 training step(s), loss on training batch is 0.00425397.
After 20455 training step(s), loss on training batch is 0.0040482.
After 20456 training step(s), loss on training batch is 0.00462163.
After 20457 training step(s), loss on training batch is 0.00399983.
After 20458 training step(s), loss on training batch is 0.00400338.
After 20459 training step(s), loss on training batch is 0.00409518.
After 20460 training step(s), loss on training batch is 0.00384683.
After 20461 training step(s), loss on training batch is 0.00365808.
After 20462 training step(s), loss on training batch is 0.00367839.
After 20463 training step(s), loss on training batch is 0.00343971.
After 20464 training step(s), loss on training batch is 0.0037562.
After 20465 training step(s), loss on training batch is 0.00360027.
After 20466 training step(s), loss on training batch is 0.00425829.
After 20467 training step(s), loss on training batch is 0.00402949.
After 20468 training step(s), loss on training batch is 0.00373662.
After 20469 training step(s), loss on training batch is 0.00381262.
After 20470 training step(s), loss on training batch is 0.00435544.
After 20471 training step(s), loss on training batch is 0.00378035.
After 20472 training step(s), loss on training batch is 0.00363478.
After 20473 training step(s), loss on training batch is 0.00469167.
After 20474 training step(s), loss on training batch is 0.00389096.
After 20475 training step(s), loss on training batch is 0.00358682.
After 20476 training step(s), loss on training batch is 0.00432324.
After 20477 training step(s), loss on training batch is 0.00376766.
After 20478 training step(s), loss on training batch is 0.00358335.
After 20479 training step(s), loss on training batch is 0.00368227.
After 20480 training step(s), loss on training batch is 0.00367642.
After 20481 training step(s), loss on training batch is 0.00340202.
After 20482 training step(s), loss on training batch is 0.00364106.
After 20483 training step(s), loss on training batch is 0.00428148.
After 20484 training step(s), loss on training batch is 0.0040926.
After 20485 training step(s), loss on training batch is 0.00355694.
After 20486 training step(s), loss on training batch is 0.00409008.
After 20487 training step(s), loss on training batch is 0.00371536.
After 20488 training step(s), loss on training batch is 0.00428536.
After 20489 training step(s), loss on training batch is 0.00430744.
After 20490 training step(s), loss on training batch is 0.00364773.
After 20491 training step(s), loss on training batch is 0.00428964.
After 20492 training step(s), loss on training batch is 0.00351766.
After 20493 training step(s), loss on training batch is 0.00360544.
After 20494 training step(s), loss on training batch is 0.00349711.
After 20495 training step(s), loss on training batch is 0.00375331.
After 20496 training step(s), loss on training batch is 0.00364912.
After 20497 training step(s), loss on training batch is 0.00354637.
After 20498 training step(s), loss on training batch is 0.00368486.
After 20499 training step(s), loss on training batch is 0.00352907.
After 20500 training step(s), loss on training batch is 0.00384419.
After 20501 training step(s), loss on training batch is 0.00421751.
After 20502 training step(s), loss on training batch is 0.00394098.
After 20503 training step(s), loss on training batch is 0.00354006.
After 20504 training step(s), loss on training batch is 0.00388092.
After 20505 training step(s), loss on training batch is 0.00365189.
After 20506 training step(s), loss on training batch is 0.00375044.
After 20507 training step(s), loss on training batch is 0.00405412.
After 20508 training step(s), loss on training batch is 0.00370187.
After 20509 training step(s), loss on training batch is 0.00389682.
After 20510 training step(s), loss on training batch is 0.00378273.
After 20511 training step(s), loss on training batch is 0.00391979.
After 20512 training step(s), loss on training batch is 0.00486209.
After 20513 training step(s), loss on training batch is 0.00415215.
After 20514 training step(s), loss on training batch is 0.00368082.
After 20515 training step(s), loss on training batch is 0.00347226.
After 20516 training step(s), loss on training batch is 0.00435843.
After 20517 training step(s), loss on training batch is 0.00399193.
After 20518 training step(s), loss on training batch is 0.00358508.
After 20519 training step(s), loss on training batch is 0.00356363.
After 20520 training step(s), loss on training batch is 0.0042082.
After 20521 training step(s), loss on training batch is 0.00389687.
After 20522 training step(s), loss on training batch is 0.00384572.
After 20523 training step(s), loss on training batch is 0.00358234.
After 20524 training step(s), loss on training batch is 0.0036773.
After 20525 training step(s), loss on training batch is 0.00360811.
After 20526 training step(s), loss on training batch is 0.0035899.
After 20527 training step(s), loss on training batch is 0.00410242.
After 20528 training step(s), loss on training batch is 0.00355751.
After 20529 training step(s), loss on training batch is 0.00416557.
After 20530 training step(s), loss on training batch is 0.00367726.
After 20531 training step(s), loss on training batch is 0.00392783.
After 20532 training step(s), loss on training batch is 0.00435149.
After 20533 training step(s), loss on training batch is 0.00380468.
After 20534 training step(s), loss on training batch is 0.00363568.
After 20535 training step(s), loss on training batch is 0.0065153.
After 20536 training step(s), loss on training batch is 0.003745.
After 20537 training step(s), loss on training batch is 0.00437999.
After 20538 training step(s), loss on training batch is 0.00427851.
After 20539 training step(s), loss on training batch is 0.00410969.
After 20540 training step(s), loss on training batch is 0.00383209.
After 20541 training step(s), loss on training batch is 0.00374435.
After 20542 training step(s), loss on training batch is 0.00391373.
After 20543 training step(s), loss on training batch is 0.00544445.
After 20544 training step(s), loss on training batch is 0.00349642.
After 20545 training step(s), loss on training batch is 0.00409363.
After 20546 training step(s), loss on training batch is 0.00416679.
After 20547 training step(s), loss on training batch is 0.00410891.
After 20548 training step(s), loss on training batch is 0.00441076.
After 20549 training step(s), loss on training batch is 0.00393367.
After 20550 training step(s), loss on training batch is 0.00356211.
After 20551 training step(s), loss on training batch is 0.00455015.
After 20552 training step(s), loss on training batch is 0.00396259.
After 20553 training step(s), loss on training batch is 0.00367137.
After 20554 training step(s), loss on training batch is 0.00442941.
After 20555 training step(s), loss on training batch is 0.00370275.
After 20556 training step(s), loss on training batch is 0.00404605.
After 20557 training step(s), loss on training batch is 0.0036321.
After 20558 training step(s), loss on training batch is 0.00348405.
After 20559 training step(s), loss on training batch is 0.00449515.
After 20560 training step(s), loss on training batch is 0.00414968.
After 20561 training step(s), loss on training batch is 0.00374356.
After 20562 training step(s), loss on training batch is 0.00381905.
After 20563 training step(s), loss on training batch is 0.00382911.
After 20564 training step(s), loss on training batch is 0.00357949.
After 20565 training step(s), loss on training batch is 0.00379057.
After 20566 training step(s), loss on training batch is 0.00387602.
After 20567 training step(s), loss on training batch is 0.00357772.
After 20568 training step(s), loss on training batch is 0.004257.
After 20569 training step(s), loss on training batch is 0.00343054.
After 20570 training step(s), loss on training batch is 0.00414195.
After 20571 training step(s), loss on training batch is 0.00333619.
After 20572 training step(s), loss on training batch is 0.00391562.
After 20573 training step(s), loss on training batch is 0.00431663.
After 20574 training step(s), loss on training batch is 0.00387262.
After 20575 training step(s), loss on training batch is 0.00383245.
After 20576 training step(s), loss on training batch is 0.00390806.
After 20577 training step(s), loss on training batch is 0.00441012.
After 20578 training step(s), loss on training batch is 0.00397313.
After 20579 training step(s), loss on training batch is 0.00346822.
After 20580 training step(s), loss on training batch is 0.00377878.
After 20581 training step(s), loss on training batch is 0.00508121.
After 20582 training step(s), loss on training batch is 0.00378675.
After 20583 training step(s), loss on training batch is 0.00576794.
After 20584 training step(s), loss on training batch is 0.00442008.
After 20585 training step(s), loss on training batch is 0.00480439.
After 20586 training step(s), loss on training batch is 0.00409156.
After 20587 training step(s), loss on training batch is 0.00420187.
After 20588 training step(s), loss on training batch is 0.00352038.
After 20589 training step(s), loss on training batch is 0.00352084.
After 20590 training step(s), loss on training batch is 0.00545205.
After 20591 training step(s), loss on training batch is 0.00381775.
After 20592 training step(s), loss on training batch is 0.0047875.
After 20593 training step(s), loss on training batch is 0.00421073.
After 20594 training step(s), loss on training batch is 0.00415387.
After 20595 training step(s), loss on training batch is 0.00380156.
After 20596 training step(s), loss on training batch is 0.00391309.
After 20597 training step(s), loss on training batch is 0.00384461.
After 20598 training step(s), loss on training batch is 0.00408707.
After 20599 training step(s), loss on training batch is 0.00373975.
After 20600 training step(s), loss on training batch is 0.00378147.
After 20601 training step(s), loss on training batch is 0.00411836.
After 20602 training step(s), loss on training batch is 0.00365769.
After 20603 training step(s), loss on training batch is 0.00358016.
After 20604 training step(s), loss on training batch is 0.00437066.
After 20605 training step(s), loss on training batch is 0.00443603.
After 20606 training step(s), loss on training batch is 0.00357016.
After 20607 training step(s), loss on training batch is 0.00368491.
After 20608 training step(s), loss on training batch is 0.0036038.
After 20609 training step(s), loss on training batch is 0.00357327.
After 20610 training step(s), loss on training batch is 0.00395023.
After 20611 training step(s), loss on training batch is 0.00371506.
After 20612 training step(s), loss on training batch is 0.0035146.
After 20613 training step(s), loss on training batch is 0.00384139.
After 20614 training step(s), loss on training batch is 0.00497853.
After 20615 training step(s), loss on training batch is 0.00387001.
After 20616 training step(s), loss on training batch is 0.00428881.
After 20617 training step(s), loss on training batch is 0.0037762.
After 20618 training step(s), loss on training batch is 0.00382107.
After 20619 training step(s), loss on training batch is 0.00358419.
After 20620 training step(s), loss on training batch is 0.00355591.
After 20621 training step(s), loss on training batch is 0.0042507.
After 20622 training step(s), loss on training batch is 0.00350989.
After 20623 training step(s), loss on training batch is 0.00394655.
After 20624 training step(s), loss on training batch is 0.00350292.
After 20625 training step(s), loss on training batch is 0.00371307.
After 20626 training step(s), loss on training batch is 0.00351728.
After 20627 training step(s), loss on training batch is 0.00378706.
After 20628 training step(s), loss on training batch is 0.00369688.
After 20629 training step(s), loss on training batch is 0.0036084.
After 20630 training step(s), loss on training batch is 0.00413385.
After 20631 training step(s), loss on training batch is 0.00423688.
After 20632 training step(s), loss on training batch is 0.00408785.
After 20633 training step(s), loss on training batch is 0.00372198.
After 20634 training step(s), loss on training batch is 0.00383603.
After 20635 training step(s), loss on training batch is 0.00351763.
After 20636 training step(s), loss on training batch is 0.00354838.
After 20637 training step(s), loss on training batch is 0.00487799.
After 20638 training step(s), loss on training batch is 0.00366981.
After 20639 training step(s), loss on training batch is 0.00358451.
After 20640 training step(s), loss on training batch is 0.00343813.
After 20641 training step(s), loss on training batch is 0.00347683.
After 20642 training step(s), loss on training batch is 0.00383358.
After 20643 training step(s), loss on training batch is 0.00430174.
After 20644 training step(s), loss on training batch is 0.00435895.
After 20645 training step(s), loss on training batch is 0.00396665.
After 20646 training step(s), loss on training batch is 0.0034075.
After 20647 training step(s), loss on training batch is 0.00365031.
After 20648 training step(s), loss on training batch is 0.0036863.
After 20649 training step(s), loss on training batch is 0.00359621.
After 20650 training step(s), loss on training batch is 0.00400565.
After 20651 training step(s), loss on training batch is 0.0036678.
After 20652 training step(s), loss on training batch is 0.00364781.
After 20653 training step(s), loss on training batch is 0.00381546.
After 20654 training step(s), loss on training batch is 0.00437157.
After 20655 training step(s), loss on training batch is 0.00373146.
After 20656 training step(s), loss on training batch is 0.0036801.
After 20657 training step(s), loss on training batch is 0.00356745.
After 20658 training step(s), loss on training batch is 0.0037487.
After 20659 training step(s), loss on training batch is 0.00342278.
After 20660 training step(s), loss on training batch is 0.00363175.
After 20661 training step(s), loss on training batch is 0.00401214.
After 20662 training step(s), loss on training batch is 0.00353469.
After 20663 training step(s), loss on training batch is 0.00397156.
After 20664 training step(s), loss on training batch is 0.00391764.
After 20665 training step(s), loss on training batch is 0.00362471.
After 20666 training step(s), loss on training batch is 0.00353775.
After 20667 training step(s), loss on training batch is 0.00350604.
After 20668 training step(s), loss on training batch is 0.00412118.
After 20669 training step(s), loss on training batch is 0.00369228.
After 20670 training step(s), loss on training batch is 0.00340868.
After 20671 training step(s), loss on training batch is 0.00455875.
After 20672 training step(s), loss on training batch is 0.00366848.
After 20673 training step(s), loss on training batch is 0.00421681.
After 20674 training step(s), loss on training batch is 0.00359277.
After 20675 training step(s), loss on training batch is 0.003535.
After 20676 training step(s), loss on training batch is 0.0035727.
After 20677 training step(s), loss on training batch is 0.00424176.
After 20678 training step(s), loss on training batch is 0.00437685.
After 20679 training step(s), loss on training batch is 0.00431936.
After 20680 training step(s), loss on training batch is 0.00773686.
After 20681 training step(s), loss on training batch is 0.0037415.
After 20682 training step(s), loss on training batch is 0.00379385.
After 20683 training step(s), loss on training batch is 0.00422642.
After 20684 training step(s), loss on training batch is 0.00347958.
After 20685 training step(s), loss on training batch is 0.00373508.
After 20686 training step(s), loss on training batch is 0.00382901.
After 20687 training step(s), loss on training batch is 0.00377337.
After 20688 training step(s), loss on training batch is 0.00400177.
After 20689 training step(s), loss on training batch is 0.00381681.
After 20690 training step(s), loss on training batch is 0.0035765.
After 20691 training step(s), loss on training batch is 0.00353563.
After 20692 training step(s), loss on training batch is 0.00408891.
After 20693 training step(s), loss on training batch is 0.00357002.
After 20694 training step(s), loss on training batch is 0.00407057.
After 20695 training step(s), loss on training batch is 0.00363601.
After 20696 training step(s), loss on training batch is 0.00406924.
After 20697 training step(s), loss on training batch is 0.00390799.
After 20698 training step(s), loss on training batch is 0.00420843.
After 20699 training step(s), loss on training batch is 0.0039141.
After 20700 training step(s), loss on training batch is 0.00387394.
After 20701 training step(s), loss on training batch is 0.00382016.
After 20702 training step(s), loss on training batch is 0.003803.
After 20703 training step(s), loss on training batch is 0.00390589.
After 20704 training step(s), loss on training batch is 0.00414016.
After 20705 training step(s), loss on training batch is 0.00365984.
After 20706 training step(s), loss on training batch is 0.0034056.
After 20707 training step(s), loss on training batch is 0.00382271.
After 20708 training step(s), loss on training batch is 0.00384571.
After 20709 training step(s), loss on training batch is 0.00442394.
After 20710 training step(s), loss on training batch is 0.00355126.
After 20711 training step(s), loss on training batch is 0.00384463.
After 20712 training step(s), loss on training batch is 0.00431314.
After 20713 training step(s), loss on training batch is 0.00367622.
After 20714 training step(s), loss on training batch is 0.0039721.
After 20715 training step(s), loss on training batch is 0.00334209.
After 20716 training step(s), loss on training batch is 0.0038178.
After 20717 training step(s), loss on training batch is 0.00453769.
After 20718 training step(s), loss on training batch is 0.0035669.
After 20719 training step(s), loss on training batch is 0.00372998.
After 20720 training step(s), loss on training batch is 0.00419033.
After 20721 training step(s), loss on training batch is 0.00358441.
After 20722 training step(s), loss on training batch is 0.00404962.
After 20723 training step(s), loss on training batch is 0.00380624.
After 20724 training step(s), loss on training batch is 0.00355013.
After 20725 training step(s), loss on training batch is 0.00367544.
After 20726 training step(s), loss on training batch is 0.00396054.
After 20727 training step(s), loss on training batch is 0.00375233.
After 20728 training step(s), loss on training batch is 0.00358857.
After 20729 training step(s), loss on training batch is 0.00420133.
After 20730 training step(s), loss on training batch is 0.00419553.
After 20731 training step(s), loss on training batch is 0.00427983.
After 20732 training step(s), loss on training batch is 0.00398619.
After 20733 training step(s), loss on training batch is 0.00410378.
After 20734 training step(s), loss on training batch is 0.00404371.
After 20735 training step(s), loss on training batch is 0.00398663.
After 20736 training step(s), loss on training batch is 0.00371229.
After 20737 training step(s), loss on training batch is 0.00488065.
After 20738 training step(s), loss on training batch is 0.00375626.
After 20739 training step(s), loss on training batch is 0.0042772.
After 20740 training step(s), loss on training batch is 0.00351793.
After 20741 training step(s), loss on training batch is 0.00396454.
After 20742 training step(s), loss on training batch is 0.00369281.
After 20743 training step(s), loss on training batch is 0.00373394.
After 20744 training step(s), loss on training batch is 0.00404218.
After 20745 training step(s), loss on training batch is 0.00379263.
After 20746 training step(s), loss on training batch is 0.00350789.
After 20747 training step(s), loss on training batch is 0.00347838.
After 20748 training step(s), loss on training batch is 0.00376884.
After 20749 training step(s), loss on training batch is 0.00405735.
After 20750 training step(s), loss on training batch is 0.00370916.
After 20751 training step(s), loss on training batch is 0.00419555.
After 20752 training step(s), loss on training batch is 0.00353683.
After 20753 training step(s), loss on training batch is 0.00446183.
After 20754 training step(s), loss on training batch is 0.00400086.
After 20755 training step(s), loss on training batch is 0.00369723.
After 20756 training step(s), loss on training batch is 0.00407209.
After 20757 training step(s), loss on training batch is 0.00384657.
After 20758 training step(s), loss on training batch is 0.00399532.
After 20759 training step(s), loss on training batch is 0.0042034.
After 20760 training step(s), loss on training batch is 0.00374644.
After 20761 training step(s), loss on training batch is 0.00358301.
After 20762 training step(s), loss on training batch is 0.00390129.
After 20763 training step(s), loss on training batch is 0.00396359.
After 20764 training step(s), loss on training batch is 0.00396867.
After 20765 training step(s), loss on training batch is 0.00363245.
After 20766 training step(s), loss on training batch is 0.00365046.
After 20767 training step(s), loss on training batch is 0.00356558.
After 20768 training step(s), loss on training batch is 0.0040211.
After 20769 training step(s), loss on training batch is 0.00334856.
After 20770 training step(s), loss on training batch is 0.00358357.
After 20771 training step(s), loss on training batch is 0.00342393.
After 20772 training step(s), loss on training batch is 0.00394299.
After 20773 training step(s), loss on training batch is 0.00378543.
After 20774 training step(s), loss on training batch is 0.00375299.
After 20775 training step(s), loss on training batch is 0.00413747.
After 20776 training step(s), loss on training batch is 0.00375247.
After 20777 training step(s), loss on training batch is 0.00426703.
After 20778 training step(s), loss on training batch is 0.0041178.
After 20779 training step(s), loss on training batch is 0.0037522.
After 20780 training step(s), loss on training batch is 0.00375009.
After 20781 training step(s), loss on training batch is 0.00351741.
After 20782 training step(s), loss on training batch is 0.00389139.
After 20783 training step(s), loss on training batch is 0.00376543.
After 20784 training step(s), loss on training batch is 0.00442626.
After 20785 training step(s), loss on training batch is 0.00398392.
After 20786 training step(s), loss on training batch is 0.00407513.
After 20787 training step(s), loss on training batch is 0.00350225.
After 20788 training step(s), loss on training batch is 0.00387435.
After 20789 training step(s), loss on training batch is 0.00371201.
After 20790 training step(s), loss on training batch is 0.00402987.
After 20791 training step(s), loss on training batch is 0.00354606.
After 20792 training step(s), loss on training batch is 0.00406951.
After 20793 training step(s), loss on training batch is 0.00403384.
After 20794 training step(s), loss on training batch is 0.00400856.
After 20795 training step(s), loss on training batch is 0.00387135.
After 20796 training step(s), loss on training batch is 0.00371901.
After 20797 training step(s), loss on training batch is 0.00390568.
After 20798 training step(s), loss on training batch is 0.00356164.
After 20799 training step(s), loss on training batch is 0.0034861.
After 20800 training step(s), loss on training batch is 0.00350203.
After 20801 training step(s), loss on training batch is 0.00410282.
After 20802 training step(s), loss on training batch is 0.00355927.
After 20803 training step(s), loss on training batch is 0.00443472.
After 20804 training step(s), loss on training batch is 0.00340548.
After 20805 training step(s), loss on training batch is 0.00379054.
After 20806 training step(s), loss on training batch is 0.00365592.
After 20807 training step(s), loss on training batch is 0.00365248.
After 20808 training step(s), loss on training batch is 0.00364653.
After 20809 training step(s), loss on training batch is 0.00360797.
After 20810 training step(s), loss on training batch is 0.00397995.
After 20811 training step(s), loss on training batch is 0.00368028.
After 20812 training step(s), loss on training batch is 0.00366713.
After 20813 training step(s), loss on training batch is 0.00363751.
After 20814 training step(s), loss on training batch is 0.00407503.
After 20815 training step(s), loss on training batch is 0.00363849.
After 20816 training step(s), loss on training batch is 0.00366567.
After 20817 training step(s), loss on training batch is 0.00491949.
After 20818 training step(s), loss on training batch is 0.00387725.
After 20819 training step(s), loss on training batch is 0.00352919.
After 20820 training step(s), loss on training batch is 0.00464825.
After 20821 training step(s), loss on training batch is 0.00366916.
After 20822 training step(s), loss on training batch is 0.00402757.
After 20823 training step(s), loss on training batch is 0.00377007.
After 20824 training step(s), loss on training batch is 0.00383037.
After 20825 training step(s), loss on training batch is 0.00356631.
After 20826 training step(s), loss on training batch is 0.00389804.
After 20827 training step(s), loss on training batch is 0.00361644.
After 20828 training step(s), loss on training batch is 0.00429381.
After 20829 training step(s), loss on training batch is 0.0035883.
After 20830 training step(s), loss on training batch is 0.00363158.
After 20831 training step(s), loss on training batch is 0.00338049.
After 20832 training step(s), loss on training batch is 0.0038124.
After 20833 training step(s), loss on training batch is 0.00371588.
After 20834 training step(s), loss on training batch is 0.00385498.
After 20835 training step(s), loss on training batch is 0.00360677.
After 20836 training step(s), loss on training batch is 0.00375506.
After 20837 training step(s), loss on training batch is 0.00376082.
After 20838 training step(s), loss on training batch is 0.00387635.
After 20839 training step(s), loss on training batch is 0.00371438.
After 20840 training step(s), loss on training batch is 0.00452919.
After 20841 training step(s), loss on training batch is 0.00351798.
After 20842 training step(s), loss on training batch is 0.00373797.
After 20843 training step(s), loss on training batch is 0.0039704.
After 20844 training step(s), loss on training batch is 0.0037875.
After 20845 training step(s), loss on training batch is 0.00373633.
After 20846 training step(s), loss on training batch is 0.00354198.
After 20847 training step(s), loss on training batch is 0.004193.
After 20848 training step(s), loss on training batch is 0.00374231.
After 20849 training step(s), loss on training batch is 0.0037938.
After 20850 training step(s), loss on training batch is 0.00427807.
After 20851 training step(s), loss on training batch is 0.00389493.
After 20852 training step(s), loss on training batch is 0.00375629.
After 20853 training step(s), loss on training batch is 0.00398362.
After 20854 training step(s), loss on training batch is 0.00471835.
After 20855 training step(s), loss on training batch is 0.00401067.
After 20856 training step(s), loss on training batch is 0.0039408.
After 20857 training step(s), loss on training batch is 0.00386061.
After 20858 training step(s), loss on training batch is 0.0038591.
After 20859 training step(s), loss on training batch is 0.00406112.
After 20860 training step(s), loss on training batch is 0.0037432.
After 20861 training step(s), loss on training batch is 0.00387525.
After 20862 training step(s), loss on training batch is 0.00349723.
After 20863 training step(s), loss on training batch is 0.00428313.
After 20864 training step(s), loss on training batch is 0.00353438.
After 20865 training step(s), loss on training batch is 0.00379129.
After 20866 training step(s), loss on training batch is 0.00387849.
After 20867 training step(s), loss on training batch is 0.00378097.
After 20868 training step(s), loss on training batch is 0.00357051.
After 20869 training step(s), loss on training batch is 0.00384327.
After 20870 training step(s), loss on training batch is 0.0035636.
After 20871 training step(s), loss on training batch is 0.00367051.
After 20872 training step(s), loss on training batch is 0.0048304.
After 20873 training step(s), loss on training batch is 0.00427955.
After 20874 training step(s), loss on training batch is 0.00390805.
After 20875 training step(s), loss on training batch is 0.00398876.
After 20876 training step(s), loss on training batch is 0.00364117.
After 20877 training step(s), loss on training batch is 0.00367806.
After 20878 training step(s), loss on training batch is 0.00384658.
After 20879 training step(s), loss on training batch is 0.00360057.
After 20880 training step(s), loss on training batch is 0.0036879.
After 20881 training step(s), loss on training batch is 0.00380812.
After 20882 training step(s), loss on training batch is 0.00382871.
After 20883 training step(s), loss on training batch is 0.00492452.
After 20884 training step(s), loss on training batch is 0.00341958.
After 20885 training step(s), loss on training batch is 0.00455541.
After 20886 training step(s), loss on training batch is 0.00484516.
After 20887 training step(s), loss on training batch is 0.0038783.
After 20888 training step(s), loss on training batch is 0.00402536.
After 20889 training step(s), loss on training batch is 0.00359664.
After 20890 training step(s), loss on training batch is 0.00397838.
After 20891 training step(s), loss on training batch is 0.00358768.
After 20892 training step(s), loss on training batch is 0.0035238.
After 20893 training step(s), loss on training batch is 0.00362497.
After 20894 training step(s), loss on training batch is 0.00379742.
After 20895 training step(s), loss on training batch is 0.0042473.
After 20896 training step(s), loss on training batch is 0.00361168.
After 20897 training step(s), loss on training batch is 0.00367256.
After 20898 training step(s), loss on training batch is 0.00417758.
After 20899 training step(s), loss on training batch is 0.00403536.
After 20900 training step(s), loss on training batch is 0.00435716.
After 20901 training step(s), loss on training batch is 0.00344585.
After 20902 training step(s), loss on training batch is 0.00356386.
After 20903 training step(s), loss on training batch is 0.00364394.
After 20904 training step(s), loss on training batch is 0.00392349.
After 20905 training step(s), loss on training batch is 0.00351733.
After 20906 training step(s), loss on training batch is 0.00371944.
After 20907 training step(s), loss on training batch is 0.0038362.
After 20908 training step(s), loss on training batch is 0.0039686.
After 20909 training step(s), loss on training batch is 0.00367101.
After 20910 training step(s), loss on training batch is 0.00408714.
After 20911 training step(s), loss on training batch is 0.00355509.
After 20912 training step(s), loss on training batch is 0.00437672.
After 20913 training step(s), loss on training batch is 0.00346606.
After 20914 training step(s), loss on training batch is 0.00353457.
After 20915 training step(s), loss on training batch is 0.00402083.
After 20916 training step(s), loss on training batch is 0.00392614.
After 20917 training step(s), loss on training batch is 0.00355988.
After 20918 training step(s), loss on training batch is 0.00356818.
After 20919 training step(s), loss on training batch is 0.003597.
After 20920 training step(s), loss on training batch is 0.00368794.
After 20921 training step(s), loss on training batch is 0.00433391.
After 20922 training step(s), loss on training batch is 0.00435142.
After 20923 training step(s), loss on training batch is 0.00349276.
After 20924 training step(s), loss on training batch is 0.0041769.
After 20925 training step(s), loss on training batch is 0.00382488.
After 20926 training step(s), loss on training batch is 0.00386971.
After 20927 training step(s), loss on training batch is 0.00389286.
After 20928 training step(s), loss on training batch is 0.00356572.
After 20929 training step(s), loss on training batch is 0.00388733.
After 20930 training step(s), loss on training batch is 0.00414728.
After 20931 training step(s), loss on training batch is 0.00379986.
After 20932 training step(s), loss on training batch is 0.00374018.
After 20933 training step(s), loss on training batch is 0.00540033.
After 20934 training step(s), loss on training batch is 0.00364784.
After 20935 training step(s), loss on training batch is 0.00383196.
After 20936 training step(s), loss on training batch is 0.0035948.
After 20937 training step(s), loss on training batch is 0.00383979.
After 20938 training step(s), loss on training batch is 0.00360905.
After 20939 training step(s), loss on training batch is 0.00360682.
After 20940 training step(s), loss on training batch is 0.00351248.
After 20941 training step(s), loss on training batch is 0.00372595.
After 20942 training step(s), loss on training batch is 0.00358962.
After 20943 training step(s), loss on training batch is 0.00391743.
After 20944 training step(s), loss on training batch is 0.00403147.
After 20945 training step(s), loss on training batch is 0.0034066.
After 20946 training step(s), loss on training batch is 0.00364316.
After 20947 training step(s), loss on training batch is 0.00383764.
After 20948 training step(s), loss on training batch is 0.0035111.
After 20949 training step(s), loss on training batch is 0.0035559.
After 20950 training step(s), loss on training batch is 0.00360712.
After 20951 training step(s), loss on training batch is 0.00363706.
After 20952 training step(s), loss on training batch is 0.00401608.
After 20953 training step(s), loss on training batch is 0.00374972.
After 20954 training step(s), loss on training batch is 0.00361519.
After 20955 training step(s), loss on training batch is 0.00392953.
After 20956 training step(s), loss on training batch is 0.00381944.
After 20957 training step(s), loss on training batch is 0.00391588.
After 20958 training step(s), loss on training batch is 0.00374402.
After 20959 training step(s), loss on training batch is 0.00342598.
After 20960 training step(s), loss on training batch is 0.00355007.
After 20961 training step(s), loss on training batch is 0.00369153.
After 20962 training step(s), loss on training batch is 0.00388429.
After 20963 training step(s), loss on training batch is 0.00355682.
After 20964 training step(s), loss on training batch is 0.00375019.
After 20965 training step(s), loss on training batch is 0.00418158.
After 20966 training step(s), loss on training batch is 0.00352346.
After 20967 training step(s), loss on training batch is 0.00368973.
After 20968 training step(s), loss on training batch is 0.00392859.
After 20969 training step(s), loss on training batch is 0.00402139.
After 20970 training step(s), loss on training batch is 0.00466215.
After 20971 training step(s), loss on training batch is 0.00360656.
After 20972 training step(s), loss on training batch is 0.00432587.
After 20973 training step(s), loss on training batch is 0.00425303.
After 20974 training step(s), loss on training batch is 0.00372617.
After 20975 training step(s), loss on training batch is 0.00354682.
After 20976 training step(s), loss on training batch is 0.00467786.
After 20977 training step(s), loss on training batch is 0.00388232.
After 20978 training step(s), loss on training batch is 0.00365371.
After 20979 training step(s), loss on training batch is 0.00403613.
After 20980 training step(s), loss on training batch is 0.00424237.
After 20981 training step(s), loss on training batch is 0.0034851.
After 20982 training step(s), loss on training batch is 0.0046086.
After 20983 training step(s), loss on training batch is 0.00683727.
After 20984 training step(s), loss on training batch is 0.00422323.
After 20985 training step(s), loss on training batch is 0.00358186.
After 20986 training step(s), loss on training batch is 0.00374975.
After 20987 training step(s), loss on training batch is 0.00376905.
After 20988 training step(s), loss on training batch is 0.00349455.
After 20989 training step(s), loss on training batch is 0.00370157.
After 20990 training step(s), loss on training batch is 0.00383397.
After 20991 training step(s), loss on training batch is 0.00378107.
After 20992 training step(s), loss on training batch is 0.00371227.
After 20993 training step(s), loss on training batch is 0.00356479.
After 20994 training step(s), loss on training batch is 0.00358678.
After 20995 training step(s), loss on training batch is 0.00391458.
After 20996 training step(s), loss on training batch is 0.00358279.
After 20997 training step(s), loss on training batch is 0.00387083.
After 20998 training step(s), loss on training batch is 0.00361403.
After 20999 training step(s), loss on training batch is 0.00378467.
After 21000 training step(s), loss on training batch is 0.00403561.
After 21001 training step(s), loss on training batch is 0.00463612.
After 21002 training step(s), loss on training batch is 0.00485281.
After 21003 training step(s), loss on training batch is 0.00347224.
After 21004 training step(s), loss on training batch is 0.00405588.
After 21005 training step(s), loss on training batch is 0.00388249.
After 21006 training step(s), loss on training batch is 0.00380246.
After 21007 training step(s), loss on training batch is 0.00371378.
After 21008 training step(s), loss on training batch is 0.00343944.
After 21009 training step(s), loss on training batch is 0.00375615.
After 21010 training step(s), loss on training batch is 0.00439647.
After 21011 training step(s), loss on training batch is 0.00394616.
After 21012 training step(s), loss on training batch is 0.00387792.
After 21013 training step(s), loss on training batch is 0.00414519.
After 21014 training step(s), loss on training batch is 0.0039515.
After 21015 training step(s), loss on training batch is 0.00343634.
After 21016 training step(s), loss on training batch is 0.00355766.
After 21017 training step(s), loss on training batch is 0.00436178.
After 21018 training step(s), loss on training batch is 0.00361488.
After 21019 training step(s), loss on training batch is 0.00365359.
After 21020 training step(s), loss on training batch is 0.0036149.
After 21021 training step(s), loss on training batch is 0.00399855.
After 21022 training step(s), loss on training batch is 0.00357114.
After 21023 training step(s), loss on training batch is 0.00379441.
After 21024 training step(s), loss on training batch is 0.00393702.
After 21025 training step(s), loss on training batch is 0.00351639.
After 21026 training step(s), loss on training batch is 0.00421449.
After 21027 training step(s), loss on training batch is 0.00398433.
After 21028 training step(s), loss on training batch is 0.00363719.
After 21029 training step(s), loss on training batch is 0.00366367.
After 21030 training step(s), loss on training batch is 0.00435359.
After 21031 training step(s), loss on training batch is 0.00365123.
After 21032 training step(s), loss on training batch is 0.00380548.
After 21033 training step(s), loss on training batch is 0.00400059.
After 21034 training step(s), loss on training batch is 0.00342212.
After 21035 training step(s), loss on training batch is 0.00391561.
After 21036 training step(s), loss on training batch is 0.00420496.
After 21037 training step(s), loss on training batch is 0.00430827.
After 21038 training step(s), loss on training batch is 0.00382691.
After 21039 training step(s), loss on training batch is 0.00430013.
After 21040 training step(s), loss on training batch is 0.00346169.
After 21041 training step(s), loss on training batch is 0.00368256.
After 21042 training step(s), loss on training batch is 0.00359091.
After 21043 training step(s), loss on training batch is 0.00386246.
After 21044 training step(s), loss on training batch is 0.00372385.
After 21045 training step(s), loss on training batch is 0.00447265.
After 21046 training step(s), loss on training batch is 0.00414309.
After 21047 training step(s), loss on training batch is 0.00346958.
After 21048 training step(s), loss on training batch is 0.00417488.
After 21049 training step(s), loss on training batch is 0.00378549.
After 21050 training step(s), loss on training batch is 0.00370664.
After 21051 training step(s), loss on training batch is 0.00339916.
After 21052 training step(s), loss on training batch is 0.00385174.
After 21053 training step(s), loss on training batch is 0.00375365.
After 21054 training step(s), loss on training batch is 0.00372713.
After 21055 training step(s), loss on training batch is 0.00380931.
After 21056 training step(s), loss on training batch is 0.00373612.
After 21057 training step(s), loss on training batch is 0.00338036.
After 21058 training step(s), loss on training batch is 0.00426402.
After 21059 training step(s), loss on training batch is 0.00355369.
After 21060 training step(s), loss on training batch is 0.00350857.
After 21061 training step(s), loss on training batch is 0.00382248.
After 21062 training step(s), loss on training batch is 0.00358613.
After 21063 training step(s), loss on training batch is 0.00364399.
After 21064 training step(s), loss on training batch is 0.00363284.
After 21065 training step(s), loss on training batch is 0.00411033.
After 21066 training step(s), loss on training batch is 0.00360262.
After 21067 training step(s), loss on training batch is 0.00350913.
After 21068 training step(s), loss on training batch is 0.00388497.
After 21069 training step(s), loss on training batch is 0.00367275.
After 21070 training step(s), loss on training batch is 0.0039142.
After 21071 training step(s), loss on training batch is 0.00355921.
After 21072 training step(s), loss on training batch is 0.00402197.
After 21073 training step(s), loss on training batch is 0.00376348.
After 21074 training step(s), loss on training batch is 0.00336316.
After 21075 training step(s), loss on training batch is 0.00397659.
After 21076 training step(s), loss on training batch is 0.00356611.
After 21077 training step(s), loss on training batch is 0.00340578.
After 21078 training step(s), loss on training batch is 0.00365473.
After 21079 training step(s), loss on training batch is 0.00398294.
After 21080 training step(s), loss on training batch is 0.00361648.
After 21081 training step(s), loss on training batch is 0.00399459.
After 21082 training step(s), loss on training batch is 0.00417731.
After 21083 training step(s), loss on training batch is 0.00379604.
After 21084 training step(s), loss on training batch is 0.00498938.
After 21085 training step(s), loss on training batch is 0.00370995.
After 21086 training step(s), loss on training batch is 0.00398145.
After 21087 training step(s), loss on training batch is 0.00337565.
After 21088 training step(s), loss on training batch is 0.00398859.
After 21089 training step(s), loss on training batch is 0.00497646.
After 21090 training step(s), loss on training batch is 0.0036239.
After 21091 training step(s), loss on training batch is 0.00413913.
After 21092 training step(s), loss on training batch is 0.00422273.
After 21093 training step(s), loss on training batch is 0.00359829.
After 21094 training step(s), loss on training batch is 0.00430574.
After 21095 training step(s), loss on training batch is 0.00397156.
After 21096 training step(s), loss on training batch is 0.0035462.
After 21097 training step(s), loss on training batch is 0.00396941.
After 21098 training step(s), loss on training batch is 0.00404071.
After 21099 training step(s), loss on training batch is 0.00393747.
After 21100 training step(s), loss on training batch is 0.00438778.
After 21101 training step(s), loss on training batch is 0.00349288.
After 21102 training step(s), loss on training batch is 0.00401261.
After 21103 training step(s), loss on training batch is 0.00363565.
After 21104 training step(s), loss on training batch is 0.00422454.
After 21105 training step(s), loss on training batch is 0.00372157.
After 21106 training step(s), loss on training batch is 0.00363874.
After 21107 training step(s), loss on training batch is 0.00368228.
After 21108 training step(s), loss on training batch is 0.00371257.
After 21109 training step(s), loss on training batch is 0.00430501.
After 21110 training step(s), loss on training batch is 0.0043005.
After 21111 training step(s), loss on training batch is 0.00366653.
After 21112 training step(s), loss on training batch is 0.00345562.
After 21113 training step(s), loss on training batch is 0.00413259.
After 21114 training step(s), loss on training batch is 0.00342821.
After 21115 training step(s), loss on training batch is 0.00397376.
After 21116 training step(s), loss on training batch is 0.00428313.
After 21117 training step(s), loss on training batch is 0.00355495.
After 21118 training step(s), loss on training batch is 0.00341855.
After 21119 training step(s), loss on training batch is 0.00375567.
After 21120 training step(s), loss on training batch is 0.00372201.
After 21121 training step(s), loss on training batch is 0.00409138.
After 21122 training step(s), loss on training batch is 0.00370478.
After 21123 training step(s), loss on training batch is 0.00428743.
After 21124 training step(s), loss on training batch is 0.00370492.
After 21125 training step(s), loss on training batch is 0.00385549.
After 21126 training step(s), loss on training batch is 0.00350087.
After 21127 training step(s), loss on training batch is 0.00373236.
After 21128 training step(s), loss on training batch is 0.00361496.
After 21129 training step(s), loss on training batch is 0.00380372.
After 21130 training step(s), loss on training batch is 0.00370927.
After 21131 training step(s), loss on training batch is 0.0037873.
After 21132 training step(s), loss on training batch is 0.00356493.
After 21133 training step(s), loss on training batch is 0.00436354.
After 21134 training step(s), loss on training batch is 0.00352123.
After 21135 training step(s), loss on training batch is 0.0034704.
After 21136 training step(s), loss on training batch is 0.00367621.
After 21137 training step(s), loss on training batch is 0.00346065.
After 21138 training step(s), loss on training batch is 0.00369078.
After 21139 training step(s), loss on training batch is 0.00401963.
After 21140 training step(s), loss on training batch is 0.00376404.
After 21141 training step(s), loss on training batch is 0.0035411.
After 21142 training step(s), loss on training batch is 0.00454372.
After 21143 training step(s), loss on training batch is 0.00367677.
After 21144 training step(s), loss on training batch is 0.00399803.
After 21145 training step(s), loss on training batch is 0.00389927.
After 21146 training step(s), loss on training batch is 0.00333483.
After 21147 training step(s), loss on training batch is 0.00387355.
After 21148 training step(s), loss on training batch is 0.00342766.
After 21149 training step(s), loss on training batch is 0.0039072.
After 21150 training step(s), loss on training batch is 0.00383239.
After 21151 training step(s), loss on training batch is 0.00452995.
After 21152 training step(s), loss on training batch is 0.00382217.
After 21153 training step(s), loss on training batch is 0.0038131.
After 21154 training step(s), loss on training batch is 0.00396279.
After 21155 training step(s), loss on training batch is 0.00346014.
After 21156 training step(s), loss on training batch is 0.00337595.
After 21157 training step(s), loss on training batch is 0.00420945.
After 21158 training step(s), loss on training batch is 0.0037504.
After 21159 training step(s), loss on training batch is 0.00400643.
After 21160 training step(s), loss on training batch is 0.00388659.
After 21161 training step(s), loss on training batch is 0.00365502.
After 21162 training step(s), loss on training batch is 0.00388619.
After 21163 training step(s), loss on training batch is 0.00378377.
After 21164 training step(s), loss on training batch is 0.00343044.
After 21165 training step(s), loss on training batch is 0.0035587.
After 21166 training step(s), loss on training batch is 0.00359453.
After 21167 training step(s), loss on training batch is 0.00416481.
After 21168 training step(s), loss on training batch is 0.00375106.
After 21169 training step(s), loss on training batch is 0.00360446.
After 21170 training step(s), loss on training batch is 0.00364552.
After 21171 training step(s), loss on training batch is 0.00353938.
After 21172 training step(s), loss on training batch is 0.00368453.
After 21173 training step(s), loss on training batch is 0.00369714.
After 21174 training step(s), loss on training batch is 0.00392202.
After 21175 training step(s), loss on training batch is 0.00348543.
After 21176 training step(s), loss on training batch is 0.00412285.
After 21177 training step(s), loss on training batch is 0.00410348.
After 21178 training step(s), loss on training batch is 0.00509423.
After 21179 training step(s), loss on training batch is 0.00349144.
After 21180 training step(s), loss on training batch is 0.00350983.
After 21181 training step(s), loss on training batch is 0.00348712.
After 21182 training step(s), loss on training batch is 0.00356082.
After 21183 training step(s), loss on training batch is 0.00350175.
After 21184 training step(s), loss on training batch is 0.00348061.
After 21185 training step(s), loss on training batch is 0.00336248.
After 21186 training step(s), loss on training batch is 0.00338816.
After 21187 training step(s), loss on training batch is 0.00398356.
After 21188 training step(s), loss on training batch is 0.00375058.
After 21189 training step(s), loss on training batch is 0.00362905.
After 21190 training step(s), loss on training batch is 0.00362326.
After 21191 training step(s), loss on training batch is 0.00343354.
After 21192 training step(s), loss on training batch is 0.00373552.
After 21193 training step(s), loss on training batch is 0.00351326.
After 21194 training step(s), loss on training batch is 0.00374598.
After 21195 training step(s), loss on training batch is 0.00357878.
After 21196 training step(s), loss on training batch is 0.00368651.
After 21197 training step(s), loss on training batch is 0.00421693.
After 21198 training step(s), loss on training batch is 0.00412251.
After 21199 training step(s), loss on training batch is 0.00351316.
After 21200 training step(s), loss on training batch is 0.00422716.
After 21201 training step(s), loss on training batch is 0.00394992.
After 21202 training step(s), loss on training batch is 0.00338395.
After 21203 training step(s), loss on training batch is 0.00341037.
After 21204 training step(s), loss on training batch is 0.0037118.
After 21205 training step(s), loss on training batch is 0.00365674.
After 21206 training step(s), loss on training batch is 0.00348426.
After 21207 training step(s), loss on training batch is 0.00462458.
After 21208 training step(s), loss on training batch is 0.00353622.
After 21209 training step(s), loss on training batch is 0.00350909.
After 21210 training step(s), loss on training batch is 0.00355232.
After 21211 training step(s), loss on training batch is 0.0037568.
After 21212 training step(s), loss on training batch is 0.00366525.
After 21213 training step(s), loss on training batch is 0.00401757.
After 21214 training step(s), loss on training batch is 0.00349811.
After 21215 training step(s), loss on training batch is 0.00334041.
After 21216 training step(s), loss on training batch is 0.00355448.
After 21217 training step(s), loss on training batch is 0.00435811.
After 21218 training step(s), loss on training batch is 0.00362316.
After 21219 training step(s), loss on training batch is 0.00349313.
After 21220 training step(s), loss on training batch is 0.00412499.
After 21221 training step(s), loss on training batch is 0.00376838.
After 21222 training step(s), loss on training batch is 0.00382522.
After 21223 training step(s), loss on training batch is 0.00391145.
After 21224 training step(s), loss on training batch is 0.00357959.
After 21225 training step(s), loss on training batch is 0.00426013.
After 21226 training step(s), loss on training batch is 0.00405231.
After 21227 training step(s), loss on training batch is 0.0036659.
After 21228 training step(s), loss on training batch is 0.0039433.
After 21229 training step(s), loss on training batch is 0.00397398.
After 21230 training step(s), loss on training batch is 0.00414862.
After 21231 training step(s), loss on training batch is 0.00342702.
After 21232 training step(s), loss on training batch is 0.00373359.
After 21233 training step(s), loss on training batch is 0.00370285.
After 21234 training step(s), loss on training batch is 0.00398337.
After 21235 training step(s), loss on training batch is 0.00384312.
After 21236 training step(s), loss on training batch is 0.00377876.
After 21237 training step(s), loss on training batch is 0.0036624.
After 21238 training step(s), loss on training batch is 0.00375487.
After 21239 training step(s), loss on training batch is 0.00355124.
After 21240 training step(s), loss on training batch is 0.00349339.
After 21241 training step(s), loss on training batch is 0.00354683.
After 21242 training step(s), loss on training batch is 0.00357387.
After 21243 training step(s), loss on training batch is 0.00330222.
After 21244 training step(s), loss on training batch is 0.00404829.
After 21245 training step(s), loss on training batch is 0.00387715.
After 21246 training step(s), loss on training batch is 0.00353782.
After 21247 training step(s), loss on training batch is 0.00387659.
After 21248 training step(s), loss on training batch is 0.0043807.
After 21249 training step(s), loss on training batch is 0.00453359.
After 21250 training step(s), loss on training batch is 0.00379791.
After 21251 training step(s), loss on training batch is 0.00342404.
After 21252 training step(s), loss on training batch is 0.00387348.
After 21253 training step(s), loss on training batch is 0.00364351.
After 21254 training step(s), loss on training batch is 0.00362592.
After 21255 training step(s), loss on training batch is 0.00383928.
After 21256 training step(s), loss on training batch is 0.0036767.
After 21257 training step(s), loss on training batch is 0.00364285.
After 21258 training step(s), loss on training batch is 0.00331972.
After 21259 training step(s), loss on training batch is 0.00335425.
After 21260 training step(s), loss on training batch is 0.00379403.
After 21261 training step(s), loss on training batch is 0.00409186.
After 21262 training step(s), loss on training batch is 0.00363246.
After 21263 training step(s), loss on training batch is 0.00398507.
After 21264 training step(s), loss on training batch is 0.0039579.
After 21265 training step(s), loss on training batch is 0.00378891.
After 21266 training step(s), loss on training batch is 0.00356003.
After 21267 training step(s), loss on training batch is 0.00375485.
After 21268 training step(s), loss on training batch is 0.00347304.
After 21269 training step(s), loss on training batch is 0.00372395.
After 21270 training step(s), loss on training batch is 0.0037437.
After 21271 training step(s), loss on training batch is 0.00361901.
After 21272 training step(s), loss on training batch is 0.00372672.
After 21273 training step(s), loss on training batch is 0.00392269.
After 21274 training step(s), loss on training batch is 0.00384425.
After 21275 training step(s), loss on training batch is 0.00401182.
After 21276 training step(s), loss on training batch is 0.00369509.
After 21277 training step(s), loss on training batch is 0.00361788.
After 21278 training step(s), loss on training batch is 0.00350684.
After 21279 training step(s), loss on training batch is 0.00351322.
After 21280 training step(s), loss on training batch is 0.00384318.
After 21281 training step(s), loss on training batch is 0.00379758.
After 21282 training step(s), loss on training batch is 0.00397665.
After 21283 training step(s), loss on training batch is 0.00364472.
After 21284 training step(s), loss on training batch is 0.00342551.
After 21285 training step(s), loss on training batch is 0.00342842.
After 21286 training step(s), loss on training batch is 0.00365596.
After 21287 training step(s), loss on training batch is 0.00355826.
After 21288 training step(s), loss on training batch is 0.00370122.
After 21289 training step(s), loss on training batch is 0.00420676.
After 21290 training step(s), loss on training batch is 0.00410457.
After 21291 training step(s), loss on training batch is 0.00372745.
After 21292 training step(s), loss on training batch is 0.00353539.
After 21293 training step(s), loss on training batch is 0.00395416.
After 21294 training step(s), loss on training batch is 0.0036739.
After 21295 training step(s), loss on training batch is 0.00380201.
After 21296 training step(s), loss on training batch is 0.00476793.
After 21297 training step(s), loss on training batch is 0.00440726.
After 21298 training step(s), loss on training batch is 0.0036303.
After 21299 training step(s), loss on training batch is 0.00352403.
After 21300 training step(s), loss on training batch is 0.00401436.
After 21301 training step(s), loss on training batch is 0.00366173.
After 21302 training step(s), loss on training batch is 0.00424813.
After 21303 training step(s), loss on training batch is 0.00412133.
After 21304 training step(s), loss on training batch is 0.00372535.
After 21305 training step(s), loss on training batch is 0.00362793.
After 21306 training step(s), loss on training batch is 0.00383372.
After 21307 training step(s), loss on training batch is 0.00396431.
After 21308 training step(s), loss on training batch is 0.00354983.
After 21309 training step(s), loss on training batch is 0.00356007.
After 21310 training step(s), loss on training batch is 0.00405763.
After 21311 training step(s), loss on training batch is 0.00351539.
After 21312 training step(s), loss on training batch is 0.00377535.
After 21313 training step(s), loss on training batch is 0.0033866.
After 21314 training step(s), loss on training batch is 0.00387392.
After 21315 training step(s), loss on training batch is 0.00344095.
After 21316 training step(s), loss on training batch is 0.00384506.
After 21317 training step(s), loss on training batch is 0.0035833.
After 21318 training step(s), loss on training batch is 0.00342288.
After 21319 training step(s), loss on training batch is 0.00395587.
After 21320 training step(s), loss on training batch is 0.00393386.
After 21321 training step(s), loss on training batch is 0.00366944.
After 21322 training step(s), loss on training batch is 0.00379082.
After 21323 training step(s), loss on training batch is 0.00369039.
After 21324 training step(s), loss on training batch is 0.00357993.
After 21325 training step(s), loss on training batch is 0.00421492.
After 21326 training step(s), loss on training batch is 0.00358136.
After 21327 training step(s), loss on training batch is 0.00370149.
After 21328 training step(s), loss on training batch is 0.0037602.
After 21329 training step(s), loss on training batch is 0.00373834.
After 21330 training step(s), loss on training batch is 0.0039447.
After 21331 training step(s), loss on training batch is 0.00368336.
After 21332 training step(s), loss on training batch is 0.00396627.
After 21333 training step(s), loss on training batch is 0.00388487.
After 21334 training step(s), loss on training batch is 0.00340642.
After 21335 training step(s), loss on training batch is 0.00340897.
After 21336 training step(s), loss on training batch is 0.00461465.
After 21337 training step(s), loss on training batch is 0.00368339.
After 21338 training step(s), loss on training batch is 0.00429685.
After 21339 training step(s), loss on training batch is 0.00366654.
After 21340 training step(s), loss on training batch is 0.00371411.
After 21341 training step(s), loss on training batch is 0.00379532.
After 21342 training step(s), loss on training batch is 0.00376542.
After 21343 training step(s), loss on training batch is 0.00368688.
After 21344 training step(s), loss on training batch is 0.00373448.
After 21345 training step(s), loss on training batch is 0.00366989.
After 21346 training step(s), loss on training batch is 0.00357905.
After 21347 training step(s), loss on training batch is 0.00334082.
After 21348 training step(s), loss on training batch is 0.00387838.
After 21349 training step(s), loss on training batch is 0.00403119.
After 21350 training step(s), loss on training batch is 0.00350543.
After 21351 training step(s), loss on training batch is 0.00507671.
After 21352 training step(s), loss on training batch is 0.00351585.
After 21353 training step(s), loss on training batch is 0.0036857.
After 21354 training step(s), loss on training batch is 0.00361397.
After 21355 training step(s), loss on training batch is 0.00354134.
After 21356 training step(s), loss on training batch is 0.00385392.
After 21357 training step(s), loss on training batch is 0.00342814.
After 21358 training step(s), loss on training batch is 0.0036941.
After 21359 training step(s), loss on training batch is 0.0036424.
After 21360 training step(s), loss on training batch is 0.00404054.
After 21361 training step(s), loss on training batch is 0.00351137.
After 21362 training step(s), loss on training batch is 0.00379042.
After 21363 training step(s), loss on training batch is 0.00417707.
After 21364 training step(s), loss on training batch is 0.0033201.
After 21365 training step(s), loss on training batch is 0.00366982.
After 21366 training step(s), loss on training batch is 0.00371808.
After 21367 training step(s), loss on training batch is 0.0033935.
After 21368 training step(s), loss on training batch is 0.0034331.
After 21369 training step(s), loss on training batch is 0.00361727.
After 21370 training step(s), loss on training batch is 0.00413329.
After 21371 training step(s), loss on training batch is 0.00356852.
After 21372 training step(s), loss on training batch is 0.00330477.
After 21373 training step(s), loss on training batch is 0.00351643.
After 21374 training step(s), loss on training batch is 0.00431065.
After 21375 training step(s), loss on training batch is 0.00378999.
After 21376 training step(s), loss on training batch is 0.00382119.
After 21377 training step(s), loss on training batch is 0.00432679.
After 21378 training step(s), loss on training batch is 0.00381005.
After 21379 training step(s), loss on training batch is 0.00363994.
After 21380 training step(s), loss on training batch is 0.00369249.
After 21381 training step(s), loss on training batch is 0.00359798.
After 21382 training step(s), loss on training batch is 0.00390091.
After 21383 training step(s), loss on training batch is 0.00362891.
After 21384 training step(s), loss on training batch is 0.00415022.
After 21385 training step(s), loss on training batch is 0.00375551.
After 21386 training step(s), loss on training batch is 0.00472103.
After 21387 training step(s), loss on training batch is 0.00373927.
After 21388 training step(s), loss on training batch is 0.00592838.
After 21389 training step(s), loss on training batch is 0.00386379.
After 21390 training step(s), loss on training batch is 0.00375298.
After 21391 training step(s), loss on training batch is 0.00356147.
After 21392 training step(s), loss on training batch is 0.00405359.
After 21393 training step(s), loss on training batch is 0.00397744.
After 21394 training step(s), loss on training batch is 0.004221.
After 21395 training step(s), loss on training batch is 0.0039217.
After 21396 training step(s), loss on training batch is 0.00372145.
After 21397 training step(s), loss on training batch is 0.00446739.
After 21398 training step(s), loss on training batch is 0.00341444.
After 21399 training step(s), loss on training batch is 0.0034174.
After 21400 training step(s), loss on training batch is 0.00417019.
After 21401 training step(s), loss on training batch is 0.00363692.
After 21402 training step(s), loss on training batch is 0.00356828.
After 21403 training step(s), loss on training batch is 0.00344391.
After 21404 training step(s), loss on training batch is 0.00399063.
After 21405 training step(s), loss on training batch is 0.0035563.
After 21406 training step(s), loss on training batch is 0.00448996.
After 21407 training step(s), loss on training batch is 0.00363189.
After 21408 training step(s), loss on training batch is 0.00373364.
After 21409 training step(s), loss on training batch is 0.00388756.
After 21410 training step(s), loss on training batch is 0.00342459.
After 21411 training step(s), loss on training batch is 0.00393112.
After 21412 training step(s), loss on training batch is 0.00450325.
After 21413 training step(s), loss on training batch is 0.00414417.
After 21414 training step(s), loss on training batch is 0.00398841.
After 21415 training step(s), loss on training batch is 0.00368339.
After 21416 training step(s), loss on training batch is 0.00380049.
After 21417 training step(s), loss on training batch is 0.00360086.
After 21418 training step(s), loss on training batch is 0.00333876.
After 21419 training step(s), loss on training batch is 0.00395957.
After 21420 training step(s), loss on training batch is 0.0037896.
After 21421 training step(s), loss on training batch is 0.00393189.
After 21422 training step(s), loss on training batch is 0.00380557.
After 21423 training step(s), loss on training batch is 0.00404355.
After 21424 training step(s), loss on training batch is 0.00375502.
After 21425 training step(s), loss on training batch is 0.00387811.
After 21426 training step(s), loss on training batch is 0.00402775.
After 21427 training step(s), loss on training batch is 0.00393514.
After 21428 training step(s), loss on training batch is 0.00388115.
After 21429 training step(s), loss on training batch is 0.00414709.
After 21430 training step(s), loss on training batch is 0.00375719.
After 21431 training step(s), loss on training batch is 0.0044158.
After 21432 training step(s), loss on training batch is 0.00382349.
After 21433 training step(s), loss on training batch is 0.00412954.
After 21434 training step(s), loss on training batch is 0.00477476.
After 21435 training step(s), loss on training batch is 0.00359575.
After 21436 training step(s), loss on training batch is 0.004238.
After 21437 training step(s), loss on training batch is 0.00381238.
After 21438 training step(s), loss on training batch is 0.00359221.
After 21439 training step(s), loss on training batch is 0.00362495.
After 21440 training step(s), loss on training batch is 0.00458105.
After 21441 training step(s), loss on training batch is 0.0039099.
After 21442 training step(s), loss on training batch is 0.00367257.
After 21443 training step(s), loss on training batch is 0.00382041.
After 21444 training step(s), loss on training batch is 0.00358061.
After 21445 training step(s), loss on training batch is 0.00386888.
After 21446 training step(s), loss on training batch is 0.00337862.
After 21447 training step(s), loss on training batch is 0.00372706.
After 21448 training step(s), loss on training batch is 0.00390137.
After 21449 training step(s), loss on training batch is 0.00355589.
After 21450 training step(s), loss on training batch is 0.0037372.
After 21451 training step(s), loss on training batch is 0.00379427.
After 21452 training step(s), loss on training batch is 0.00361305.
After 21453 training step(s), loss on training batch is 0.00348935.
After 21454 training step(s), loss on training batch is 0.00389881.
After 21455 training step(s), loss on training batch is 0.00347143.
After 21456 training step(s), loss on training batch is 0.00398841.
After 21457 training step(s), loss on training batch is 0.0039352.
After 21458 training step(s), loss on training batch is 0.00361025.
After 21459 training step(s), loss on training batch is 0.00361163.
After 21460 training step(s), loss on training batch is 0.00337117.
After 21461 training step(s), loss on training batch is 0.00337266.
After 21462 training step(s), loss on training batch is 0.00358973.
After 21463 training step(s), loss on training batch is 0.00341057.
After 21464 training step(s), loss on training batch is 0.003514.
After 21465 training step(s), loss on training batch is 0.00398517.
After 21466 training step(s), loss on training batch is 0.00333828.
After 21467 training step(s), loss on training batch is 0.00388788.
After 21468 training step(s), loss on training batch is 0.00348637.
After 21469 training step(s), loss on training batch is 0.00358317.
After 21470 training step(s), loss on training batch is 0.00361459.
After 21471 training step(s), loss on training batch is 0.00352945.
After 21472 training step(s), loss on training batch is 0.00358265.
After 21473 training step(s), loss on training batch is 0.00371321.
After 21474 training step(s), loss on training batch is 0.00348496.
After 21475 training step(s), loss on training batch is 0.00332216.
After 21476 training step(s), loss on training batch is 0.00340656.
After 21477 training step(s), loss on training batch is 0.00375668.
After 21478 training step(s), loss on training batch is 0.00363982.
After 21479 training step(s), loss on training batch is 0.00350859.
After 21480 training step(s), loss on training batch is 0.00361292.
After 21481 training step(s), loss on training batch is 0.00362538.
After 21482 training step(s), loss on training batch is 0.00382528.
After 21483 training step(s), loss on training batch is 0.00355682.
After 21484 training step(s), loss on training batch is 0.00372784.
After 21485 training step(s), loss on training batch is 0.00352733.
After 21486 training step(s), loss on training batch is 0.00353363.
After 21487 training step(s), loss on training batch is 0.00334227.
After 21488 training step(s), loss on training batch is 0.00373567.
After 21489 training step(s), loss on training batch is 0.00385073.
After 21490 training step(s), loss on training batch is 0.00390829.
After 21491 training step(s), loss on training batch is 0.00351363.
After 21492 training step(s), loss on training batch is 0.00354694.
After 21493 training step(s), loss on training batch is 0.00380917.
After 21494 training step(s), loss on training batch is 0.00357274.
After 21495 training step(s), loss on training batch is 0.00360927.
After 21496 training step(s), loss on training batch is 0.00398305.
After 21497 training step(s), loss on training batch is 0.0039809.
After 21498 training step(s), loss on training batch is 0.00386443.
After 21499 training step(s), loss on training batch is 0.00345789.
After 21500 training step(s), loss on training batch is 0.00372513.
After 21501 training step(s), loss on training batch is 0.00335388.
After 21502 training step(s), loss on training batch is 0.00349693.
After 21503 training step(s), loss on training batch is 0.00408785.
After 21504 training step(s), loss on training batch is 0.00357212.
After 21505 training step(s), loss on training batch is 0.0035128.
After 21506 training step(s), loss on training batch is 0.00386211.
After 21507 training step(s), loss on training batch is 0.00390322.
After 21508 training step(s), loss on training batch is 0.00411612.
After 21509 training step(s), loss on training batch is 0.00390118.
After 21510 training step(s), loss on training batch is 0.00453469.
After 21511 training step(s), loss on training batch is 0.0039393.
After 21512 training step(s), loss on training batch is 0.00345085.
After 21513 training step(s), loss on training batch is 0.0035565.
After 21514 training step(s), loss on training batch is 0.00378556.
After 21515 training step(s), loss on training batch is 0.00415143.
After 21516 training step(s), loss on training batch is 0.00409676.
After 21517 training step(s), loss on training batch is 0.00385676.
After 21518 training step(s), loss on training batch is 0.00353562.
After 21519 training step(s), loss on training batch is 0.00388902.
After 21520 training step(s), loss on training batch is 0.00422437.
After 21521 training step(s), loss on training batch is 0.0035014.
After 21522 training step(s), loss on training batch is 0.0038032.
After 21523 training step(s), loss on training batch is 0.00355282.
After 21524 training step(s), loss on training batch is 0.00343062.
After 21525 training step(s), loss on training batch is 0.00346532.
After 21526 training step(s), loss on training batch is 0.00376765.
After 21527 training step(s), loss on training batch is 0.00372468.
After 21528 training step(s), loss on training batch is 0.00387646.
After 21529 training step(s), loss on training batch is 0.00329624.
After 21530 training step(s), loss on training batch is 0.00367789.
After 21531 training step(s), loss on training batch is 0.00347828.
After 21532 training step(s), loss on training batch is 0.00344575.
After 21533 training step(s), loss on training batch is 0.00380101.
After 21534 training step(s), loss on training batch is 0.00423477.
After 21535 training step(s), loss on training batch is 0.00376982.
After 21536 training step(s), loss on training batch is 0.00398681.
After 21537 training step(s), loss on training batch is 0.00388915.
After 21538 training step(s), loss on training batch is 0.00383158.
After 21539 training step(s), loss on training batch is 0.00376268.
After 21540 training step(s), loss on training batch is 0.00368099.
After 21541 training step(s), loss on training batch is 0.00418515.
After 21542 training step(s), loss on training batch is 0.00354417.
After 21543 training step(s), loss on training batch is 0.00354342.
After 21544 training step(s), loss on training batch is 0.00358292.
After 21545 training step(s), loss on training batch is 0.00363992.
After 21546 training step(s), loss on training batch is 0.00361985.
After 21547 training step(s), loss on training batch is 0.00373262.
After 21548 training step(s), loss on training batch is 0.00374331.
After 21549 training step(s), loss on training batch is 0.0051101.
After 21550 training step(s), loss on training batch is 0.00328224.
After 21551 training step(s), loss on training batch is 0.00374104.
After 21552 training step(s), loss on training batch is 0.00364774.
After 21553 training step(s), loss on training batch is 0.00411073.
After 21554 training step(s), loss on training batch is 0.00376169.
After 21555 training step(s), loss on training batch is 0.00358424.
After 21556 training step(s), loss on training batch is 0.00397988.
After 21557 training step(s), loss on training batch is 0.00359307.
After 21558 training step(s), loss on training batch is 0.00356422.
After 21559 training step(s), loss on training batch is 0.0037978.
After 21560 training step(s), loss on training batch is 0.00409154.
After 21561 training step(s), loss on training batch is 0.00392971.
After 21562 training step(s), loss on training batch is 0.00380138.
After 21563 training step(s), loss on training batch is 0.00349731.
After 21564 training step(s), loss on training batch is 0.00379674.
After 21565 training step(s), loss on training batch is 0.00364597.
After 21566 training step(s), loss on training batch is 0.00355931.
After 21567 training step(s), loss on training batch is 0.00353183.
After 21568 training step(s), loss on training batch is 0.0034634.
After 21569 training step(s), loss on training batch is 0.0041746.
After 21570 training step(s), loss on training batch is 0.00408647.
After 21571 training step(s), loss on training batch is 0.00459185.
After 21572 training step(s), loss on training batch is 0.00371205.
After 21573 training step(s), loss on training batch is 0.00365834.
After 21574 training step(s), loss on training batch is 0.00328586.
After 21575 training step(s), loss on training batch is 0.0035802.
After 21576 training step(s), loss on training batch is 0.00349441.
After 21577 training step(s), loss on training batch is 0.00352263.
After 21578 training step(s), loss on training batch is 0.00393988.
After 21579 training step(s), loss on training batch is 0.00402334.
After 21580 training step(s), loss on training batch is 0.00379923.
After 21581 training step(s), loss on training batch is 0.00357686.
After 21582 training step(s), loss on training batch is 0.00349427.
After 21583 training step(s), loss on training batch is 0.00397307.
After 21584 training step(s), loss on training batch is 0.00413484.
After 21585 training step(s), loss on training batch is 0.00347884.
After 21586 training step(s), loss on training batch is 0.00357725.
After 21587 training step(s), loss on training batch is 0.00346508.
After 21588 training step(s), loss on training batch is 0.00362856.
After 21589 training step(s), loss on training batch is 0.00416328.
After 21590 training step(s), loss on training batch is 0.00429269.
After 21591 training step(s), loss on training batch is 0.00361408.
After 21592 training step(s), loss on training batch is 0.00418203.
After 21593 training step(s), loss on training batch is 0.00329973.
After 21594 training step(s), loss on training batch is 0.00575871.
After 21595 training step(s), loss on training batch is 0.00346065.
After 21596 training step(s), loss on training batch is 0.00376033.
After 21597 training step(s), loss on training batch is 0.00367002.
After 21598 training step(s), loss on training batch is 0.00341679.
After 21599 training step(s), loss on training batch is 0.00462011.
After 21600 training step(s), loss on training batch is 0.00362158.
After 21601 training step(s), loss on training batch is 0.00378676.
After 21602 training step(s), loss on training batch is 0.00336907.
After 21603 training step(s), loss on training batch is 0.00390135.
After 21604 training step(s), loss on training batch is 0.00360251.
After 21605 training step(s), loss on training batch is 0.00386339.
After 21606 training step(s), loss on training batch is 0.0043873.
After 21607 training step(s), loss on training batch is 0.00384944.
After 21608 training step(s), loss on training batch is 0.00381059.
After 21609 training step(s), loss on training batch is 0.00359233.
After 21610 training step(s), loss on training batch is 0.00378643.
After 21611 training step(s), loss on training batch is 0.00353906.
After 21612 training step(s), loss on training batch is 0.00385487.
After 21613 training step(s), loss on training batch is 0.00373712.
After 21614 training step(s), loss on training batch is 0.0035412.
After 21615 training step(s), loss on training batch is 0.00353961.
After 21616 training step(s), loss on training batch is 0.00398234.
After 21617 training step(s), loss on training batch is 0.00388758.
After 21618 training step(s), loss on training batch is 0.00375795.
After 21619 training step(s), loss on training batch is 0.00353292.
After 21620 training step(s), loss on training batch is 0.00401924.
After 21621 training step(s), loss on training batch is 0.00356275.
After 21622 training step(s), loss on training batch is 0.0037455.
After 21623 training step(s), loss on training batch is 0.00347796.
After 21624 training step(s), loss on training batch is 0.00360245.
After 21625 training step(s), loss on training batch is 0.00351439.
After 21626 training step(s), loss on training batch is 0.0038581.
After 21627 training step(s), loss on training batch is 0.003517.
After 21628 training step(s), loss on training batch is 0.0037961.
After 21629 training step(s), loss on training batch is 0.00385862.
After 21630 training step(s), loss on training batch is 0.00355493.
After 21631 training step(s), loss on training batch is 0.00378628.
After 21632 training step(s), loss on training batch is 0.00377808.
After 21633 training step(s), loss on training batch is 0.00392009.
After 21634 training step(s), loss on training batch is 0.00344404.
After 21635 training step(s), loss on training batch is 0.00443972.
After 21636 training step(s), loss on training batch is 0.00416407.
After 21637 training step(s), loss on training batch is 0.00426826.
After 21638 training step(s), loss on training batch is 0.00447258.
After 21639 training step(s), loss on training batch is 0.00406065.
After 21640 training step(s), loss on training batch is 0.00376849.
After 21641 training step(s), loss on training batch is 0.00389349.
After 21642 training step(s), loss on training batch is 0.00349763.
After 21643 training step(s), loss on training batch is 0.00363292.
After 21644 training step(s), loss on training batch is 0.00389912.
After 21645 training step(s), loss on training batch is 0.00361088.
After 21646 training step(s), loss on training batch is 0.00343345.
After 21647 training step(s), loss on training batch is 0.00376576.
After 21648 training step(s), loss on training batch is 0.00397993.
After 21649 training step(s), loss on training batch is 0.00364777.
After 21650 training step(s), loss on training batch is 0.00377122.
After 21651 training step(s), loss on training batch is 0.00372966.
After 21652 training step(s), loss on training batch is 0.00351956.
After 21653 training step(s), loss on training batch is 0.00354015.
After 21654 training step(s), loss on training batch is 0.003486.
After 21655 training step(s), loss on training batch is 0.00379746.
After 21656 training step(s), loss on training batch is 0.00404565.
After 21657 training step(s), loss on training batch is 0.00354242.
After 21658 training step(s), loss on training batch is 0.00356715.
After 21659 training step(s), loss on training batch is 0.00355999.
After 21660 training step(s), loss on training batch is 0.00363195.
After 21661 training step(s), loss on training batch is 0.0035098.
After 21662 training step(s), loss on training batch is 0.00344294.
After 21663 training step(s), loss on training batch is 0.00368424.
After 21664 training step(s), loss on training batch is 0.00352422.
After 21665 training step(s), loss on training batch is 0.00377046.
After 21666 training step(s), loss on training batch is 0.00405228.
After 21667 training step(s), loss on training batch is 0.00355994.
After 21668 training step(s), loss on training batch is 0.00354544.
After 21669 training step(s), loss on training batch is 0.00383614.
After 21670 training step(s), loss on training batch is 0.00357874.
After 21671 training step(s), loss on training batch is 0.00349976.
After 21672 training step(s), loss on training batch is 0.00370604.
After 21673 training step(s), loss on training batch is 0.00354615.
After 21674 training step(s), loss on training batch is 0.00440221.
After 21675 training step(s), loss on training batch is 0.0035698.
After 21676 training step(s), loss on training batch is 0.00351693.
After 21677 training step(s), loss on training batch is 0.0035198.
After 21678 training step(s), loss on training batch is 0.00360985.
After 21679 training step(s), loss on training batch is 0.0036105.
After 21680 training step(s), loss on training batch is 0.003986.
After 21681 training step(s), loss on training batch is 0.00416817.
After 21682 training step(s), loss on training batch is 0.00340357.
After 21683 training step(s), loss on training batch is 0.00366354.
After 21684 training step(s), loss on training batch is 0.00365403.
After 21685 training step(s), loss on training batch is 0.00388719.
After 21686 training step(s), loss on training batch is 0.00405922.
After 21687 training step(s), loss on training batch is 0.00408792.
After 21688 training step(s), loss on training batch is 0.00426802.
After 21689 training step(s), loss on training batch is 0.00441624.
After 21690 training step(s), loss on training batch is 0.00388897.
After 21691 training step(s), loss on training batch is 0.00325908.
After 21692 training step(s), loss on training batch is 0.00356515.
After 21693 training step(s), loss on training batch is 0.00361673.
After 21694 training step(s), loss on training batch is 0.00372117.
After 21695 training step(s), loss on training batch is 0.00381993.
After 21696 training step(s), loss on training batch is 0.00375002.
After 21697 training step(s), loss on training batch is 0.00356524.
After 21698 training step(s), loss on training batch is 0.00337458.
After 21699 training step(s), loss on training batch is 0.00419669.
After 21700 training step(s), loss on training batch is 0.0033765.
After 21701 training step(s), loss on training batch is 0.00435553.
After 21702 training step(s), loss on training batch is 0.00445713.
After 21703 training step(s), loss on training batch is 0.00386471.
After 21704 training step(s), loss on training batch is 0.00351569.
After 21705 training step(s), loss on training batch is 0.00364256.
After 21706 training step(s), loss on training batch is 0.00397935.
After 21707 training step(s), loss on training batch is 0.00388128.
After 21708 training step(s), loss on training batch is 0.00359901.
After 21709 training step(s), loss on training batch is 0.00353252.
After 21710 training step(s), loss on training batch is 0.00380664.
After 21711 training step(s), loss on training batch is 0.00342529.
After 21712 training step(s), loss on training batch is 0.00415044.
After 21713 training step(s), loss on training batch is 0.00435268.
After 21714 training step(s), loss on training batch is 0.00390659.
After 21715 training step(s), loss on training batch is 0.00381161.
After 21716 training step(s), loss on training batch is 0.00392982.
After 21717 training step(s), loss on training batch is 0.0041736.
After 21718 training step(s), loss on training batch is 0.00366054.
After 21719 training step(s), loss on training batch is 0.00514331.
After 21720 training step(s), loss on training batch is 0.00372685.
After 21721 training step(s), loss on training batch is 0.00351492.
After 21722 training step(s), loss on training batch is 0.00383873.
After 21723 training step(s), loss on training batch is 0.00345493.
After 21724 training step(s), loss on training batch is 0.00334627.
After 21725 training step(s), loss on training batch is 0.00395754.
After 21726 training step(s), loss on training batch is 0.00351358.
After 21727 training step(s), loss on training batch is 0.0036493.
After 21728 training step(s), loss on training batch is 0.00365725.
After 21729 training step(s), loss on training batch is 0.00349713.
After 21730 training step(s), loss on training batch is 0.00350823.
After 21731 training step(s), loss on training batch is 0.00360006.
After 21732 training step(s), loss on training batch is 0.00332667.
After 21733 training step(s), loss on training batch is 0.00352877.
After 21734 training step(s), loss on training batch is 0.00349819.
After 21735 training step(s), loss on training batch is 0.00450562.
After 21736 training step(s), loss on training batch is 0.00336149.
After 21737 training step(s), loss on training batch is 0.00362288.
After 21738 training step(s), loss on training batch is 0.00338208.
After 21739 training step(s), loss on training batch is 0.00354657.
After 21740 training step(s), loss on training batch is 0.00427647.
After 21741 training step(s), loss on training batch is 0.00360572.
After 21742 training step(s), loss on training batch is 0.00360309.
After 21743 training step(s), loss on training batch is 0.00340514.
After 21744 training step(s), loss on training batch is 0.00386336.
After 21745 training step(s), loss on training batch is 0.00355081.
After 21746 training step(s), loss on training batch is 0.00349729.
After 21747 training step(s), loss on training batch is 0.00355426.
After 21748 training step(s), loss on training batch is 0.00325135.
After 21749 training step(s), loss on training batch is 0.00346642.
After 21750 training step(s), loss on training batch is 0.00342587.
After 21751 training step(s), loss on training batch is 0.00390153.
After 21752 training step(s), loss on training batch is 0.00338995.
After 21753 training step(s), loss on training batch is 0.00344697.
After 21754 training step(s), loss on training batch is 0.00379894.
After 21755 training step(s), loss on training batch is 0.0037867.
After 21756 training step(s), loss on training batch is 0.00369667.
After 21757 training step(s), loss on training batch is 0.00385382.
After 21758 training step(s), loss on training batch is 0.00355266.
After 21759 training step(s), loss on training batch is 0.00456524.
After 21760 training step(s), loss on training batch is 0.0035671.
After 21761 training step(s), loss on training batch is 0.00418582.
After 21762 training step(s), loss on training batch is 0.00351845.
After 21763 training step(s), loss on training batch is 0.00389007.
After 21764 training step(s), loss on training batch is 0.00367349.
After 21765 training step(s), loss on training batch is 0.00419654.
After 21766 training step(s), loss on training batch is 0.00355391.
After 21767 training step(s), loss on training batch is 0.00360927.
After 21768 training step(s), loss on training batch is 0.00433129.
After 21769 training step(s), loss on training batch is 0.00395566.
After 21770 training step(s), loss on training batch is 0.00377437.
After 21771 training step(s), loss on training batch is 0.00375214.
After 21772 training step(s), loss on training batch is 0.00400952.
After 21773 training step(s), loss on training batch is 0.00387377.
After 21774 training step(s), loss on training batch is 0.0038413.
After 21775 training step(s), loss on training batch is 0.00344257.
After 21776 training step(s), loss on training batch is 0.00325471.
After 21777 training step(s), loss on training batch is 0.00384372.
After 21778 training step(s), loss on training batch is 0.00369511.
After 21779 training step(s), loss on training batch is 0.00376837.
After 21780 training step(s), loss on training batch is 0.00348533.
After 21781 training step(s), loss on training batch is 0.00358664.
After 21782 training step(s), loss on training batch is 0.00419246.
After 21783 training step(s), loss on training batch is 0.00351597.
After 21784 training step(s), loss on training batch is 0.0036093.
After 21785 training step(s), loss on training batch is 0.00401693.
After 21786 training step(s), loss on training batch is 0.00359652.
After 21787 training step(s), loss on training batch is 0.00376129.
After 21788 training step(s), loss on training batch is 0.00358743.
After 21789 training step(s), loss on training batch is 0.00413971.
After 21790 training step(s), loss on training batch is 0.00368927.
After 21791 training step(s), loss on training batch is 0.00374313.
After 21792 training step(s), loss on training batch is 0.00375716.
After 21793 training step(s), loss on training batch is 0.00412444.
After 21794 training step(s), loss on training batch is 0.003694.
After 21795 training step(s), loss on training batch is 0.00344186.
After 21796 training step(s), loss on training batch is 0.00417549.
After 21797 training step(s), loss on training batch is 0.00387837.
After 21798 training step(s), loss on training batch is 0.00337864.
After 21799 training step(s), loss on training batch is 0.00385568.
After 21800 training step(s), loss on training batch is 0.00371838.
After 21801 training step(s), loss on training batch is 0.00353545.
After 21802 training step(s), loss on training batch is 0.00370628.
After 21803 training step(s), loss on training batch is 0.00397228.
After 21804 training step(s), loss on training batch is 0.00352689.
After 21805 training step(s), loss on training batch is 0.00395878.
After 21806 training step(s), loss on training batch is 0.003667.
After 21807 training step(s), loss on training batch is 0.00333833.
After 21808 training step(s), loss on training batch is 0.00335997.
After 21809 training step(s), loss on training batch is 0.00345333.
After 21810 training step(s), loss on training batch is 0.00354728.
After 21811 training step(s), loss on training batch is 0.0035998.
After 21812 training step(s), loss on training batch is 0.00412877.
After 21813 training step(s), loss on training batch is 0.00458072.
After 21814 training step(s), loss on training batch is 0.00374497.
After 21815 training step(s), loss on training batch is 0.00375149.
After 21816 training step(s), loss on training batch is 0.00372471.
After 21817 training step(s), loss on training batch is 0.00367185.
After 21818 training step(s), loss on training batch is 0.00354775.
After 21819 training step(s), loss on training batch is 0.0041713.
After 21820 training step(s), loss on training batch is 0.00372249.
After 21821 training step(s), loss on training batch is 0.00363014.
After 21822 training step(s), loss on training batch is 0.00380153.
After 21823 training step(s), loss on training batch is 0.00349599.
After 21824 training step(s), loss on training batch is 0.00381878.
After 21825 training step(s), loss on training batch is 0.00333886.
After 21826 training step(s), loss on training batch is 0.00421011.
After 21827 training step(s), loss on training batch is 0.00389171.
After 21828 training step(s), loss on training batch is 0.00369261.
After 21829 training step(s), loss on training batch is 0.00367728.
After 21830 training step(s), loss on training batch is 0.00343973.
After 21831 training step(s), loss on training batch is 0.00346313.
After 21832 training step(s), loss on training batch is 0.00336402.
After 21833 training step(s), loss on training batch is 0.00380823.
After 21834 training step(s), loss on training batch is 0.00350101.
After 21835 training step(s), loss on training batch is 0.00346286.
After 21836 training step(s), loss on training batch is 0.00350216.
After 21837 training step(s), loss on training batch is 0.00343467.
After 21838 training step(s), loss on training batch is 0.0042268.
After 21839 training step(s), loss on training batch is 0.00382465.
After 21840 training step(s), loss on training batch is 0.00353196.
After 21841 training step(s), loss on training batch is 0.0035916.
After 21842 training step(s), loss on training batch is 0.00341752.
After 21843 training step(s), loss on training batch is 0.00351917.
After 21844 training step(s), loss on training batch is 0.00362224.
After 21845 training step(s), loss on training batch is 0.00373536.
After 21846 training step(s), loss on training batch is 0.00375463.
After 21847 training step(s), loss on training batch is 0.00374886.
After 21848 training step(s), loss on training batch is 0.00434675.
After 21849 training step(s), loss on training batch is 0.00327632.
After 21850 training step(s), loss on training batch is 0.00370591.
After 21851 training step(s), loss on training batch is 0.00341376.
After 21852 training step(s), loss on training batch is 0.00332575.
After 21853 training step(s), loss on training batch is 0.00346163.
After 21854 training step(s), loss on training batch is 0.00357739.
After 21855 training step(s), loss on training batch is 0.0034526.
After 21856 training step(s), loss on training batch is 0.00375981.
After 21857 training step(s), loss on training batch is 0.00369368.
After 21858 training step(s), loss on training batch is 0.00356032.
After 21859 training step(s), loss on training batch is 0.00545747.
After 21860 training step(s), loss on training batch is 0.00377143.
After 21861 training step(s), loss on training batch is 0.00328618.
After 21862 training step(s), loss on training batch is 0.00376657.
After 21863 training step(s), loss on training batch is 0.0033541.
After 21864 training step(s), loss on training batch is 0.00371532.
After 21865 training step(s), loss on training batch is 0.00410781.
After 21866 training step(s), loss on training batch is 0.00373188.
After 21867 training step(s), loss on training batch is 0.00406916.
After 21868 training step(s), loss on training batch is 0.00405308.
After 21869 training step(s), loss on training batch is 0.00449816.
After 21870 training step(s), loss on training batch is 0.00376996.
After 21871 training step(s), loss on training batch is 0.00344526.
After 21872 training step(s), loss on training batch is 0.00405803.
After 21873 training step(s), loss on training batch is 0.00362759.
After 21874 training step(s), loss on training batch is 0.00441945.
After 21875 training step(s), loss on training batch is 0.00381982.
After 21876 training step(s), loss on training batch is 0.00355924.
After 21877 training step(s), loss on training batch is 0.00373626.
After 21878 training step(s), loss on training batch is 0.00405277.
After 21879 training step(s), loss on training batch is 0.0042012.
After 21880 training step(s), loss on training batch is 0.0036772.
After 21881 training step(s), loss on training batch is 0.00347386.
After 21882 training step(s), loss on training batch is 0.00359316.
After 21883 training step(s), loss on training batch is 0.00355209.
After 21884 training step(s), loss on training batch is 0.00356409.
After 21885 training step(s), loss on training batch is 0.00358182.
After 21886 training step(s), loss on training batch is 0.00389631.
After 21887 training step(s), loss on training batch is 0.00353233.
After 21888 training step(s), loss on training batch is 0.00353728.
After 21889 training step(s), loss on training batch is 0.00375358.
After 21890 training step(s), loss on training batch is 0.00409291.
After 21891 training step(s), loss on training batch is 0.00328463.
After 21892 training step(s), loss on training batch is 0.00361978.
After 21893 training step(s), loss on training batch is 0.00353013.
After 21894 training step(s), loss on training batch is 0.00362835.
After 21895 training step(s), loss on training batch is 0.00349415.
After 21896 training step(s), loss on training batch is 0.00382516.
After 21897 training step(s), loss on training batch is 0.00358593.
After 21898 training step(s), loss on training batch is 0.00359718.
After 21899 training step(s), loss on training batch is 0.00396207.
After 21900 training step(s), loss on training batch is 0.00382072.
After 21901 training step(s), loss on training batch is 0.00351044.
After 21902 training step(s), loss on training batch is 0.00356324.
After 21903 training step(s), loss on training batch is 0.00357689.
After 21904 training step(s), loss on training batch is 0.00394848.
After 21905 training step(s), loss on training batch is 0.00350955.
After 21906 training step(s), loss on training batch is 0.00528475.
After 21907 training step(s), loss on training batch is 0.00368315.
After 21908 training step(s), loss on training batch is 0.00368462.
After 21909 training step(s), loss on training batch is 0.00360733.
After 21910 training step(s), loss on training batch is 0.00539617.
After 21911 training step(s), loss on training batch is 0.0033908.
After 21912 training step(s), loss on training batch is 0.00379103.
After 21913 training step(s), loss on training batch is 0.00353642.
After 21914 training step(s), loss on training batch is 0.00534512.
After 21915 training step(s), loss on training batch is 0.00368079.
After 21916 training step(s), loss on training batch is 0.00362826.
After 21917 training step(s), loss on training batch is 0.00407486.
After 21918 training step(s), loss on training batch is 0.00364937.
After 21919 training step(s), loss on training batch is 0.00369551.
After 21920 training step(s), loss on training batch is 0.00365718.
After 21921 training step(s), loss on training batch is 0.00322737.
After 21922 training step(s), loss on training batch is 0.00363493.
After 21923 training step(s), loss on training batch is 0.00383994.
After 21924 training step(s), loss on training batch is 0.00349243.
After 21925 training step(s), loss on training batch is 0.00358957.
After 21926 training step(s), loss on training batch is 0.00358109.
After 21927 training step(s), loss on training batch is 0.00353605.
After 21928 training step(s), loss on training batch is 0.00345754.
After 21929 training step(s), loss on training batch is 0.00355098.
After 21930 training step(s), loss on training batch is 0.00374694.
After 21931 training step(s), loss on training batch is 0.00332005.
After 21932 training step(s), loss on training batch is 0.00334933.
After 21933 training step(s), loss on training batch is 0.00385358.
After 21934 training step(s), loss on training batch is 0.00400764.
After 21935 training step(s), loss on training batch is 0.00376378.
After 21936 training step(s), loss on training batch is 0.00395752.
After 21937 training step(s), loss on training batch is 0.00373524.
After 21938 training step(s), loss on training batch is 0.00335021.
After 21939 training step(s), loss on training batch is 0.00377316.
After 21940 training step(s), loss on training batch is 0.00390251.
After 21941 training step(s), loss on training batch is 0.00412981.
After 21942 training step(s), loss on training batch is 0.00378737.
After 21943 training step(s), loss on training batch is 0.00400291.
After 21944 training step(s), loss on training batch is 0.00350718.
After 21945 training step(s), loss on training batch is 0.00433243.
After 21946 training step(s), loss on training batch is 0.00376847.
After 21947 training step(s), loss on training batch is 0.00360357.
After 21948 training step(s), loss on training batch is 0.00352486.
After 21949 training step(s), loss on training batch is 0.00353903.
After 21950 training step(s), loss on training batch is 0.00376106.
After 21951 training step(s), loss on training batch is 0.00351094.
After 21952 training step(s), loss on training batch is 0.00479339.
After 21953 training step(s), loss on training batch is 0.00394641.
After 21954 training step(s), loss on training batch is 0.00366781.
After 21955 training step(s), loss on training batch is 0.00364895.
After 21956 training step(s), loss on training batch is 0.00394619.
After 21957 training step(s), loss on training batch is 0.00351092.
After 21958 training step(s), loss on training batch is 0.00365471.
After 21959 training step(s), loss on training batch is 0.00351493.
After 21960 training step(s), loss on training batch is 0.00385117.
After 21961 training step(s), loss on training batch is 0.00359134.
After 21962 training step(s), loss on training batch is 0.00345473.
After 21963 training step(s), loss on training batch is 0.00394337.
After 21964 training step(s), loss on training batch is 0.00339252.
After 21965 training step(s), loss on training batch is 0.00336794.
After 21966 training step(s), loss on training batch is 0.00329957.
After 21967 training step(s), loss on training batch is 0.00362835.
After 21968 training step(s), loss on training batch is 0.00351904.
After 21969 training step(s), loss on training batch is 0.0039497.
After 21970 training step(s), loss on training batch is 0.00414581.
After 21971 training step(s), loss on training batch is 0.0038069.
After 21972 training step(s), loss on training batch is 0.00396781.
After 21973 training step(s), loss on training batch is 0.00385439.
After 21974 training step(s), loss on training batch is 0.00347851.
After 21975 training step(s), loss on training batch is 0.00336992.
After 21976 training step(s), loss on training batch is 0.00376124.
After 21977 training step(s), loss on training batch is 0.00345808.
After 21978 training step(s), loss on training batch is 0.00424736.
After 21979 training step(s), loss on training batch is 0.00394737.
After 21980 training step(s), loss on training batch is 0.00347002.
After 21981 training step(s), loss on training batch is 0.00467923.
After 21982 training step(s), loss on training batch is 0.00373297.
After 21983 training step(s), loss on training batch is 0.00337265.
After 21984 training step(s), loss on training batch is 0.003596.
After 21985 training step(s), loss on training batch is 0.00445129.
After 21986 training step(s), loss on training batch is 0.00403988.
After 21987 training step(s), loss on training batch is 0.00334032.
After 21988 training step(s), loss on training batch is 0.00377305.
After 21989 training step(s), loss on training batch is 0.00364204.
After 21990 training step(s), loss on training batch is 0.00406237.
After 21991 training step(s), loss on training batch is 0.00380093.
After 21992 training step(s), loss on training batch is 0.00458911.
After 21993 training step(s), loss on training batch is 0.00394922.
After 21994 training step(s), loss on training batch is 0.00399503.
After 21995 training step(s), loss on training batch is 0.00356069.
After 21996 training step(s), loss on training batch is 0.0037237.
After 21997 training step(s), loss on training batch is 0.00377535.
After 21998 training step(s), loss on training batch is 0.00336616.
After 21999 training step(s), loss on training batch is 0.00385438.
After 22000 training step(s), loss on training batch is 0.00384599.
After 22001 training step(s), loss on training batch is 0.00358615.
After 22002 training step(s), loss on training batch is 0.00351673.
After 22003 training step(s), loss on training batch is 0.00414067.
After 22004 training step(s), loss on training batch is 0.00334395.
After 22005 training step(s), loss on training batch is 0.00391551.
After 22006 training step(s), loss on training batch is 0.00343162.
After 22007 training step(s), loss on training batch is 0.00361576.
After 22008 training step(s), loss on training batch is 0.00350613.
After 22009 training step(s), loss on training batch is 0.00362899.
After 22010 training step(s), loss on training batch is 0.00346532.
After 22011 training step(s), loss on training batch is 0.00375822.
After 22012 training step(s), loss on training batch is 0.00430048.
After 22013 training step(s), loss on training batch is 0.00399977.
After 22014 training step(s), loss on training batch is 0.00353895.
After 22015 training step(s), loss on training batch is 0.00386237.
After 22016 training step(s), loss on training batch is 0.00375644.
After 22017 training step(s), loss on training batch is 0.00351077.
After 22018 training step(s), loss on training batch is 0.00388322.
After 22019 training step(s), loss on training batch is 0.00341391.
After 22020 training step(s), loss on training batch is 0.00410796.
After 22021 training step(s), loss on training batch is 0.00406616.
After 22022 training step(s), loss on training batch is 0.00357032.
After 22023 training step(s), loss on training batch is 0.00339162.
After 22024 training step(s), loss on training batch is 0.00370622.
After 22025 training step(s), loss on training batch is 0.00358785.
After 22026 training step(s), loss on training batch is 0.00407378.
After 22027 training step(s), loss on training batch is 0.00399082.
After 22028 training step(s), loss on training batch is 0.00389326.
After 22029 training step(s), loss on training batch is 0.00361049.
After 22030 training step(s), loss on training batch is 0.00392445.
After 22031 training step(s), loss on training batch is 0.0036117.
After 22032 training step(s), loss on training batch is 0.00398067.
After 22033 training step(s), loss on training batch is 0.00356177.
After 22034 training step(s), loss on training batch is 0.00342833.
After 22035 training step(s), loss on training batch is 0.00357722.
After 22036 training step(s), loss on training batch is 0.00346916.
After 22037 training step(s), loss on training batch is 0.00402688.
After 22038 training step(s), loss on training batch is 0.00356173.
After 22039 training step(s), loss on training batch is 0.00376495.
After 22040 training step(s), loss on training batch is 0.00370516.
After 22041 training step(s), loss on training batch is 0.00378141.
After 22042 training step(s), loss on training batch is 0.00354498.
After 22043 training step(s), loss on training batch is 0.00357874.
After 22044 training step(s), loss on training batch is 0.00362465.
After 22045 training step(s), loss on training batch is 0.00363102.
After 22046 training step(s), loss on training batch is 0.00350676.
After 22047 training step(s), loss on training batch is 0.0040075.
After 22048 training step(s), loss on training batch is 0.00337451.
After 22049 training step(s), loss on training batch is 0.00378649.
After 22050 training step(s), loss on training batch is 0.00326504.
After 22051 training step(s), loss on training batch is 0.00381087.
After 22052 training step(s), loss on training batch is 0.00358809.
After 22053 training step(s), loss on training batch is 0.00407327.
After 22054 training step(s), loss on training batch is 0.00354945.
After 22055 training step(s), loss on training batch is 0.00362263.
After 22056 training step(s), loss on training batch is 0.00362066.
After 22057 training step(s), loss on training batch is 0.00355102.
After 22058 training step(s), loss on training batch is 0.00362835.
After 22059 training step(s), loss on training batch is 0.00378117.
After 22060 training step(s), loss on training batch is 0.00331428.
After 22061 training step(s), loss on training batch is 0.00375447.
After 22062 training step(s), loss on training batch is 0.00352433.
After 22063 training step(s), loss on training batch is 0.00378721.
After 22064 training step(s), loss on training batch is 0.00323411.
After 22065 training step(s), loss on training batch is 0.00325952.
After 22066 training step(s), loss on training batch is 0.00385151.
After 22067 training step(s), loss on training batch is 0.00373465.
After 22068 training step(s), loss on training batch is 0.00389748.
After 22069 training step(s), loss on training batch is 0.00470063.
After 22070 training step(s), loss on training batch is 0.00370925.
After 22071 training step(s), loss on training batch is 0.00393801.
After 22072 training step(s), loss on training batch is 0.00344305.
After 22073 training step(s), loss on training batch is 0.00338657.
After 22074 training step(s), loss on training batch is 0.00413939.
After 22075 training step(s), loss on training batch is 0.00342564.
After 22076 training step(s), loss on training batch is 0.00409981.
After 22077 training step(s), loss on training batch is 0.00359318.
After 22078 training step(s), loss on training batch is 0.00384701.
After 22079 training step(s), loss on training batch is 0.00394564.
After 22080 training step(s), loss on training batch is 0.0035558.
After 22081 training step(s), loss on training batch is 0.00383723.
After 22082 training step(s), loss on training batch is 0.00332262.
After 22083 training step(s), loss on training batch is 0.00360198.
After 22084 training step(s), loss on training batch is 0.00403184.
After 22085 training step(s), loss on training batch is 0.00356307.
After 22086 training step(s), loss on training batch is 0.0033845.
After 22087 training step(s), loss on training batch is 0.00335046.
After 22088 training step(s), loss on training batch is 0.00358909.
After 22089 training step(s), loss on training batch is 0.00362203.
After 22090 training step(s), loss on training batch is 0.00392545.
After 22091 training step(s), loss on training batch is 0.00410781.
After 22092 training step(s), loss on training batch is 0.00352083.
After 22093 training step(s), loss on training batch is 0.00366064.
After 22094 training step(s), loss on training batch is 0.00428728.
After 22095 training step(s), loss on training batch is 0.00431043.
After 22096 training step(s), loss on training batch is 0.00391705.
After 22097 training step(s), loss on training batch is 0.00343253.
After 22098 training step(s), loss on training batch is 0.00364722.
After 22099 training step(s), loss on training batch is 0.00367328.
After 22100 training step(s), loss on training batch is 0.00369905.
After 22101 training step(s), loss on training batch is 0.00368877.
After 22102 training step(s), loss on training batch is 0.00340779.
After 22103 training step(s), loss on training batch is 0.00329888.
After 22104 training step(s), loss on training batch is 0.00350184.
After 22105 training step(s), loss on training batch is 0.00408268.
After 22106 training step(s), loss on training batch is 0.00385068.
After 22107 training step(s), loss on training batch is 0.00394518.
After 22108 training step(s), loss on training batch is 0.00370489.
After 22109 training step(s), loss on training batch is 0.0033568.
After 22110 training step(s), loss on training batch is 0.0037537.
After 22111 training step(s), loss on training batch is 0.00393273.
After 22112 training step(s), loss on training batch is 0.00349968.
After 22113 training step(s), loss on training batch is 0.00353768.
After 22114 training step(s), loss on training batch is 0.00362144.
After 22115 training step(s), loss on training batch is 0.00371867.
After 22116 training step(s), loss on training batch is 0.00369451.
After 22117 training step(s), loss on training batch is 0.00390331.
After 22118 training step(s), loss on training batch is 0.00353856.
After 22119 training step(s), loss on training batch is 0.00336112.
After 22120 training step(s), loss on training batch is 0.00391557.
After 22121 training step(s), loss on training batch is 0.00405179.
After 22122 training step(s), loss on training batch is 0.00356147.
After 22123 training step(s), loss on training batch is 0.00349755.
After 22124 training step(s), loss on training batch is 0.0037019.
After 22125 training step(s), loss on training batch is 0.00347681.
After 22126 training step(s), loss on training batch is 0.0034338.
After 22127 training step(s), loss on training batch is 0.00365171.
After 22128 training step(s), loss on training batch is 0.00339964.
After 22129 training step(s), loss on training batch is 0.00371767.
After 22130 training step(s), loss on training batch is 0.00391167.
After 22131 training step(s), loss on training batch is 0.00348149.
After 22132 training step(s), loss on training batch is 0.00427874.
After 22133 training step(s), loss on training batch is 0.00380339.
After 22134 training step(s), loss on training batch is 0.0033169.
After 22135 training step(s), loss on training batch is 0.00403869.
After 22136 training step(s), loss on training batch is 0.00370844.
After 22137 training step(s), loss on training batch is 0.00341248.
After 22138 training step(s), loss on training batch is 0.0035522.
After 22139 training step(s), loss on training batch is 0.00352124.
After 22140 training step(s), loss on training batch is 0.00331757.
After 22141 training step(s), loss on training batch is 0.00346131.
After 22142 training step(s), loss on training batch is 0.00491426.
After 22143 training step(s), loss on training batch is 0.00389191.
After 22144 training step(s), loss on training batch is 0.00337796.
After 22145 training step(s), loss on training batch is 0.00383207.
After 22146 training step(s), loss on training batch is 0.00337986.
After 22147 training step(s), loss on training batch is 0.00423531.
After 22148 training step(s), loss on training batch is 0.00358036.
After 22149 training step(s), loss on training batch is 0.00410907.
After 22150 training step(s), loss on training batch is 0.00333291.
After 22151 training step(s), loss on training batch is 0.0047.
After 22152 training step(s), loss on training batch is 0.00358262.
After 22153 training step(s), loss on training batch is 0.00327791.
After 22154 training step(s), loss on training batch is 0.00360845.
After 22155 training step(s), loss on training batch is 0.00362124.
After 22156 training step(s), loss on training batch is 0.00334592.
After 22157 training step(s), loss on training batch is 0.00376857.
After 22158 training step(s), loss on training batch is 0.00385591.
After 22159 training step(s), loss on training batch is 0.00339678.
After 22160 training step(s), loss on training batch is 0.0033414.
After 22161 training step(s), loss on training batch is 0.00336735.
After 22162 training step(s), loss on training batch is 0.00442872.
After 22163 training step(s), loss on training batch is 0.00345216.
After 22164 training step(s), loss on training batch is 0.00349546.
After 22165 training step(s), loss on training batch is 0.00416565.
After 22166 training step(s), loss on training batch is 0.00337686.
After 22167 training step(s), loss on training batch is 0.00328686.
After 22168 training step(s), loss on training batch is 0.00338441.
After 22169 training step(s), loss on training batch is 0.00441714.
After 22170 training step(s), loss on training batch is 0.00423806.
After 22171 training step(s), loss on training batch is 0.0033278.
After 22172 training step(s), loss on training batch is 0.00409941.
After 22173 training step(s), loss on training batch is 0.00370471.
After 22174 training step(s), loss on training batch is 0.00360678.
After 22175 training step(s), loss on training batch is 0.00393311.
After 22176 training step(s), loss on training batch is 0.00359748.
After 22177 training step(s), loss on training batch is 0.00442469.
After 22178 training step(s), loss on training batch is 0.00383684.
After 22179 training step(s), loss on training batch is 0.00361753.
After 22180 training step(s), loss on training batch is 0.00338182.
After 22181 training step(s), loss on training batch is 0.0035891.
After 22182 training step(s), loss on training batch is 0.00410645.
After 22183 training step(s), loss on training batch is 0.00354123.
After 22184 training step(s), loss on training batch is 0.00346958.
After 22185 training step(s), loss on training batch is 0.00385901.
After 22186 training step(s), loss on training batch is 0.00381978.
After 22187 training step(s), loss on training batch is 0.00348569.
After 22188 training step(s), loss on training batch is 0.00353941.
After 22189 training step(s), loss on training batch is 0.00369598.
After 22190 training step(s), loss on training batch is 0.00353967.
After 22191 training step(s), loss on training batch is 0.00345275.
After 22192 training step(s), loss on training batch is 0.00386599.
After 22193 training step(s), loss on training batch is 0.00365658.
After 22194 training step(s), loss on training batch is 0.00350386.
After 22195 training step(s), loss on training batch is 0.00340249.
After 22196 training step(s), loss on training batch is 0.00339033.
After 22197 training step(s), loss on training batch is 0.00364744.
After 22198 training step(s), loss on training batch is 0.00407936.
After 22199 training step(s), loss on training batch is 0.00398677.
After 22200 training step(s), loss on training batch is 0.0039117.
After 22201 training step(s), loss on training batch is 0.00429969.
After 22202 training step(s), loss on training batch is 0.00417646.
After 22203 training step(s), loss on training batch is 0.00502309.
After 22204 training step(s), loss on training batch is 0.00339677.
After 22205 training step(s), loss on training batch is 0.00323658.
After 22206 training step(s), loss on training batch is 0.00346076.
After 22207 training step(s), loss on training batch is 0.00358464.
After 22208 training step(s), loss on training batch is 0.00428479.
After 22209 training step(s), loss on training batch is 0.00363378.
After 22210 training step(s), loss on training batch is 0.00391535.
After 22211 training step(s), loss on training batch is 0.00351609.
After 22212 training step(s), loss on training batch is 0.00374985.
After 22213 training step(s), loss on training batch is 0.00382232.
After 22214 training step(s), loss on training batch is 0.00354352.
After 22215 training step(s), loss on training batch is 0.00404744.
After 22216 training step(s), loss on training batch is 0.00343945.
After 22217 training step(s), loss on training batch is 0.00354243.
After 22218 training step(s), loss on training batch is 0.00348479.
After 22219 training step(s), loss on training batch is 0.00372164.
After 22220 training step(s), loss on training batch is 0.0036688.
After 22221 training step(s), loss on training batch is 0.00372825.
After 22222 training step(s), loss on training batch is 0.00341439.
After 22223 training step(s), loss on training batch is 0.00353712.
After 22224 training step(s), loss on training batch is 0.0038545.
After 22225 training step(s), loss on training batch is 0.0034979.
After 22226 training step(s), loss on training batch is 0.00379316.
After 22227 training step(s), loss on training batch is 0.00389607.
After 22228 training step(s), loss on training batch is 0.00331065.
After 22229 training step(s), loss on training batch is 0.00383151.
After 22230 training step(s), loss on training batch is 0.00321639.
After 22231 training step(s), loss on training batch is 0.00386514.
After 22232 training step(s), loss on training batch is 0.0032738.
After 22233 training step(s), loss on training batch is 0.0035002.
After 22234 training step(s), loss on training batch is 0.00435449.
After 22235 training step(s), loss on training batch is 0.00355093.
After 22236 training step(s), loss on training batch is 0.00379039.
After 22237 training step(s), loss on training batch is 0.0045717.
After 22238 training step(s), loss on training batch is 0.00360343.
After 22239 training step(s), loss on training batch is 0.00366482.
After 22240 training step(s), loss on training batch is 0.00481689.
After 22241 training step(s), loss on training batch is 0.00361091.
After 22242 training step(s), loss on training batch is 0.00361.
After 22243 training step(s), loss on training batch is 0.00365602.
After 22244 training step(s), loss on training batch is 0.00363262.
After 22245 training step(s), loss on training batch is 0.0034517.
After 22246 training step(s), loss on training batch is 0.00324012.
After 22247 training step(s), loss on training batch is 0.00340951.
After 22248 training step(s), loss on training batch is 0.00321214.
After 22249 training step(s), loss on training batch is 0.00451854.
After 22250 training step(s), loss on training batch is 0.00355987.
After 22251 training step(s), loss on training batch is 0.00373784.
After 22252 training step(s), loss on training batch is 0.00356025.
After 22253 training step(s), loss on training batch is 0.00342992.
After 22254 training step(s), loss on training batch is 0.00354927.
After 22255 training step(s), loss on training batch is 0.00355929.
After 22256 training step(s), loss on training batch is 0.00378829.
After 22257 training step(s), loss on training batch is 0.0035949.
After 22258 training step(s), loss on training batch is 0.0034721.
After 22259 training step(s), loss on training batch is 0.00378357.
After 22260 training step(s), loss on training batch is 0.00348553.
After 22261 training step(s), loss on training batch is 0.00345908.
After 22262 training step(s), loss on training batch is 0.00562094.
After 22263 training step(s), loss on training batch is 0.00350324.
After 22264 training step(s), loss on training batch is 0.00340868.
After 22265 training step(s), loss on training batch is 0.0039143.
After 22266 training step(s), loss on training batch is 0.00348101.
After 22267 training step(s), loss on training batch is 0.00359926.
After 22268 training step(s), loss on training batch is 0.00347246.
After 22269 training step(s), loss on training batch is 0.00388199.
After 22270 training step(s), loss on training batch is 0.0044736.
After 22271 training step(s), loss on training batch is 0.00385544.
After 22272 training step(s), loss on training batch is 0.00386453.
After 22273 training step(s), loss on training batch is 0.004334.
After 22274 training step(s), loss on training batch is 0.00355131.
After 22275 training step(s), loss on training batch is 0.00351892.
After 22276 training step(s), loss on training batch is 0.00385022.
After 22277 training step(s), loss on training batch is 0.00347398.
After 22278 training step(s), loss on training batch is 0.00383182.
After 22279 training step(s), loss on training batch is 0.00348816.
After 22280 training step(s), loss on training batch is 0.00337127.
After 22281 training step(s), loss on training batch is 0.00333879.
After 22282 training step(s), loss on training batch is 0.00352125.
After 22283 training step(s), loss on training batch is 0.00332661.
After 22284 training step(s), loss on training batch is 0.00374147.
After 22285 training step(s), loss on training batch is 0.00333808.
After 22286 training step(s), loss on training batch is 0.00373731.
After 22287 training step(s), loss on training batch is 0.00356448.
After 22288 training step(s), loss on training batch is 0.00393005.
After 22289 training step(s), loss on training batch is 0.00363903.
After 22290 training step(s), loss on training batch is 0.00344916.
After 22291 training step(s), loss on training batch is 0.0033159.
After 22292 training step(s), loss on training batch is 0.00369198.
After 22293 training step(s), loss on training batch is 0.00383903.
After 22294 training step(s), loss on training batch is 0.00342084.
After 22295 training step(s), loss on training batch is 0.00383102.
After 22296 training step(s), loss on training batch is 0.00362229.
After 22297 training step(s), loss on training batch is 0.00355045.
After 22298 training step(s), loss on training batch is 0.00328159.
After 22299 training step(s), loss on training batch is 0.00366197.
After 22300 training step(s), loss on training batch is 0.00342331.
After 22301 training step(s), loss on training batch is 0.0036642.
After 22302 training step(s), loss on training batch is 0.00353058.
After 22303 training step(s), loss on training batch is 0.00343038.
After 22304 training step(s), loss on training batch is 0.00459838.
After 22305 training step(s), loss on training batch is 0.00406874.
After 22306 training step(s), loss on training batch is 0.00370494.
After 22307 training step(s), loss on training batch is 0.0048976.
After 22308 training step(s), loss on training batch is 0.0032636.
After 22309 training step(s), loss on training batch is 0.00368313.
After 22310 training step(s), loss on training batch is 0.00384504.
After 22311 training step(s), loss on training batch is 0.00376327.
After 22312 training step(s), loss on training batch is 0.00345973.
After 22313 training step(s), loss on training batch is 0.00346382.
After 22314 training step(s), loss on training batch is 0.00342633.
After 22315 training step(s), loss on training batch is 0.0048164.
After 22316 training step(s), loss on training batch is 0.00342397.
After 22317 training step(s), loss on training batch is 0.00345974.
After 22318 training step(s), loss on training batch is 0.00341035.
After 22319 training step(s), loss on training batch is 0.00341085.
After 22320 training step(s), loss on training batch is 0.00351145.
After 22321 training step(s), loss on training batch is 0.00361907.
After 22322 training step(s), loss on training batch is 0.0034461.
After 22323 training step(s), loss on training batch is 0.00348319.
After 22324 training step(s), loss on training batch is 0.00486135.
After 22325 training step(s), loss on training batch is 0.00374943.
After 22326 training step(s), loss on training batch is 0.00420441.
After 22327 training step(s), loss on training batch is 0.00420885.
After 22328 training step(s), loss on training batch is 0.00379007.
After 22329 training step(s), loss on training batch is 0.00386269.
After 22330 training step(s), loss on training batch is 0.00342232.
After 22331 training step(s), loss on training batch is 0.00391829.
After 22332 training step(s), loss on training batch is 0.00403405.
After 22333 training step(s), loss on training batch is 0.00354841.
After 22334 training step(s), loss on training batch is 0.00377605.
After 22335 training step(s), loss on training batch is 0.00326553.
After 22336 training step(s), loss on training batch is 0.0037243.
After 22337 training step(s), loss on training batch is 0.00336161.
After 22338 training step(s), loss on training batch is 0.00344516.
After 22339 training step(s), loss on training batch is 0.00349534.
After 22340 training step(s), loss on training batch is 0.00337142.
After 22341 training step(s), loss on training batch is 0.00398995.
After 22342 training step(s), loss on training batch is 0.00368267.
After 22343 training step(s), loss on training batch is 0.00448748.
After 22344 training step(s), loss on training batch is 0.0032892.
After 22345 training step(s), loss on training batch is 0.00344587.
After 22346 training step(s), loss on training batch is 0.00330458.
After 22347 training step(s), loss on training batch is 0.00326303.
After 22348 training step(s), loss on training batch is 0.00339727.
After 22349 training step(s), loss on training batch is 0.0041219.
After 22350 training step(s), loss on training batch is 0.00410647.
After 22351 training step(s), loss on training batch is 0.00366727.
After 22352 training step(s), loss on training batch is 0.00372035.
After 22353 training step(s), loss on training batch is 0.00359074.
After 22354 training step(s), loss on training batch is 0.00349167.
After 22355 training step(s), loss on training batch is 0.00352847.
After 22356 training step(s), loss on training batch is 0.00379943.
After 22357 training step(s), loss on training batch is 0.00438579.
After 22358 training step(s), loss on training batch is 0.00358469.
After 22359 training step(s), loss on training batch is 0.0034683.
After 22360 training step(s), loss on training batch is 0.00364105.
After 22361 training step(s), loss on training batch is 0.00387823.
After 22362 training step(s), loss on training batch is 0.00382384.
After 22363 training step(s), loss on training batch is 0.00384243.
After 22364 training step(s), loss on training batch is 0.00331016.
After 22365 training step(s), loss on training batch is 0.00395901.
After 22366 training step(s), loss on training batch is 0.00393212.
After 22367 training step(s), loss on training batch is 0.00375751.
After 22368 training step(s), loss on training batch is 0.00365143.
After 22369 training step(s), loss on training batch is 0.00364216.
After 22370 training step(s), loss on training batch is 0.00395346.
After 22371 training step(s), loss on training batch is 0.00377779.
After 22372 training step(s), loss on training batch is 0.00366789.
After 22373 training step(s), loss on training batch is 0.00409218.
After 22374 training step(s), loss on training batch is 0.00383682.
After 22375 training step(s), loss on training batch is 0.00360552.
After 22376 training step(s), loss on training batch is 0.00418518.
After 22377 training step(s), loss on training batch is 0.0033199.
After 22378 training step(s), loss on training batch is 0.00354762.
After 22379 training step(s), loss on training batch is 0.00403025.
After 22380 training step(s), loss on training batch is 0.00337162.
After 22381 training step(s), loss on training batch is 0.00363182.
After 22382 training step(s), loss on training batch is 0.00374837.
After 22383 training step(s), loss on training batch is 0.00340361.
After 22384 training step(s), loss on training batch is 0.00351381.
After 22385 training step(s), loss on training batch is 0.00378245.
After 22386 training step(s), loss on training batch is 0.00349766.
After 22387 training step(s), loss on training batch is 0.0034129.
After 22388 training step(s), loss on training batch is 0.00371638.
After 22389 training step(s), loss on training batch is 0.00364931.
After 22390 training step(s), loss on training batch is 0.00390975.
After 22391 training step(s), loss on training batch is 0.00398261.
After 22392 training step(s), loss on training batch is 0.00345444.
After 22393 training step(s), loss on training batch is 0.00377952.
After 22394 training step(s), loss on training batch is 0.00350228.
After 22395 training step(s), loss on training batch is 0.00402111.
After 22396 training step(s), loss on training batch is 0.00370645.
After 22397 training step(s), loss on training batch is 0.00323659.
After 22398 training step(s), loss on training batch is 0.00374087.
After 22399 training step(s), loss on training batch is 0.0033468.
After 22400 training step(s), loss on training batch is 0.00332528.
After 22401 training step(s), loss on training batch is 0.00372537.
After 22402 training step(s), loss on training batch is 0.00362899.
After 22403 training step(s), loss on training batch is 0.00412727.
After 22404 training step(s), loss on training batch is 0.00381822.
After 22405 training step(s), loss on training batch is 0.00398199.
After 22406 training step(s), loss on training batch is 0.00364873.
After 22407 training step(s), loss on training batch is 0.00375655.
After 22408 training step(s), loss on training batch is 0.0041614.
After 22409 training step(s), loss on training batch is 0.00350985.
After 22410 training step(s), loss on training batch is 0.00328681.
After 22411 training step(s), loss on training batch is 0.00380286.
After 22412 training step(s), loss on training batch is 0.00338595.
After 22413 training step(s), loss on training batch is 0.0033754.
After 22414 training step(s), loss on training batch is 0.00363613.
After 22415 training step(s), loss on training batch is 0.00340614.
After 22416 training step(s), loss on training batch is 0.00382982.
After 22417 training step(s), loss on training batch is 0.00350425.
After 22418 training step(s), loss on training batch is 0.00364532.
After 22419 training step(s), loss on training batch is 0.00394004.
After 22420 training step(s), loss on training batch is 0.00376933.
After 22421 training step(s), loss on training batch is 0.00399661.
After 22422 training step(s), loss on training batch is 0.00375544.
After 22423 training step(s), loss on training batch is 0.00335397.
After 22424 training step(s), loss on training batch is 0.0034476.
After 22425 training step(s), loss on training batch is 0.00349249.
After 22426 training step(s), loss on training batch is 0.00398514.
After 22427 training step(s), loss on training batch is 0.00339229.
After 22428 training step(s), loss on training batch is 0.00361977.
After 22429 training step(s), loss on training batch is 0.0036011.
After 22430 training step(s), loss on training batch is 0.00384985.
After 22431 training step(s), loss on training batch is 0.00345811.
After 22432 training step(s), loss on training batch is 0.00559099.
After 22433 training step(s), loss on training batch is 0.00367693.
After 22434 training step(s), loss on training batch is 0.00336082.
After 22435 training step(s), loss on training batch is 0.00384419.
After 22436 training step(s), loss on training batch is 0.00348528.
After 22437 training step(s), loss on training batch is 0.00519987.
After 22438 training step(s), loss on training batch is 0.00360989.
After 22439 training step(s), loss on training batch is 0.00350368.
After 22440 training step(s), loss on training batch is 0.00330043.
After 22441 training step(s), loss on training batch is 0.00363386.
After 22442 training step(s), loss on training batch is 0.00335232.
After 22443 training step(s), loss on training batch is 0.00548363.
After 22444 training step(s), loss on training batch is 0.0033808.
After 22445 training step(s), loss on training batch is 0.00408216.
After 22446 training step(s), loss on training batch is 0.00428095.
After 22447 training step(s), loss on training batch is 0.00379991.
After 22448 training step(s), loss on training batch is 0.00318232.
After 22449 training step(s), loss on training batch is 0.00382697.
After 22450 training step(s), loss on training batch is 0.00388998.
After 22451 training step(s), loss on training batch is 0.00356688.
After 22452 training step(s), loss on training batch is 0.00362161.
After 22453 training step(s), loss on training batch is 0.00394096.
After 22454 training step(s), loss on training batch is 0.00468571.
After 22455 training step(s), loss on training batch is 0.00379906.
After 22456 training step(s), loss on training batch is 0.0037694.
After 22457 training step(s), loss on training batch is 0.0040344.
After 22458 training step(s), loss on training batch is 0.0035612.
After 22459 training step(s), loss on training batch is 0.00397439.
After 22460 training step(s), loss on training batch is 0.00385233.
After 22461 training step(s), loss on training batch is 0.00392243.
After 22462 training step(s), loss on training batch is 0.00350137.
After 22463 training step(s), loss on training batch is 0.00367485.
After 22464 training step(s), loss on training batch is 0.00345273.
After 22465 training step(s), loss on training batch is 0.0036069.
After 22466 training step(s), loss on training batch is 0.00389615.
After 22467 training step(s), loss on training batch is 0.0033503.
After 22468 training step(s), loss on training batch is 0.00443694.
After 22469 training step(s), loss on training batch is 0.00340444.
After 22470 training step(s), loss on training batch is 0.00379088.
After 22471 training step(s), loss on training batch is 0.00349243.
After 22472 training step(s), loss on training batch is 0.00407079.
After 22473 training step(s), loss on training batch is 0.00366918.
After 22474 training step(s), loss on training batch is 0.00353694.
After 22475 training step(s), loss on training batch is 0.00343915.
After 22476 training step(s), loss on training batch is 0.00374913.
After 22477 training step(s), loss on training batch is 0.00376201.
After 22478 training step(s), loss on training batch is 0.00339481.
After 22479 training step(s), loss on training batch is 0.00362667.
After 22480 training step(s), loss on training batch is 0.00351843.
After 22481 training step(s), loss on training batch is 0.00365533.
After 22482 training step(s), loss on training batch is 0.00373743.
After 22483 training step(s), loss on training batch is 0.00361789.
After 22484 training step(s), loss on training batch is 0.00384866.
After 22485 training step(s), loss on training batch is 0.00381756.
After 22486 training step(s), loss on training batch is 0.00357245.
After 22487 training step(s), loss on training batch is 0.00344881.
After 22488 training step(s), loss on training batch is 0.00344573.
After 22489 training step(s), loss on training batch is 0.00329397.
After 22490 training step(s), loss on training batch is 0.00388704.
After 22491 training step(s), loss on training batch is 0.00354158.
After 22492 training step(s), loss on training batch is 0.0041871.
After 22493 training step(s), loss on training batch is 0.00361344.
After 22494 training step(s), loss on training batch is 0.00334774.
After 22495 training step(s), loss on training batch is 0.00345355.
After 22496 training step(s), loss on training batch is 0.0037297.
After 22497 training step(s), loss on training batch is 0.00332791.
After 22498 training step(s), loss on training batch is 0.00374228.
After 22499 training step(s), loss on training batch is 0.00374361.
After 22500 training step(s), loss on training batch is 0.00344361.
After 22501 training step(s), loss on training batch is 0.00366055.
After 22502 training step(s), loss on training batch is 0.00375964.
After 22503 training step(s), loss on training batch is 0.00370817.
After 22504 training step(s), loss on training batch is 0.00413247.
After 22505 training step(s), loss on training batch is 0.00357699.
After 22506 training step(s), loss on training batch is 0.00364638.
After 22507 training step(s), loss on training batch is 0.00341393.
After 22508 training step(s), loss on training batch is 0.0035901.
After 22509 training step(s), loss on training batch is 0.00349762.
After 22510 training step(s), loss on training batch is 0.00338094.
After 22511 training step(s), loss on training batch is 0.00400026.
After 22512 training step(s), loss on training batch is 0.00357955.
After 22513 training step(s), loss on training batch is 0.00344762.
After 22514 training step(s), loss on training batch is 0.00349222.
After 22515 training step(s), loss on training batch is 0.00427017.
After 22516 training step(s), loss on training batch is 0.0036985.
After 22517 training step(s), loss on training batch is 0.00381444.
After 22518 training step(s), loss on training batch is 0.00371085.
After 22519 training step(s), loss on training batch is 0.00379642.
After 22520 training step(s), loss on training batch is 0.003257.
After 22521 training step(s), loss on training batch is 0.00391198.
After 22522 training step(s), loss on training batch is 0.00408729.
After 22523 training step(s), loss on training batch is 0.00339429.
After 22524 training step(s), loss on training batch is 0.00346586.
After 22525 training step(s), loss on training batch is 0.00370579.
After 22526 training step(s), loss on training batch is 0.00335537.
After 22527 training step(s), loss on training batch is 0.0033082.
After 22528 training step(s), loss on training batch is 0.00329971.
After 22529 training step(s), loss on training batch is 0.00353312.
After 22530 training step(s), loss on training batch is 0.00341645.
After 22531 training step(s), loss on training batch is 0.00332015.
After 22532 training step(s), loss on training batch is 0.00408026.
After 22533 training step(s), loss on training batch is 0.00416062.
After 22534 training step(s), loss on training batch is 0.00321895.
After 22535 training step(s), loss on training batch is 0.00332599.
After 22536 training step(s), loss on training batch is 0.00339045.
After 22537 training step(s), loss on training batch is 0.00334395.
After 22538 training step(s), loss on training batch is 0.00373861.
After 22539 training step(s), loss on training batch is 0.00391565.
After 22540 training step(s), loss on training batch is 0.00389139.
After 22541 training step(s), loss on training batch is 0.00362718.
After 22542 training step(s), loss on training batch is 0.00419717.
After 22543 training step(s), loss on training batch is 0.00363708.
After 22544 training step(s), loss on training batch is 0.00338766.
After 22545 training step(s), loss on training batch is 0.00354246.
After 22546 training step(s), loss on training batch is 0.00357728.
After 22547 training step(s), loss on training batch is 0.0034378.
After 22548 training step(s), loss on training batch is 0.00344798.
After 22549 training step(s), loss on training batch is 0.00365924.
After 22550 training step(s), loss on training batch is 0.00333243.
After 22551 training step(s), loss on training batch is 0.00331231.
After 22552 training step(s), loss on training batch is 0.00409874.
After 22553 training step(s), loss on training batch is 0.00327228.
After 22554 training step(s), loss on training batch is 0.00358026.
After 22555 training step(s), loss on training batch is 0.004347.
After 22556 training step(s), loss on training batch is 0.00317587.
After 22557 training step(s), loss on training batch is 0.00370671.
After 22558 training step(s), loss on training batch is 0.00429278.
After 22559 training step(s), loss on training batch is 0.00353403.
After 22560 training step(s), loss on training batch is 0.00350225.
After 22561 training step(s), loss on training batch is 0.00327298.
After 22562 training step(s), loss on training batch is 0.00348852.
After 22563 training step(s), loss on training batch is 0.00358152.
After 22564 training step(s), loss on training batch is 0.00349445.
After 22565 training step(s), loss on training batch is 0.00374632.
After 22566 training step(s), loss on training batch is 0.00339504.
After 22567 training step(s), loss on training batch is 0.00378106.
After 22568 training step(s), loss on training batch is 0.00430431.
After 22569 training step(s), loss on training batch is 0.00344209.
After 22570 training step(s), loss on training batch is 0.00339459.
After 22571 training step(s), loss on training batch is 0.00346655.
After 22572 training step(s), loss on training batch is 0.00349345.
After 22573 training step(s), loss on training batch is 0.00388554.
After 22574 training step(s), loss on training batch is 0.00381815.
After 22575 training step(s), loss on training batch is 0.00374636.
After 22576 training step(s), loss on training batch is 0.00336328.
After 22577 training step(s), loss on training batch is 0.00363295.
After 22578 training step(s), loss on training batch is 0.00351729.
After 22579 training step(s), loss on training batch is 0.00404246.
After 22580 training step(s), loss on training batch is 0.0036642.
After 22581 training step(s), loss on training batch is 0.00394337.
After 22582 training step(s), loss on training batch is 0.00346622.
After 22583 training step(s), loss on training batch is 0.00351899.
After 22584 training step(s), loss on training batch is 0.00399771.
After 22585 training step(s), loss on training batch is 0.0033649.
After 22586 training step(s), loss on training batch is 0.0034444.
After 22587 training step(s), loss on training batch is 0.00380487.
After 22588 training step(s), loss on training batch is 0.00358431.
After 22589 training step(s), loss on training batch is 0.00348424.
After 22590 training step(s), loss on training batch is 0.00351061.
After 22591 training step(s), loss on training batch is 0.00333318.
After 22592 training step(s), loss on training batch is 0.00419127.
After 22593 training step(s), loss on training batch is 0.00342263.
After 22594 training step(s), loss on training batch is 0.00366516.
After 22595 training step(s), loss on training batch is 0.0034607.
After 22596 training step(s), loss on training batch is 0.00341715.
After 22597 training step(s), loss on training batch is 0.00329636.
After 22598 training step(s), loss on training batch is 0.0034422.
After 22599 training step(s), loss on training batch is 0.00355865.
After 22600 training step(s), loss on training batch is 0.00501129.
After 22601 training step(s), loss on training batch is 0.00391291.
After 22602 training step(s), loss on training batch is 0.00368676.
After 22603 training step(s), loss on training batch is 0.0036761.
After 22604 training step(s), loss on training batch is 0.00368744.
After 22605 training step(s), loss on training batch is 0.00355598.
After 22606 training step(s), loss on training batch is 0.00361855.
After 22607 training step(s), loss on training batch is 0.00342293.
After 22608 training step(s), loss on training batch is 0.00352742.
After 22609 training step(s), loss on training batch is 0.00337168.
After 22610 training step(s), loss on training batch is 0.00365543.
After 22611 training step(s), loss on training batch is 0.00328215.
After 22612 training step(s), loss on training batch is 0.00384911.
After 22613 training step(s), loss on training batch is 0.0038037.
After 22614 training step(s), loss on training batch is 0.00394741.
After 22615 training step(s), loss on training batch is 0.00347664.
After 22616 training step(s), loss on training batch is 0.00336768.
After 22617 training step(s), loss on training batch is 0.00346103.
After 22618 training step(s), loss on training batch is 0.00414081.
After 22619 training step(s), loss on training batch is 0.00331759.
After 22620 training step(s), loss on training batch is 0.00347929.
After 22621 training step(s), loss on training batch is 0.00339207.
After 22622 training step(s), loss on training batch is 0.00489136.
After 22623 training step(s), loss on training batch is 0.00397091.
After 22624 training step(s), loss on training batch is 0.00370654.
After 22625 training step(s), loss on training batch is 0.00422115.
After 22626 training step(s), loss on training batch is 0.00425884.
After 22627 training step(s), loss on training batch is 0.00339233.
After 22628 training step(s), loss on training batch is 0.00341582.
After 22629 training step(s), loss on training batch is 0.00367312.
After 22630 training step(s), loss on training batch is 0.00382328.
After 22631 training step(s), loss on training batch is 0.00352627.
After 22632 training step(s), loss on training batch is 0.00463469.
After 22633 training step(s), loss on training batch is 0.00351981.
After 22634 training step(s), loss on training batch is 0.00391265.
After 22635 training step(s), loss on training batch is 0.00356331.
After 22636 training step(s), loss on training batch is 0.00350188.
After 22637 training step(s), loss on training batch is 0.00354341.
After 22638 training step(s), loss on training batch is 0.003316.
After 22639 training step(s), loss on training batch is 0.00412581.
After 22640 training step(s), loss on training batch is 0.00382244.
After 22641 training step(s), loss on training batch is 0.00375713.
After 22642 training step(s), loss on training batch is 0.00379492.
After 22643 training step(s), loss on training batch is 0.00374951.
After 22644 training step(s), loss on training batch is 0.00349888.
After 22645 training step(s), loss on training batch is 0.00359118.
After 22646 training step(s), loss on training batch is 0.00341927.
After 22647 training step(s), loss on training batch is 0.00341812.
After 22648 training step(s), loss on training batch is 0.00378978.
After 22649 training step(s), loss on training batch is 0.00339927.
After 22650 training step(s), loss on training batch is 0.00354366.
After 22651 training step(s), loss on training batch is 0.00325148.
After 22652 training step(s), loss on training batch is 0.00343563.
After 22653 training step(s), loss on training batch is 0.00477727.
After 22654 training step(s), loss on training batch is 0.00391912.
After 22655 training step(s), loss on training batch is 0.0043793.
After 22656 training step(s), loss on training batch is 0.00365371.
After 22657 training step(s), loss on training batch is 0.00372665.
After 22658 training step(s), loss on training batch is 0.00410177.
After 22659 training step(s), loss on training batch is 0.00353573.
After 22660 training step(s), loss on training batch is 0.00380865.
After 22661 training step(s), loss on training batch is 0.00363992.
After 22662 training step(s), loss on training batch is 0.00348275.
After 22663 training step(s), loss on training batch is 0.00334003.
After 22664 training step(s), loss on training batch is 0.00349588.
After 22665 training step(s), loss on training batch is 0.00354145.
After 22666 training step(s), loss on training batch is 0.00413552.
After 22667 training step(s), loss on training batch is 0.00335658.
After 22668 training step(s), loss on training batch is 0.00371584.
After 22669 training step(s), loss on training batch is 0.00446911.
After 22670 training step(s), loss on training batch is 0.00350832.
After 22671 training step(s), loss on training batch is 0.00379654.
After 22672 training step(s), loss on training batch is 0.00333378.
After 22673 training step(s), loss on training batch is 0.00388221.
After 22674 training step(s), loss on training batch is 0.00353561.
After 22675 training step(s), loss on training batch is 0.00376293.
After 22676 training step(s), loss on training batch is 0.00384211.
After 22677 training step(s), loss on training batch is 0.00397832.
After 22678 training step(s), loss on training batch is 0.00386508.
After 22679 training step(s), loss on training batch is 0.00384743.
After 22680 training step(s), loss on training batch is 0.00411196.
After 22681 training step(s), loss on training batch is 0.0043762.
After 22682 training step(s), loss on training batch is 0.00350549.
After 22683 training step(s), loss on training batch is 0.00343941.
After 22684 training step(s), loss on training batch is 0.00344696.
After 22685 training step(s), loss on training batch is 0.00350451.
After 22686 training step(s), loss on training batch is 0.00369096.
After 22687 training step(s), loss on training batch is 0.00358137.
After 22688 training step(s), loss on training batch is 0.00378235.
After 22689 training step(s), loss on training batch is 0.0034304.
After 22690 training step(s), loss on training batch is 0.00347156.
After 22691 training step(s), loss on training batch is 0.0035261.
After 22692 training step(s), loss on training batch is 0.00329126.
After 22693 training step(s), loss on training batch is 0.00349376.
After 22694 training step(s), loss on training batch is 0.00364369.
After 22695 training step(s), loss on training batch is 0.00364036.
After 22696 training step(s), loss on training batch is 0.00375552.
After 22697 training step(s), loss on training batch is 0.00347842.
After 22698 training step(s), loss on training batch is 0.00327867.
After 22699 training step(s), loss on training batch is 0.00348777.
After 22700 training step(s), loss on training batch is 0.00345836.
After 22701 training step(s), loss on training batch is 0.00365098.
After 22702 training step(s), loss on training batch is 0.00349473.
After 22703 training step(s), loss on training batch is 0.00368487.
After 22704 training step(s), loss on training batch is 0.0036465.
After 22705 training step(s), loss on training batch is 0.0035163.
After 22706 training step(s), loss on training batch is 0.00339013.
After 22707 training step(s), loss on training batch is 0.00360623.
After 22708 training step(s), loss on training batch is 0.00340015.
After 22709 training step(s), loss on training batch is 0.00365214.
After 22710 training step(s), loss on training batch is 0.00429378.
After 22711 training step(s), loss on training batch is 0.00354516.
After 22712 training step(s), loss on training batch is 0.0033494.
After 22713 training step(s), loss on training batch is 0.00326268.
After 22714 training step(s), loss on training batch is 0.00350977.
After 22715 training step(s), loss on training batch is 0.00329438.
After 22716 training step(s), loss on training batch is 0.00349792.
After 22717 training step(s), loss on training batch is 0.00324592.
After 22718 training step(s), loss on training batch is 0.00343419.
After 22719 training step(s), loss on training batch is 0.00351666.
After 22720 training step(s), loss on training batch is 0.00336008.
After 22721 training step(s), loss on training batch is 0.00332656.
After 22722 training step(s), loss on training batch is 0.00338926.
After 22723 training step(s), loss on training batch is 0.00352756.
After 22724 training step(s), loss on training batch is 0.00354989.
After 22725 training step(s), loss on training batch is 0.00329725.
After 22726 training step(s), loss on training batch is 0.00388388.
After 22727 training step(s), loss on training batch is 0.00366523.
After 22728 training step(s), loss on training batch is 0.00359619.
After 22729 training step(s), loss on training batch is 0.00345683.
After 22730 training step(s), loss on training batch is 0.00427958.
After 22731 training step(s), loss on training batch is 0.00342119.
After 22732 training step(s), loss on training batch is 0.00368828.
After 22733 training step(s), loss on training batch is 0.00396177.
After 22734 training step(s), loss on training batch is 0.00347758.
After 22735 training step(s), loss on training batch is 0.00333083.
After 22736 training step(s), loss on training batch is 0.00363969.
After 22737 training step(s), loss on training batch is 0.00354571.
After 22738 training step(s), loss on training batch is 0.0039414.
After 22739 training step(s), loss on training batch is 0.00341333.
After 22740 training step(s), loss on training batch is 0.00336317.
After 22741 training step(s), loss on training batch is 0.00415786.
After 22742 training step(s), loss on training batch is 0.00361666.
After 22743 training step(s), loss on training batch is 0.00482884.
After 22744 training step(s), loss on training batch is 0.00357202.
After 22745 training step(s), loss on training batch is 0.00355796.
After 22746 training step(s), loss on training batch is 0.00389643.
After 22747 training step(s), loss on training batch is 0.0036386.
After 22748 training step(s), loss on training batch is 0.00366408.
After 22749 training step(s), loss on training batch is 0.00327169.
After 22750 training step(s), loss on training batch is 0.00430141.
After 22751 training step(s), loss on training batch is 0.00387311.
After 22752 training step(s), loss on training batch is 0.00339421.
After 22753 training step(s), loss on training batch is 0.00416076.
After 22754 training step(s), loss on training batch is 0.0034367.
After 22755 training step(s), loss on training batch is 0.004595.
After 22756 training step(s), loss on training batch is 0.00353034.
After 22757 training step(s), loss on training batch is 0.00358717.
After 22758 training step(s), loss on training batch is 0.00353582.
After 22759 training step(s), loss on training batch is 0.00352373.
After 22760 training step(s), loss on training batch is 0.00355972.
After 22761 training step(s), loss on training batch is 0.00376302.
After 22762 training step(s), loss on training batch is 0.00353577.
After 22763 training step(s), loss on training batch is 0.00354121.
After 22764 training step(s), loss on training batch is 0.00350171.
After 22765 training step(s), loss on training batch is 0.00329911.
After 22766 training step(s), loss on training batch is 0.00370115.
After 22767 training step(s), loss on training batch is 0.00346425.
After 22768 training step(s), loss on training batch is 0.00360511.
After 22769 training step(s), loss on training batch is 0.00363078.
After 22770 training step(s), loss on training batch is 0.00330207.
After 22771 training step(s), loss on training batch is 0.00397291.
After 22772 training step(s), loss on training batch is 0.00337432.
After 22773 training step(s), loss on training batch is 0.00355202.
After 22774 training step(s), loss on training batch is 0.00330446.
After 22775 training step(s), loss on training batch is 0.00397628.
After 22776 training step(s), loss on training batch is 0.00351136.
After 22777 training step(s), loss on training batch is 0.00390828.
After 22778 training step(s), loss on training batch is 0.00360058.
After 22779 training step(s), loss on training batch is 0.00359814.
After 22780 training step(s), loss on training batch is 0.00531476.
After 22781 training step(s), loss on training batch is 0.00323737.
After 22782 training step(s), loss on training batch is 0.00384688.
After 22783 training step(s), loss on training batch is 0.00326493.
After 22784 training step(s), loss on training batch is 0.00388974.
After 22785 training step(s), loss on training batch is 0.00367608.
After 22786 training step(s), loss on training batch is 0.00406787.
After 22787 training step(s), loss on training batch is 0.00321735.
After 22788 training step(s), loss on training batch is 0.00376763.
After 22789 training step(s), loss on training batch is 0.00338946.
After 22790 training step(s), loss on training batch is 0.00430239.
After 22791 training step(s), loss on training batch is 0.00339421.
After 22792 training step(s), loss on training batch is 0.00354753.
After 22793 training step(s), loss on training batch is 0.00349045.
After 22794 training step(s), loss on training batch is 0.00353333.
After 22795 training step(s), loss on training batch is 0.00415301.
After 22796 training step(s), loss on training batch is 0.00427678.
After 22797 training step(s), loss on training batch is 0.00348198.
After 22798 training step(s), loss on training batch is 0.00371655.
After 22799 training step(s), loss on training batch is 0.00421595.
After 22800 training step(s), loss on training batch is 0.00337343.
After 22801 training step(s), loss on training batch is 0.00373107.
After 22802 training step(s), loss on training batch is 0.00342836.
After 22803 training step(s), loss on training batch is 0.00388575.
After 22804 training step(s), loss on training batch is 0.00339314.
After 22805 training step(s), loss on training batch is 0.0035582.
After 22806 training step(s), loss on training batch is 0.00352278.
After 22807 training step(s), loss on training batch is 0.00363745.
After 22808 training step(s), loss on training batch is 0.0033694.
After 22809 training step(s), loss on training batch is 0.00350773.
After 22810 training step(s), loss on training batch is 0.0034308.
After 22811 training step(s), loss on training batch is 0.00378529.
After 22812 training step(s), loss on training batch is 0.00394378.
After 22813 training step(s), loss on training batch is 0.00381613.
After 22814 training step(s), loss on training batch is 0.00410514.
After 22815 training step(s), loss on training batch is 0.00398997.
After 22816 training step(s), loss on training batch is 0.00331965.
After 22817 training step(s), loss on training batch is 0.00393842.
After 22818 training step(s), loss on training batch is 0.00371392.
After 22819 training step(s), loss on training batch is 0.00328406.
After 22820 training step(s), loss on training batch is 0.00374269.
After 22821 training step(s), loss on training batch is 0.00330684.
After 22822 training step(s), loss on training batch is 0.00340274.
After 22823 training step(s), loss on training batch is 0.00348011.
After 22824 training step(s), loss on training batch is 0.00333329.
After 22825 training step(s), loss on training batch is 0.00337017.
After 22826 training step(s), loss on training batch is 0.00362437.
After 22827 training step(s), loss on training batch is 0.00326508.
After 22828 training step(s), loss on training batch is 0.00368624.
After 22829 training step(s), loss on training batch is 0.00406215.
After 22830 training step(s), loss on training batch is 0.00345288.
After 22831 training step(s), loss on training batch is 0.00363785.
After 22832 training step(s), loss on training batch is 0.00396342.
After 22833 training step(s), loss on training batch is 0.0032551.
After 22834 training step(s), loss on training batch is 0.00343934.
After 22835 training step(s), loss on training batch is 0.00352257.
After 22836 training step(s), loss on training batch is 0.0033184.
After 22837 training step(s), loss on training batch is 0.00330395.
After 22838 training step(s), loss on training batch is 0.00365321.
After 22839 training step(s), loss on training batch is 0.003231.
After 22840 training step(s), loss on training batch is 0.00344488.
After 22841 training step(s), loss on training batch is 0.0034843.
After 22842 training step(s), loss on training batch is 0.00359478.
After 22843 training step(s), loss on training batch is 0.0043638.
After 22844 training step(s), loss on training batch is 0.00346745.
After 22845 training step(s), loss on training batch is 0.00381486.
After 22846 training step(s), loss on training batch is 0.00332577.
After 22847 training step(s), loss on training batch is 0.00328064.
After 22848 training step(s), loss on training batch is 0.00361489.
After 22849 training step(s), loss on training batch is 0.00382542.
After 22850 training step(s), loss on training batch is 0.00362468.
After 22851 training step(s), loss on training batch is 0.00343204.
After 22852 training step(s), loss on training batch is 0.00401322.
After 22853 training step(s), loss on training batch is 0.00334132.
After 22854 training step(s), loss on training batch is 0.0033975.
After 22855 training step(s), loss on training batch is 0.00386958.
After 22856 training step(s), loss on training batch is 0.0036362.
After 22857 training step(s), loss on training batch is 0.00389666.
After 22858 training step(s), loss on training batch is 0.00333926.
After 22859 training step(s), loss on training batch is 0.00380403.
After 22860 training step(s), loss on training batch is 0.0036951.
After 22861 training step(s), loss on training batch is 0.00403401.
After 22862 training step(s), loss on training batch is 0.00364445.
After 22863 training step(s), loss on training batch is 0.00378035.
After 22864 training step(s), loss on training batch is 0.00386283.
After 22865 training step(s), loss on training batch is 0.0033963.
After 22866 training step(s), loss on training batch is 0.00337615.
After 22867 training step(s), loss on training batch is 0.00317193.
After 22868 training step(s), loss on training batch is 0.00336927.
After 22869 training step(s), loss on training batch is 0.00342213.
After 22870 training step(s), loss on training batch is 0.00343365.
After 22871 training step(s), loss on training batch is 0.00383448.
After 22872 training step(s), loss on training batch is 0.00363884.
After 22873 training step(s), loss on training batch is 0.00380767.
After 22874 training step(s), loss on training batch is 0.00383314.
After 22875 training step(s), loss on training batch is 0.00365053.
After 22876 training step(s), loss on training batch is 0.00374651.
After 22877 training step(s), loss on training batch is 0.00424962.
After 22878 training step(s), loss on training batch is 0.00338865.
After 22879 training step(s), loss on training batch is 0.00363546.
After 22880 training step(s), loss on training batch is 0.003476.
After 22881 training step(s), loss on training batch is 0.00380874.
After 22882 training step(s), loss on training batch is 0.00332692.
After 22883 training step(s), loss on training batch is 0.00356187.
After 22884 training step(s), loss on training batch is 0.00335282.
After 22885 training step(s), loss on training batch is 0.00350302.
After 22886 training step(s), loss on training batch is 0.00347663.
After 22887 training step(s), loss on training batch is 0.00342499.
After 22888 training step(s), loss on training batch is 0.00368482.
After 22889 training step(s), loss on training batch is 0.00352994.
After 22890 training step(s), loss on training batch is 0.00341912.
After 22891 training step(s), loss on training batch is 0.00370996.
After 22892 training step(s), loss on training batch is 0.00335157.
After 22893 training step(s), loss on training batch is 0.00379398.
After 22894 training step(s), loss on training batch is 0.00349831.
After 22895 training step(s), loss on training batch is 0.004001.
After 22896 training step(s), loss on training batch is 0.00373562.
After 22897 training step(s), loss on training batch is 0.00539881.
After 22898 training step(s), loss on training batch is 0.00345197.
After 22899 training step(s), loss on training batch is 0.00355552.
After 22900 training step(s), loss on training batch is 0.00363386.
After 22901 training step(s), loss on training batch is 0.00342305.
After 22902 training step(s), loss on training batch is 0.00355226.
After 22903 training step(s), loss on training batch is 0.0036758.
After 22904 training step(s), loss on training batch is 0.00323455.
After 22905 training step(s), loss on training batch is 0.00333335.
After 22906 training step(s), loss on training batch is 0.00336677.
After 22907 training step(s), loss on training batch is 0.00369979.
After 22908 training step(s), loss on training batch is 0.00355698.
After 22909 training step(s), loss on training batch is 0.00392243.
After 22910 training step(s), loss on training batch is 0.00394339.
After 22911 training step(s), loss on training batch is 0.00350636.
After 22912 training step(s), loss on training batch is 0.00354593.
After 22913 training step(s), loss on training batch is 0.0034479.
After 22914 training step(s), loss on training batch is 0.00358573.
After 22915 training step(s), loss on training batch is 0.00344693.
After 22916 training step(s), loss on training batch is 0.00316386.
After 22917 training step(s), loss on training batch is 0.0034717.
After 22918 training step(s), loss on training batch is 0.00361045.
After 22919 training step(s), loss on training batch is 0.00382725.
After 22920 training step(s), loss on training batch is 0.00329529.
After 22921 training step(s), loss on training batch is 0.0038615.
After 22922 training step(s), loss on training batch is 0.00340028.
After 22923 training step(s), loss on training batch is 0.00344284.
After 22924 training step(s), loss on training batch is 0.00340028.
After 22925 training step(s), loss on training batch is 0.00322866.
After 22926 training step(s), loss on training batch is 0.0036006.
After 22927 training step(s), loss on training batch is 0.00359004.
After 22928 training step(s), loss on training batch is 0.00424323.
After 22929 training step(s), loss on training batch is 0.00369423.
After 22930 training step(s), loss on training batch is 0.00341335.
After 22931 training step(s), loss on training batch is 0.00379589.
After 22932 training step(s), loss on training batch is 0.0033799.
After 22933 training step(s), loss on training batch is 0.00350143.
After 22934 training step(s), loss on training batch is 0.00438582.
After 22935 training step(s), loss on training batch is 0.00335582.
After 22936 training step(s), loss on training batch is 0.00334343.
After 22937 training step(s), loss on training batch is 0.00370205.
After 22938 training step(s), loss on training batch is 0.00312542.
After 22939 training step(s), loss on training batch is 0.00370762.
After 22940 training step(s), loss on training batch is 0.00350696.
After 22941 training step(s), loss on training batch is 0.00358614.
After 22942 training step(s), loss on training batch is 0.0036229.
After 22943 training step(s), loss on training batch is 0.0036466.
After 22944 training step(s), loss on training batch is 0.00332621.
After 22945 training step(s), loss on training batch is 0.00324497.
After 22946 training step(s), loss on training batch is 0.0037093.
After 22947 training step(s), loss on training batch is 0.00334298.
After 22948 training step(s), loss on training batch is 0.00336433.
After 22949 training step(s), loss on training batch is 0.00345013.
After 22950 training step(s), loss on training batch is 0.00325787.
After 22951 training step(s), loss on training batch is 0.00383661.
After 22952 training step(s), loss on training batch is 0.00366329.
After 22953 training step(s), loss on training batch is 0.00387719.
After 22954 training step(s), loss on training batch is 0.00351828.
After 22955 training step(s), loss on training batch is 0.00358959.
After 22956 training step(s), loss on training batch is 0.00349238.
After 22957 training step(s), loss on training batch is 0.00374418.
After 22958 training step(s), loss on training batch is 0.00355548.
After 22959 training step(s), loss on training batch is 0.00338447.
After 22960 training step(s), loss on training batch is 0.00362522.
After 22961 training step(s), loss on training batch is 0.00359182.
After 22962 training step(s), loss on training batch is 0.00321811.
After 22963 training step(s), loss on training batch is 0.00390596.
After 22964 training step(s), loss on training batch is 0.0038261.
After 22965 training step(s), loss on training batch is 0.00367078.
After 22966 training step(s), loss on training batch is 0.00389365.
After 22967 training step(s), loss on training batch is 0.00374622.
After 22968 training step(s), loss on training batch is 0.0034204.
After 22969 training step(s), loss on training batch is 0.00338074.
After 22970 training step(s), loss on training batch is 0.0039929.
After 22971 training step(s), loss on training batch is 0.00376722.
After 22972 training step(s), loss on training batch is 0.00404264.
After 22973 training step(s), loss on training batch is 0.00344472.
After 22974 training step(s), loss on training batch is 0.0035829.
After 22975 training step(s), loss on training batch is 0.00363607.
After 22976 training step(s), loss on training batch is 0.00444599.
After 22977 training step(s), loss on training batch is 0.00358358.
After 22978 training step(s), loss on training batch is 0.00359012.
After 22979 training step(s), loss on training batch is 0.00382313.
After 22980 training step(s), loss on training batch is 0.00332179.
After 22981 training step(s), loss on training batch is 0.00331652.
After 22982 training step(s), loss on training batch is 0.00342803.
After 22983 training step(s), loss on training batch is 0.00406758.
After 22984 training step(s), loss on training batch is 0.00359806.
After 22985 training step(s), loss on training batch is 0.00355463.
After 22986 training step(s), loss on training batch is 0.00367115.
After 22987 training step(s), loss on training batch is 0.00323282.
After 22988 training step(s), loss on training batch is 0.00394057.
After 22989 training step(s), loss on training batch is 0.00393008.
After 22990 training step(s), loss on training batch is 0.00363319.
After 22991 training step(s), loss on training batch is 0.0033979.
After 22992 training step(s), loss on training batch is 0.00382995.
After 22993 training step(s), loss on training batch is 0.00343129.
After 22994 training step(s), loss on training batch is 0.00322117.
After 22995 training step(s), loss on training batch is 0.00397237.
After 22996 training step(s), loss on training batch is 0.00367821.
After 22997 training step(s), loss on training batch is 0.00363591.
After 22998 training step(s), loss on training batch is 0.00370084.
After 22999 training step(s), loss on training batch is 0.00344467.
After 23000 training step(s), loss on training batch is 0.00359615.
After 23001 training step(s), loss on training batch is 0.00363019.
After 23002 training step(s), loss on training batch is 0.00349148.
After 23003 training step(s), loss on training batch is 0.00343629.
After 23004 training step(s), loss on training batch is 0.00361111.
After 23005 training step(s), loss on training batch is 0.00322757.
After 23006 training step(s), loss on training batch is 0.00333392.
After 23007 training step(s), loss on training batch is 0.00355494.
After 23008 training step(s), loss on training batch is 0.00439234.
After 23009 training step(s), loss on training batch is 0.00350061.
After 23010 training step(s), loss on training batch is 0.00352127.
After 23011 training step(s), loss on training batch is 0.0033662.
After 23012 training step(s), loss on training batch is 0.00363922.
After 23013 training step(s), loss on training batch is 0.00373464.
After 23014 training step(s), loss on training batch is 0.00361276.
After 23015 training step(s), loss on training batch is 0.00349575.
After 23016 training step(s), loss on training batch is 0.00390839.
After 23017 training step(s), loss on training batch is 0.00357017.
After 23018 training step(s), loss on training batch is 0.00338502.
After 23019 training step(s), loss on training batch is 0.00360965.
After 23020 training step(s), loss on training batch is 0.00383806.
After 23021 training step(s), loss on training batch is 0.00344351.
After 23022 training step(s), loss on training batch is 0.0038093.
After 23023 training step(s), loss on training batch is 0.00345211.
After 23024 training step(s), loss on training batch is 0.0036353.
After 23025 training step(s), loss on training batch is 0.00341249.
After 23026 training step(s), loss on training batch is 0.00338817.
After 23027 training step(s), loss on training batch is 0.00565798.
After 23028 training step(s), loss on training batch is 0.00341415.
After 23029 training step(s), loss on training batch is 0.00452112.
After 23030 training step(s), loss on training batch is 0.00399799.
After 23031 training step(s), loss on training batch is 0.00334629.
After 23032 training step(s), loss on training batch is 0.00415597.
After 23033 training step(s), loss on training batch is 0.00324126.
After 23034 training step(s), loss on training batch is 0.00335252.
After 23035 training step(s), loss on training batch is 0.00340728.
After 23036 training step(s), loss on training batch is 0.00344846.
After 23037 training step(s), loss on training batch is 0.00368176.
After 23038 training step(s), loss on training batch is 0.00368464.
After 23039 training step(s), loss on training batch is 0.00352352.
After 23040 training step(s), loss on training batch is 0.00370082.
After 23041 training step(s), loss on training batch is 0.004262.
After 23042 training step(s), loss on training batch is 0.00356262.
After 23043 training step(s), loss on training batch is 0.00365498.
After 23044 training step(s), loss on training batch is 0.00360852.
After 23045 training step(s), loss on training batch is 0.00331407.
After 23046 training step(s), loss on training batch is 0.00391722.
After 23047 training step(s), loss on training batch is 0.00373622.
After 23048 training step(s), loss on training batch is 0.00357314.
After 23049 training step(s), loss on training batch is 0.00407814.
After 23050 training step(s), loss on training batch is 0.00368618.
After 23051 training step(s), loss on training batch is 0.00394101.
After 23052 training step(s), loss on training batch is 0.00367741.
After 23053 training step(s), loss on training batch is 0.00372354.
After 23054 training step(s), loss on training batch is 0.00389667.
After 23055 training step(s), loss on training batch is 0.00363762.
After 23056 training step(s), loss on training batch is 0.00336684.
After 23057 training step(s), loss on training batch is 0.00363144.
After 23058 training step(s), loss on training batch is 0.00360863.
After 23059 training step(s), loss on training batch is 0.00330414.
After 23060 training step(s), loss on training batch is 0.00372271.
After 23061 training step(s), loss on training batch is 0.00377071.
After 23062 training step(s), loss on training batch is 0.00546836.
After 23063 training step(s), loss on training batch is 0.00322672.
After 23064 training step(s), loss on training batch is 0.00348111.
After 23065 training step(s), loss on training batch is 0.00353063.
After 23066 training step(s), loss on training batch is 0.00321584.
After 23067 training step(s), loss on training batch is 0.00356393.
After 23068 training step(s), loss on training batch is 0.00366857.
After 23069 training step(s), loss on training batch is 0.00367011.
After 23070 training step(s), loss on training batch is 0.00354642.
After 23071 training step(s), loss on training batch is 0.00381901.
After 23072 training step(s), loss on training batch is 0.00375278.
After 23073 training step(s), loss on training batch is 0.00332525.
After 23074 training step(s), loss on training batch is 0.00344122.
After 23075 training step(s), loss on training batch is 0.00333843.
After 23076 training step(s), loss on training batch is 0.00377736.
After 23077 training step(s), loss on training batch is 0.00422606.
After 23078 training step(s), loss on training batch is 0.00346926.
After 23079 training step(s), loss on training batch is 0.00383545.
After 23080 training step(s), loss on training batch is 0.00354639.
After 23081 training step(s), loss on training batch is 0.00346565.
After 23082 training step(s), loss on training batch is 0.00352236.
After 23083 training step(s), loss on training batch is 0.00356467.
After 23084 training step(s), loss on training batch is 0.00349676.
After 23085 training step(s), loss on training batch is 0.00368118.
After 23086 training step(s), loss on training batch is 0.00355986.
After 23087 training step(s), loss on training batch is 0.00327478.
After 23088 training step(s), loss on training batch is 0.00376532.
After 23089 training step(s), loss on training batch is 0.00333239.
After 23090 training step(s), loss on training batch is 0.00339298.
After 23091 training step(s), loss on training batch is 0.0045191.
After 23092 training step(s), loss on training batch is 0.00396937.
After 23093 training step(s), loss on training batch is 0.00366854.
After 23094 training step(s), loss on training batch is 0.00356002.
After 23095 training step(s), loss on training batch is 0.00331094.
After 23096 training step(s), loss on training batch is 0.00379475.
After 23097 training step(s), loss on training batch is 0.00384337.
After 23098 training step(s), loss on training batch is 0.00337501.
After 23099 training step(s), loss on training batch is 0.0034913.
After 23100 training step(s), loss on training batch is 0.00350715.
After 23101 training step(s), loss on training batch is 0.00381211.
After 23102 training step(s), loss on training batch is 0.00402773.
After 23103 training step(s), loss on training batch is 0.00356315.
After 23104 training step(s), loss on training batch is 0.00318978.
After 23105 training step(s), loss on training batch is 0.00351911.
After 23106 training step(s), loss on training batch is 0.00365802.
After 23107 training step(s), loss on training batch is 0.00333829.
After 23108 training step(s), loss on training batch is 0.00341256.
After 23109 training step(s), loss on training batch is 0.00354007.
After 23110 training step(s), loss on training batch is 0.00353929.
After 23111 training step(s), loss on training batch is 0.00352044.
After 23112 training step(s), loss on training batch is 0.00357229.
After 23113 training step(s), loss on training batch is 0.00335957.
After 23114 training step(s), loss on training batch is 0.00428079.
After 23115 training step(s), loss on training batch is 0.00384564.
After 23116 training step(s), loss on training batch is 0.00368168.
After 23117 training step(s), loss on training batch is 0.00333454.
After 23118 training step(s), loss on training batch is 0.0035369.
After 23119 training step(s), loss on training batch is 0.00336919.
After 23120 training step(s), loss on training batch is 0.00334437.
After 23121 training step(s), loss on training batch is 0.00404328.
After 23122 training step(s), loss on training batch is 0.00378866.
After 23123 training step(s), loss on training batch is 0.00340317.
After 23124 training step(s), loss on training batch is 0.00358219.
After 23125 training step(s), loss on training batch is 0.00331714.
After 23126 training step(s), loss on training batch is 0.00340549.
After 23127 training step(s), loss on training batch is 0.00354061.
After 23128 training step(s), loss on training batch is 0.00318808.
After 23129 training step(s), loss on training batch is 0.00390906.
After 23130 training step(s), loss on training batch is 0.00342911.
After 23131 training step(s), loss on training batch is 0.00362131.
After 23132 training step(s), loss on training batch is 0.00336903.
After 23133 training step(s), loss on training batch is 0.00387641.
After 23134 training step(s), loss on training batch is 0.00356808.
After 23135 training step(s), loss on training batch is 0.00377927.
After 23136 training step(s), loss on training batch is 0.00385885.
After 23137 training step(s), loss on training batch is 0.00610454.
After 23138 training step(s), loss on training batch is 0.00314734.
After 23139 training step(s), loss on training batch is 0.00319805.
After 23140 training step(s), loss on training batch is 0.00372308.
After 23141 training step(s), loss on training batch is 0.00351727.
After 23142 training step(s), loss on training batch is 0.00369055.
After 23143 training step(s), loss on training batch is 0.00364234.
After 23144 training step(s), loss on training batch is 0.00315359.
After 23145 training step(s), loss on training batch is 0.0033298.
After 23146 training step(s), loss on training batch is 0.00339753.
After 23147 training step(s), loss on training batch is 0.00337779.
After 23148 training step(s), loss on training batch is 0.00362085.
After 23149 training step(s), loss on training batch is 0.00328878.
After 23150 training step(s), loss on training batch is 0.003438.
After 23151 training step(s), loss on training batch is 0.0034719.
After 23152 training step(s), loss on training batch is 0.00360448.
After 23153 training step(s), loss on training batch is 0.0037733.
After 23154 training step(s), loss on training batch is 0.00377536.
After 23155 training step(s), loss on training batch is 0.00374651.
After 23156 training step(s), loss on training batch is 0.00359504.
After 23157 training step(s), loss on training batch is 0.00565281.
After 23158 training step(s), loss on training batch is 0.00366382.
After 23159 training step(s), loss on training batch is 0.00357395.
After 23160 training step(s), loss on training batch is 0.00337581.
After 23161 training step(s), loss on training batch is 0.00341004.
After 23162 training step(s), loss on training batch is 0.00358684.
After 23163 training step(s), loss on training batch is 0.00347003.
After 23164 training step(s), loss on training batch is 0.00372564.
After 23165 training step(s), loss on training batch is 0.00323676.
After 23166 training step(s), loss on training batch is 0.00337798.
After 23167 training step(s), loss on training batch is 0.00383592.
After 23168 training step(s), loss on training batch is 0.00345319.
After 23169 training step(s), loss on training batch is 0.0037111.
After 23170 training step(s), loss on training batch is 0.0034778.
After 23171 training step(s), loss on training batch is 0.00337746.
After 23172 training step(s), loss on training batch is 0.0037129.
After 23173 training step(s), loss on training batch is 0.00330589.
After 23174 training step(s), loss on training batch is 0.00365675.
After 23175 training step(s), loss on training batch is 0.00367951.
After 23176 training step(s), loss on training batch is 0.00332447.
After 23177 training step(s), loss on training batch is 0.00356461.
After 23178 training step(s), loss on training batch is 0.00335901.
After 23179 training step(s), loss on training batch is 0.00361338.
After 23180 training step(s), loss on training batch is 0.00333843.
After 23181 training step(s), loss on training batch is 0.00362358.
After 23182 training step(s), loss on training batch is 0.00367244.
After 23183 training step(s), loss on training batch is 0.00358564.
After 23184 training step(s), loss on training batch is 0.00372694.
After 23185 training step(s), loss on training batch is 0.00348505.
After 23186 training step(s), loss on training batch is 0.00374879.
After 23187 training step(s), loss on training batch is 0.00398191.
After 23188 training step(s), loss on training batch is 0.00380913.
After 23189 training step(s), loss on training batch is 0.00343929.
After 23190 training step(s), loss on training batch is 0.00366252.
After 23191 training step(s), loss on training batch is 0.00357733.
After 23192 training step(s), loss on training batch is 0.00351097.
After 23193 training step(s), loss on training batch is 0.0033027.
After 23194 training step(s), loss on training batch is 0.00341787.
After 23195 training step(s), loss on training batch is 0.00384922.
After 23196 training step(s), loss on training batch is 0.00365767.
After 23197 training step(s), loss on training batch is 0.0040149.
After 23198 training step(s), loss on training batch is 0.00358689.
After 23199 training step(s), loss on training batch is 0.00346327.
After 23200 training step(s), loss on training batch is 0.00390308.
After 23201 training step(s), loss on training batch is 0.00396156.
After 23202 training step(s), loss on training batch is 0.00352276.
After 23203 training step(s), loss on training batch is 0.00345706.
After 23204 training step(s), loss on training batch is 0.00367018.
After 23205 training step(s), loss on training batch is 0.0038715.
After 23206 training step(s), loss on training batch is 0.00346336.
After 23207 training step(s), loss on training batch is 0.0033946.
After 23208 training step(s), loss on training batch is 0.00323279.
After 23209 training step(s), loss on training batch is 0.0038552.
After 23210 training step(s), loss on training batch is 0.00329642.
After 23211 training step(s), loss on training batch is 0.00348779.
After 23212 training step(s), loss on training batch is 0.00360854.
After 23213 training step(s), loss on training batch is 0.00336311.
After 23214 training step(s), loss on training batch is 0.00329009.
After 23215 training step(s), loss on training batch is 0.00428883.
After 23216 training step(s), loss on training batch is 0.00325131.
After 23217 training step(s), loss on training batch is 0.00335775.
After 23218 training step(s), loss on training batch is 0.00474334.
After 23219 training step(s), loss on training batch is 0.00339675.
After 23220 training step(s), loss on training batch is 0.00372709.
After 23221 training step(s), loss on training batch is 0.00381859.
After 23222 training step(s), loss on training batch is 0.00338404.
After 23223 training step(s), loss on training batch is 0.00379687.
After 23224 training step(s), loss on training batch is 0.00339344.
After 23225 training step(s), loss on training batch is 0.00354881.
After 23226 training step(s), loss on training batch is 0.00341722.
After 23227 training step(s), loss on training batch is 0.0032599.
After 23228 training step(s), loss on training batch is 0.00320793.
After 23229 training step(s), loss on training batch is 0.00391825.
After 23230 training step(s), loss on training batch is 0.00351997.
After 23231 training step(s), loss on training batch is 0.0037523.
After 23232 training step(s), loss on training batch is 0.00351321.
After 23233 training step(s), loss on training batch is 0.00417127.
After 23234 training step(s), loss on training batch is 0.00373765.
After 23235 training step(s), loss on training batch is 0.00327311.
After 23236 training step(s), loss on training batch is 0.00349603.
After 23237 training step(s), loss on training batch is 0.00385683.
After 23238 training step(s), loss on training batch is 0.00332909.
After 23239 training step(s), loss on training batch is 0.00356406.
After 23240 training step(s), loss on training batch is 0.0032926.
After 23241 training step(s), loss on training batch is 0.00346421.
After 23242 training step(s), loss on training batch is 0.00416588.
After 23243 training step(s), loss on training batch is 0.00350929.
After 23244 training step(s), loss on training batch is 0.00350858.
After 23245 training step(s), loss on training batch is 0.00327067.
After 23246 training step(s), loss on training batch is 0.004202.
After 23247 training step(s), loss on training batch is 0.00350648.
After 23248 training step(s), loss on training batch is 0.00342971.
After 23249 training step(s), loss on training batch is 0.00329353.
After 23250 training step(s), loss on training batch is 0.00377178.
After 23251 training step(s), loss on training batch is 0.00348001.
After 23252 training step(s), loss on training batch is 0.00341351.
After 23253 training step(s), loss on training batch is 0.00352542.
After 23254 training step(s), loss on training batch is 0.00383972.
After 23255 training step(s), loss on training batch is 0.00375167.
After 23256 training step(s), loss on training batch is 0.00413066.
After 23257 training step(s), loss on training batch is 0.00350893.
After 23258 training step(s), loss on training batch is 0.00364066.
After 23259 training step(s), loss on training batch is 0.00400545.
After 23260 training step(s), loss on training batch is 0.00322747.
After 23261 training step(s), loss on training batch is 0.00326152.
After 23262 training step(s), loss on training batch is 0.00328908.
After 23263 training step(s), loss on training batch is 0.00325952.
After 23264 training step(s), loss on training batch is 0.00450421.
After 23265 training step(s), loss on training batch is 0.00408122.
After 23266 training step(s), loss on training batch is 0.00371637.
After 23267 training step(s), loss on training batch is 0.0037736.
After 23268 training step(s), loss on training batch is 0.00315146.
After 23269 training step(s), loss on training batch is 0.00351928.
After 23270 training step(s), loss on training batch is 0.00329927.
After 23271 training step(s), loss on training batch is 0.00328457.
After 23272 training step(s), loss on training batch is 0.00320357.
After 23273 training step(s), loss on training batch is 0.00360604.
After 23274 training step(s), loss on training batch is 0.00352016.
After 23275 training step(s), loss on training batch is 0.00372833.
After 23276 training step(s), loss on training batch is 0.00407799.
After 23277 training step(s), loss on training batch is 0.00337988.
After 23278 training step(s), loss on training batch is 0.00336619.
After 23279 training step(s), loss on training batch is 0.00371797.
After 23280 training step(s), loss on training batch is 0.00347334.
After 23281 training step(s), loss on training batch is 0.00324407.
After 23282 training step(s), loss on training batch is 0.00358205.
After 23283 training step(s), loss on training batch is 0.00365171.
After 23284 training step(s), loss on training batch is 0.00351369.
After 23285 training step(s), loss on training batch is 0.00383017.
After 23286 training step(s), loss on training batch is 0.00334226.
After 23287 training step(s), loss on training batch is 0.00414836.
After 23288 training step(s), loss on training batch is 0.00336447.
After 23289 training step(s), loss on training batch is 0.00363536.
After 23290 training step(s), loss on training batch is 0.00381537.
After 23291 training step(s), loss on training batch is 0.00349644.
After 23292 training step(s), loss on training batch is 0.00373167.
After 23293 training step(s), loss on training batch is 0.00354886.
After 23294 training step(s), loss on training batch is 0.00317836.
After 23295 training step(s), loss on training batch is 0.00361376.
After 23296 training step(s), loss on training batch is 0.00333515.
After 23297 training step(s), loss on training batch is 0.00317168.
After 23298 training step(s), loss on training batch is 0.00373818.
After 23299 training step(s), loss on training batch is 0.00359147.
After 23300 training step(s), loss on training batch is 0.00339308.
After 23301 training step(s), loss on training batch is 0.00319129.
After 23302 training step(s), loss on training batch is 0.00378636.
After 23303 training step(s), loss on training batch is 0.00413134.
After 23304 training step(s), loss on training batch is 0.00341186.
After 23305 training step(s), loss on training batch is 0.0033234.
After 23306 training step(s), loss on training batch is 0.00369557.
After 23307 training step(s), loss on training batch is 0.0038617.
After 23308 training step(s), loss on training batch is 0.00351882.
After 23309 training step(s), loss on training batch is 0.00340695.
After 23310 training step(s), loss on training batch is 0.00344811.
After 23311 training step(s), loss on training batch is 0.0040226.
After 23312 training step(s), loss on training batch is 0.00345021.
After 23313 training step(s), loss on training batch is 0.00400692.
After 23314 training step(s), loss on training batch is 0.00398984.
After 23315 training step(s), loss on training batch is 0.00350007.
After 23316 training step(s), loss on training batch is 0.00368031.
After 23317 training step(s), loss on training batch is 0.00349705.
After 23318 training step(s), loss on training batch is 0.00354958.
After 23319 training step(s), loss on training batch is 0.00355372.
After 23320 training step(s), loss on training batch is 0.00338586.
After 23321 training step(s), loss on training batch is 0.00335684.
After 23322 training step(s), loss on training batch is 0.00380659.
After 23323 training step(s), loss on training batch is 0.0033103.
After 23324 training step(s), loss on training batch is 0.00370364.
After 23325 training step(s), loss on training batch is 0.00348581.
After 23326 training step(s), loss on training batch is 0.00338741.
After 23327 training step(s), loss on training batch is 0.00367264.
After 23328 training step(s), loss on training batch is 0.00353545.
After 23329 training step(s), loss on training batch is 0.00373056.
After 23330 training step(s), loss on training batch is 0.0032552.
After 23331 training step(s), loss on training batch is 0.00352054.
After 23332 training step(s), loss on training batch is 0.00368836.
After 23333 training step(s), loss on training batch is 0.00352929.
After 23334 training step(s), loss on training batch is 0.0037261.
After 23335 training step(s), loss on training batch is 0.00337256.
After 23336 training step(s), loss on training batch is 0.00362885.
After 23337 training step(s), loss on training batch is 0.00444828.
After 23338 training step(s), loss on training batch is 0.0034616.
After 23339 training step(s), loss on training batch is 0.00345849.
After 23340 training step(s), loss on training batch is 0.00368649.
After 23341 training step(s), loss on training batch is 0.00342242.
After 23342 training step(s), loss on training batch is 0.00329243.
After 23343 training step(s), loss on training batch is 0.00333078.
After 23344 training step(s), loss on training batch is 0.00346462.
After 23345 training step(s), loss on training batch is 0.00335917.
After 23346 training step(s), loss on training batch is 0.00356369.
After 23347 training step(s), loss on training batch is 0.00395993.
After 23348 training step(s), loss on training batch is 0.00327749.
After 23349 training step(s), loss on training batch is 0.00384006.
After 23350 training step(s), loss on training batch is 0.00351501.
After 23351 training step(s), loss on training batch is 0.00354745.
After 23352 training step(s), loss on training batch is 0.00343772.
After 23353 training step(s), loss on training batch is 0.00330465.
After 23354 training step(s), loss on training batch is 0.00379539.
After 23355 training step(s), loss on training batch is 0.00345626.
After 23356 training step(s), loss on training batch is 0.00413069.
After 23357 training step(s), loss on training batch is 0.00352197.
After 23358 training step(s), loss on training batch is 0.00386948.
After 23359 training step(s), loss on training batch is 0.00381969.
After 23360 training step(s), loss on training batch is 0.00384905.
After 23361 training step(s), loss on training batch is 0.00334072.
After 23362 training step(s), loss on training batch is 0.00343197.
After 23363 training step(s), loss on training batch is 0.00362676.
After 23364 training step(s), loss on training batch is 0.00337198.
After 23365 training step(s), loss on training batch is 0.00357073.
After 23366 training step(s), loss on training batch is 0.00425418.
After 23367 training step(s), loss on training batch is 0.00359332.
After 23368 training step(s), loss on training batch is 0.00327865.
After 23369 training step(s), loss on training batch is 0.00332683.
After 23370 training step(s), loss on training batch is 0.0036219.
After 23371 training step(s), loss on training batch is 0.00331935.
After 23372 training step(s), loss on training batch is 0.00352387.
After 23373 training step(s), loss on training batch is 0.00398865.
After 23374 training step(s), loss on training batch is 0.0032305.
After 23375 training step(s), loss on training batch is 0.00436876.
After 23376 training step(s), loss on training batch is 0.00395958.
After 23377 training step(s), loss on training batch is 0.00380685.
After 23378 training step(s), loss on training batch is 0.00365877.
After 23379 training step(s), loss on training batch is 0.00385765.
After 23380 training step(s), loss on training batch is 0.00428216.
After 23381 training step(s), loss on training batch is 0.00485882.
After 23382 training step(s), loss on training batch is 0.00343403.
After 23383 training step(s), loss on training batch is 0.00361952.
After 23384 training step(s), loss on training batch is 0.00387832.
After 23385 training step(s), loss on training batch is 0.00334085.
After 23386 training step(s), loss on training batch is 0.00392723.
After 23387 training step(s), loss on training batch is 0.00347757.
After 23388 training step(s), loss on training batch is 0.00382376.
After 23389 training step(s), loss on training batch is 0.00355508.
After 23390 training step(s), loss on training batch is 0.00324055.
After 23391 training step(s), loss on training batch is 0.00357203.
After 23392 training step(s), loss on training batch is 0.00332906.
After 23393 training step(s), loss on training batch is 0.00372537.
After 23394 training step(s), loss on training batch is 0.00395544.
After 23395 training step(s), loss on training batch is 0.00354793.
After 23396 training step(s), loss on training batch is 0.00340632.
After 23397 training step(s), loss on training batch is 0.0033662.
After 23398 training step(s), loss on training batch is 0.00350847.
After 23399 training step(s), loss on training batch is 0.00371481.
After 23400 training step(s), loss on training batch is 0.00359385.
After 23401 training step(s), loss on training batch is 0.00422745.
After 23402 training step(s), loss on training batch is 0.00330675.
After 23403 training step(s), loss on training batch is 0.00322341.
After 23404 training step(s), loss on training batch is 0.00347727.
After 23405 training step(s), loss on training batch is 0.00314923.
After 23406 training step(s), loss on training batch is 0.00332789.
After 23407 training step(s), loss on training batch is 0.00323156.
After 23408 training step(s), loss on training batch is 0.00437062.
After 23409 training step(s), loss on training batch is 0.00344634.
After 23410 training step(s), loss on training batch is 0.00320516.
After 23411 training step(s), loss on training batch is 0.00334249.
After 23412 training step(s), loss on training batch is 0.00330362.
After 23413 training step(s), loss on training batch is 0.00355833.
After 23414 training step(s), loss on training batch is 0.00349333.
After 23415 training step(s), loss on training batch is 0.0033751.
After 23416 training step(s), loss on training batch is 0.00360939.
After 23417 training step(s), loss on training batch is 0.00329022.
After 23418 training step(s), loss on training batch is 0.00372054.
After 23419 training step(s), loss on training batch is 0.00395881.
After 23420 training step(s), loss on training batch is 0.00340957.
After 23421 training step(s), loss on training batch is 0.00357969.
After 23422 training step(s), loss on training batch is 0.00360417.
After 23423 training step(s), loss on training batch is 0.00338556.
After 23424 training step(s), loss on training batch is 0.00391327.
After 23425 training step(s), loss on training batch is 0.00378201.
After 23426 training step(s), loss on training batch is 0.00354534.
After 23427 training step(s), loss on training batch is 0.00417671.
After 23428 training step(s), loss on training batch is 0.00340196.
After 23429 training step(s), loss on training batch is 0.00345093.
After 23430 training step(s), loss on training batch is 0.00346209.
After 23431 training step(s), loss on training batch is 0.00312167.
After 23432 training step(s), loss on training batch is 0.00342378.
After 23433 training step(s), loss on training batch is 0.00338244.
After 23434 training step(s), loss on training batch is 0.00366571.
After 23435 training step(s), loss on training batch is 0.00372607.
After 23436 training step(s), loss on training batch is 0.00374524.
After 23437 training step(s), loss on training batch is 0.00327996.
After 23438 training step(s), loss on training batch is 0.0032882.
After 23439 training step(s), loss on training batch is 0.00342888.
After 23440 training step(s), loss on training batch is 0.00357369.
After 23441 training step(s), loss on training batch is 0.00346815.
After 23442 training step(s), loss on training batch is 0.00345719.
After 23443 training step(s), loss on training batch is 0.00329137.
After 23444 training step(s), loss on training batch is 0.00330957.
After 23445 training step(s), loss on training batch is 0.00332777.
After 23446 training step(s), loss on training batch is 0.00345537.
After 23447 training step(s), loss on training batch is 0.00345478.
After 23448 training step(s), loss on training batch is 0.00347469.
After 23449 training step(s), loss on training batch is 0.00341836.
After 23450 training step(s), loss on training batch is 0.00346042.
After 23451 training step(s), loss on training batch is 0.00323946.
After 23452 training step(s), loss on training batch is 0.00335113.
After 23453 training step(s), loss on training batch is 0.00340768.
After 23454 training step(s), loss on training batch is 0.00341789.
After 23455 training step(s), loss on training batch is 0.00356631.
After 23456 training step(s), loss on training batch is 0.00330194.
After 23457 training step(s), loss on training batch is 0.00351244.
After 23458 training step(s), loss on training batch is 0.00408416.
After 23459 training step(s), loss on training batch is 0.00351571.
After 23460 training step(s), loss on training batch is 0.0033396.
After 23461 training step(s), loss on training batch is 0.00366848.
After 23462 training step(s), loss on training batch is 0.00351315.
After 23463 training step(s), loss on training batch is 0.00377052.
After 23464 training step(s), loss on training batch is 0.00387911.
After 23465 training step(s), loss on training batch is 0.00345898.
After 23466 training step(s), loss on training batch is 0.0032805.
After 23467 training step(s), loss on training batch is 0.00364161.
After 23468 training step(s), loss on training batch is 0.00323722.
After 23469 training step(s), loss on training batch is 0.00381041.
After 23470 training step(s), loss on training batch is 0.00404747.
After 23471 training step(s), loss on training batch is 0.00366106.
After 23472 training step(s), loss on training batch is 0.00366527.
After 23473 training step(s), loss on training batch is 0.00334197.
After 23474 training step(s), loss on training batch is 0.00330722.
After 23475 training step(s), loss on training batch is 0.00328959.
After 23476 training step(s), loss on training batch is 0.00342343.
After 23477 training step(s), loss on training batch is 0.00331082.
After 23478 training step(s), loss on training batch is 0.00354057.
After 23479 training step(s), loss on training batch is 0.0042923.
After 23480 training step(s), loss on training batch is 0.00372644.
After 23481 training step(s), loss on training batch is 0.00353096.
After 23482 training step(s), loss on training batch is 0.00368674.
After 23483 training step(s), loss on training batch is 0.00332669.
After 23484 training step(s), loss on training batch is 0.00333635.
After 23485 training step(s), loss on training batch is 0.00503457.
After 23486 training step(s), loss on training batch is 0.00347611.
After 23487 training step(s), loss on training batch is 0.00380216.
After 23488 training step(s), loss on training batch is 0.00345515.
After 23489 training step(s), loss on training batch is 0.00335865.
After 23490 training step(s), loss on training batch is 0.00391402.
After 23491 training step(s), loss on training batch is 0.00555801.
After 23492 training step(s), loss on training batch is 0.00356415.
After 23493 training step(s), loss on training batch is 0.00358411.
After 23494 training step(s), loss on training batch is 0.00337812.
After 23495 training step(s), loss on training batch is 0.00340479.
After 23496 training step(s), loss on training batch is 0.00338863.
After 23497 training step(s), loss on training batch is 0.00365317.
After 23498 training step(s), loss on training batch is 0.00398608.
After 23499 training step(s), loss on training batch is 0.00328445.
After 23500 training step(s), loss on training batch is 0.00325034.
After 23501 training step(s), loss on training batch is 0.00390643.
After 23502 training step(s), loss on training batch is 0.00346991.
After 23503 training step(s), loss on training batch is 0.00320013.
After 23504 training step(s), loss on training batch is 0.0031504.
After 23505 training step(s), loss on training batch is 0.00368246.
After 23506 training step(s), loss on training batch is 0.0033051.
After 23507 training step(s), loss on training batch is 0.00351589.
After 23508 training step(s), loss on training batch is 0.00383597.
After 23509 training step(s), loss on training batch is 0.00322222.
After 23510 training step(s), loss on training batch is 0.00406021.
After 23511 training step(s), loss on training batch is 0.00350323.
After 23512 training step(s), loss on training batch is 0.00356561.
After 23513 training step(s), loss on training batch is 0.00337524.
After 23514 training step(s), loss on training batch is 0.00397023.
After 23515 training step(s), loss on training batch is 0.0035236.
After 23516 training step(s), loss on training batch is 0.00320276.
After 23517 training step(s), loss on training batch is 0.00352876.
After 23518 training step(s), loss on training batch is 0.00355581.
After 23519 training step(s), loss on training batch is 0.00357603.
After 23520 training step(s), loss on training batch is 0.00340089.
After 23521 training step(s), loss on training batch is 0.00370707.
After 23522 training step(s), loss on training batch is 0.00324745.
After 23523 training step(s), loss on training batch is 0.00374536.
After 23524 training step(s), loss on training batch is 0.00346013.
After 23525 training step(s), loss on training batch is 0.00395649.
After 23526 training step(s), loss on training batch is 0.00379494.
After 23527 training step(s), loss on training batch is 0.00315756.
After 23528 training step(s), loss on training batch is 0.00357719.
After 23529 training step(s), loss on training batch is 0.00334121.
After 23530 training step(s), loss on training batch is 0.00370228.
After 23531 training step(s), loss on training batch is 0.00335863.
After 23532 training step(s), loss on training batch is 0.00335847.
After 23533 training step(s), loss on training batch is 0.00378146.
After 23534 training step(s), loss on training batch is 0.00333464.
After 23535 training step(s), loss on training batch is 0.00327712.
After 23536 training step(s), loss on training batch is 0.00386764.
After 23537 training step(s), loss on training batch is 0.00339426.
After 23538 training step(s), loss on training batch is 0.00465643.
After 23539 training step(s), loss on training batch is 0.00325297.
After 23540 training step(s), loss on training batch is 0.00348802.
After 23541 training step(s), loss on training batch is 0.00467374.
After 23542 training step(s), loss on training batch is 0.00332364.
After 23543 training step(s), loss on training batch is 0.00350099.
After 23544 training step(s), loss on training batch is 0.00336411.
After 23545 training step(s), loss on training batch is 0.00351478.
After 23546 training step(s), loss on training batch is 0.0036426.
After 23547 training step(s), loss on training batch is 0.0036629.
After 23548 training step(s), loss on training batch is 0.00353876.
After 23549 training step(s), loss on training batch is 0.00369951.
After 23550 training step(s), loss on training batch is 0.00394694.
After 23551 training step(s), loss on training batch is 0.00356421.
After 23552 training step(s), loss on training batch is 0.0034807.
After 23553 training step(s), loss on training batch is 0.00377171.
After 23554 training step(s), loss on training batch is 0.00346993.
After 23555 training step(s), loss on training batch is 0.00335084.
After 23556 training step(s), loss on training batch is 0.00336265.
After 23557 training step(s), loss on training batch is 0.00371184.
After 23558 training step(s), loss on training batch is 0.00343253.
After 23559 training step(s), loss on training batch is 0.00359283.
After 23560 training step(s), loss on training batch is 0.00313197.
After 23561 training step(s), loss on training batch is 0.00353845.
After 23562 training step(s), loss on training batch is 0.00375555.
After 23563 training step(s), loss on training batch is 0.00361018.
After 23564 training step(s), loss on training batch is 0.00340644.
After 23565 training step(s), loss on training batch is 0.00481375.
After 23566 training step(s), loss on training batch is 0.00390622.
After 23567 training step(s), loss on training batch is 0.00325568.
After 23568 training step(s), loss on training batch is 0.00358561.
After 23569 training step(s), loss on training batch is 0.00383643.
After 23570 training step(s), loss on training batch is 0.00351938.
After 23571 training step(s), loss on training batch is 0.00360416.
After 23572 training step(s), loss on training batch is 0.00372024.
After 23573 training step(s), loss on training batch is 0.00347283.
After 23574 training step(s), loss on training batch is 0.00329116.
After 23575 training step(s), loss on training batch is 0.00406419.
After 23576 training step(s), loss on training batch is 0.00324177.
After 23577 training step(s), loss on training batch is 0.00334919.
After 23578 training step(s), loss on training batch is 0.00345226.
After 23579 training step(s), loss on training batch is 0.00366455.
After 23580 training step(s), loss on training batch is 0.00344031.
After 23581 training step(s), loss on training batch is 0.00466575.
After 23582 training step(s), loss on training batch is 0.00323431.
After 23583 training step(s), loss on training batch is 0.00399271.
After 23584 training step(s), loss on training batch is 0.00311378.
After 23585 training step(s), loss on training batch is 0.00339201.
After 23586 training step(s), loss on training batch is 0.00337435.
After 23587 training step(s), loss on training batch is 0.00350765.
After 23588 training step(s), loss on training batch is 0.00381244.
After 23589 training step(s), loss on training batch is 0.0034368.
After 23590 training step(s), loss on training batch is 0.00393473.
After 23591 training step(s), loss on training batch is 0.00364286.
After 23592 training step(s), loss on training batch is 0.00346954.
After 23593 training step(s), loss on training batch is 0.00421337.
After 23594 training step(s), loss on training batch is 0.00353108.
After 23595 training step(s), loss on training batch is 0.00322189.
After 23596 training step(s), loss on training batch is 0.00330882.
After 23597 training step(s), loss on training batch is 0.00364281.
After 23598 training step(s), loss on training batch is 0.00359031.
After 23599 training step(s), loss on training batch is 0.00340669.
After 23600 training step(s), loss on training batch is 0.00364872.
After 23601 training step(s), loss on training batch is 0.00400868.
After 23602 training step(s), loss on training batch is 0.00333608.
After 23603 training step(s), loss on training batch is 0.00344862.
After 23604 training step(s), loss on training batch is 0.00340824.
After 23605 training step(s), loss on training batch is 0.00340317.
After 23606 training step(s), loss on training batch is 0.00334428.
After 23607 training step(s), loss on training batch is 0.00341304.
After 23608 training step(s), loss on training batch is 0.00343118.
After 23609 training step(s), loss on training batch is 0.00321411.
After 23610 training step(s), loss on training batch is 0.0035064.
After 23611 training step(s), loss on training batch is 0.00344431.
After 23612 training step(s), loss on training batch is 0.00337354.
After 23613 training step(s), loss on training batch is 0.0033499.
After 23614 training step(s), loss on training batch is 0.00353168.
After 23615 training step(s), loss on training batch is 0.00350543.
After 23616 training step(s), loss on training batch is 0.00311233.
After 23617 training step(s), loss on training batch is 0.00408298.
After 23618 training step(s), loss on training batch is 0.00350951.
After 23619 training step(s), loss on training batch is 0.00354599.
After 23620 training step(s), loss on training batch is 0.00326143.
After 23621 training step(s), loss on training batch is 0.00410292.
After 23622 training step(s), loss on training batch is 0.00354043.
After 23623 training step(s), loss on training batch is 0.00407928.
After 23624 training step(s), loss on training batch is 0.00354784.
After 23625 training step(s), loss on training batch is 0.00369751.
After 23626 training step(s), loss on training batch is 0.00435788.
After 23627 training step(s), loss on training batch is 0.00330069.
After 23628 training step(s), loss on training batch is 0.00383126.
After 23629 training step(s), loss on training batch is 0.00338997.
After 23630 training step(s), loss on training batch is 0.00321326.
After 23631 training step(s), loss on training batch is 0.00354.
After 23632 training step(s), loss on training batch is 0.00341668.
After 23633 training step(s), loss on training batch is 0.0035969.
After 23634 training step(s), loss on training batch is 0.00360295.
After 23635 training step(s), loss on training batch is 0.00398929.
After 23636 training step(s), loss on training batch is 0.0035132.
After 23637 training step(s), loss on training batch is 0.00395368.
After 23638 training step(s), loss on training batch is 0.00384698.
After 23639 training step(s), loss on training batch is 0.00383497.
After 23640 training step(s), loss on training batch is 0.00329109.
After 23641 training step(s), loss on training batch is 0.0038456.
After 23642 training step(s), loss on training batch is 0.00379088.
After 23643 training step(s), loss on training batch is 0.00369437.
After 23644 training step(s), loss on training batch is 0.00360412.
After 23645 training step(s), loss on training batch is 0.00387595.
After 23646 training step(s), loss on training batch is 0.00359828.
After 23647 training step(s), loss on training batch is 0.0034912.
After 23648 training step(s), loss on training batch is 0.00446805.
After 23649 training step(s), loss on training batch is 0.0037599.
After 23650 training step(s), loss on training batch is 0.00323615.
After 23651 training step(s), loss on training batch is 0.00408296.
After 23652 training step(s), loss on training batch is 0.00392094.
After 23653 training step(s), loss on training batch is 0.0037643.
After 23654 training step(s), loss on training batch is 0.00318504.
After 23655 training step(s), loss on training batch is 0.00332143.
After 23656 training step(s), loss on training batch is 0.00362479.
After 23657 training step(s), loss on training batch is 0.00325057.
After 23658 training step(s), loss on training batch is 0.00344891.
After 23659 training step(s), loss on training batch is 0.00360486.
After 23660 training step(s), loss on training batch is 0.00349485.
After 23661 training step(s), loss on training batch is 0.00347886.
After 23662 training step(s), loss on training batch is 0.00326814.
After 23663 training step(s), loss on training batch is 0.00349552.
After 23664 training step(s), loss on training batch is 0.00456711.
After 23665 training step(s), loss on training batch is 0.00335489.
After 23666 training step(s), loss on training batch is 0.00343328.
After 23667 training step(s), loss on training batch is 0.00439986.
After 23668 training step(s), loss on training batch is 0.00385306.
After 23669 training step(s), loss on training batch is 0.0035423.
After 23670 training step(s), loss on training batch is 0.00374012.
After 23671 training step(s), loss on training batch is 0.00362149.
After 23672 training step(s), loss on training batch is 0.0033294.
After 23673 training step(s), loss on training batch is 0.00349792.
After 23674 training step(s), loss on training batch is 0.00348602.
After 23675 training step(s), loss on training batch is 0.00327398.
After 23676 training step(s), loss on training batch is 0.00320095.
After 23677 training step(s), loss on training batch is 0.00384017.
After 23678 training step(s), loss on training batch is 0.00379247.
After 23679 training step(s), loss on training batch is 0.0035853.
After 23680 training step(s), loss on training batch is 0.00411086.
After 23681 training step(s), loss on training batch is 0.00340043.
After 23682 training step(s), loss on training batch is 0.00340676.
After 23683 training step(s), loss on training batch is 0.00376796.
After 23684 training step(s), loss on training batch is 0.00315985.
After 23685 training step(s), loss on training batch is 0.00348853.
After 23686 training step(s), loss on training batch is 0.00399061.
After 23687 training step(s), loss on training batch is 0.0035167.
After 23688 training step(s), loss on training batch is 0.00359021.
After 23689 training step(s), loss on training batch is 0.00325466.
After 23690 training step(s), loss on training batch is 0.00401488.
After 23691 training step(s), loss on training batch is 0.00421973.
After 23692 training step(s), loss on training batch is 0.00362319.
After 23693 training step(s), loss on training batch is 0.00364909.
After 23694 training step(s), loss on training batch is 0.00384704.
After 23695 training step(s), loss on training batch is 0.0036604.
After 23696 training step(s), loss on training batch is 0.0037804.
After 23697 training step(s), loss on training batch is 0.00335286.
After 23698 training step(s), loss on training batch is 0.00337845.
After 23699 training step(s), loss on training batch is 0.00352739.
After 23700 training step(s), loss on training batch is 0.00322185.
After 23701 training step(s), loss on training batch is 0.00314867.
After 23702 training step(s), loss on training batch is 0.00335372.
After 23703 training step(s), loss on training batch is 0.00353702.
After 23704 training step(s), loss on training batch is 0.00353802.
After 23705 training step(s), loss on training batch is 0.00353707.
After 23706 training step(s), loss on training batch is 0.00355634.
After 23707 training step(s), loss on training batch is 0.00351437.
After 23708 training step(s), loss on training batch is 0.00350526.
After 23709 training step(s), loss on training batch is 0.0034683.
After 23710 training step(s), loss on training batch is 0.00340402.
After 23711 training step(s), loss on training batch is 0.00336259.
After 23712 training step(s), loss on training batch is 0.0034343.
After 23713 training step(s), loss on training batch is 0.00334888.
After 23714 training step(s), loss on training batch is 0.00347967.
After 23715 training step(s), loss on training batch is 0.00368432.
After 23716 training step(s), loss on training batch is 0.0030671.
After 23717 training step(s), loss on training batch is 0.0034164.
After 23718 training step(s), loss on training batch is 0.00339007.
After 23719 training step(s), loss on training batch is 0.00341146.
After 23720 training step(s), loss on training batch is 0.00354424.
After 23721 training step(s), loss on training batch is 0.00343574.
After 23722 training step(s), loss on training batch is 0.00359268.
After 23723 training step(s), loss on training batch is 0.00353628.
After 23724 training step(s), loss on training batch is 0.00338087.
After 23725 training step(s), loss on training batch is 0.00334392.
After 23726 training step(s), loss on training batch is 0.00399533.
After 23727 training step(s), loss on training batch is 0.00337687.
After 23728 training step(s), loss on training batch is 0.00389555.
After 23729 training step(s), loss on training batch is 0.00333079.
After 23730 training step(s), loss on training batch is 0.00358822.
After 23731 training step(s), loss on training batch is 0.00339615.
After 23732 training step(s), loss on training batch is 0.00358175.
After 23733 training step(s), loss on training batch is 0.00345816.
After 23734 training step(s), loss on training batch is 0.00325798.
After 23735 training step(s), loss on training batch is 0.00354431.
After 23736 training step(s), loss on training batch is 0.00348343.
After 23737 training step(s), loss on training batch is 0.00385757.
After 23738 training step(s), loss on training batch is 0.00331598.
After 23739 training step(s), loss on training batch is 0.00391715.
After 23740 training step(s), loss on training batch is 0.00444549.
After 23741 training step(s), loss on training batch is 0.00385674.
After 23742 training step(s), loss on training batch is 0.00351559.
After 23743 training step(s), loss on training batch is 0.0032278.
After 23744 training step(s), loss on training batch is 0.00362921.
After 23745 training step(s), loss on training batch is 0.00332488.
After 23746 training step(s), loss on training batch is 0.00366094.
After 23747 training step(s), loss on training batch is 0.00350785.
After 23748 training step(s), loss on training batch is 0.00330375.
After 23749 training step(s), loss on training batch is 0.00378954.
After 23750 training step(s), loss on training batch is 0.00368355.
After 23751 training step(s), loss on training batch is 0.00338967.
After 23752 training step(s), loss on training batch is 0.00333265.
After 23753 training step(s), loss on training batch is 0.00356592.
After 23754 training step(s), loss on training batch is 0.00351983.
After 23755 training step(s), loss on training batch is 0.00338703.
After 23756 training step(s), loss on training batch is 0.00369766.
After 23757 training step(s), loss on training batch is 0.00361099.
After 23758 training step(s), loss on training batch is 0.00357191.
After 23759 training step(s), loss on training batch is 0.00366844.
After 23760 training step(s), loss on training batch is 0.00343188.
After 23761 training step(s), loss on training batch is 0.00353574.
After 23762 training step(s), loss on training batch is 0.00332063.
After 23763 training step(s), loss on training batch is 0.00359423.
After 23764 training step(s), loss on training batch is 0.0035987.
After 23765 training step(s), loss on training batch is 0.00357905.
After 23766 training step(s), loss on training batch is 0.00330616.
After 23767 training step(s), loss on training batch is 0.00341423.
After 23768 training step(s), loss on training batch is 0.00339343.
After 23769 training step(s), loss on training batch is 0.00334392.
After 23770 training step(s), loss on training batch is 0.00376537.
After 23771 training step(s), loss on training batch is 0.00380882.
After 23772 training step(s), loss on training batch is 0.00347513.
After 23773 training step(s), loss on training batch is 0.0034054.
After 23774 training step(s), loss on training batch is 0.00397689.
After 23775 training step(s), loss on training batch is 0.00327925.
After 23776 training step(s), loss on training batch is 0.00342393.
After 23777 training step(s), loss on training batch is 0.00343395.
After 23778 training step(s), loss on training batch is 0.00333527.
After 23779 training step(s), loss on training batch is 0.00341064.
After 23780 training step(s), loss on training batch is 0.00357997.
After 23781 training step(s), loss on training batch is 0.00348221.
After 23782 training step(s), loss on training batch is 0.00378637.
After 23783 training step(s), loss on training batch is 0.00326801.
After 23784 training step(s), loss on training batch is 0.00358536.
After 23785 training step(s), loss on training batch is 0.00337151.
After 23786 training step(s), loss on training batch is 0.00354338.
After 23787 training step(s), loss on training batch is 0.00324771.
After 23788 training step(s), loss on training batch is 0.00366317.
After 23789 training step(s), loss on training batch is 0.00346544.
After 23790 training step(s), loss on training batch is 0.00381409.
After 23791 training step(s), loss on training batch is 0.00318692.
After 23792 training step(s), loss on training batch is 0.00333224.
After 23793 training step(s), loss on training batch is 0.0033424.
After 23794 training step(s), loss on training batch is 0.0031017.
After 23795 training step(s), loss on training batch is 0.00433917.
After 23796 training step(s), loss on training batch is 0.00331983.
After 23797 training step(s), loss on training batch is 0.00329967.
After 23798 training step(s), loss on training batch is 0.00348085.
After 23799 training step(s), loss on training batch is 0.00362372.
After 23800 training step(s), loss on training batch is 0.00665789.
After 23801 training step(s), loss on training batch is 0.00376451.
After 23802 training step(s), loss on training batch is 0.00354777.
After 23803 training step(s), loss on training batch is 0.00418585.
After 23804 training step(s), loss on training batch is 0.00337089.
After 23805 training step(s), loss on training batch is 0.00337473.
After 23806 training step(s), loss on training batch is 0.00367045.
After 23807 training step(s), loss on training batch is 0.00355717.
After 23808 training step(s), loss on training batch is 0.00341684.
After 23809 training step(s), loss on training batch is 0.00356618.
After 23810 training step(s), loss on training batch is 0.00395553.
After 23811 training step(s), loss on training batch is 0.00350333.
After 23812 training step(s), loss on training batch is 0.0032973.
After 23813 training step(s), loss on training batch is 0.00417147.
After 23814 training step(s), loss on training batch is 0.00341019.
After 23815 training step(s), loss on training batch is 0.0032922.
After 23816 training step(s), loss on training batch is 0.00338179.
After 23817 training step(s), loss on training batch is 0.00377995.
After 23818 training step(s), loss on training batch is 0.00379415.
After 23819 training step(s), loss on training batch is 0.00340702.
After 23820 training step(s), loss on training batch is 0.0035231.
After 23821 training step(s), loss on training batch is 0.0038211.
After 23822 training step(s), loss on training batch is 0.00374503.
After 23823 training step(s), loss on training batch is 0.00386867.
After 23824 training step(s), loss on training batch is 0.00361874.
After 23825 training step(s), loss on training batch is 0.00413984.
After 23826 training step(s), loss on training batch is 0.003764.
After 23827 training step(s), loss on training batch is 0.00346452.
After 23828 training step(s), loss on training batch is 0.00353546.
After 23829 training step(s), loss on training batch is 0.00424787.
After 23830 training step(s), loss on training batch is 0.00350656.
After 23831 training step(s), loss on training batch is 0.00347227.
After 23832 training step(s), loss on training batch is 0.00333212.
After 23833 training step(s), loss on training batch is 0.00335549.
After 23834 training step(s), loss on training batch is 0.00379667.
After 23835 training step(s), loss on training batch is 0.00353233.
After 23836 training step(s), loss on training batch is 0.00364742.
After 23837 training step(s), loss on training batch is 0.0032907.
After 23838 training step(s), loss on training batch is 0.00323615.
After 23839 training step(s), loss on training batch is 0.00341568.
After 23840 training step(s), loss on training batch is 0.0035874.
After 23841 training step(s), loss on training batch is 0.00330743.
After 23842 training step(s), loss on training batch is 0.00357959.
After 23843 training step(s), loss on training batch is 0.00355428.
After 23844 training step(s), loss on training batch is 0.0032804.
After 23845 training step(s), loss on training batch is 0.00316924.
After 23846 training step(s), loss on training batch is 0.00369259.
After 23847 training step(s), loss on training batch is 0.00347029.
After 23848 training step(s), loss on training batch is 0.00339093.
After 23849 training step(s), loss on training batch is 0.00335414.
After 23850 training step(s), loss on training batch is 0.0037792.
After 23851 training step(s), loss on training batch is 0.00441166.
After 23852 training step(s), loss on training batch is 0.00361264.
After 23853 training step(s), loss on training batch is 0.00356412.
After 23854 training step(s), loss on training batch is 0.00347722.
After 23855 training step(s), loss on training batch is 0.00316571.
After 23856 training step(s), loss on training batch is 0.00363201.
After 23857 training step(s), loss on training batch is 0.00331736.
After 23858 training step(s), loss on training batch is 0.00327982.
After 23859 training step(s), loss on training batch is 0.0032267.
After 23860 training step(s), loss on training batch is 0.00312838.
After 23861 training step(s), loss on training batch is 0.00332063.
After 23862 training step(s), loss on training batch is 0.00331963.
After 23863 training step(s), loss on training batch is 0.00321026.
After 23864 training step(s), loss on training batch is 0.00394226.
After 23865 training step(s), loss on training batch is 0.0032203.
After 23866 training step(s), loss on training batch is 0.00350692.
After 23867 training step(s), loss on training batch is 0.00315317.
After 23868 training step(s), loss on training batch is 0.00360817.
After 23869 training step(s), loss on training batch is 0.00353488.
After 23870 training step(s), loss on training batch is 0.00342903.
After 23871 training step(s), loss on training batch is 0.00357383.
After 23872 training step(s), loss on training batch is 0.00352937.
After 23873 training step(s), loss on training batch is 0.00345049.
After 23874 training step(s), loss on training batch is 0.00324117.
After 23875 training step(s), loss on training batch is 0.0031681.
After 23876 training step(s), loss on training batch is 0.0044547.
After 23877 training step(s), loss on training batch is 0.00353425.
After 23878 training step(s), loss on training batch is 0.00328023.
After 23879 training step(s), loss on training batch is 0.00359653.
After 23880 training step(s), loss on training batch is 0.00344999.
After 23881 training step(s), loss on training batch is 0.00330907.
After 23882 training step(s), loss on training batch is 0.00347463.
After 23883 training step(s), loss on training batch is 0.0040196.
After 23884 training step(s), loss on training batch is 0.00333037.
After 23885 training step(s), loss on training batch is 0.00360826.
After 23886 training step(s), loss on training batch is 0.00338131.
After 23887 training step(s), loss on training batch is 0.00418909.
After 23888 training step(s), loss on training batch is 0.00347761.
After 23889 training step(s), loss on training batch is 0.00465963.
After 23890 training step(s), loss on training batch is 0.00334184.
After 23891 training step(s), loss on training batch is 0.0037417.
After 23892 training step(s), loss on training batch is 0.00357207.
After 23893 training step(s), loss on training batch is 0.00365004.
After 23894 training step(s), loss on training batch is 0.00370639.
After 23895 training step(s), loss on training batch is 0.00349943.
After 23896 training step(s), loss on training batch is 0.00375907.
After 23897 training step(s), loss on training batch is 0.00332977.
After 23898 training step(s), loss on training batch is 0.00407253.
After 23899 training step(s), loss on training batch is 0.00353992.
After 23900 training step(s), loss on training batch is 0.00309699.
After 23901 training step(s), loss on training batch is 0.00354843.
After 23902 training step(s), loss on training batch is 0.00393674.
After 23903 training step(s), loss on training batch is 0.00334116.
After 23904 training step(s), loss on training batch is 0.0037028.
After 23905 training step(s), loss on training batch is 0.00314891.
After 23906 training step(s), loss on training batch is 0.00410704.
After 23907 training step(s), loss on training batch is 0.00384819.
After 23908 training step(s), loss on training batch is 0.00352454.
After 23909 training step(s), loss on training batch is 0.00329313.
After 23910 training step(s), loss on training batch is 0.00346161.
After 23911 training step(s), loss on training batch is 0.0031876.
After 23912 training step(s), loss on training batch is 0.00416113.
After 23913 training step(s), loss on training batch is 0.00314274.
After 23914 training step(s), loss on training batch is 0.00371262.
After 23915 training step(s), loss on training batch is 0.00373241.
After 23916 training step(s), loss on training batch is 0.00392366.
After 23917 training step(s), loss on training batch is 0.00327367.
After 23918 training step(s), loss on training batch is 0.00366421.
After 23919 training step(s), loss on training batch is 0.00335232.
After 23920 training step(s), loss on training batch is 0.00419989.
After 23921 training step(s), loss on training batch is 0.00358117.
After 23922 training step(s), loss on training batch is 0.0034976.
After 23923 training step(s), loss on training batch is 0.00323715.
After 23924 training step(s), loss on training batch is 0.00366999.
After 23925 training step(s), loss on training batch is 0.0037644.
After 23926 training step(s), loss on training batch is 0.00345814.
After 23927 training step(s), loss on training batch is 0.00371625.
After 23928 training step(s), loss on training batch is 0.00424718.
After 23929 training step(s), loss on training batch is 0.00324405.
After 23930 training step(s), loss on training batch is 0.0035583.
After 23931 training step(s), loss on training batch is 0.00419787.
After 23932 training step(s), loss on training batch is 0.00340294.
After 23933 training step(s), loss on training batch is 0.00385565.
After 23934 training step(s), loss on training batch is 0.003404.
After 23935 training step(s), loss on training batch is 0.00345969.
After 23936 training step(s), loss on training batch is 0.00323622.
After 23937 training step(s), loss on training batch is 0.00323909.
After 23938 training step(s), loss on training batch is 0.00342954.
After 23939 training step(s), loss on training batch is 0.0032496.
After 23940 training step(s), loss on training batch is 0.00383612.
After 23941 training step(s), loss on training batch is 0.00341784.
After 23942 training step(s), loss on training batch is 0.00349193.
After 23943 training step(s), loss on training batch is 0.0035137.
After 23944 training step(s), loss on training batch is 0.00377717.
After 23945 training step(s), loss on training batch is 0.00334524.
After 23946 training step(s), loss on training batch is 0.00362466.
After 23947 training step(s), loss on training batch is 0.0030695.
After 23948 training step(s), loss on training batch is 0.00328009.
After 23949 training step(s), loss on training batch is 0.00343496.
After 23950 training step(s), loss on training batch is 0.00317956.
After 23951 training step(s), loss on training batch is 0.00330444.
After 23952 training step(s), loss on training batch is 0.00381604.
After 23953 training step(s), loss on training batch is 0.00326153.
After 23954 training step(s), loss on training batch is 0.00341781.
After 23955 training step(s), loss on training batch is 0.00313123.
After 23956 training step(s), loss on training batch is 0.00318076.
After 23957 training step(s), loss on training batch is 0.00361554.
After 23958 training step(s), loss on training batch is 0.00355027.
After 23959 training step(s), loss on training batch is 0.00369878.
After 23960 training step(s), loss on training batch is 0.00321295.
After 23961 training step(s), loss on training batch is 0.003853.
After 23962 training step(s), loss on training batch is 0.0032769.
After 23963 training step(s), loss on training batch is 0.00350779.
After 23964 training step(s), loss on training batch is 0.00349397.
After 23965 training step(s), loss on training batch is 0.00333912.
After 23966 training step(s), loss on training batch is 0.00358938.
After 23967 training step(s), loss on training batch is 0.00320692.
After 23968 training step(s), loss on training batch is 0.0034502.
After 23969 training step(s), loss on training batch is 0.00365212.
After 23970 training step(s), loss on training batch is 0.00316347.
After 23971 training step(s), loss on training batch is 0.00348891.
After 23972 training step(s), loss on training batch is 0.00416407.
After 23973 training step(s), loss on training batch is 0.00393861.
After 23974 training step(s), loss on training batch is 0.0032643.
After 23975 training step(s), loss on training batch is 0.00385812.
After 23976 training step(s), loss on training batch is 0.00344554.
After 23977 training step(s), loss on training batch is 0.00347479.
After 23978 training step(s), loss on training batch is 0.0034163.
After 23979 training step(s), loss on training batch is 0.00323785.
After 23980 training step(s), loss on training batch is 0.00363772.
After 23981 training step(s), loss on training batch is 0.00330458.
After 23982 training step(s), loss on training batch is 0.00354471.
After 23983 training step(s), loss on training batch is 0.00349382.
After 23984 training step(s), loss on training batch is 0.00395539.
After 23985 training step(s), loss on training batch is 0.00437951.
After 23986 training step(s), loss on training batch is 0.00342582.
After 23987 training step(s), loss on training batch is 0.00325677.
After 23988 training step(s), loss on training batch is 0.00349285.
After 23989 training step(s), loss on training batch is 0.00342617.
After 23990 training step(s), loss on training batch is 0.00341486.
After 23991 training step(s), loss on training batch is 0.00375327.
After 23992 training step(s), loss on training batch is 0.00350557.
After 23993 training step(s), loss on training batch is 0.00341033.
After 23994 training step(s), loss on training batch is 0.00360003.
After 23995 training step(s), loss on training batch is 0.00313042.
After 23996 training step(s), loss on training batch is 0.00376714.
After 23997 training step(s), loss on training batch is 0.00386777.
After 23998 training step(s), loss on training batch is 0.00318583.
After 23999 training step(s), loss on training batch is 0.00325249.
After 24000 training step(s), loss on training batch is 0.00355138.
After 24001 training step(s), loss on training batch is 0.00358325.
After 24002 training step(s), loss on training batch is 0.00339849.
After 24003 training step(s), loss on training batch is 0.00363592.
After 24004 training step(s), loss on training batch is 0.00388637.
After 24005 training step(s), loss on training batch is 0.00374221.
After 24006 training step(s), loss on training batch is 0.00415385.
After 24007 training step(s), loss on training batch is 0.00390925.
After 24008 training step(s), loss on training batch is 0.00336757.
After 24009 training step(s), loss on training batch is 0.00337091.
After 24010 training step(s), loss on training batch is 0.00327387.
After 24011 training step(s), loss on training batch is 0.00331321.
After 24012 training step(s), loss on training batch is 0.00325161.
After 24013 training step(s), loss on training batch is 0.00358022.
After 24014 training step(s), loss on training batch is 0.00363005.
After 24015 training step(s), loss on training batch is 0.00375521.
After 24016 training step(s), loss on training batch is 0.00345896.
After 24017 training step(s), loss on training batch is 0.00366177.
After 24018 training step(s), loss on training batch is 0.00334806.
After 24019 training step(s), loss on training batch is 0.00371777.
After 24020 training step(s), loss on training batch is 0.00345474.
After 24021 training step(s), loss on training batch is 0.00390775.
After 24022 training step(s), loss on training batch is 0.00355049.
After 24023 training step(s), loss on training batch is 0.00337619.
After 24024 training step(s), loss on training batch is 0.00333373.
After 24025 training step(s), loss on training batch is 0.00401153.
After 24026 training step(s), loss on training batch is 0.00330798.
After 24027 training step(s), loss on training batch is 0.00375666.
After 24028 training step(s), loss on training batch is 0.00358207.
After 24029 training step(s), loss on training batch is 0.00366805.
After 24030 training step(s), loss on training batch is 0.00345905.
After 24031 training step(s), loss on training batch is 0.00363534.
After 24032 training step(s), loss on training batch is 0.00316646.
After 24033 training step(s), loss on training batch is 0.0034394.
After 24034 training step(s), loss on training batch is 0.00329568.
After 24035 training step(s), loss on training batch is 0.00361239.
After 24036 training step(s), loss on training batch is 0.00354953.
After 24037 training step(s), loss on training batch is 0.00352119.
After 24038 training step(s), loss on training batch is 0.00347658.
After 24039 training step(s), loss on training batch is 0.00335963.
After 24040 training step(s), loss on training batch is 0.00400894.
After 24041 training step(s), loss on training batch is 0.00345976.
After 24042 training step(s), loss on training batch is 0.0033558.
After 24043 training step(s), loss on training batch is 0.00361578.
After 24044 training step(s), loss on training batch is 0.00387313.
After 24045 training step(s), loss on training batch is 0.00351602.
After 24046 training step(s), loss on training batch is 0.00372323.
After 24047 training step(s), loss on training batch is 0.00341934.
After 24048 training step(s), loss on training batch is 0.00341533.
After 24049 training step(s), loss on training batch is 0.00347417.
After 24050 training step(s), loss on training batch is 0.00342506.
After 24051 training step(s), loss on training batch is 0.0037214.
After 24052 training step(s), loss on training batch is 0.00323824.
After 24053 training step(s), loss on training batch is 0.00319148.
After 24054 training step(s), loss on training batch is 0.00334229.
After 24055 training step(s), loss on training batch is 0.00349111.
After 24056 training step(s), loss on training batch is 0.00393677.
After 24057 training step(s), loss on training batch is 0.00339231.
After 24058 training step(s), loss on training batch is 0.00344918.
After 24059 training step(s), loss on training batch is 0.00370919.
After 24060 training step(s), loss on training batch is 0.00319566.
After 24061 training step(s), loss on training batch is 0.00362612.
After 24062 training step(s), loss on training batch is 0.00325057.
After 24063 training step(s), loss on training batch is 0.00329954.
After 24064 training step(s), loss on training batch is 0.00363628.
After 24065 training step(s), loss on training batch is 0.00361443.
After 24066 training step(s), loss on training batch is 0.00397956.
After 24067 training step(s), loss on training batch is 0.00356119.
After 24068 training step(s), loss on training batch is 0.00396044.
After 24069 training step(s), loss on training batch is 0.0048643.
After 24070 training step(s), loss on training batch is 0.00423494.
After 24071 training step(s), loss on training batch is 0.00441503.
After 24072 training step(s), loss on training batch is 0.00411549.
After 24073 training step(s), loss on training batch is 0.00374958.
After 24074 training step(s), loss on training batch is 0.00394048.
After 24075 training step(s), loss on training batch is 0.00337262.
After 24076 training step(s), loss on training batch is 0.00336648.
After 24077 training step(s), loss on training batch is 0.00453808.
After 24078 training step(s), loss on training batch is 0.00346485.
After 24079 training step(s), loss on training batch is 0.00335192.
After 24080 training step(s), loss on training batch is 0.00326832.
After 24081 training step(s), loss on training batch is 0.00332122.
After 24082 training step(s), loss on training batch is 0.00390804.
After 24083 training step(s), loss on training batch is 0.00342505.
After 24084 training step(s), loss on training batch is 0.00387185.
After 24085 training step(s), loss on training batch is 0.00361721.
After 24086 training step(s), loss on training batch is 0.00328091.
After 24087 training step(s), loss on training batch is 0.00412259.
After 24088 training step(s), loss on training batch is 0.0033239.
After 24089 training step(s), loss on training batch is 0.00348562.
After 24090 training step(s), loss on training batch is 0.00367837.
After 24091 training step(s), loss on training batch is 0.00363897.
After 24092 training step(s), loss on training batch is 0.00314465.
After 24093 training step(s), loss on training batch is 0.00359188.
After 24094 training step(s), loss on training batch is 0.00334834.
After 24095 training step(s), loss on training batch is 0.00343141.
After 24096 training step(s), loss on training batch is 0.00346454.
After 24097 training step(s), loss on training batch is 0.00405574.
After 24098 training step(s), loss on training batch is 0.00350428.
After 24099 training step(s), loss on training batch is 0.00334487.
After 24100 training step(s), loss on training batch is 0.00321285.
After 24101 training step(s), loss on training batch is 0.00341436.
After 24102 training step(s), loss on training batch is 0.00371267.
After 24103 training step(s), loss on training batch is 0.00334924.
After 24104 training step(s), loss on training batch is 0.00341947.
After 24105 training step(s), loss on training batch is 0.00432862.
After 24106 training step(s), loss on training batch is 0.0040338.
After 24107 training step(s), loss on training batch is 0.00354367.
After 24108 training step(s), loss on training batch is 0.00371673.
After 24109 training step(s), loss on training batch is 0.00383324.
After 24110 training step(s), loss on training batch is 0.00318569.
After 24111 training step(s), loss on training batch is 0.00353425.
After 24112 training step(s), loss on training batch is 0.00321449.
After 24113 training step(s), loss on training batch is 0.00351956.
After 24114 training step(s), loss on training batch is 0.00311637.
After 24115 training step(s), loss on training batch is 0.00356355.
After 24116 training step(s), loss on training batch is 0.00319285.
After 24117 training step(s), loss on training batch is 0.00351472.
After 24118 training step(s), loss on training batch is 0.00324978.
After 24119 training step(s), loss on training batch is 0.00338337.
After 24120 training step(s), loss on training batch is 0.00357422.
After 24121 training step(s), loss on training batch is 0.00329811.
After 24122 training step(s), loss on training batch is 0.00326636.
After 24123 training step(s), loss on training batch is 0.00370036.
After 24124 training step(s), loss on training batch is 0.00499878.
After 24125 training step(s), loss on training batch is 0.00358003.
After 24126 training step(s), loss on training batch is 0.0033173.
After 24127 training step(s), loss on training batch is 0.00394926.
After 24128 training step(s), loss on training batch is 0.00367683.
After 24129 training step(s), loss on training batch is 0.00329762.
After 24130 training step(s), loss on training batch is 0.00370533.
After 24131 training step(s), loss on training batch is 0.00326898.
After 24132 training step(s), loss on training batch is 0.00383799.
After 24133 training step(s), loss on training batch is 0.00355342.
After 24134 training step(s), loss on training batch is 0.00338635.
After 24135 training step(s), loss on training batch is 0.0036245.
After 24136 training step(s), loss on training batch is 0.0032903.
After 24137 training step(s), loss on training batch is 0.00353696.
After 24138 training step(s), loss on training batch is 0.00321659.
After 24139 training step(s), loss on training batch is 0.0031913.
After 24140 training step(s), loss on training batch is 0.00319192.
After 24141 training step(s), loss on training batch is 0.00326281.
After 24142 training step(s), loss on training batch is 0.0037174.
After 24143 training step(s), loss on training batch is 0.00420963.
After 24144 training step(s), loss on training batch is 0.00335815.
After 24145 training step(s), loss on training batch is 0.00355585.
After 24146 training step(s), loss on training batch is 0.00340534.
After 24147 training step(s), loss on training batch is 0.00323635.
After 24148 training step(s), loss on training batch is 0.00336686.
After 24149 training step(s), loss on training batch is 0.00360285.
After 24150 training step(s), loss on training batch is 0.00320909.
After 24151 training step(s), loss on training batch is 0.00333087.
After 24152 training step(s), loss on training batch is 0.00353587.
After 24153 training step(s), loss on training batch is 0.00382188.
After 24154 training step(s), loss on training batch is 0.00364884.
After 24155 training step(s), loss on training batch is 0.0035489.
After 24156 training step(s), loss on training batch is 0.00343857.
After 24157 training step(s), loss on training batch is 0.00354663.
After 24158 training step(s), loss on training batch is 0.00374525.
After 24159 training step(s), loss on training batch is 0.0038255.
After 24160 training step(s), loss on training batch is 0.00357788.
After 24161 training step(s), loss on training batch is 0.00335966.
After 24162 training step(s), loss on training batch is 0.00390652.
After 24163 training step(s), loss on training batch is 0.00347877.
After 24164 training step(s), loss on training batch is 0.00374763.
After 24165 training step(s), loss on training batch is 0.00312052.
After 24166 training step(s), loss on training batch is 0.00316416.
After 24167 training step(s), loss on training batch is 0.00374203.
After 24168 training step(s), loss on training batch is 0.00322072.
After 24169 training step(s), loss on training batch is 0.00371268.
After 24170 training step(s), loss on training batch is 0.0038819.
After 24171 training step(s), loss on training batch is 0.00354237.
After 24172 training step(s), loss on training batch is 0.0033087.
After 24173 training step(s), loss on training batch is 0.00339203.
After 24174 training step(s), loss on training batch is 0.00324218.
After 24175 training step(s), loss on training batch is 0.00333543.
After 24176 training step(s), loss on training batch is 0.0034408.
After 24177 training step(s), loss on training batch is 0.00339066.
After 24178 training step(s), loss on training batch is 0.00340287.
After 24179 training step(s), loss on training batch is 0.00358794.
After 24180 training step(s), loss on training batch is 0.00327863.
After 24181 training step(s), loss on training batch is 0.00352101.
After 24182 training step(s), loss on training batch is 0.00370748.
After 24183 training step(s), loss on training batch is 0.0039538.
After 24184 training step(s), loss on training batch is 0.00366691.
After 24185 training step(s), loss on training batch is 0.00404289.
After 24186 training step(s), loss on training batch is 0.00369068.
After 24187 training step(s), loss on training batch is 0.00307047.
After 24188 training step(s), loss on training batch is 0.00351163.
After 24189 training step(s), loss on training batch is 0.00311635.
After 24190 training step(s), loss on training batch is 0.00374353.
After 24191 training step(s), loss on training batch is 0.00313143.
After 24192 training step(s), loss on training batch is 0.00347552.
After 24193 training step(s), loss on training batch is 0.0036948.
After 24194 training step(s), loss on training batch is 0.00411793.
After 24195 training step(s), loss on training batch is 0.00356297.
After 24196 training step(s), loss on training batch is 0.00320096.
After 24197 training step(s), loss on training batch is 0.00336929.
After 24198 training step(s), loss on training batch is 0.00373322.
After 24199 training step(s), loss on training batch is 0.00316646.
After 24200 training step(s), loss on training batch is 0.00385548.
After 24201 training step(s), loss on training batch is 0.00351423.
After 24202 training step(s), loss on training batch is 0.00340106.
After 24203 training step(s), loss on training batch is 0.00333337.
After 24204 training step(s), loss on training batch is 0.00324835.
After 24205 training step(s), loss on training batch is 0.00340575.
After 24206 training step(s), loss on training batch is 0.00379617.
After 24207 training step(s), loss on training batch is 0.00350244.
After 24208 training step(s), loss on training batch is 0.00330676.
After 24209 training step(s), loss on training batch is 0.00333842.
After 24210 training step(s), loss on training batch is 0.00337303.
After 24211 training step(s), loss on training batch is 0.00341971.
After 24212 training step(s), loss on training batch is 0.00319434.
After 24213 training step(s), loss on training batch is 0.00321356.
After 24214 training step(s), loss on training batch is 0.00366209.
After 24215 training step(s), loss on training batch is 0.00324203.
After 24216 training step(s), loss on training batch is 0.00317383.
After 24217 training step(s), loss on training batch is 0.00359643.
After 24218 training step(s), loss on training batch is 0.00321721.
After 24219 training step(s), loss on training batch is 0.00351087.
After 24220 training step(s), loss on training batch is 0.0032618.
After 24221 training step(s), loss on training batch is 0.00326224.
After 24222 training step(s), loss on training batch is 0.00347362.
After 24223 training step(s), loss on training batch is 0.0033029.
After 24224 training step(s), loss on training batch is 0.00336434.
After 24225 training step(s), loss on training batch is 0.00378496.
After 24226 training step(s), loss on training batch is 0.00340378.
After 24227 training step(s), loss on training batch is 0.00352658.
After 24228 training step(s), loss on training batch is 0.0032431.
After 24229 training step(s), loss on training batch is 0.00369295.
After 24230 training step(s), loss on training batch is 0.00312932.
After 24231 training step(s), loss on training batch is 0.00404401.
After 24232 training step(s), loss on training batch is 0.00362754.
After 24233 training step(s), loss on training batch is 0.00345552.
After 24234 training step(s), loss on training batch is 0.00360655.
After 24235 training step(s), loss on training batch is 0.00325172.
After 24236 training step(s), loss on training batch is 0.00356893.
After 24237 training step(s), loss on training batch is 0.00333749.
After 24238 training step(s), loss on training batch is 0.00325422.
After 24239 training step(s), loss on training batch is 0.00432484.
After 24240 training step(s), loss on training batch is 0.00355117.
After 24241 training step(s), loss on training batch is 0.00341533.
After 24242 training step(s), loss on training batch is 0.00365812.
After 24243 training step(s), loss on training batch is 0.00351588.
After 24244 training step(s), loss on training batch is 0.00346944.
After 24245 training step(s), loss on training batch is 0.00327018.
After 24246 training step(s), loss on training batch is 0.00386351.
After 24247 training step(s), loss on training batch is 0.00349132.
After 24248 training step(s), loss on training batch is 0.00389713.
After 24249 training step(s), loss on training batch is 0.00405937.
After 24250 training step(s), loss on training batch is 0.00318884.
After 24251 training step(s), loss on training batch is 0.00360604.
After 24252 training step(s), loss on training batch is 0.0034742.
After 24253 training step(s), loss on training batch is 0.00373387.
After 24254 training step(s), loss on training batch is 0.00528581.
After 24255 training step(s), loss on training batch is 0.00354852.
After 24256 training step(s), loss on training batch is 0.00394752.
After 24257 training step(s), loss on training batch is 0.00356556.
After 24258 training step(s), loss on training batch is 0.00367641.
After 24259 training step(s), loss on training batch is 0.00310703.
After 24260 training step(s), loss on training batch is 0.0037288.
After 24261 training step(s), loss on training batch is 0.00339524.
After 24262 training step(s), loss on training batch is 0.00359623.
After 24263 training step(s), loss on training batch is 0.00318605.
After 24264 training step(s), loss on training batch is 0.00327746.
After 24265 training step(s), loss on training batch is 0.00365659.
After 24266 training step(s), loss on training batch is 0.00353858.
After 24267 training step(s), loss on training batch is 0.00325626.
After 24268 training step(s), loss on training batch is 0.00343294.
After 24269 training step(s), loss on training batch is 0.00323469.
After 24270 training step(s), loss on training batch is 0.00325092.
After 24271 training step(s), loss on training batch is 0.00337556.
After 24272 training step(s), loss on training batch is 0.00324012.
After 24273 training step(s), loss on training batch is 0.00350358.
After 24274 training step(s), loss on training batch is 0.00380771.
After 24275 training step(s), loss on training batch is 0.00378037.
After 24276 training step(s), loss on training batch is 0.00381075.
After 24277 training step(s), loss on training batch is 0.00320073.
After 24278 training step(s), loss on training batch is 0.00321453.
After 24279 training step(s), loss on training batch is 0.00321939.
After 24280 training step(s), loss on training batch is 0.0039664.
After 24281 training step(s), loss on training batch is 0.00354221.
After 24282 training step(s), loss on training batch is 0.00358963.
After 24283 training step(s), loss on training batch is 0.00317222.
After 24284 training step(s), loss on training batch is 0.00396432.
After 24285 training step(s), loss on training batch is 0.0032982.
After 24286 training step(s), loss on training batch is 0.00392753.
After 24287 training step(s), loss on training batch is 0.00361347.
After 24288 training step(s), loss on training batch is 0.00386353.
After 24289 training step(s), loss on training batch is 0.00360627.
After 24290 training step(s), loss on training batch is 0.00318981.
After 24291 training step(s), loss on training batch is 0.00318967.
After 24292 training step(s), loss on training batch is 0.00331237.
After 24293 training step(s), loss on training batch is 0.00376653.
After 24294 training step(s), loss on training batch is 0.00371334.
After 24295 training step(s), loss on training batch is 0.00345619.
After 24296 training step(s), loss on training batch is 0.003864.
After 24297 training step(s), loss on training batch is 0.0033358.
After 24298 training step(s), loss on training batch is 0.00371274.
After 24299 training step(s), loss on training batch is 0.00310995.
After 24300 training step(s), loss on training batch is 0.00354839.
After 24301 training step(s), loss on training batch is 0.00366.
After 24302 training step(s), loss on training batch is 0.00415008.
After 24303 training step(s), loss on training batch is 0.0030934.
After 24304 training step(s), loss on training batch is 0.00374652.
After 24305 training step(s), loss on training batch is 0.00373141.
After 24306 training step(s), loss on training batch is 0.00342198.
After 24307 training step(s), loss on training batch is 0.00336907.
After 24308 training step(s), loss on training batch is 0.00357612.
After 24309 training step(s), loss on training batch is 0.00351899.
After 24310 training step(s), loss on training batch is 0.003685.
After 24311 training step(s), loss on training batch is 0.00355319.
After 24312 training step(s), loss on training batch is 0.00320106.
After 24313 training step(s), loss on training batch is 0.00325238.
After 24314 training step(s), loss on training batch is 0.00350284.
After 24315 training step(s), loss on training batch is 0.00367846.
After 24316 training step(s), loss on training batch is 0.00338181.
After 24317 training step(s), loss on training batch is 0.00346656.
After 24318 training step(s), loss on training batch is 0.00403191.
After 24319 training step(s), loss on training batch is 0.00333587.
After 24320 training step(s), loss on training batch is 0.00321492.
After 24321 training step(s), loss on training batch is 0.00338162.
After 24322 training step(s), loss on training batch is 0.00365874.
After 24323 training step(s), loss on training batch is 0.00370486.
After 24324 training step(s), loss on training batch is 0.00328245.
After 24325 training step(s), loss on training batch is 0.00368692.
After 24326 training step(s), loss on training batch is 0.00374429.
After 24327 training step(s), loss on training batch is 0.00328903.
After 24328 training step(s), loss on training batch is 0.00363966.
After 24329 training step(s), loss on training batch is 0.0036256.
After 24330 training step(s), loss on training batch is 0.0036867.
After 24331 training step(s), loss on training batch is 0.00334363.
After 24332 training step(s), loss on training batch is 0.0038768.
After 24333 training step(s), loss on training batch is 0.00423475.
After 24334 training step(s), loss on training batch is 0.0034444.
After 24335 training step(s), loss on training batch is 0.00385696.
After 24336 training step(s), loss on training batch is 0.00311934.
After 24337 training step(s), loss on training batch is 0.00353282.
After 24338 training step(s), loss on training batch is 0.00317108.
After 24339 training step(s), loss on training batch is 0.00473862.
After 24340 training step(s), loss on training batch is 0.00313756.
After 24341 training step(s), loss on training batch is 0.0032035.
After 24342 training step(s), loss on training batch is 0.00338331.
After 24343 training step(s), loss on training batch is 0.0033259.
After 24344 training step(s), loss on training batch is 0.00426323.
After 24345 training step(s), loss on training batch is 0.00348743.
After 24346 training step(s), loss on training batch is 0.00321974.
After 24347 training step(s), loss on training batch is 0.00322113.
After 24348 training step(s), loss on training batch is 0.00337868.
After 24349 training step(s), loss on training batch is 0.00366892.
After 24350 training step(s), loss on training batch is 0.00324685.
After 24351 training step(s), loss on training batch is 0.00376147.
After 24352 training step(s), loss on training batch is 0.00358536.
After 24353 training step(s), loss on training batch is 0.0034041.
After 24354 training step(s), loss on training batch is 0.00334368.
After 24355 training step(s), loss on training batch is 0.00324586.
After 24356 training step(s), loss on training batch is 0.00323287.
After 24357 training step(s), loss on training batch is 0.00407962.
After 24358 training step(s), loss on training batch is 0.00312971.
After 24359 training step(s), loss on training batch is 0.00354984.
After 24360 training step(s), loss on training batch is 0.00327323.
After 24361 training step(s), loss on training batch is 0.00332272.
After 24362 training step(s), loss on training batch is 0.00363547.
After 24363 training step(s), loss on training batch is 0.00309132.
After 24364 training step(s), loss on training batch is 0.00369765.
After 24365 training step(s), loss on training batch is 0.00341754.
After 24366 training step(s), loss on training batch is 0.00341337.
After 24367 training step(s), loss on training batch is 0.0036722.
After 24368 training step(s), loss on training batch is 0.0032702.
After 24369 training step(s), loss on training batch is 0.00346678.
After 24370 training step(s), loss on training batch is 0.00318954.
After 24371 training step(s), loss on training batch is 0.0036284.
After 24372 training step(s), loss on training batch is 0.00384842.
After 24373 training step(s), loss on training batch is 0.00338049.
After 24374 training step(s), loss on training batch is 0.0036851.
After 24375 training step(s), loss on training batch is 0.00367759.
After 24376 training step(s), loss on training batch is 0.00344795.
After 24377 training step(s), loss on training batch is 0.00397644.
After 24378 training step(s), loss on training batch is 0.00340828.
After 24379 training step(s), loss on training batch is 0.00356358.
After 24380 training step(s), loss on training batch is 0.00365639.
After 24381 training step(s), loss on training batch is 0.00375224.
After 24382 training step(s), loss on training batch is 0.00336372.
After 24383 training step(s), loss on training batch is 0.00315.
After 24384 training step(s), loss on training batch is 0.003364.
After 24385 training step(s), loss on training batch is 0.00323788.
After 24386 training step(s), loss on training batch is 0.0036391.
After 24387 training step(s), loss on training batch is 0.00328558.
After 24388 training step(s), loss on training batch is 0.00328502.
After 24389 training step(s), loss on training batch is 0.00309988.
After 24390 training step(s), loss on training batch is 0.00363486.
After 24391 training step(s), loss on training batch is 0.00357835.
After 24392 training step(s), loss on training batch is 0.00374767.
After 24393 training step(s), loss on training batch is 0.00347864.
After 24394 training step(s), loss on training batch is 0.00382741.
After 24395 training step(s), loss on training batch is 0.00372573.
After 24396 training step(s), loss on training batch is 0.00332063.
After 24397 training step(s), loss on training batch is 0.00350217.
After 24398 training step(s), loss on training batch is 0.00386159.
After 24399 training step(s), loss on training batch is 0.00327111.
After 24400 training step(s), loss on training batch is 0.00342808.
After 24401 training step(s), loss on training batch is 0.00368276.
After 24402 training step(s), loss on training batch is 0.00363347.
After 24403 training step(s), loss on training batch is 0.00474092.
After 24404 training step(s), loss on training batch is 0.00355228.
After 24405 training step(s), loss on training batch is 0.00348881.
After 24406 training step(s), loss on training batch is 0.00336831.
After 24407 training step(s), loss on training batch is 0.00322716.
After 24408 training step(s), loss on training batch is 0.00323795.
After 24409 training step(s), loss on training batch is 0.00329466.
After 24410 training step(s), loss on training batch is 0.00395151.
After 24411 training step(s), loss on training batch is 0.00405637.
After 24412 training step(s), loss on training batch is 0.00351243.
After 24413 training step(s), loss on training batch is 0.00309746.
After 24414 training step(s), loss on training batch is 0.00370148.
After 24415 training step(s), loss on training batch is 0.00329047.
After 24416 training step(s), loss on training batch is 0.0032122.
After 24417 training step(s), loss on training batch is 0.00352469.
After 24418 training step(s), loss on training batch is 0.00359121.
After 24419 training step(s), loss on training batch is 0.00361537.
After 24420 training step(s), loss on training batch is 0.00351366.
After 24421 training step(s), loss on training batch is 0.00370696.
After 24422 training step(s), loss on training batch is 0.00373413.
After 24423 training step(s), loss on training batch is 0.0033494.
After 24424 training step(s), loss on training batch is 0.00340528.
After 24425 training step(s), loss on training batch is 0.00342072.
After 24426 training step(s), loss on training batch is 0.00378465.
After 24427 training step(s), loss on training batch is 0.00371765.
After 24428 training step(s), loss on training batch is 0.00335434.
After 24429 training step(s), loss on training batch is 0.003437.
After 24430 training step(s), loss on training batch is 0.00346904.
After 24431 training step(s), loss on training batch is 0.00332131.
After 24432 training step(s), loss on training batch is 0.00351406.
After 24433 training step(s), loss on training batch is 0.00314424.
After 24434 training step(s), loss on training batch is 0.00397998.
After 24435 training step(s), loss on training batch is 0.00321866.
After 24436 training step(s), loss on training batch is 0.00348926.
After 24437 training step(s), loss on training batch is 0.00313573.
After 24438 training step(s), loss on training batch is 0.00372552.
After 24439 training step(s), loss on training batch is 0.00419556.
After 24440 training step(s), loss on training batch is 0.00345248.
After 24441 training step(s), loss on training batch is 0.00328897.
After 24442 training step(s), loss on training batch is 0.00364541.
After 24443 training step(s), loss on training batch is 0.00310877.
After 24444 training step(s), loss on training batch is 0.00320805.
After 24445 training step(s), loss on training batch is 0.00362274.
After 24446 training step(s), loss on training batch is 0.0032455.
After 24447 training step(s), loss on training batch is 0.00357345.
After 24448 training step(s), loss on training batch is 0.00318391.
After 24449 training step(s), loss on training batch is 0.00344428.
After 24450 training step(s), loss on training batch is 0.00321238.
After 24451 training step(s), loss on training batch is 0.00343151.
After 24452 training step(s), loss on training batch is 0.00344147.
After 24453 training step(s), loss on training batch is 0.0034632.
After 24454 training step(s), loss on training batch is 0.0032739.
After 24455 training step(s), loss on training batch is 0.0034418.
After 24456 training step(s), loss on training batch is 0.00397003.
After 24457 training step(s), loss on training batch is 0.00369099.
After 24458 training step(s), loss on training batch is 0.00340394.
After 24459 training step(s), loss on training batch is 0.00362506.
After 24460 training step(s), loss on training batch is 0.00364726.
After 24461 training step(s), loss on training batch is 0.00349953.
After 24462 training step(s), loss on training batch is 0.00344682.
After 24463 training step(s), loss on training batch is 0.00347786.
After 24464 training step(s), loss on training batch is 0.00339823.
After 24465 training step(s), loss on training batch is 0.00322909.
After 24466 training step(s), loss on training batch is 0.00330372.
After 24467 training step(s), loss on training batch is 0.0034435.
After 24468 training step(s), loss on training batch is 0.0034479.
After 24469 training step(s), loss on training batch is 0.00351288.
After 24470 training step(s), loss on training batch is 0.00418271.
After 24471 training step(s), loss on training batch is 0.00337301.
After 24472 training step(s), loss on training batch is 0.00403213.
After 24473 training step(s), loss on training batch is 0.00354187.
After 24474 training step(s), loss on training batch is 0.00331143.
After 24475 training step(s), loss on training batch is 0.00404071.
After 24476 training step(s), loss on training batch is 0.00339097.
After 24477 training step(s), loss on training batch is 0.00387787.
After 24478 training step(s), loss on training batch is 0.00361364.
After 24479 training step(s), loss on training batch is 0.00339249.
After 24480 training step(s), loss on training batch is 0.00344054.
After 24481 training step(s), loss on training batch is 0.00326471.
After 24482 training step(s), loss on training batch is 0.0038326.
After 24483 training step(s), loss on training batch is 0.00343227.
After 24484 training step(s), loss on training batch is 0.00357823.
After 24485 training step(s), loss on training batch is 0.00348388.
After 24486 training step(s), loss on training batch is 0.00432039.
After 24487 training step(s), loss on training batch is 0.00314614.
After 24488 training step(s), loss on training batch is 0.00320677.
After 24489 training step(s), loss on training batch is 0.00342089.
After 24490 training step(s), loss on training batch is 0.00335628.
After 24491 training step(s), loss on training batch is 0.00336825.
After 24492 training step(s), loss on training batch is 0.00359937.
After 24493 training step(s), loss on training batch is 0.00320823.
After 24494 training step(s), loss on training batch is 0.00350119.
After 24495 training step(s), loss on training batch is 0.00313207.
After 24496 training step(s), loss on training batch is 0.0033187.
After 24497 training step(s), loss on training batch is 0.00335502.
After 24498 training step(s), loss on training batch is 0.00341821.
After 24499 training step(s), loss on training batch is 0.00326704.
After 24500 training step(s), loss on training batch is 0.00321873.
After 24501 training step(s), loss on training batch is 0.0032291.
After 24502 training step(s), loss on training batch is 0.00340187.
After 24503 training step(s), loss on training batch is 0.00403504.
After 24504 training step(s), loss on training batch is 0.0036632.
After 24505 training step(s), loss on training batch is 0.00351059.
After 24506 training step(s), loss on training batch is 0.00340976.
After 24507 training step(s), loss on training batch is 0.0033827.
After 24508 training step(s), loss on training batch is 0.0034932.
After 24509 training step(s), loss on training batch is 0.00431287.
After 24510 training step(s), loss on training batch is 0.00379676.
After 24511 training step(s), loss on training batch is 0.00339697.
After 24512 training step(s), loss on training batch is 0.00324789.
After 24513 training step(s), loss on training batch is 0.00339033.
After 24514 training step(s), loss on training batch is 0.0034793.
After 24515 training step(s), loss on training batch is 0.00338975.
After 24516 training step(s), loss on training batch is 0.00339288.
After 24517 training step(s), loss on training batch is 0.00330533.
After 24518 training step(s), loss on training batch is 0.00318591.
After 24519 training step(s), loss on training batch is 0.00348459.
After 24520 training step(s), loss on training batch is 0.00317585.
After 24521 training step(s), loss on training batch is 0.0035738.
After 24522 training step(s), loss on training batch is 0.00395271.
After 24523 training step(s), loss on training batch is 0.00333278.
After 24524 training step(s), loss on training batch is 0.00339309.
After 24525 training step(s), loss on training batch is 0.00405368.
After 24526 training step(s), loss on training batch is 0.00355132.
After 24527 training step(s), loss on training batch is 0.00318593.
After 24528 training step(s), loss on training batch is 0.00328899.
After 24529 training step(s), loss on training batch is 0.00322056.
After 24530 training step(s), loss on training batch is 0.00349318.
After 24531 training step(s), loss on training batch is 0.00357364.
After 24532 training step(s), loss on training batch is 0.00359924.
After 24533 training step(s), loss on training batch is 0.00338605.
After 24534 training step(s), loss on training batch is 0.00338168.
After 24535 training step(s), loss on training batch is 0.00353371.
After 24536 training step(s), loss on training batch is 0.00381302.
After 24537 training step(s), loss on training batch is 0.00338857.
After 24538 training step(s), loss on training batch is 0.00327131.
After 24539 training step(s), loss on training batch is 0.00323897.
After 24540 training step(s), loss on training batch is 0.00329821.
After 24541 training step(s), loss on training batch is 0.00366281.
After 24542 training step(s), loss on training batch is 0.00349497.
After 24543 training step(s), loss on training batch is 0.00384721.
After 24544 training step(s), loss on training batch is 0.00318314.
After 24545 training step(s), loss on training batch is 0.00332933.
After 24546 training step(s), loss on training batch is 0.00357643.
After 24547 training step(s), loss on training batch is 0.00332301.
After 24548 training step(s), loss on training batch is 0.00364179.
After 24549 training step(s), loss on training batch is 0.00358669.
After 24550 training step(s), loss on training batch is 0.00353728.
After 24551 training step(s), loss on training batch is 0.00335254.
After 24552 training step(s), loss on training batch is 0.00373248.
After 24553 training step(s), loss on training batch is 0.00398971.
After 24554 training step(s), loss on training batch is 0.00340153.
After 24555 training step(s), loss on training batch is 0.0033622.
After 24556 training step(s), loss on training batch is 0.00313288.
After 24557 training step(s), loss on training batch is 0.00339114.
After 24558 training step(s), loss on training batch is 0.00331037.
After 24559 training step(s), loss on training batch is 0.00362653.
After 24560 training step(s), loss on training batch is 0.0042768.
After 24561 training step(s), loss on training batch is 0.0034154.
After 24562 training step(s), loss on training batch is 0.00333015.
After 24563 training step(s), loss on training batch is 0.00334074.
After 24564 training step(s), loss on training batch is 0.00381032.
After 24565 training step(s), loss on training batch is 0.00322673.
After 24566 training step(s), loss on training batch is 0.00323496.
After 24567 training step(s), loss on training batch is 0.00314897.
After 24568 training step(s), loss on training batch is 0.00473047.
After 24569 training step(s), loss on training batch is 0.00317124.
After 24570 training step(s), loss on training batch is 0.0035216.
After 24571 training step(s), loss on training batch is 0.00322868.
After 24572 training step(s), loss on training batch is 0.00330967.
After 24573 training step(s), loss on training batch is 0.0034442.
After 24574 training step(s), loss on training batch is 0.00346884.
After 24575 training step(s), loss on training batch is 0.00323563.
After 24576 training step(s), loss on training batch is 0.00353372.
After 24577 training step(s), loss on training batch is 0.00385738.
After 24578 training step(s), loss on training batch is 0.00337242.
After 24579 training step(s), loss on training batch is 0.00322827.
After 24580 training step(s), loss on training batch is 0.00324033.
After 24581 training step(s), loss on training batch is 0.00329364.
After 24582 training step(s), loss on training batch is 0.0034632.
After 24583 training step(s), loss on training batch is 0.00316098.
After 24584 training step(s), loss on training batch is 0.00316887.
After 24585 training step(s), loss on training batch is 0.00354498.
After 24586 training step(s), loss on training batch is 0.00323461.
After 24587 training step(s), loss on training batch is 0.00356028.
After 24588 training step(s), loss on training batch is 0.00339989.
After 24589 training step(s), loss on training batch is 0.00339454.
After 24590 training step(s), loss on training batch is 0.00317638.
After 24591 training step(s), loss on training batch is 0.00324332.
After 24592 training step(s), loss on training batch is 0.00404531.
After 24593 training step(s), loss on training batch is 0.00344965.
After 24594 training step(s), loss on training batch is 0.00381236.
After 24595 training step(s), loss on training batch is 0.00363784.
After 24596 training step(s), loss on training batch is 0.00340896.
After 24597 training step(s), loss on training batch is 0.00361786.
After 24598 training step(s), loss on training batch is 0.00340405.
After 24599 training step(s), loss on training batch is 0.00313223.
After 24600 training step(s), loss on training batch is 0.00372212.
After 24601 training step(s), loss on training batch is 0.00400775.
After 24602 training step(s), loss on training batch is 0.0032814.
After 24603 training step(s), loss on training batch is 0.00326335.
After 24604 training step(s), loss on training batch is 0.00340878.
After 24605 training step(s), loss on training batch is 0.00409605.
After 24606 training step(s), loss on training batch is 0.00364473.
After 24607 training step(s), loss on training batch is 0.00321686.
After 24608 training step(s), loss on training batch is 0.00337326.
After 24609 training step(s), loss on training batch is 0.00378887.
After 24610 training step(s), loss on training batch is 0.00315665.
After 24611 training step(s), loss on training batch is 0.00370112.
After 24612 training step(s), loss on training batch is 0.00327366.
After 24613 training step(s), loss on training batch is 0.00374701.
After 24614 training step(s), loss on training batch is 0.00368766.
After 24615 training step(s), loss on training batch is 0.00379515.
After 24616 training step(s), loss on training batch is 0.00358388.
After 24617 training step(s), loss on training batch is 0.00330167.
After 24618 training step(s), loss on training batch is 0.00376937.
After 24619 training step(s), loss on training batch is 0.00315977.
After 24620 training step(s), loss on training batch is 0.00321547.
After 24621 training step(s), loss on training batch is 0.0034855.
After 24622 training step(s), loss on training batch is 0.00354491.
After 24623 training step(s), loss on training batch is 0.00346507.
After 24624 training step(s), loss on training batch is 0.0035011.
After 24625 training step(s), loss on training batch is 0.0034415.
After 24626 training step(s), loss on training batch is 0.0032509.
After 24627 training step(s), loss on training batch is 0.00382209.
After 24628 training step(s), loss on training batch is 0.00347616.
After 24629 training step(s), loss on training batch is 0.00342099.
After 24630 training step(s), loss on training batch is 0.0035314.
After 24631 training step(s), loss on training batch is 0.00346485.
After 24632 training step(s), loss on training batch is 0.0033904.
After 24633 training step(s), loss on training batch is 0.00344827.
After 24634 training step(s), loss on training batch is 0.00346522.
After 24635 training step(s), loss on training batch is 0.00354334.
After 24636 training step(s), loss on training batch is 0.0037454.
After 24637 training step(s), loss on training batch is 0.00339203.
After 24638 training step(s), loss on training batch is 0.00323099.
After 24639 training step(s), loss on training batch is 0.00325563.
After 24640 training step(s), loss on training batch is 0.00338603.
After 24641 training step(s), loss on training batch is 0.00334645.
After 24642 training step(s), loss on training batch is 0.00330512.
After 24643 training step(s), loss on training batch is 0.00366491.
After 24644 training step(s), loss on training batch is 0.00338026.
After 24645 training step(s), loss on training batch is 0.00325026.
After 24646 training step(s), loss on training batch is 0.00345969.
After 24647 training step(s), loss on training batch is 0.00349185.
After 24648 training step(s), loss on training batch is 0.0036418.
After 24649 training step(s), loss on training batch is 0.00334102.
After 24650 training step(s), loss on training batch is 0.00330609.
After 24651 training step(s), loss on training batch is 0.00313149.
After 24652 training step(s), loss on training batch is 0.00335677.
After 24653 training step(s), loss on training batch is 0.00331888.
After 24654 training step(s), loss on training batch is 0.00373892.
After 24655 training step(s), loss on training batch is 0.00307207.
After 24656 training step(s), loss on training batch is 0.00358083.
After 24657 training step(s), loss on training batch is 0.00375926.
After 24658 training step(s), loss on training batch is 0.00360857.
After 24659 training step(s), loss on training batch is 0.00341647.
After 24660 training step(s), loss on training batch is 0.00326192.
After 24661 training step(s), loss on training batch is 0.00387589.
After 24662 training step(s), loss on training batch is 0.0032556.
After 24663 training step(s), loss on training batch is 0.00366161.
After 24664 training step(s), loss on training batch is 0.00346959.
After 24665 training step(s), loss on training batch is 0.0032842.
After 24666 training step(s), loss on training batch is 0.00378392.
After 24667 training step(s), loss on training batch is 0.00326019.
After 24668 training step(s), loss on training batch is 0.00433434.
After 24669 training step(s), loss on training batch is 0.00316422.
After 24670 training step(s), loss on training batch is 0.00353097.
After 24671 training step(s), loss on training batch is 0.00361805.
After 24672 training step(s), loss on training batch is 0.00381892.
After 24673 training step(s), loss on training batch is 0.00369513.
After 24674 training step(s), loss on training batch is 0.00344784.
After 24675 training step(s), loss on training batch is 0.0034983.
After 24676 training step(s), loss on training batch is 0.00343361.
After 24677 training step(s), loss on training batch is 0.00365726.
After 24678 training step(s), loss on training batch is 0.00349452.
After 24679 training step(s), loss on training batch is 0.00321577.
After 24680 training step(s), loss on training batch is 0.00353015.
After 24681 training step(s), loss on training batch is 0.00352501.
After 24682 training step(s), loss on training batch is 0.00343606.
After 24683 training step(s), loss on training batch is 0.00342148.
After 24684 training step(s), loss on training batch is 0.00344554.
After 24685 training step(s), loss on training batch is 0.0037736.
After 24686 training step(s), loss on training batch is 0.00322478.
After 24687 training step(s), loss on training batch is 0.00371449.
After 24688 training step(s), loss on training batch is 0.00400222.
After 24689 training step(s), loss on training batch is 0.00346544.
After 24690 training step(s), loss on training batch is 0.0034788.
After 24691 training step(s), loss on training batch is 0.00342883.
After 24692 training step(s), loss on training batch is 0.00316386.
After 24693 training step(s), loss on training batch is 0.00330837.
After 24694 training step(s), loss on training batch is 0.00307535.
After 24695 training step(s), loss on training batch is 0.00331453.
After 24696 training step(s), loss on training batch is 0.0032082.
After 24697 training step(s), loss on training batch is 0.00343863.
After 24698 training step(s), loss on training batch is 0.00350983.
After 24699 training step(s), loss on training batch is 0.00358551.
After 24700 training step(s), loss on training batch is 0.00369063.
After 24701 training step(s), loss on training batch is 0.00373789.
After 24702 training step(s), loss on training batch is 0.00358932.
After 24703 training step(s), loss on training batch is 0.00318863.
After 24704 training step(s), loss on training batch is 0.0033803.
After 24705 training step(s), loss on training batch is 0.00341951.
After 24706 training step(s), loss on training batch is 0.00337766.
After 24707 training step(s), loss on training batch is 0.00342049.
After 24708 training step(s), loss on training batch is 0.00374096.
After 24709 training step(s), loss on training batch is 0.00370428.
After 24710 training step(s), loss on training batch is 0.00333458.
After 24711 training step(s), loss on training batch is 0.00330776.
After 24712 training step(s), loss on training batch is 0.00368283.
After 24713 training step(s), loss on training batch is 0.00322515.
After 24714 training step(s), loss on training batch is 0.00357608.
After 24715 training step(s), loss on training batch is 0.00405.
After 24716 training step(s), loss on training batch is 0.00379969.
After 24717 training step(s), loss on training batch is 0.00397694.
After 24718 training step(s), loss on training batch is 0.00381937.
After 24719 training step(s), loss on training batch is 0.00347332.
After 24720 training step(s), loss on training batch is 0.00332147.
After 24721 training step(s), loss on training batch is 0.00321878.
After 24722 training step(s), loss on training batch is 0.00302084.
After 24723 training step(s), loss on training batch is 0.00319233.
After 24724 training step(s), loss on training batch is 0.0033303.
After 24725 training step(s), loss on training batch is 0.00352944.
After 24726 training step(s), loss on training batch is 0.00316574.
After 24727 training step(s), loss on training batch is 0.00329173.
After 24728 training step(s), loss on training batch is 0.00466323.
After 24729 training step(s), loss on training batch is 0.00506953.
After 24730 training step(s), loss on training batch is 0.00370313.
After 24731 training step(s), loss on training batch is 0.00414727.
After 24732 training step(s), loss on training batch is 0.00353032.
After 24733 training step(s), loss on training batch is 0.00352288.
After 24734 training step(s), loss on training batch is 0.00425636.
After 24735 training step(s), loss on training batch is 0.00341693.
After 24736 training step(s), loss on training batch is 0.00335878.
After 24737 training step(s), loss on training batch is 0.00369876.
After 24738 training step(s), loss on training batch is 0.00371568.
After 24739 training step(s), loss on training batch is 0.00345291.
After 24740 training step(s), loss on training batch is 0.00319309.
After 24741 training step(s), loss on training batch is 0.00358206.
After 24742 training step(s), loss on training batch is 0.00329961.
After 24743 training step(s), loss on training batch is 0.00337113.
After 24744 training step(s), loss on training batch is 0.00325429.
After 24745 training step(s), loss on training batch is 0.00358728.
After 24746 training step(s), loss on training batch is 0.00300165.
After 24747 training step(s), loss on training batch is 0.0036933.
After 24748 training step(s), loss on training batch is 0.00369091.
After 24749 training step(s), loss on training batch is 0.00319415.
After 24750 training step(s), loss on training batch is 0.00367896.
After 24751 training step(s), loss on training batch is 0.00355028.
After 24752 training step(s), loss on training batch is 0.00355085.
After 24753 training step(s), loss on training batch is 0.00332309.
After 24754 training step(s), loss on training batch is 0.00337577.
After 24755 training step(s), loss on training batch is 0.00347018.
After 24756 training step(s), loss on training batch is 0.0032482.
After 24757 training step(s), loss on training batch is 0.00374651.
After 24758 training step(s), loss on training batch is 0.0037968.
After 24759 training step(s), loss on training batch is 0.00335589.
After 24760 training step(s), loss on training batch is 0.00338275.
After 24761 training step(s), loss on training batch is 0.0034102.
After 24762 training step(s), loss on training batch is 0.00338948.
After 24763 training step(s), loss on training batch is 0.00377564.
After 24764 training step(s), loss on training batch is 0.00303411.
After 24765 training step(s), loss on training batch is 0.00336027.
After 24766 training step(s), loss on training batch is 0.0036129.
After 24767 training step(s), loss on training batch is 0.00329419.
After 24768 training step(s), loss on training batch is 0.00311083.
After 24769 training step(s), loss on training batch is 0.00349868.
After 24770 training step(s), loss on training batch is 0.00335475.
After 24771 training step(s), loss on training batch is 0.00340983.
After 24772 training step(s), loss on training batch is 0.00322366.
After 24773 training step(s), loss on training batch is 0.00338549.
After 24774 training step(s), loss on training batch is 0.00379473.
After 24775 training step(s), loss on training batch is 0.00311666.
After 24776 training step(s), loss on training batch is 0.00348247.
After 24777 training step(s), loss on training batch is 0.00350462.
After 24778 training step(s), loss on training batch is 0.00343366.
After 24779 training step(s), loss on training batch is 0.00350724.
After 24780 training step(s), loss on training batch is 0.00355488.
After 24781 training step(s), loss on training batch is 0.00327505.
After 24782 training step(s), loss on training batch is 0.00326368.
After 24783 training step(s), loss on training batch is 0.00347277.
After 24784 training step(s), loss on training batch is 0.00328957.
After 24785 training step(s), loss on training batch is 0.00350252.
After 24786 training step(s), loss on training batch is 0.00315465.
After 24787 training step(s), loss on training batch is 0.00312758.
After 24788 training step(s), loss on training batch is 0.00344466.
After 24789 training step(s), loss on training batch is 0.00341051.
After 24790 training step(s), loss on training batch is 0.00369548.
After 24791 training step(s), loss on training batch is 0.00351465.
After 24792 training step(s), loss on training batch is 0.00334609.
After 24793 training step(s), loss on training batch is 0.00343767.
After 24794 training step(s), loss on training batch is 0.00357303.
After 24795 training step(s), loss on training batch is 0.00324112.
After 24796 training step(s), loss on training batch is 0.00349416.
After 24797 training step(s), loss on training batch is 0.0032917.
After 24798 training step(s), loss on training batch is 0.00391686.
After 24799 training step(s), loss on training batch is 0.00332483.
After 24800 training step(s), loss on training batch is 0.00317129.
After 24801 training step(s), loss on training batch is 0.00340367.
After 24802 training step(s), loss on training batch is 0.00318417.
After 24803 training step(s), loss on training batch is 0.0030618.
After 24804 training step(s), loss on training batch is 0.00333705.
After 24805 training step(s), loss on training batch is 0.0035072.
After 24806 training step(s), loss on training batch is 0.00330686.
After 24807 training step(s), loss on training batch is 0.0032909.
After 24808 training step(s), loss on training batch is 0.0032197.
After 24809 training step(s), loss on training batch is 0.0036443.
After 24810 training step(s), loss on training batch is 0.00346443.
After 24811 training step(s), loss on training batch is 0.00329115.
After 24812 training step(s), loss on training batch is 0.00300747.
After 24813 training step(s), loss on training batch is 0.00346105.
After 24814 training step(s), loss on training batch is 0.00339886.
After 24815 training step(s), loss on training batch is 0.00327296.
After 24816 training step(s), loss on training batch is 0.00332801.
After 24817 training step(s), loss on training batch is 0.00335813.
After 24818 training step(s), loss on training batch is 0.00417303.
After 24819 training step(s), loss on training batch is 0.00308778.
After 24820 training step(s), loss on training batch is 0.00368143.
After 24821 training step(s), loss on training batch is 0.00349713.
After 24822 training step(s), loss on training batch is 0.00309414.
After 24823 training step(s), loss on training batch is 0.00326928.
After 24824 training step(s), loss on training batch is 0.00377538.
After 24825 training step(s), loss on training batch is 0.00308113.
After 24826 training step(s), loss on training batch is 0.00334875.
After 24827 training step(s), loss on training batch is 0.00341878.
After 24828 training step(s), loss on training batch is 0.00341543.
After 24829 training step(s), loss on training batch is 0.00325342.
After 24830 training step(s), loss on training batch is 0.00354093.
After 24831 training step(s), loss on training batch is 0.00332478.
After 24832 training step(s), loss on training batch is 0.0037164.
After 24833 training step(s), loss on training batch is 0.00358886.
After 24834 training step(s), loss on training batch is 0.00375987.
After 24835 training step(s), loss on training batch is 0.00374793.
After 24836 training step(s), loss on training batch is 0.0031604.
After 24837 training step(s), loss on training batch is 0.00333321.
After 24838 training step(s), loss on training batch is 0.00369497.
After 24839 training step(s), loss on training batch is 0.00349013.
After 24840 training step(s), loss on training batch is 0.00349016.
After 24841 training step(s), loss on training batch is 0.00330283.
After 24842 training step(s), loss on training batch is 0.00333385.
After 24843 training step(s), loss on training batch is 0.00336429.
After 24844 training step(s), loss on training batch is 0.0034549.
After 24845 training step(s), loss on training batch is 0.00319691.
After 24846 training step(s), loss on training batch is 0.00345991.
After 24847 training step(s), loss on training batch is 0.00358739.
After 24848 training step(s), loss on training batch is 0.00368189.
After 24849 training step(s), loss on training batch is 0.00361256.
After 24850 training step(s), loss on training batch is 0.00366008.
After 24851 training step(s), loss on training batch is 0.00332563.
After 24852 training step(s), loss on training batch is 0.00375972.
After 24853 training step(s), loss on training batch is 0.00355485.
After 24854 training step(s), loss on training batch is 0.00326207.
After 24855 training step(s), loss on training batch is 0.0030737.
After 24856 training step(s), loss on training batch is 0.00344139.
After 24857 training step(s), loss on training batch is 0.00334434.
After 24858 training step(s), loss on training batch is 0.00318019.
After 24859 training step(s), loss on training batch is 0.00341736.
After 24860 training step(s), loss on training batch is 0.00339226.
After 24861 training step(s), loss on training batch is 0.00356144.
After 24862 training step(s), loss on training batch is 0.00310957.
After 24863 training step(s), loss on training batch is 0.00313622.
After 24864 training step(s), loss on training batch is 0.0036511.
After 24865 training step(s), loss on training batch is 0.00326559.
After 24866 training step(s), loss on training batch is 0.00355135.
After 24867 training step(s), loss on training batch is 0.00324805.
After 24868 training step(s), loss on training batch is 0.00384193.
After 24869 training step(s), loss on training batch is 0.0038281.
After 24870 training step(s), loss on training batch is 0.00363882.
After 24871 training step(s), loss on training batch is 0.00331004.
After 24872 training step(s), loss on training batch is 0.00389435.
After 24873 training step(s), loss on training batch is 0.00375714.
After 24874 training step(s), loss on training batch is 0.00394122.
After 24875 training step(s), loss on training batch is 0.00328548.
After 24876 training step(s), loss on training batch is 0.00335549.
After 24877 training step(s), loss on training batch is 0.00318686.
After 24878 training step(s), loss on training batch is 0.0032501.
After 24879 training step(s), loss on training batch is 0.00337189.
After 24880 training step(s), loss on training batch is 0.00334922.
After 24881 training step(s), loss on training batch is 0.00325268.
After 24882 training step(s), loss on training batch is 0.00353341.
After 24883 training step(s), loss on training batch is 0.00327438.
After 24884 training step(s), loss on training batch is 0.00394815.
After 24885 training step(s), loss on training batch is 0.00408838.
After 24886 training step(s), loss on training batch is 0.00451317.
After 24887 training step(s), loss on training batch is 0.00346172.
After 24888 training step(s), loss on training batch is 0.00304573.
After 24889 training step(s), loss on training batch is 0.00332325.
After 24890 training step(s), loss on training batch is 0.00344967.
After 24891 training step(s), loss on training batch is 0.00309985.
After 24892 training step(s), loss on training batch is 0.00309095.
After 24893 training step(s), loss on training batch is 0.00341747.
After 24894 training step(s), loss on training batch is 0.00324598.
After 24895 training step(s), loss on training batch is 0.00358819.
After 24896 training step(s), loss on training batch is 0.00343734.
After 24897 training step(s), loss on training batch is 0.00352705.
After 24898 training step(s), loss on training batch is 0.00353813.
After 24899 training step(s), loss on training batch is 0.00325748.
After 24900 training step(s), loss on training batch is 0.00384306.
After 24901 training step(s), loss on training batch is 0.00357864.
After 24902 training step(s), loss on training batch is 0.00346957.
After 24903 training step(s), loss on training batch is 0.00374671.
After 24904 training step(s), loss on training batch is 0.00558451.
After 24905 training step(s), loss on training batch is 0.0035229.
After 24906 training step(s), loss on training batch is 0.00352863.
After 24907 training step(s), loss on training batch is 0.00328748.
After 24908 training step(s), loss on training batch is 0.00302334.
After 24909 training step(s), loss on training batch is 0.0033712.
After 24910 training step(s), loss on training batch is 0.00315031.
After 24911 training step(s), loss on training batch is 0.00319299.
After 24912 training step(s), loss on training batch is 0.00389608.
After 24913 training step(s), loss on training batch is 0.00317685.
After 24914 training step(s), loss on training batch is 0.00391559.
After 24915 training step(s), loss on training batch is 0.00323267.
After 24916 training step(s), loss on training batch is 0.00375939.
After 24917 training step(s), loss on training batch is 0.00358808.
After 24918 training step(s), loss on training batch is 0.00430809.
After 24919 training step(s), loss on training batch is 0.00360703.
After 24920 training step(s), loss on training batch is 0.00342207.
After 24921 training step(s), loss on training batch is 0.00313596.
After 24922 training step(s), loss on training batch is 0.00348258.
After 24923 training step(s), loss on training batch is 0.00387707.
After 24924 training step(s), loss on training batch is 0.00315332.
After 24925 training step(s), loss on training batch is 0.00341151.
After 24926 training step(s), loss on training batch is 0.00350893.
After 24927 training step(s), loss on training batch is 0.00342922.
After 24928 training step(s), loss on training batch is 0.00323754.
After 24929 training step(s), loss on training batch is 0.00368549.
After 24930 training step(s), loss on training batch is 0.00367732.
After 24931 training step(s), loss on training batch is 0.00311221.
After 24932 training step(s), loss on training batch is 0.00351288.
After 24933 training step(s), loss on training batch is 0.00392077.
After 24934 training step(s), loss on training batch is 0.00379393.
After 24935 training step(s), loss on training batch is 0.00359485.
After 24936 training step(s), loss on training batch is 0.00322491.
After 24937 training step(s), loss on training batch is 0.00358335.
After 24938 training step(s), loss on training batch is 0.00322774.
After 24939 training step(s), loss on training batch is 0.00358604.
After 24940 training step(s), loss on training batch is 0.00356006.
After 24941 training step(s), loss on training batch is 0.00392438.
After 24942 training step(s), loss on training batch is 0.00348558.
After 24943 training step(s), loss on training batch is 0.00430437.
After 24944 training step(s), loss on training batch is 0.00367732.
After 24945 training step(s), loss on training batch is 0.00332609.
After 24946 training step(s), loss on training batch is 0.00318351.
After 24947 training step(s), loss on training batch is 0.00316075.
After 24948 training step(s), loss on training batch is 0.00403607.
After 24949 training step(s), loss on training batch is 0.00333049.
After 24950 training step(s), loss on training batch is 0.00364298.
After 24951 training step(s), loss on training batch is 0.00361877.
After 24952 training step(s), loss on training batch is 0.00345169.
After 24953 training step(s), loss on training batch is 0.00341309.
After 24954 training step(s), loss on training batch is 0.00327866.
After 24955 training step(s), loss on training batch is 0.00329068.
After 24956 training step(s), loss on training batch is 0.00336993.
After 24957 training step(s), loss on training batch is 0.00310558.
After 24958 training step(s), loss on training batch is 0.00334407.
After 24959 training step(s), loss on training batch is 0.00337257.
After 24960 training step(s), loss on training batch is 0.00349155.
After 24961 training step(s), loss on training batch is 0.00327956.
After 24962 training step(s), loss on training batch is 0.00443806.
After 24963 training step(s), loss on training batch is 0.00373346.
After 24964 training step(s), loss on training batch is 0.00346401.
After 24965 training step(s), loss on training batch is 0.00335466.
After 24966 training step(s), loss on training batch is 0.00345809.
After 24967 training step(s), loss on training batch is 0.00346082.
After 24968 training step(s), loss on training batch is 0.00341545.
After 24969 training step(s), loss on training batch is 0.00322449.
After 24970 training step(s), loss on training batch is 0.00329194.
After 24971 training step(s), loss on training batch is 0.00363837.
After 24972 training step(s), loss on training batch is 0.00383646.
After 24973 training step(s), loss on training batch is 0.00323097.
After 24974 training step(s), loss on training batch is 0.00348189.
After 24975 training step(s), loss on training batch is 0.00417303.
After 24976 training step(s), loss on training batch is 0.00320318.
After 24977 training step(s), loss on training batch is 0.00381584.
After 24978 training step(s), loss on training batch is 0.00319709.
After 24979 training step(s), loss on training batch is 0.0034251.
After 24980 training step(s), loss on training batch is 0.00317441.
After 24981 training step(s), loss on training batch is 0.00406348.
After 24982 training step(s), loss on training batch is 0.00351284.
After 24983 training step(s), loss on training batch is 0.0032987.
After 24984 training step(s), loss on training batch is 0.00366878.
After 24985 training step(s), loss on training batch is 0.00378896.
After 24986 training step(s), loss on training batch is 0.00337581.
After 24987 training step(s), loss on training batch is 0.00361713.
After 24988 training step(s), loss on training batch is 0.00323136.
After 24989 training step(s), loss on training batch is 0.00399654.
After 24990 training step(s), loss on training batch is 0.00322771.
After 24991 training step(s), loss on training batch is 0.00308618.
After 24992 training step(s), loss on training batch is 0.00343982.
After 24993 training step(s), loss on training batch is 0.00345812.
After 24994 training step(s), loss on training batch is 0.00343924.
After 24995 training step(s), loss on training batch is 0.00328253.
After 24996 training step(s), loss on training batch is 0.0037823.
After 24997 training step(s), loss on training batch is 0.00364451.
After 24998 training step(s), loss on training batch is 0.00329055.
After 24999 training step(s), loss on training batch is 0.00344224.
After 25000 training step(s), loss on training batch is 0.00326029.
After 25001 training step(s), loss on training batch is 0.00359724.
After 25002 training step(s), loss on training batch is 0.00348234.
After 25003 training step(s), loss on training batch is 0.00306797.
After 25004 training step(s), loss on training batch is 0.00351542.
After 25005 training step(s), loss on training batch is 0.00333763.
After 25006 training step(s), loss on training batch is 0.00373391.
After 25007 training step(s), loss on training batch is 0.00305991.
After 25008 training step(s), loss on training batch is 0.00334991.
After 25009 training step(s), loss on training batch is 0.00361999.
After 25010 training step(s), loss on training batch is 0.00309986.
After 25011 training step(s), loss on training batch is 0.00323595.
After 25012 training step(s), loss on training batch is 0.00357034.
After 25013 training step(s), loss on training batch is 0.00333438.
After 25014 training step(s), loss on training batch is 0.00320962.
After 25015 training step(s), loss on training batch is 0.00319555.
After 25016 training step(s), loss on training batch is 0.00445014.
After 25017 training step(s), loss on training batch is 0.00314084.
After 25018 training step(s), loss on training batch is 0.00386861.
After 25019 training step(s), loss on training batch is 0.00341012.
After 25020 training step(s), loss on training batch is 0.00360373.
After 25021 training step(s), loss on training batch is 0.00350069.
After 25022 training step(s), loss on training batch is 0.00353292.
After 25023 training step(s), loss on training batch is 0.00315.
After 25024 training step(s), loss on training batch is 0.00330203.
After 25025 training step(s), loss on training batch is 0.0039417.
After 25026 training step(s), loss on training batch is 0.00322141.
After 25027 training step(s), loss on training batch is 0.00321731.
After 25028 training step(s), loss on training batch is 0.0032183.
After 25029 training step(s), loss on training batch is 0.00331417.
After 25030 training step(s), loss on training batch is 0.00377215.
After 25031 training step(s), loss on training batch is 0.00356694.
After 25032 training step(s), loss on training batch is 0.00341145.
After 25033 training step(s), loss on training batch is 0.00345324.
After 25034 training step(s), loss on training batch is 0.00331411.
After 25035 training step(s), loss on training batch is 0.00330719.
After 25036 training step(s), loss on training batch is 0.00331843.
After 25037 training step(s), loss on training batch is 0.00347008.
After 25038 training step(s), loss on training batch is 0.00347334.
After 25039 training step(s), loss on training batch is 0.00387688.
After 25040 training step(s), loss on training batch is 0.00332592.
After 25041 training step(s), loss on training batch is 0.00333995.
After 25042 training step(s), loss on training batch is 0.0033377.
After 25043 training step(s), loss on training batch is 0.00325742.
After 25044 training step(s), loss on training batch is 0.00361708.
After 25045 training step(s), loss on training batch is 0.00341501.
After 25046 training step(s), loss on training batch is 0.00343799.
After 25047 training step(s), loss on training batch is 0.0035629.
After 25048 training step(s), loss on training batch is 0.00380441.
After 25049 training step(s), loss on training batch is 0.00368641.
After 25050 training step(s), loss on training batch is 0.00318514.
After 25051 training step(s), loss on training batch is 0.00345591.
After 25052 training step(s), loss on training batch is 0.0035799.
After 25053 training step(s), loss on training batch is 0.00341818.
After 25054 training step(s), loss on training batch is 0.00316548.
After 25055 training step(s), loss on training batch is 0.00328576.
After 25056 training step(s), loss on training batch is 0.00348476.
After 25057 training step(s), loss on training batch is 0.00326601.
After 25058 training step(s), loss on training batch is 0.00310176.
After 25059 training step(s), loss on training batch is 0.0034235.
After 25060 training step(s), loss on training batch is 0.00332888.
After 25061 training step(s), loss on training batch is 0.00311785.
After 25062 training step(s), loss on training batch is 0.00325851.
After 25063 training step(s), loss on training batch is 0.00335751.
After 25064 training step(s), loss on training batch is 0.00354847.
After 25065 training step(s), loss on training batch is 0.00348879.
After 25066 training step(s), loss on training batch is 0.00353729.
After 25067 training step(s), loss on training batch is 0.00333369.
After 25068 training step(s), loss on training batch is 0.0052329.
After 25069 training step(s), loss on training batch is 0.00344515.
After 25070 training step(s), loss on training batch is 0.00316001.
After 25071 training step(s), loss on training batch is 0.00327822.
After 25072 training step(s), loss on training batch is 0.00371632.
After 25073 training step(s), loss on training batch is 0.00346099.
After 25074 training step(s), loss on training batch is 0.00345668.
After 25075 training step(s), loss on training batch is 0.003332.
After 25076 training step(s), loss on training batch is 0.00393034.
After 25077 training step(s), loss on training batch is 0.00321605.
After 25078 training step(s), loss on training batch is 0.00360503.
After 25079 training step(s), loss on training batch is 0.00350792.
After 25080 training step(s), loss on training batch is 0.00337004.
After 25081 training step(s), loss on training batch is 0.00394198.
After 25082 training step(s), loss on training batch is 0.00399016.
After 25083 training step(s), loss on training batch is 0.00360557.
After 25084 training step(s), loss on training batch is 0.00341129.
After 25085 training step(s), loss on training batch is 0.00324829.
After 25086 training step(s), loss on training batch is 0.00319754.
After 25087 training step(s), loss on training batch is 0.00363818.
After 25088 training step(s), loss on training batch is 0.00327276.
After 25089 training step(s), loss on training batch is 0.00314548.
After 25090 training step(s), loss on training batch is 0.00328443.
After 25091 training step(s), loss on training batch is 0.0035406.
After 25092 training step(s), loss on training batch is 0.00348567.
After 25093 training step(s), loss on training batch is 0.00340449.
After 25094 training step(s), loss on training batch is 0.00319137.
After 25095 training step(s), loss on training batch is 0.0034184.
After 25096 training step(s), loss on training batch is 0.00323812.
After 25097 training step(s), loss on training batch is 0.00389458.
After 25098 training step(s), loss on training batch is 0.00319118.
After 25099 training step(s), loss on training batch is 0.00318507.
After 25100 training step(s), loss on training batch is 0.00337823.
After 25101 training step(s), loss on training batch is 0.00366279.
After 25102 training step(s), loss on training batch is 0.00357306.
After 25103 training step(s), loss on training batch is 0.00310994.
After 25104 training step(s), loss on training batch is 0.00369847.
After 25105 training step(s), loss on training batch is 0.00351953.
After 25106 training step(s), loss on training batch is 0.0034078.
After 25107 training step(s), loss on training batch is 0.00376584.
After 25108 training step(s), loss on training batch is 0.00343888.
After 25109 training step(s), loss on training batch is 0.00335635.
After 25110 training step(s), loss on training batch is 0.00331028.
After 25111 training step(s), loss on training batch is 0.00366473.
After 25112 training step(s), loss on training batch is 0.00309389.
After 25113 training step(s), loss on training batch is 0.00339957.
After 25114 training step(s), loss on training batch is 0.00373.
After 25115 training step(s), loss on training batch is 0.00351517.
After 25116 training step(s), loss on training batch is 0.00331488.
After 25117 training step(s), loss on training batch is 0.00335054.
After 25118 training step(s), loss on training batch is 0.00350109.
After 25119 training step(s), loss on training batch is 0.00314284.
After 25120 training step(s), loss on training batch is 0.00443004.
After 25121 training step(s), loss on training batch is 0.00322092.
After 25122 training step(s), loss on training batch is 0.00327582.
After 25123 training step(s), loss on training batch is 0.00350027.
After 25124 training step(s), loss on training batch is 0.00327592.
After 25125 training step(s), loss on training batch is 0.00341471.
After 25126 training step(s), loss on training batch is 0.00333567.
After 25127 training step(s), loss on training batch is 0.00342002.
After 25128 training step(s), loss on training batch is 0.00344679.
After 25129 training step(s), loss on training batch is 0.00354115.
After 25130 training step(s), loss on training batch is 0.00335032.
After 25131 training step(s), loss on training batch is 0.00332318.
After 25132 training step(s), loss on training batch is 0.00374339.
After 25133 training step(s), loss on training batch is 0.00352183.
After 25134 training step(s), loss on training batch is 0.00324672.
After 25135 training step(s), loss on training batch is 0.00310064.
After 25136 training step(s), loss on training batch is 0.00337467.
After 25137 training step(s), loss on training batch is 0.00355483.
After 25138 training step(s), loss on training batch is 0.00319442.
After 25139 training step(s), loss on training batch is 0.00335512.
After 25140 training step(s), loss on training batch is 0.00325336.
After 25141 training step(s), loss on training batch is 0.00384336.
After 25142 training step(s), loss on training batch is 0.00305582.
After 25143 training step(s), loss on training batch is 0.0033837.
After 25144 training step(s), loss on training batch is 0.00401863.
After 25145 training step(s), loss on training batch is 0.00338336.
After 25146 training step(s), loss on training batch is 0.00341075.
After 25147 training step(s), loss on training batch is 0.00310239.
After 25148 training step(s), loss on training batch is 0.00356323.
After 25149 training step(s), loss on training batch is 0.00336053.
After 25150 training step(s), loss on training batch is 0.00324979.
After 25151 training step(s), loss on training batch is 0.00338523.
After 25152 training step(s), loss on training batch is 0.00331526.
After 25153 training step(s), loss on training batch is 0.00366223.
After 25154 training step(s), loss on training batch is 0.00355451.
After 25155 training step(s), loss on training batch is 0.00323574.
After 25156 training step(s), loss on training batch is 0.00385775.
After 25157 training step(s), loss on training batch is 0.00330725.
After 25158 training step(s), loss on training batch is 0.00364491.
After 25159 training step(s), loss on training batch is 0.00299381.
After 25160 training step(s), loss on training batch is 0.0031837.
After 25161 training step(s), loss on training batch is 0.0031775.
After 25162 training step(s), loss on training batch is 0.00356644.
After 25163 training step(s), loss on training batch is 0.00407589.
After 25164 training step(s), loss on training batch is 0.00363286.
After 25165 training step(s), loss on training batch is 0.00394794.
After 25166 training step(s), loss on training batch is 0.00319158.
After 25167 training step(s), loss on training batch is 0.00381825.
After 25168 training step(s), loss on training batch is 0.00356424.
After 25169 training step(s), loss on training batch is 0.0032181.
After 25170 training step(s), loss on training batch is 0.00328648.
After 25171 training step(s), loss on training batch is 0.00315346.
After 25172 training step(s), loss on training batch is 0.00320786.
After 25173 training step(s), loss on training batch is 0.00335049.
After 25174 training step(s), loss on training batch is 0.00314559.
After 25175 training step(s), loss on training batch is 0.003487.
After 25176 training step(s), loss on training batch is 0.00333001.
After 25177 training step(s), loss on training batch is 0.00333957.
After 25178 training step(s), loss on training batch is 0.00335128.
After 25179 training step(s), loss on training batch is 0.00341237.
After 25180 training step(s), loss on training batch is 0.00311028.
After 25181 training step(s), loss on training batch is 0.00378636.
After 25182 training step(s), loss on training batch is 0.00349617.
After 25183 training step(s), loss on training batch is 0.00315808.
After 25184 training step(s), loss on training batch is 0.00352922.
After 25185 training step(s), loss on training batch is 0.00384494.
After 25186 training step(s), loss on training batch is 0.00364365.
After 25187 training step(s), loss on training batch is 0.00374517.
After 25188 training step(s), loss on training batch is 0.0032005.
After 25189 training step(s), loss on training batch is 0.00379404.
After 25190 training step(s), loss on training batch is 0.00377273.
After 25191 training step(s), loss on training batch is 0.00337854.
After 25192 training step(s), loss on training batch is 0.00334822.
After 25193 training step(s), loss on training batch is 0.00345823.
After 25194 training step(s), loss on training batch is 0.00381394.
After 25195 training step(s), loss on training batch is 0.00332703.
After 25196 training step(s), loss on training batch is 0.00324785.
After 25197 training step(s), loss on training batch is 0.00332641.
After 25198 training step(s), loss on training batch is 0.00316638.
After 25199 training step(s), loss on training batch is 0.00370414.
After 25200 training step(s), loss on training batch is 0.00301352.
After 25201 training step(s), loss on training batch is 0.00311748.
After 25202 training step(s), loss on training batch is 0.00330691.
After 25203 training step(s), loss on training batch is 0.00342684.
After 25204 training step(s), loss on training batch is 0.00383323.
After 25205 training step(s), loss on training batch is 0.00313464.
After 25206 training step(s), loss on training batch is 0.00366039.
After 25207 training step(s), loss on training batch is 0.00318914.
After 25208 training step(s), loss on training batch is 0.0035383.
After 25209 training step(s), loss on training batch is 0.00305228.
After 25210 training step(s), loss on training batch is 0.00343697.
After 25211 training step(s), loss on training batch is 0.00393477.
After 25212 training step(s), loss on training batch is 0.00370807.
After 25213 training step(s), loss on training batch is 0.00309792.
After 25214 training step(s), loss on training batch is 0.00345983.
After 25215 training step(s), loss on training batch is 0.00334831.
After 25216 training step(s), loss on training batch is 0.00332083.
After 25217 training step(s), loss on training batch is 0.00305292.
After 25218 training step(s), loss on training batch is 0.00343876.
After 25219 training step(s), loss on training batch is 0.00314285.
After 25220 training step(s), loss on training batch is 0.00375408.
After 25221 training step(s), loss on training batch is 0.00368953.
After 25222 training step(s), loss on training batch is 0.00334137.
After 25223 training step(s), loss on training batch is 0.00522335.
After 25224 training step(s), loss on training batch is 0.00356596.
After 25225 training step(s), loss on training batch is 0.00311728.
After 25226 training step(s), loss on training batch is 0.00390269.
After 25227 training step(s), loss on training batch is 0.00328423.
After 25228 training step(s), loss on training batch is 0.00322167.
After 25229 training step(s), loss on training batch is 0.00338273.
After 25230 training step(s), loss on training batch is 0.0033155.
After 25231 training step(s), loss on training batch is 0.00352773.
After 25232 training step(s), loss on training batch is 0.0035464.
After 25233 training step(s), loss on training batch is 0.00352299.
After 25234 training step(s), loss on training batch is 0.00310212.
After 25235 training step(s), loss on training batch is 0.00314206.
After 25236 training step(s), loss on training batch is 0.00344089.
After 25237 training step(s), loss on training batch is 0.00408126.
After 25238 training step(s), loss on training batch is 0.0034776.
After 25239 training step(s), loss on training batch is 0.0046308.
After 25240 training step(s), loss on training batch is 0.00352534.
After 25241 training step(s), loss on training batch is 0.00308071.
After 25242 training step(s), loss on training batch is 0.00323382.
After 25243 training step(s), loss on training batch is 0.00377047.
After 25244 training step(s), loss on training batch is 0.00326894.
After 25245 training step(s), loss on training batch is 0.00422975.
After 25246 training step(s), loss on training batch is 0.00324592.
After 25247 training step(s), loss on training batch is 0.00312546.
After 25248 training step(s), loss on training batch is 0.00330086.
After 25249 training step(s), loss on training batch is 0.00340779.
After 25250 training step(s), loss on training batch is 0.00374086.
After 25251 training step(s), loss on training batch is 0.00356318.
After 25252 training step(s), loss on training batch is 0.00403212.
After 25253 training step(s), loss on training batch is 0.00339896.
After 25254 training step(s), loss on training batch is 0.00314299.
After 25255 training step(s), loss on training batch is 0.00318599.
After 25256 training step(s), loss on training batch is 0.00332747.
After 25257 training step(s), loss on training batch is 0.00375558.
After 25258 training step(s), loss on training batch is 0.00397792.
After 25259 training step(s), loss on training batch is 0.00340385.
After 25260 training step(s), loss on training batch is 0.0031341.
After 25261 training step(s), loss on training batch is 0.00338808.
After 25262 training step(s), loss on training batch is 0.00314322.
After 25263 training step(s), loss on training batch is 0.00336132.
After 25264 training step(s), loss on training batch is 0.00411272.
After 25265 training step(s), loss on training batch is 0.00338355.
After 25266 training step(s), loss on training batch is 0.00369212.
After 25267 training step(s), loss on training batch is 0.00336532.
After 25268 training step(s), loss on training batch is 0.00348987.
After 25269 training step(s), loss on training batch is 0.00326702.
After 25270 training step(s), loss on training batch is 0.00403563.
After 25271 training step(s), loss on training batch is 0.00338764.
After 25272 training step(s), loss on training batch is 0.00324521.
After 25273 training step(s), loss on training batch is 0.00383808.
After 25274 training step(s), loss on training batch is 0.00316182.
After 25275 training step(s), loss on training batch is 0.00331968.
After 25276 training step(s), loss on training batch is 0.0055852.
After 25277 training step(s), loss on training batch is 0.00363626.
After 25278 training step(s), loss on training batch is 0.00356795.
After 25279 training step(s), loss on training batch is 0.00327801.
After 25280 training step(s), loss on training batch is 0.00344242.
After 25281 training step(s), loss on training batch is 0.00350119.
After 25282 training step(s), loss on training batch is 0.00381678.
After 25283 training step(s), loss on training batch is 0.00369686.
After 25284 training step(s), loss on training batch is 0.00318843.
After 25285 training step(s), loss on training batch is 0.00309074.
After 25286 training step(s), loss on training batch is 0.00327193.
After 25287 training step(s), loss on training batch is 0.00331016.
After 25288 training step(s), loss on training batch is 0.00328111.
After 25289 training step(s), loss on training batch is 0.00323528.
After 25290 training step(s), loss on training batch is 0.00360246.
After 25291 training step(s), loss on training batch is 0.00341464.
After 25292 training step(s), loss on training batch is 0.00325338.
After 25293 training step(s), loss on training batch is 0.0033598.
After 25294 training step(s), loss on training batch is 0.00429139.
After 25295 training step(s), loss on training batch is 0.00373725.
After 25296 training step(s), loss on training batch is 0.00377825.
After 25297 training step(s), loss on training batch is 0.00357051.
After 25298 training step(s), loss on training batch is 0.00313184.
After 25299 training step(s), loss on training batch is 0.00406101.
After 25300 training step(s), loss on training batch is 0.00313907.
After 25301 training step(s), loss on training batch is 0.00337909.
After 25302 training step(s), loss on training batch is 0.00325828.
After 25303 training step(s), loss on training batch is 0.0035388.
After 25304 training step(s), loss on training batch is 0.00361934.
After 25305 training step(s), loss on training batch is 0.00351708.
After 25306 training step(s), loss on training batch is 0.00367166.
After 25307 training step(s), loss on training batch is 0.00404923.
After 25308 training step(s), loss on training batch is 0.00334161.
After 25309 training step(s), loss on training batch is 0.00325611.
After 25310 training step(s), loss on training batch is 0.00323417.
After 25311 training step(s), loss on training batch is 0.00350355.
After 25312 training step(s), loss on training batch is 0.0033503.
After 25313 training step(s), loss on training batch is 0.00376509.
After 25314 training step(s), loss on training batch is 0.00337082.
After 25315 training step(s), loss on training batch is 0.00316197.
After 25316 training step(s), loss on training batch is 0.00329505.
After 25317 training step(s), loss on training batch is 0.0043437.
After 25318 training step(s), loss on training batch is 0.00353169.
After 25319 training step(s), loss on training batch is 0.0038385.
After 25320 training step(s), loss on training batch is 0.00362764.
After 25321 training step(s), loss on training batch is 0.00342638.
After 25322 training step(s), loss on training batch is 0.00318632.
After 25323 training step(s), loss on training batch is 0.0031636.
After 25324 training step(s), loss on training batch is 0.00309188.
After 25325 training step(s), loss on training batch is 0.00337572.
After 25326 training step(s), loss on training batch is 0.00356052.
After 25327 training step(s), loss on training batch is 0.00353302.
After 25328 training step(s), loss on training batch is 0.00324222.
After 25329 training step(s), loss on training batch is 0.00325438.
After 25330 training step(s), loss on training batch is 0.00330482.
After 25331 training step(s), loss on training batch is 0.00328516.
After 25332 training step(s), loss on training batch is 0.0036463.
After 25333 training step(s), loss on training batch is 0.00363862.
After 25334 training step(s), loss on training batch is 0.00322121.
After 25335 training step(s), loss on training batch is 0.00334085.
After 25336 training step(s), loss on training batch is 0.00353616.
After 25337 training step(s), loss on training batch is 0.00326224.
After 25338 training step(s), loss on training batch is 0.00337874.
After 25339 training step(s), loss on training batch is 0.00338239.
After 25340 training step(s), loss on training batch is 0.00388231.
After 25341 training step(s), loss on training batch is 0.00371056.
After 25342 training step(s), loss on training batch is 0.00327447.
After 25343 training step(s), loss on training batch is 0.00350835.
After 25344 training step(s), loss on training batch is 0.00315254.
After 25345 training step(s), loss on training batch is 0.00312716.
After 25346 training step(s), loss on training batch is 0.00370478.
After 25347 training step(s), loss on training batch is 0.00352626.
After 25348 training step(s), loss on training batch is 0.00329861.
After 25349 training step(s), loss on training batch is 0.00327893.
After 25350 training step(s), loss on training batch is 0.00365483.
After 25351 training step(s), loss on training batch is 0.00360329.
After 25352 training step(s), loss on training batch is 0.00364104.
After 25353 training step(s), loss on training batch is 0.00332578.
After 25354 training step(s), loss on training batch is 0.00327391.
After 25355 training step(s), loss on training batch is 0.00322335.
After 25356 training step(s), loss on training batch is 0.00324855.
After 25357 training step(s), loss on training batch is 0.00328098.
After 25358 training step(s), loss on training batch is 0.00324175.
After 25359 training step(s), loss on training batch is 0.00317251.
After 25360 training step(s), loss on training batch is 0.00299497.
After 25361 training step(s), loss on training batch is 0.00322373.
After 25362 training step(s), loss on training batch is 0.00346802.
After 25363 training step(s), loss on training batch is 0.00356314.
After 25364 training step(s), loss on training batch is 0.00311913.
After 25365 training step(s), loss on training batch is 0.00357091.
After 25366 training step(s), loss on training batch is 0.00341899.
After 25367 training step(s), loss on training batch is 0.00313512.
After 25368 training step(s), loss on training batch is 0.00346333.
After 25369 training step(s), loss on training batch is 0.00305874.
After 25370 training step(s), loss on training batch is 0.00341691.
After 25371 training step(s), loss on training batch is 0.00337839.
After 25372 training step(s), loss on training batch is 0.00388871.
After 25373 training step(s), loss on training batch is 0.00326536.
After 25374 training step(s), loss on training batch is 0.00378595.
After 25375 training step(s), loss on training batch is 0.0037064.
After 25376 training step(s), loss on training batch is 0.00346039.
After 25377 training step(s), loss on training batch is 0.00381901.
After 25378 training step(s), loss on training batch is 0.00364572.
After 25379 training step(s), loss on training batch is 0.00375626.
After 25380 training step(s), loss on training batch is 0.00344358.
After 25381 training step(s), loss on training batch is 0.00310708.
After 25382 training step(s), loss on training batch is 0.00363177.
After 25383 training step(s), loss on training batch is 0.00346362.
After 25384 training step(s), loss on training batch is 0.00385841.
After 25385 training step(s), loss on training batch is 0.00354367.
After 25386 training step(s), loss on training batch is 0.00373914.
After 25387 training step(s), loss on training batch is 0.00386265.
After 25388 training step(s), loss on training batch is 0.00371429.
After 25389 training step(s), loss on training batch is 0.00314667.
After 25390 training step(s), loss on training batch is 0.00341319.
After 25391 training step(s), loss on training batch is 0.00318828.
After 25392 training step(s), loss on training batch is 0.00344792.
After 25393 training step(s), loss on training batch is 0.00358802.
After 25394 training step(s), loss on training batch is 0.00361295.
After 25395 training step(s), loss on training batch is 0.00365304.
After 25396 training step(s), loss on training batch is 0.00354574.
After 25397 training step(s), loss on training batch is 0.00329734.
After 25398 training step(s), loss on training batch is 0.00350623.
After 25399 training step(s), loss on training batch is 0.00323636.
After 25400 training step(s), loss on training batch is 0.00330298.
After 25401 training step(s), loss on training batch is 0.00321666.
After 25402 training step(s), loss on training batch is 0.00315044.
After 25403 training step(s), loss on training batch is 0.00302286.
After 25404 training step(s), loss on training batch is 0.00356758.
After 25405 training step(s), loss on training batch is 0.00343676.
After 25406 training step(s), loss on training batch is 0.00327979.
After 25407 training step(s), loss on training batch is 0.00333209.
After 25408 training step(s), loss on training batch is 0.00311326.
After 25409 training step(s), loss on training batch is 0.00321079.
After 25410 training step(s), loss on training batch is 0.00344277.
After 25411 training step(s), loss on training batch is 0.00321284.
After 25412 training step(s), loss on training batch is 0.00357583.
After 25413 training step(s), loss on training batch is 0.00401864.
After 25414 training step(s), loss on training batch is 0.00329292.
After 25415 training step(s), loss on training batch is 0.00334473.
After 25416 training step(s), loss on training batch is 0.00360676.
After 25417 training step(s), loss on training batch is 0.00371198.
After 25418 training step(s), loss on training batch is 0.00317011.
After 25419 training step(s), loss on training batch is 0.0036346.
After 25420 training step(s), loss on training batch is 0.003267.
After 25421 training step(s), loss on training batch is 0.00346764.
After 25422 training step(s), loss on training batch is 0.00314447.
After 25423 training step(s), loss on training batch is 0.00422185.
After 25424 training step(s), loss on training batch is 0.00303824.
After 25425 training step(s), loss on training batch is 0.00347308.
After 25426 training step(s), loss on training batch is 0.00403284.
After 25427 training step(s), loss on training batch is 0.00339996.
After 25428 training step(s), loss on training batch is 0.00319568.
After 25429 training step(s), loss on training batch is 0.00313407.
After 25430 training step(s), loss on training batch is 0.00343094.
After 25431 training step(s), loss on training batch is 0.0034741.
After 25432 training step(s), loss on training batch is 0.00322537.
After 25433 training step(s), loss on training batch is 0.00351852.
After 25434 training step(s), loss on training batch is 0.00323353.
After 25435 training step(s), loss on training batch is 0.00321132.
After 25436 training step(s), loss on training batch is 0.00325587.
After 25437 training step(s), loss on training batch is 0.00346762.
After 25438 training step(s), loss on training batch is 0.00317061.
After 25439 training step(s), loss on training batch is 0.00342293.
After 25440 training step(s), loss on training batch is 0.00322883.
After 25441 training step(s), loss on training batch is 0.00328026.
After 25442 training step(s), loss on training batch is 0.00371382.
After 25443 training step(s), loss on training batch is 0.00337389.
After 25444 training step(s), loss on training batch is 0.00341888.
After 25445 training step(s), loss on training batch is 0.0031594.
After 25446 training step(s), loss on training batch is 0.00340806.
After 25447 training step(s), loss on training batch is 0.00388258.
After 25448 training step(s), loss on training batch is 0.00339461.
After 25449 training step(s), loss on training batch is 0.0037169.
After 25450 training step(s), loss on training batch is 0.00334626.
After 25451 training step(s), loss on training batch is 0.00346297.
After 25452 training step(s), loss on training batch is 0.00306034.
After 25453 training step(s), loss on training batch is 0.00322898.
After 25454 training step(s), loss on training batch is 0.00318146.
After 25455 training step(s), loss on training batch is 0.00349042.
After 25456 training step(s), loss on training batch is 0.00311707.
After 25457 training step(s), loss on training batch is 0.0031516.
After 25458 training step(s), loss on training batch is 0.00303719.
After 25459 training step(s), loss on training batch is 0.00373226.
After 25460 training step(s), loss on training batch is 0.00352991.
After 25461 training step(s), loss on training batch is 0.00342204.
After 25462 training step(s), loss on training batch is 0.00359837.
After 25463 training step(s), loss on training batch is 0.00319059.
After 25464 training step(s), loss on training batch is 0.00324598.
After 25465 training step(s), loss on training batch is 0.00357507.
After 25466 training step(s), loss on training batch is 0.0038709.
After 25467 training step(s), loss on training batch is 0.00368764.
After 25468 training step(s), loss on training batch is 0.00356408.
After 25469 training step(s), loss on training batch is 0.00375212.
After 25470 training step(s), loss on training batch is 0.00356995.
After 25471 training step(s), loss on training batch is 0.003521.
After 25472 training step(s), loss on training batch is 0.00319473.
After 25473 training step(s), loss on training batch is 0.0032832.
After 25474 training step(s), loss on training batch is 0.00319187.
After 25475 training step(s), loss on training batch is 0.00359664.
After 25476 training step(s), loss on training batch is 0.00314168.
After 25477 training step(s), loss on training batch is 0.00372505.
After 25478 training step(s), loss on training batch is 0.0033954.
After 25479 training step(s), loss on training batch is 0.00337781.
After 25480 training step(s), loss on training batch is 0.0035595.
After 25481 training step(s), loss on training batch is 0.00404045.
After 25482 training step(s), loss on training batch is 0.00378692.
After 25483 training step(s), loss on training batch is 0.00312261.
After 25484 training step(s), loss on training batch is 0.00351522.
After 25485 training step(s), loss on training batch is 0.00335302.
After 25486 training step(s), loss on training batch is 0.00305151.
After 25487 training step(s), loss on training batch is 0.00313218.
After 25488 training step(s), loss on training batch is 0.00321055.
After 25489 training step(s), loss on training batch is 0.00357286.
After 25490 training step(s), loss on training batch is 0.00325918.
After 25491 training step(s), loss on training batch is 0.00342711.
After 25492 training step(s), loss on training batch is 0.00367695.
After 25493 training step(s), loss on training batch is 0.00321573.
After 25494 training step(s), loss on training batch is 0.00320454.
After 25495 training step(s), loss on training batch is 0.00319085.
After 25496 training step(s), loss on training batch is 0.00387019.
After 25497 training step(s), loss on training batch is 0.00313923.
After 25498 training step(s), loss on training batch is 0.00322292.
After 25499 training step(s), loss on training batch is 0.00336677.
After 25500 training step(s), loss on training batch is 0.00379142.
After 25501 training step(s), loss on training batch is 0.00339462.
After 25502 training step(s), loss on training batch is 0.00329491.
After 25503 training step(s), loss on training batch is 0.00310834.
After 25504 training step(s), loss on training batch is 0.00326658.
After 25505 training step(s), loss on training batch is 0.0031308.
After 25506 training step(s), loss on training batch is 0.00396899.
After 25507 training step(s), loss on training batch is 0.00329112.
After 25508 training step(s), loss on training batch is 0.00353013.
After 25509 training step(s), loss on training batch is 0.00313675.
After 25510 training step(s), loss on training batch is 0.00334302.
After 25511 training step(s), loss on training batch is 0.00312676.
After 25512 training step(s), loss on training batch is 0.00329625.
After 25513 training step(s), loss on training batch is 0.00338008.
After 25514 training step(s), loss on training batch is 0.00331057.
After 25515 training step(s), loss on training batch is 0.00374011.
After 25516 training step(s), loss on training batch is 0.00343935.
After 25517 training step(s), loss on training batch is 0.00365408.
After 25518 training step(s), loss on training batch is 0.0033936.
After 25519 training step(s), loss on training batch is 0.00342587.
After 25520 training step(s), loss on training batch is 0.00374529.
After 25521 training step(s), loss on training batch is 0.00390086.
After 25522 training step(s), loss on training batch is 0.00308083.
After 25523 training step(s), loss on training batch is 0.00352346.
After 25524 training step(s), loss on training batch is 0.00311435.
After 25525 training step(s), loss on training batch is 0.00367957.
After 25526 training step(s), loss on training batch is 0.00379371.
After 25527 training step(s), loss on training batch is 0.00364395.
After 25528 training step(s), loss on training batch is 0.00365184.
After 25529 training step(s), loss on training batch is 0.00362027.
After 25530 training step(s), loss on training batch is 0.00331678.
After 25531 training step(s), loss on training batch is 0.00304565.
After 25532 training step(s), loss on training batch is 0.00319311.
After 25533 training step(s), loss on training batch is 0.00316384.
After 25534 training step(s), loss on training batch is 0.0031293.
After 25535 training step(s), loss on training batch is 0.00323528.
After 25536 training step(s), loss on training batch is 0.00327132.
After 25537 training step(s), loss on training batch is 0.00384953.
After 25538 training step(s), loss on training batch is 0.00350345.
After 25539 training step(s), loss on training batch is 0.00343262.
After 25540 training step(s), loss on training batch is 0.00316069.
After 25541 training step(s), loss on training batch is 0.0039419.
After 25542 training step(s), loss on training batch is 0.00316631.
After 25543 training step(s), loss on training batch is 0.00359445.
After 25544 training step(s), loss on training batch is 0.00357557.
After 25545 training step(s), loss on training batch is 0.00583334.
After 25546 training step(s), loss on training batch is 0.00318048.
After 25547 training step(s), loss on training batch is 0.00309389.
After 25548 training step(s), loss on training batch is 0.00374834.
After 25549 training step(s), loss on training batch is 0.00661937.
After 25550 training step(s), loss on training batch is 0.00366528.
After 25551 training step(s), loss on training batch is 0.00312245.
After 25552 training step(s), loss on training batch is 0.00341064.
After 25553 training step(s), loss on training batch is 0.00327663.
After 25554 training step(s), loss on training batch is 0.00324576.
After 25555 training step(s), loss on training batch is 0.00332314.
After 25556 training step(s), loss on training batch is 0.00319667.
After 25557 training step(s), loss on training batch is 0.00343807.
After 25558 training step(s), loss on training batch is 0.00366361.
After 25559 training step(s), loss on training batch is 0.00373831.
After 25560 training step(s), loss on training batch is 0.00369546.
After 25561 training step(s), loss on training batch is 0.00319572.
After 25562 training step(s), loss on training batch is 0.00416421.
After 25563 training step(s), loss on training batch is 0.00317197.
After 25564 training step(s), loss on training batch is 0.00313895.
After 25565 training step(s), loss on training batch is 0.00409753.
After 25566 training step(s), loss on training batch is 0.00355555.
After 25567 training step(s), loss on training batch is 0.00327575.
After 25568 training step(s), loss on training batch is 0.0033025.
After 25569 training step(s), loss on training batch is 0.00864735.
After 25570 training step(s), loss on training batch is 0.00354347.
After 25571 training step(s), loss on training batch is 0.00365031.
After 25572 training step(s), loss on training batch is 0.00398577.
After 25573 training step(s), loss on training batch is 0.0037881.
After 25574 training step(s), loss on training batch is 0.0036486.
After 25575 training step(s), loss on training batch is 0.00318166.
After 25576 training step(s), loss on training batch is 0.00316235.
After 25577 training step(s), loss on training batch is 0.00351233.
After 25578 training step(s), loss on training batch is 0.0032718.
After 25579 training step(s), loss on training batch is 0.00351985.
After 25580 training step(s), loss on training batch is 0.00403917.
After 25581 training step(s), loss on training batch is 0.00313442.
After 25582 training step(s), loss on training batch is 0.00337507.
After 25583 training step(s), loss on training batch is 0.0040632.
After 25584 training step(s), loss on training batch is 0.00325716.
After 25585 training step(s), loss on training batch is 0.00342642.
After 25586 training step(s), loss on training batch is 0.00342545.
After 25587 training step(s), loss on training batch is 0.00336716.
After 25588 training step(s), loss on training batch is 0.0031266.
After 25589 training step(s), loss on training batch is 0.0037728.
After 25590 training step(s), loss on training batch is 0.00317715.
After 25591 training step(s), loss on training batch is 0.00317786.
After 25592 training step(s), loss on training batch is 0.00327081.
After 25593 training step(s), loss on training batch is 0.00329832.
After 25594 training step(s), loss on training batch is 0.00353903.
After 25595 training step(s), loss on training batch is 0.00322632.
After 25596 training step(s), loss on training batch is 0.00345952.
After 25597 training step(s), loss on training batch is 0.00312982.
After 25598 training step(s), loss on training batch is 0.00315922.
After 25599 training step(s), loss on training batch is 0.00340354.
After 25600 training step(s), loss on training batch is 0.00313583.
After 25601 training step(s), loss on training batch is 0.00314407.
After 25602 training step(s), loss on training batch is 0.00346481.
After 25603 training step(s), loss on training batch is 0.00336006.
After 25604 training step(s), loss on training batch is 0.00371247.
After 25605 training step(s), loss on training batch is 0.00317689.
After 25606 training step(s), loss on training batch is 0.00318105.
After 25607 training step(s), loss on training batch is 0.0032229.
After 25608 training step(s), loss on training batch is 0.00354182.
After 25609 training step(s), loss on training batch is 0.00332775.
After 25610 training step(s), loss on training batch is 0.00348175.
After 25611 training step(s), loss on training batch is 0.00336406.
After 25612 training step(s), loss on training batch is 0.00343633.
After 25613 training step(s), loss on training batch is 0.0030623.
After 25614 training step(s), loss on training batch is 0.00454067.
After 25615 training step(s), loss on training batch is 0.00361453.
After 25616 training step(s), loss on training batch is 0.00378686.
After 25617 training step(s), loss on training batch is 0.00310044.
After 25618 training step(s), loss on training batch is 0.00323059.
After 25619 training step(s), loss on training batch is 0.00336319.
After 25620 training step(s), loss on training batch is 0.00312535.
After 25621 training step(s), loss on training batch is 0.0034428.
After 25622 training step(s), loss on training batch is 0.00368582.
After 25623 training step(s), loss on training batch is 0.00327165.
After 25624 training step(s), loss on training batch is 0.0035133.
After 25625 training step(s), loss on training batch is 0.00327344.
After 25626 training step(s), loss on training batch is 0.0030378.
After 25627 training step(s), loss on training batch is 0.00312533.
After 25628 training step(s), loss on training batch is 0.00349311.
After 25629 training step(s), loss on training batch is 0.00323089.
After 25630 training step(s), loss on training batch is 0.00335539.
After 25631 training step(s), loss on training batch is 0.003272.
After 25632 training step(s), loss on training batch is 0.0033133.
After 25633 training step(s), loss on training batch is 0.00313899.
After 25634 training step(s), loss on training batch is 0.00317395.
After 25635 training step(s), loss on training batch is 0.00367481.
After 25636 training step(s), loss on training batch is 0.00334149.
After 25637 training step(s), loss on training batch is 0.00370098.
After 25638 training step(s), loss on training batch is 0.00360791.
After 25639 training step(s), loss on training batch is 0.00369566.
After 25640 training step(s), loss on training batch is 0.00374315.
After 25641 training step(s), loss on training batch is 0.00346613.
After 25642 training step(s), loss on training batch is 0.00333787.
After 25643 training step(s), loss on training batch is 0.00322025.
After 25644 training step(s), loss on training batch is 0.00310215.
After 25645 training step(s), loss on training batch is 0.00338612.
After 25646 training step(s), loss on training batch is 0.00328942.
After 25647 training step(s), loss on training batch is 0.00359019.
After 25648 training step(s), loss on training batch is 0.0031241.
After 25649 training step(s), loss on training batch is 0.00361839.
After 25650 training step(s), loss on training batch is 0.00323279.
After 25651 training step(s), loss on training batch is 0.00381231.
After 25652 training step(s), loss on training batch is 0.00320673.
After 25653 training step(s), loss on training batch is 0.00318295.
After 25654 training step(s), loss on training batch is 0.00339545.
After 25655 training step(s), loss on training batch is 0.00361491.
After 25656 training step(s), loss on training batch is 0.00320306.
After 25657 training step(s), loss on training batch is 0.00348705.
After 25658 training step(s), loss on training batch is 0.00324626.
After 25659 training step(s), loss on training batch is 0.0033343.
After 25660 training step(s), loss on training batch is 0.00374226.
After 25661 training step(s), loss on training batch is 0.00350873.
After 25662 training step(s), loss on training batch is 0.00330537.
After 25663 training step(s), loss on training batch is 0.0037564.
After 25664 training step(s), loss on training batch is 0.00328741.
After 25665 training step(s), loss on training batch is 0.00318883.
After 25666 training step(s), loss on training batch is 0.00341842.
After 25667 training step(s), loss on training batch is 0.00329666.
After 25668 training step(s), loss on training batch is 0.00323028.
After 25669 training step(s), loss on training batch is 0.00317267.
After 25670 training step(s), loss on training batch is 0.00312931.
After 25671 training step(s), loss on training batch is 0.00347156.
After 25672 training step(s), loss on training batch is 0.00350695.
After 25673 training step(s), loss on training batch is 0.00330006.
After 25674 training step(s), loss on training batch is 0.00332189.
After 25675 training step(s), loss on training batch is 0.0033982.
After 25676 training step(s), loss on training batch is 0.00303814.
After 25677 training step(s), loss on training batch is 0.00322374.
After 25678 training step(s), loss on training batch is 0.00372529.
After 25679 training step(s), loss on training batch is 0.00347998.
After 25680 training step(s), loss on training batch is 0.00333647.
After 25681 training step(s), loss on training batch is 0.00344422.
After 25682 training step(s), loss on training batch is 0.00316778.
After 25683 training step(s), loss on training batch is 0.00322657.
After 25684 training step(s), loss on training batch is 0.00334597.
After 25685 training step(s), loss on training batch is 0.00322206.
After 25686 training step(s), loss on training batch is 0.00308287.
After 25687 training step(s), loss on training batch is 0.00326421.
After 25688 training step(s), loss on training batch is 0.00324365.
After 25689 training step(s), loss on training batch is 0.00322208.
After 25690 training step(s), loss on training batch is 0.00342092.
After 25691 training step(s), loss on training batch is 0.00406698.
After 25692 training step(s), loss on training batch is 0.00360856.
After 25693 training step(s), loss on training batch is 0.00335912.
After 25694 training step(s), loss on training batch is 0.00336683.
After 25695 training step(s), loss on training batch is 0.00319736.
After 25696 training step(s), loss on training batch is 0.00342576.
After 25697 training step(s), loss on training batch is 0.00306712.
After 25698 training step(s), loss on training batch is 0.00349306.
After 25699 training step(s), loss on training batch is 0.00305122.
After 25700 training step(s), loss on training batch is 0.00307225.
After 25701 training step(s), loss on training batch is 0.00321274.
After 25702 training step(s), loss on training batch is 0.00378447.
After 25703 training step(s), loss on training batch is 0.00323886.
After 25704 training step(s), loss on training batch is 0.00362111.
After 25705 training step(s), loss on training batch is 0.0040243.
After 25706 training step(s), loss on training batch is 0.00356015.
After 25707 training step(s), loss on training batch is 0.00469144.
After 25708 training step(s), loss on training batch is 0.003395.
After 25709 training step(s), loss on training batch is 0.00383826.
After 25710 training step(s), loss on training batch is 0.00316033.
After 25711 training step(s), loss on training batch is 0.00358698.
After 25712 training step(s), loss on training batch is 0.00363359.
After 25713 training step(s), loss on training batch is 0.00314163.
After 25714 training step(s), loss on training batch is 0.00344566.
After 25715 training step(s), loss on training batch is 0.00329493.
After 25716 training step(s), loss on training batch is 0.00343216.
After 25717 training step(s), loss on training batch is 0.00365909.
After 25718 training step(s), loss on training batch is 0.00319178.
After 25719 training step(s), loss on training batch is 0.00312122.
After 25720 training step(s), loss on training batch is 0.00351767.
After 25721 training step(s), loss on training batch is 0.00423348.
After 25722 training step(s), loss on training batch is 0.00353965.
After 25723 training step(s), loss on training batch is 0.0034335.
After 25724 training step(s), loss on training batch is 0.0034379.
After 25725 training step(s), loss on training batch is 0.00334322.
After 25726 training step(s), loss on training batch is 0.00403195.
After 25727 training step(s), loss on training batch is 0.00332098.
After 25728 training step(s), loss on training batch is 0.00345716.
After 25729 training step(s), loss on training batch is 0.00333956.
After 25730 training step(s), loss on training batch is 0.0034283.
After 25731 training step(s), loss on training batch is 0.003083.
After 25732 training step(s), loss on training batch is 0.0033056.
After 25733 training step(s), loss on training batch is 0.00351043.
After 25734 training step(s), loss on training batch is 0.003267.
After 25735 training step(s), loss on training batch is 0.00320779.
After 25736 training step(s), loss on training batch is 0.00327907.
After 25737 training step(s), loss on training batch is 0.00308823.
After 25738 training step(s), loss on training batch is 0.00323976.
After 25739 training step(s), loss on training batch is 0.00299152.
After 25740 training step(s), loss on training batch is 0.00339339.
After 25741 training step(s), loss on training batch is 0.00314373.
After 25742 training step(s), loss on training batch is 0.00315178.
After 25743 training step(s), loss on training batch is 0.00376004.
After 25744 training step(s), loss on training batch is 0.00349366.
After 25745 training step(s), loss on training batch is 0.00347746.
After 25746 training step(s), loss on training batch is 0.003252.
After 25747 training step(s), loss on training batch is 0.00354794.
After 25748 training step(s), loss on training batch is 0.00350591.
After 25749 training step(s), loss on training batch is 0.00337108.
After 25750 training step(s), loss on training batch is 0.00334963.
After 25751 training step(s), loss on training batch is 0.00354669.
After 25752 training step(s), loss on training batch is 0.0034971.
After 25753 training step(s), loss on training batch is 0.00365646.
After 25754 training step(s), loss on training batch is 0.00313558.
After 25755 training step(s), loss on training batch is 0.00399723.
After 25756 training step(s), loss on training batch is 0.00336725.
After 25757 training step(s), loss on training batch is 0.00380251.
After 25758 training step(s), loss on training batch is 0.00334271.
After 25759 training step(s), loss on training batch is 0.00426656.
After 25760 training step(s), loss on training batch is 0.00369354.
After 25761 training step(s), loss on training batch is 0.00370662.
After 25762 training step(s), loss on training batch is 0.00390842.
After 25763 training step(s), loss on training batch is 0.00320272.
After 25764 training step(s), loss on training batch is 0.00318299.
After 25765 training step(s), loss on training batch is 0.00388326.
After 25766 training step(s), loss on training batch is 0.00454768.
After 25767 training step(s), loss on training batch is 0.00353757.
After 25768 training step(s), loss on training batch is 0.00325873.
After 25769 training step(s), loss on training batch is 0.00349884.
After 25770 training step(s), loss on training batch is 0.00360193.
After 25771 training step(s), loss on training batch is 0.00355472.
After 25772 training step(s), loss on training batch is 0.0033579.
After 25773 training step(s), loss on training batch is 0.0033365.
After 25774 training step(s), loss on training batch is 0.00334218.
After 25775 training step(s), loss on training batch is 0.00389199.
After 25776 training step(s), loss on training batch is 0.00344156.
After 25777 training step(s), loss on training batch is 0.0033768.
After 25778 training step(s), loss on training batch is 0.00311508.
After 25779 training step(s), loss on training batch is 0.00379232.
After 25780 training step(s), loss on training batch is 0.00361264.
After 25781 training step(s), loss on training batch is 0.00311139.
After 25782 training step(s), loss on training batch is 0.00321238.
After 25783 training step(s), loss on training batch is 0.00391468.
After 25784 training step(s), loss on training batch is 0.00338056.
After 25785 training step(s), loss on training batch is 0.00354166.
After 25786 training step(s), loss on training batch is 0.00332185.
After 25787 training step(s), loss on training batch is 0.00312465.
After 25788 training step(s), loss on training batch is 0.00329917.
After 25789 training step(s), loss on training batch is 0.00367656.
After 25790 training step(s), loss on training batch is 0.00302219.
After 25791 training step(s), loss on training batch is 0.00308873.
After 25792 training step(s), loss on training batch is 0.00375945.
After 25793 training step(s), loss on training batch is 0.0033179.
After 25794 training step(s), loss on training batch is 0.00338562.
After 25795 training step(s), loss on training batch is 0.00337245.
After 25796 training step(s), loss on training batch is 0.00356027.
After 25797 training step(s), loss on training batch is 0.00351895.
After 25798 training step(s), loss on training batch is 0.00335162.
After 25799 training step(s), loss on training batch is 0.00369267.
After 25800 training step(s), loss on training batch is 0.00337083.
After 25801 training step(s), loss on training batch is 0.00317081.
After 25802 training step(s), loss on training batch is 0.00413891.
After 25803 training step(s), loss on training batch is 0.00326609.
After 25804 training step(s), loss on training batch is 0.00312657.
After 25805 training step(s), loss on training batch is 0.00342231.
After 25806 training step(s), loss on training batch is 0.0034933.
After 25807 training step(s), loss on training batch is 0.00375818.
After 25808 training step(s), loss on training batch is 0.00318187.
After 25809 training step(s), loss on training batch is 0.00383388.
After 25810 training step(s), loss on training batch is 0.00334847.
After 25811 training step(s), loss on training batch is 0.00338849.
After 25812 training step(s), loss on training batch is 0.00341843.
After 25813 training step(s), loss on training batch is 0.00329553.
After 25814 training step(s), loss on training batch is 0.00315112.
After 25815 training step(s), loss on training batch is 0.00344118.
After 25816 training step(s), loss on training batch is 0.00356859.
After 25817 training step(s), loss on training batch is 0.00313381.
After 25818 training step(s), loss on training batch is 0.0033287.
After 25819 training step(s), loss on training batch is 0.003372.
After 25820 training step(s), loss on training batch is 0.00332186.
After 25821 training step(s), loss on training batch is 0.00337068.
After 25822 training step(s), loss on training batch is 0.00372001.
After 25823 training step(s), loss on training batch is 0.00338244.
After 25824 training step(s), loss on training batch is 0.00353375.
After 25825 training step(s), loss on training batch is 0.00357109.
After 25826 training step(s), loss on training batch is 0.003797.
After 25827 training step(s), loss on training batch is 0.00339907.
After 25828 training step(s), loss on training batch is 0.00320846.
After 25829 training step(s), loss on training batch is 0.0037995.
After 25830 training step(s), loss on training batch is 0.00302786.
After 25831 training step(s), loss on training batch is 0.00302194.
After 25832 training step(s), loss on training batch is 0.00336304.
After 25833 training step(s), loss on training batch is 0.00378481.
After 25834 training step(s), loss on training batch is 0.00352914.
After 25835 training step(s), loss on training batch is 0.00340151.
After 25836 training step(s), loss on training batch is 0.00344481.
After 25837 training step(s), loss on training batch is 0.00342279.
After 25838 training step(s), loss on training batch is 0.00328831.
After 25839 training step(s), loss on training batch is 0.00354041.
After 25840 training step(s), loss on training batch is 0.00336296.
After 25841 training step(s), loss on training batch is 0.00360483.
After 25842 training step(s), loss on training batch is 0.00331849.
After 25843 training step(s), loss on training batch is 0.00350519.
After 25844 training step(s), loss on training batch is 0.00328931.
After 25845 training step(s), loss on training batch is 0.00352389.
After 25846 training step(s), loss on training batch is 0.00354033.
After 25847 training step(s), loss on training batch is 0.00334343.
After 25848 training step(s), loss on training batch is 0.00427037.
After 25849 training step(s), loss on training batch is 0.00382487.
After 25850 training step(s), loss on training batch is 0.00322526.
After 25851 training step(s), loss on training batch is 0.00313977.
After 25852 training step(s), loss on training batch is 0.00348012.
After 25853 training step(s), loss on training batch is 0.00312121.
After 25854 training step(s), loss on training batch is 0.00432667.
After 25855 training step(s), loss on training batch is 0.00323332.
After 25856 training step(s), loss on training batch is 0.00322215.
After 25857 training step(s), loss on training batch is 0.00340944.
After 25858 training step(s), loss on training batch is 0.0035875.
After 25859 training step(s), loss on training batch is 0.00305822.
After 25860 training step(s), loss on training batch is 0.0036309.
After 25861 training step(s), loss on training batch is 0.00397605.
After 25862 training step(s), loss on training batch is 0.00341813.
After 25863 training step(s), loss on training batch is 0.00356494.
After 25864 training step(s), loss on training batch is 0.00319826.
After 25865 training step(s), loss on training batch is 0.00349837.
After 25866 training step(s), loss on training batch is 0.00332365.
After 25867 training step(s), loss on training batch is 0.00341017.
After 25868 training step(s), loss on training batch is 0.00316294.
After 25869 training step(s), loss on training batch is 0.00351118.
After 25870 training step(s), loss on training batch is 0.00318646.
After 25871 training step(s), loss on training batch is 0.00356817.
After 25872 training step(s), loss on training batch is 0.00330736.
After 25873 training step(s), loss on training batch is 0.00347192.
After 25874 training step(s), loss on training batch is 0.00324941.
After 25875 training step(s), loss on training batch is 0.00314488.
After 25876 training step(s), loss on training batch is 0.00345053.
After 25877 training step(s), loss on training batch is 0.00321955.
After 25878 training step(s), loss on training batch is 0.00318515.
After 25879 training step(s), loss on training batch is 0.00359318.
After 25880 training step(s), loss on training batch is 0.00370403.
After 25881 training step(s), loss on training batch is 0.00332391.
After 25882 training step(s), loss on training batch is 0.00360941.
After 25883 training step(s), loss on training batch is 0.00343175.
After 25884 training step(s), loss on training batch is 0.00341803.
After 25885 training step(s), loss on training batch is 0.00335807.
After 25886 training step(s), loss on training batch is 0.00339689.
After 25887 training step(s), loss on training batch is 0.0033136.
After 25888 training step(s), loss on training batch is 0.0032539.
After 25889 training step(s), loss on training batch is 0.00309326.
After 25890 training step(s), loss on training batch is 0.00348957.
After 25891 training step(s), loss on training batch is 0.00323392.
After 25892 training step(s), loss on training batch is 0.0034993.
After 25893 training step(s), loss on training batch is 0.00323393.
After 25894 training step(s), loss on training batch is 0.00321079.
After 25895 training step(s), loss on training batch is 0.00462805.
After 25896 training step(s), loss on training batch is 0.00355547.
After 25897 training step(s), loss on training batch is 0.0035392.
After 25898 training step(s), loss on training batch is 0.00464692.
After 25899 training step(s), loss on training batch is 0.00362307.
After 25900 training step(s), loss on training batch is 0.00338025.
After 25901 training step(s), loss on training batch is 0.0036146.
After 25902 training step(s), loss on training batch is 0.00342586.
After 25903 training step(s), loss on training batch is 0.00328346.
After 25904 training step(s), loss on training batch is 0.00339192.
After 25905 training step(s), loss on training batch is 0.00335133.
After 25906 training step(s), loss on training batch is 0.00301581.
After 25907 training step(s), loss on training batch is 0.00351728.
After 25908 training step(s), loss on training batch is 0.00330731.
After 25909 training step(s), loss on training batch is 0.00306176.
After 25910 training step(s), loss on training batch is 0.00318035.
After 25911 training step(s), loss on training batch is 0.00342955.
After 25912 training step(s), loss on training batch is 0.00327251.
After 25913 training step(s), loss on training batch is 0.00341193.
After 25914 training step(s), loss on training batch is 0.00320191.
After 25915 training step(s), loss on training batch is 0.00359053.
After 25916 training step(s), loss on training batch is 0.00361082.
After 25917 training step(s), loss on training batch is 0.00327372.
After 25918 training step(s), loss on training batch is 0.00306037.
After 25919 training step(s), loss on training batch is 0.00337028.
After 25920 training step(s), loss on training batch is 0.00306626.
After 25921 training step(s), loss on training batch is 0.00340341.
After 25922 training step(s), loss on training batch is 0.00355017.
After 25923 training step(s), loss on training batch is 0.00383364.
After 25924 training step(s), loss on training batch is 0.0031679.
After 25925 training step(s), loss on training batch is 0.00322656.
After 25926 training step(s), loss on training batch is 0.00379591.
After 25927 training step(s), loss on training batch is 0.00340404.
After 25928 training step(s), loss on training batch is 0.00322145.
After 25929 training step(s), loss on training batch is 0.00326296.
After 25930 training step(s), loss on training batch is 0.00329819.
After 25931 training step(s), loss on training batch is 0.00359823.
After 25932 training step(s), loss on training batch is 0.00346869.
After 25933 training step(s), loss on training batch is 0.00358147.
After 25934 training step(s), loss on training batch is 0.00313453.
After 25935 training step(s), loss on training batch is 0.00324997.
After 25936 training step(s), loss on training batch is 0.00362209.
After 25937 training step(s), loss on training batch is 0.00337583.
After 25938 training step(s), loss on training batch is 0.00341797.
After 25939 training step(s), loss on training batch is 0.00348241.
After 25940 training step(s), loss on training batch is 0.00354763.
After 25941 training step(s), loss on training batch is 0.00344696.
After 25942 training step(s), loss on training batch is 0.00383653.
After 25943 training step(s), loss on training batch is 0.00327111.
After 25944 training step(s), loss on training batch is 0.00327611.
After 25945 training step(s), loss on training batch is 0.00360039.
After 25946 training step(s), loss on training batch is 0.00382062.
After 25947 training step(s), loss on training batch is 0.00361846.
After 25948 training step(s), loss on training batch is 0.00306788.
After 25949 training step(s), loss on training batch is 0.00316914.
After 25950 training step(s), loss on training batch is 0.00321663.
After 25951 training step(s), loss on training batch is 0.00385119.
After 25952 training step(s), loss on training batch is 0.00317634.
After 25953 training step(s), loss on training batch is 0.00328262.
After 25954 training step(s), loss on training batch is 0.00324918.
After 25955 training step(s), loss on training batch is 0.00330585.
After 25956 training step(s), loss on training batch is 0.00348696.
After 25957 training step(s), loss on training batch is 0.0034212.
After 25958 training step(s), loss on training batch is 0.00322991.
After 25959 training step(s), loss on training batch is 0.00310249.
After 25960 training step(s), loss on training batch is 0.00331106.
After 25961 training step(s), loss on training batch is 0.00340556.
After 25962 training step(s), loss on training batch is 0.0037069.
After 25963 training step(s), loss on training batch is 0.00388276.
After 25964 training step(s), loss on training batch is 0.00354633.
After 25965 training step(s), loss on training batch is 0.00320212.
After 25966 training step(s), loss on training batch is 0.00350192.
After 25967 training step(s), loss on training batch is 0.00307962.
After 25968 training step(s), loss on training batch is 0.00353794.
After 25969 training step(s), loss on training batch is 0.00374442.
After 25970 training step(s), loss on training batch is 0.0032239.
After 25971 training step(s), loss on training batch is 0.00375871.
After 25972 training step(s), loss on training batch is 0.00314375.
After 25973 training step(s), loss on training batch is 0.00332204.
After 25974 training step(s), loss on training batch is 0.00321487.
After 25975 training step(s), loss on training batch is 0.00340283.
After 25976 training step(s), loss on training batch is 0.00318236.
After 25977 training step(s), loss on training batch is 0.00343237.
After 25978 training step(s), loss on training batch is 0.00394081.
After 25979 training step(s), loss on training batch is 0.00337615.
After 25980 training step(s), loss on training batch is 0.00336429.
After 25981 training step(s), loss on training batch is 0.00337542.
After 25982 training step(s), loss on training batch is 0.00343811.
After 25983 training step(s), loss on training batch is 0.00331551.
After 25984 training step(s), loss on training batch is 0.00369909.
After 25985 training step(s), loss on training batch is 0.00315234.
After 25986 training step(s), loss on training batch is 0.00318508.
After 25987 training step(s), loss on training batch is 0.00326445.
After 25988 training step(s), loss on training batch is 0.00321779.
After 25989 training step(s), loss on training batch is 0.00315846.
After 25990 training step(s), loss on training batch is 0.00344783.
After 25991 training step(s), loss on training batch is 0.00356795.
After 25992 training step(s), loss on training batch is 0.00326053.
After 25993 training step(s), loss on training batch is 0.00358562.
After 25994 training step(s), loss on training batch is 0.00327677.
After 25995 training step(s), loss on training batch is 0.00309605.
After 25996 training step(s), loss on training batch is 0.00327522.
After 25997 training step(s), loss on training batch is 0.00317052.
After 25998 training step(s), loss on training batch is 0.00333224.
After 25999 training step(s), loss on training batch is 0.00346149.
After 26000 training step(s), loss on training batch is 0.00356969.
After 26001 training step(s), loss on training batch is 0.00356983.
After 26002 training step(s), loss on training batch is 0.00315538.
After 26003 training step(s), loss on training batch is 0.00368713.
After 26004 training step(s), loss on training batch is 0.00352735.
After 26005 training step(s), loss on training batch is 0.00317122.
After 26006 training step(s), loss on training batch is 0.00305123.
After 26007 training step(s), loss on training batch is 0.00323447.
After 26008 training step(s), loss on training batch is 0.00327323.
After 26009 training step(s), loss on training batch is 0.0035452.
After 26010 training step(s), loss on training batch is 0.00351246.
After 26011 training step(s), loss on training batch is 0.0032772.
After 26012 training step(s), loss on training batch is 0.00300718.
After 26013 training step(s), loss on training batch is 0.00346956.
After 26014 training step(s), loss on training batch is 0.00322116.
After 26015 training step(s), loss on training batch is 0.00341622.
After 26016 training step(s), loss on training batch is 0.00331296.
After 26017 training step(s), loss on training batch is 0.00345783.
After 26018 training step(s), loss on training batch is 0.00328705.
After 26019 training step(s), loss on training batch is 0.0033544.
After 26020 training step(s), loss on training batch is 0.00357157.
After 26021 training step(s), loss on training batch is 0.00338486.
After 26022 training step(s), loss on training batch is 0.0032356.
After 26023 training step(s), loss on training batch is 0.00355073.
After 26024 training step(s), loss on training batch is 0.00319012.
After 26025 training step(s), loss on training batch is 0.00365628.
After 26026 training step(s), loss on training batch is 0.00355273.
After 26027 training step(s), loss on training batch is 0.00302853.
After 26028 training step(s), loss on training batch is 0.00356194.
After 26029 training step(s), loss on training batch is 0.00372981.
After 26030 training step(s), loss on training batch is 0.00345467.
After 26031 training step(s), loss on training batch is 0.0032751.
After 26032 training step(s), loss on training batch is 0.00368757.
After 26033 training step(s), loss on training batch is 0.00329105.
After 26034 training step(s), loss on training batch is 0.00312079.
After 26035 training step(s), loss on training batch is 0.00321836.
After 26036 training step(s), loss on training batch is 0.00353831.
After 26037 training step(s), loss on training batch is 0.00425168.
After 26038 training step(s), loss on training batch is 0.00320933.
After 26039 training step(s), loss on training batch is 0.00367314.
After 26040 training step(s), loss on training batch is 0.00336417.
After 26041 training step(s), loss on training batch is 0.0036114.
After 26042 training step(s), loss on training batch is 0.00308781.
After 26043 training step(s), loss on training batch is 0.00341066.
After 26044 training step(s), loss on training batch is 0.00336155.
After 26045 training step(s), loss on training batch is 0.00364939.
After 26046 training step(s), loss on training batch is 0.0034159.
After 26047 training step(s), loss on training batch is 0.00326383.
After 26048 training step(s), loss on training batch is 0.0035128.
After 26049 training step(s), loss on training batch is 0.00318236.
After 26050 training step(s), loss on training batch is 0.00382613.
After 26051 training step(s), loss on training batch is 0.00341116.
After 26052 training step(s), loss on training batch is 0.00318098.
After 26053 training step(s), loss on training batch is 0.00354623.
After 26054 training step(s), loss on training batch is 0.00338736.
After 26055 training step(s), loss on training batch is 0.00344772.
After 26056 training step(s), loss on training batch is 0.00326617.
After 26057 training step(s), loss on training batch is 0.00336262.
After 26058 training step(s), loss on training batch is 0.00308756.
After 26059 training step(s), loss on training batch is 0.00349832.
After 26060 training step(s), loss on training batch is 0.0036158.
After 26061 training step(s), loss on training batch is 0.00302744.
After 26062 training step(s), loss on training batch is 0.00338135.
After 26063 training step(s), loss on training batch is 0.00323007.
After 26064 training step(s), loss on training batch is 0.00311373.
After 26065 training step(s), loss on training batch is 0.00319713.
After 26066 training step(s), loss on training batch is 0.00327392.
After 26067 training step(s), loss on training batch is 0.00350994.
After 26068 training step(s), loss on training batch is 0.00330072.
After 26069 training step(s), loss on training batch is 0.00351398.
After 26070 training step(s), loss on training batch is 0.00303574.
After 26071 training step(s), loss on training batch is 0.00308399.
After 26072 training step(s), loss on training batch is 0.00351294.
After 26073 training step(s), loss on training batch is 0.0035959.
After 26074 training step(s), loss on training batch is 0.00346604.
After 26075 training step(s), loss on training batch is 0.00341592.
After 26076 training step(s), loss on training batch is 0.00338563.
After 26077 training step(s), loss on training batch is 0.00348428.
After 26078 training step(s), loss on training batch is 0.00360651.
After 26079 training step(s), loss on training batch is 0.00324696.
After 26080 training step(s), loss on training batch is 0.00315493.
After 26081 training step(s), loss on training batch is 0.00357167.
After 26082 training step(s), loss on training batch is 0.00335554.
After 26083 training step(s), loss on training batch is 0.00335995.
After 26084 training step(s), loss on training batch is 0.00332263.
After 26085 training step(s), loss on training batch is 0.00381583.
After 26086 training step(s), loss on training batch is 0.0033548.
After 26087 training step(s), loss on training batch is 0.00355279.
After 26088 training step(s), loss on training batch is 0.00341686.
After 26089 training step(s), loss on training batch is 0.00321217.
After 26090 training step(s), loss on training batch is 0.00368512.
After 26091 training step(s), loss on training batch is 0.00307712.
After 26092 training step(s), loss on training batch is 0.00336763.
After 26093 training step(s), loss on training batch is 0.00300513.
After 26094 training step(s), loss on training batch is 0.00327386.
After 26095 training step(s), loss on training batch is 0.0035723.
After 26096 training step(s), loss on training batch is 0.00340468.
After 26097 training step(s), loss on training batch is 0.00316043.
After 26098 training step(s), loss on training batch is 0.00369255.
After 26099 training step(s), loss on training batch is 0.00320859.
After 26100 training step(s), loss on training batch is 0.00365329.
After 26101 training step(s), loss on training batch is 0.0035201.
After 26102 training step(s), loss on training batch is 0.00302542.
After 26103 training step(s), loss on training batch is 0.00355837.
After 26104 training step(s), loss on training batch is 0.00316404.
After 26105 training step(s), loss on training batch is 0.00377227.
After 26106 training step(s), loss on training batch is 0.00343783.
After 26107 training step(s), loss on training batch is 0.0031976.
After 26108 training step(s), loss on training batch is 0.00336923.
After 26109 training step(s), loss on training batch is 0.00338444.
After 26110 training step(s), loss on training batch is 0.00327121.
After 26111 training step(s), loss on training batch is 0.00349951.
After 26112 training step(s), loss on training batch is 0.00395156.
After 26113 training step(s), loss on training batch is 0.00313831.
After 26114 training step(s), loss on training batch is 0.00311255.
After 26115 training step(s), loss on training batch is 0.00322274.
After 26116 training step(s), loss on training batch is 0.00318904.
After 26117 training step(s), loss on training batch is 0.00303067.
After 26118 training step(s), loss on training batch is 0.00356737.
After 26119 training step(s), loss on training batch is 0.00331905.
After 26120 training step(s), loss on training batch is 0.00417254.
After 26121 training step(s), loss on training batch is 0.00387245.
After 26122 training step(s), loss on training batch is 0.0034817.
After 26123 training step(s), loss on training batch is 0.0042017.
After 26124 training step(s), loss on training batch is 0.00344753.
After 26125 training step(s), loss on training batch is 0.00373482.
After 26126 training step(s), loss on training batch is 0.00386612.
After 26127 training step(s), loss on training batch is 0.00325957.
After 26128 training step(s), loss on training batch is 0.00375312.
After 26129 training step(s), loss on training batch is 0.00347028.
After 26130 training step(s), loss on training batch is 0.003522.
After 26131 training step(s), loss on training batch is 0.00315193.
After 26132 training step(s), loss on training batch is 0.00330215.
After 26133 training step(s), loss on training batch is 0.00320093.
After 26134 training step(s), loss on training batch is 0.0034496.
After 26135 training step(s), loss on training batch is 0.003538.
After 26136 training step(s), loss on training batch is 0.00372631.
After 26137 training step(s), loss on training batch is 0.00309384.
After 26138 training step(s), loss on training batch is 0.0035543.
After 26139 training step(s), loss on training batch is 0.00333864.
After 26140 training step(s), loss on training batch is 0.00345826.
After 26141 training step(s), loss on training batch is 0.00324777.
After 26142 training step(s), loss on training batch is 0.00306039.
After 26143 training step(s), loss on training batch is 0.00331109.
After 26144 training step(s), loss on training batch is 0.00311295.
After 26145 training step(s), loss on training batch is 0.00306897.
After 26146 training step(s), loss on training batch is 0.00354125.
After 26147 training step(s), loss on training batch is 0.00339113.
After 26148 training step(s), loss on training batch is 0.00328485.
After 26149 training step(s), loss on training batch is 0.0038012.
After 26150 training step(s), loss on training batch is 0.00313535.
After 26151 training step(s), loss on training batch is 0.00324984.
After 26152 training step(s), loss on training batch is 0.00332013.
After 26153 training step(s), loss on training batch is 0.00309179.
After 26154 training step(s), loss on training batch is 0.00412781.
After 26155 training step(s), loss on training batch is 0.00324238.
After 26156 training step(s), loss on training batch is 0.00304294.
After 26157 training step(s), loss on training batch is 0.00368493.
After 26158 training step(s), loss on training batch is 0.0040061.
After 26159 training step(s), loss on training batch is 0.00331957.
After 26160 training step(s), loss on training batch is 0.00323205.
After 26161 training step(s), loss on training batch is 0.0032018.
After 26162 training step(s), loss on training batch is 0.00343508.
After 26163 training step(s), loss on training batch is 0.00375814.
After 26164 training step(s), loss on training batch is 0.00336324.
After 26165 training step(s), loss on training batch is 0.00321977.
After 26166 training step(s), loss on training batch is 0.00361283.
After 26167 training step(s), loss on training batch is 0.00364723.
After 26168 training step(s), loss on training batch is 0.00377296.
After 26169 training step(s), loss on training batch is 0.00321107.
After 26170 training step(s), loss on training batch is 0.00327625.
After 26171 training step(s), loss on training batch is 0.00373983.
After 26172 training step(s), loss on training batch is 0.00313467.
After 26173 training step(s), loss on training batch is 0.00342248.
After 26174 training step(s), loss on training batch is 0.00312254.
After 26175 training step(s), loss on training batch is 0.00312406.
After 26176 training step(s), loss on training batch is 0.00355099.
After 26177 training step(s), loss on training batch is 0.00326826.
After 26178 training step(s), loss on training batch is 0.0030491.
After 26179 training step(s), loss on training batch is 0.00318678.
After 26180 training step(s), loss on training batch is 0.00327045.
After 26181 training step(s), loss on training batch is 0.00328343.
After 26182 training step(s), loss on training batch is 0.00393029.
After 26183 training step(s), loss on training batch is 0.00353921.
After 26184 training step(s), loss on training batch is 0.0034202.
After 26185 training step(s), loss on training batch is 0.00340266.
After 26186 training step(s), loss on training batch is 0.00337499.
After 26187 training step(s), loss on training batch is 0.00304142.
After 26188 training step(s), loss on training batch is 0.00325264.
After 26189 training step(s), loss on training batch is 0.00339465.
After 26190 training step(s), loss on training batch is 0.00346148.
After 26191 training step(s), loss on training batch is 0.0033518.
After 26192 training step(s), loss on training batch is 0.0034129.
After 26193 training step(s), loss on training batch is 0.00330027.
After 26194 training step(s), loss on training batch is 0.00360209.
After 26195 training step(s), loss on training batch is 0.00323948.
After 26196 training step(s), loss on training batch is 0.00317022.
After 26197 training step(s), loss on training batch is 0.00333265.
After 26198 training step(s), loss on training batch is 0.00326453.
After 26199 training step(s), loss on training batch is 0.00337219.
After 26200 training step(s), loss on training batch is 0.00311197.
After 26201 training step(s), loss on training batch is 0.0035068.
After 26202 training step(s), loss on training batch is 0.00307231.
After 26203 training step(s), loss on training batch is 0.00308142.
After 26204 training step(s), loss on training batch is 0.00311418.
After 26205 training step(s), loss on training batch is 0.00359576.
After 26206 training step(s), loss on training batch is 0.00333355.
After 26207 training step(s), loss on training batch is 0.00332929.
After 26208 training step(s), loss on training batch is 0.00338688.
After 26209 training step(s), loss on training batch is 0.00337331.
After 26210 training step(s), loss on training batch is 0.00332632.
After 26211 training step(s), loss on training batch is 0.00330856.
After 26212 training step(s), loss on training batch is 0.00347919.
After 26213 training step(s), loss on training batch is 0.00321501.
After 26214 training step(s), loss on training batch is 0.00332329.
After 26215 training step(s), loss on training batch is 0.00311814.
After 26216 training step(s), loss on training batch is 0.0032916.
After 26217 training step(s), loss on training batch is 0.00330328.
After 26218 training step(s), loss on training batch is 0.00338319.
After 26219 training step(s), loss on training batch is 0.00353717.
After 26220 training step(s), loss on training batch is 0.00500951.
After 26221 training step(s), loss on training batch is 0.00375155.
After 26222 training step(s), loss on training batch is 0.0036142.
After 26223 training step(s), loss on training batch is 0.00339703.
After 26224 training step(s), loss on training batch is 0.00358705.
After 26225 training step(s), loss on training batch is 0.00312458.
After 26226 training step(s), loss on training batch is 0.00308213.
After 26227 training step(s), loss on training batch is 0.0032169.
After 26228 training step(s), loss on training batch is 0.00326105.
After 26229 training step(s), loss on training batch is 0.00314035.
After 26230 training step(s), loss on training batch is 0.00322074.
After 26231 training step(s), loss on training batch is 0.0035647.
After 26232 training step(s), loss on training batch is 0.00354651.
After 26233 training step(s), loss on training batch is 0.00314882.
After 26234 training step(s), loss on training batch is 0.00370763.
After 26235 training step(s), loss on training batch is 0.0033054.
After 26236 training step(s), loss on training batch is 0.0034862.
After 26237 training step(s), loss on training batch is 0.00349673.
After 26238 training step(s), loss on training batch is 0.00345068.
After 26239 training step(s), loss on training batch is 0.00319491.
After 26240 training step(s), loss on training batch is 0.00318898.
After 26241 training step(s), loss on training batch is 0.00322511.
After 26242 training step(s), loss on training batch is 0.00330747.
After 26243 training step(s), loss on training batch is 0.00341983.
After 26244 training step(s), loss on training batch is 0.00314345.
After 26245 training step(s), loss on training batch is 0.00310325.
After 26246 training step(s), loss on training batch is 0.00327465.
After 26247 training step(s), loss on training batch is 0.00412292.
After 26248 training step(s), loss on training batch is 0.00358395.
After 26249 training step(s), loss on training batch is 0.00328199.
After 26250 training step(s), loss on training batch is 0.00325551.
After 26251 training step(s), loss on training batch is 0.00337804.
After 26252 training step(s), loss on training batch is 0.00351291.
After 26253 training step(s), loss on training batch is 0.00329471.
After 26254 training step(s), loss on training batch is 0.00380639.
After 26255 training step(s), loss on training batch is 0.0031368.
After 26256 training step(s), loss on training batch is 0.0034919.
After 26257 training step(s), loss on training batch is 0.00318886.
After 26258 training step(s), loss on training batch is 0.00334224.
After 26259 training step(s), loss on training batch is 0.00316067.
After 26260 training step(s), loss on training batch is 0.0031123.
After 26261 training step(s), loss on training batch is 0.00319169.
After 26262 training step(s), loss on training batch is 0.00310263.
After 26263 training step(s), loss on training batch is 0.00342466.
After 26264 training step(s), loss on training batch is 0.00399192.
After 26265 training step(s), loss on training batch is 0.00353548.
After 26266 training step(s), loss on training batch is 0.00355635.
After 26267 training step(s), loss on training batch is 0.00341315.
After 26268 training step(s), loss on training batch is 0.00323495.
After 26269 training step(s), loss on training batch is 0.00325745.
After 26270 training step(s), loss on training batch is 0.00344161.
After 26271 training step(s), loss on training batch is 0.00342975.
After 26272 training step(s), loss on training batch is 0.00319504.
After 26273 training step(s), loss on training batch is 0.00320844.
After 26274 training step(s), loss on training batch is 0.00308278.
After 26275 training step(s), loss on training batch is 0.00345363.
After 26276 training step(s), loss on training batch is 0.00330424.
After 26277 training step(s), loss on training batch is 0.00337534.
After 26278 training step(s), loss on training batch is 0.00369581.
After 26279 training step(s), loss on training batch is 0.00387773.
After 26280 training step(s), loss on training batch is 0.00435112.
After 26281 training step(s), loss on training batch is 0.00345588.
After 26282 training step(s), loss on training batch is 0.0032697.
After 26283 training step(s), loss on training batch is 0.00304533.
After 26284 training step(s), loss on training batch is 0.00351501.
After 26285 training step(s), loss on training batch is 0.00347631.
After 26286 training step(s), loss on training batch is 0.00315089.
After 26287 training step(s), loss on training batch is 0.00323657.
After 26288 training step(s), loss on training batch is 0.00357933.
After 26289 training step(s), loss on training batch is 0.00311096.
After 26290 training step(s), loss on training batch is 0.00315986.
After 26291 training step(s), loss on training batch is 0.00345629.
After 26292 training step(s), loss on training batch is 0.00355558.
After 26293 training step(s), loss on training batch is 0.00329498.
After 26294 training step(s), loss on training batch is 0.00350805.
After 26295 training step(s), loss on training batch is 0.00312601.
After 26296 training step(s), loss on training batch is 0.00343419.
After 26297 training step(s), loss on training batch is 0.00391961.
After 26298 training step(s), loss on training batch is 0.00356753.
After 26299 training step(s), loss on training batch is 0.00318612.
After 26300 training step(s), loss on training batch is 0.00322672.
After 26301 training step(s), loss on training batch is 0.00350949.
After 26302 training step(s), loss on training batch is 0.00308744.
After 26303 training step(s), loss on training batch is 0.00311464.
After 26304 training step(s), loss on training batch is 0.003349.
After 26305 training step(s), loss on training batch is 0.00298782.
After 26306 training step(s), loss on training batch is 0.00305787.
After 26307 training step(s), loss on training batch is 0.00306302.
After 26308 training step(s), loss on training batch is 0.00329685.
After 26309 training step(s), loss on training batch is 0.00297783.
After 26310 training step(s), loss on training batch is 0.00348475.
After 26311 training step(s), loss on training batch is 0.00358643.
After 26312 training step(s), loss on training batch is 0.00386894.
After 26313 training step(s), loss on training batch is 0.0032644.
After 26314 training step(s), loss on training batch is 0.00336147.
After 26315 training step(s), loss on training batch is 0.00310434.
After 26316 training step(s), loss on training batch is 0.00329192.
After 26317 training step(s), loss on training batch is 0.00333643.
After 26318 training step(s), loss on training batch is 0.003642.
After 26319 training step(s), loss on training batch is 0.00332248.
After 26320 training step(s), loss on training batch is 0.00305534.
After 26321 training step(s), loss on training batch is 0.00347741.
After 26322 training step(s), loss on training batch is 0.00318876.
After 26323 training step(s), loss on training batch is 0.00310053.
After 26324 training step(s), loss on training batch is 0.00338668.
After 26325 training step(s), loss on training batch is 0.00338303.
After 26326 training step(s), loss on training batch is 0.00327417.
After 26327 training step(s), loss on training batch is 0.00308962.
After 26328 training step(s), loss on training batch is 0.00398956.
After 26329 training step(s), loss on training batch is 0.0030786.
After 26330 training step(s), loss on training batch is 0.00384957.
After 26331 training step(s), loss on training batch is 0.00349507.
After 26332 training step(s), loss on training batch is 0.00508423.
After 26333 training step(s), loss on training batch is 0.00338967.
After 26334 training step(s), loss on training batch is 0.00320706.
After 26335 training step(s), loss on training batch is 0.00346909.
After 26336 training step(s), loss on training batch is 0.0035578.
After 26337 training step(s), loss on training batch is 0.00335708.
After 26338 training step(s), loss on training batch is 0.0033464.
After 26339 training step(s), loss on training batch is 0.00350301.
After 26340 training step(s), loss on training batch is 0.00343478.
After 26341 training step(s), loss on training batch is 0.00320589.
After 26342 training step(s), loss on training batch is 0.00336396.
After 26343 training step(s), loss on training batch is 0.0042278.
After 26344 training step(s), loss on training batch is 0.00313131.
After 26345 training step(s), loss on training batch is 0.00335957.
After 26346 training step(s), loss on training batch is 0.00316665.
After 26347 training step(s), loss on training batch is 0.00330594.
After 26348 training step(s), loss on training batch is 0.00332033.
After 26349 training step(s), loss on training batch is 0.0035039.
After 26350 training step(s), loss on training batch is 0.00329981.
After 26351 training step(s), loss on training batch is 0.00329919.
After 26352 training step(s), loss on training batch is 0.00323938.
After 26353 training step(s), loss on training batch is 0.00324769.
After 26354 training step(s), loss on training batch is 0.00355322.
After 26355 training step(s), loss on training batch is 0.00345681.
After 26356 training step(s), loss on training batch is 0.00323194.
After 26357 training step(s), loss on training batch is 0.00336689.
After 26358 training step(s), loss on training batch is 0.00303003.
After 26359 training step(s), loss on training batch is 0.00313143.
After 26360 training step(s), loss on training batch is 0.00327079.
After 26361 training step(s), loss on training batch is 0.00337677.
After 26362 training step(s), loss on training batch is 0.00329168.
After 26363 training step(s), loss on training batch is 0.00314469.
After 26364 training step(s), loss on training batch is 0.00318432.
After 26365 training step(s), loss on training batch is 0.00354347.
After 26366 training step(s), loss on training batch is 0.00323686.
After 26367 training step(s), loss on training batch is 0.00405818.
After 26368 training step(s), loss on training batch is 0.00320429.
After 26369 training step(s), loss on training batch is 0.00342109.
After 26370 training step(s), loss on training batch is 0.00341871.
After 26371 training step(s), loss on training batch is 0.00338612.
After 26372 training step(s), loss on training batch is 0.00380338.
After 26373 training step(s), loss on training batch is 0.00351956.
After 26374 training step(s), loss on training batch is 0.00331723.
After 26375 training step(s), loss on training batch is 0.00373904.
After 26376 training step(s), loss on training batch is 0.00315031.
After 26377 training step(s), loss on training batch is 0.00320623.
After 26378 training step(s), loss on training batch is 0.00323883.
After 26379 training step(s), loss on training batch is 0.00369582.
After 26380 training step(s), loss on training batch is 0.00308583.
After 26381 training step(s), loss on training batch is 0.00342575.
After 26382 training step(s), loss on training batch is 0.00319727.
After 26383 training step(s), loss on training batch is 0.00319846.
After 26384 training step(s), loss on training batch is 0.0031237.
After 26385 training step(s), loss on training batch is 0.0032498.
After 26386 training step(s), loss on training batch is 0.00333038.
After 26387 training step(s), loss on training batch is 0.00317815.
After 26388 training step(s), loss on training batch is 0.00350519.
After 26389 training step(s), loss on training batch is 0.00314106.
After 26390 training step(s), loss on training batch is 0.00372077.
After 26391 training step(s), loss on training batch is 0.00371915.
After 26392 training step(s), loss on training batch is 0.00367371.
After 26393 training step(s), loss on training batch is 0.00390592.
After 26394 training step(s), loss on training batch is 0.00323414.
After 26395 training step(s), loss on training batch is 0.00397834.
After 26396 training step(s), loss on training batch is 0.003414.
After 26397 training step(s), loss on training batch is 0.00315554.
After 26398 training step(s), loss on training batch is 0.00327237.
After 26399 training step(s), loss on training batch is 0.00317538.
After 26400 training step(s), loss on training batch is 0.00345911.
After 26401 training step(s), loss on training batch is 0.00318956.
After 26402 training step(s), loss on training batch is 0.00339585.
After 26403 training step(s), loss on training batch is 0.00305953.
After 26404 training step(s), loss on training batch is 0.00313949.
After 26405 training step(s), loss on training batch is 0.00329699.
After 26406 training step(s), loss on training batch is 0.0030622.
After 26407 training step(s), loss on training batch is 0.00343811.
After 26408 training step(s), loss on training batch is 0.00308151.
After 26409 training step(s), loss on training batch is 0.00345196.
After 26410 training step(s), loss on training batch is 0.00333317.
After 26411 training step(s), loss on training batch is 0.0033714.
After 26412 training step(s), loss on training batch is 0.00501727.
After 26413 training step(s), loss on training batch is 0.00345258.
After 26414 training step(s), loss on training batch is 0.00318633.
After 26415 training step(s), loss on training batch is 0.00329695.
After 26416 training step(s), loss on training batch is 0.00321577.
After 26417 training step(s), loss on training batch is 0.00348304.
After 26418 training step(s), loss on training batch is 0.00367896.
After 26419 training step(s), loss on training batch is 0.00378396.
After 26420 training step(s), loss on training batch is 0.00366659.
After 26421 training step(s), loss on training batch is 0.00343237.
After 26422 training step(s), loss on training batch is 0.00358939.
After 26423 training step(s), loss on training batch is 0.00301363.
After 26424 training step(s), loss on training batch is 0.00319445.
After 26425 training step(s), loss on training batch is 0.00329699.
After 26426 training step(s), loss on training batch is 0.00340676.
After 26427 training step(s), loss on training batch is 0.00343967.
After 26428 training step(s), loss on training batch is 0.00345806.
After 26429 training step(s), loss on training batch is 0.00319662.
After 26430 training step(s), loss on training batch is 0.00328914.
After 26431 training step(s), loss on training batch is 0.00318757.
After 26432 training step(s), loss on training batch is 0.00323292.
After 26433 training step(s), loss on training batch is 0.00321468.
After 26434 training step(s), loss on training batch is 0.00332089.
After 26435 training step(s), loss on training batch is 0.00322729.
After 26436 training step(s), loss on training batch is 0.00306309.
After 26437 training step(s), loss on training batch is 0.00302707.
After 26438 training step(s), loss on training batch is 0.00312638.
After 26439 training step(s), loss on training batch is 0.00329949.
After 26440 training step(s), loss on training batch is 0.00388096.
After 26441 training step(s), loss on training batch is 0.00344486.
After 26442 training step(s), loss on training batch is 0.00403185.
After 26443 training step(s), loss on training batch is 0.0038459.
After 26444 training step(s), loss on training batch is 0.0032157.
After 26445 training step(s), loss on training batch is 0.00333684.
After 26446 training step(s), loss on training batch is 0.00360324.
After 26447 training step(s), loss on training batch is 0.00388752.
After 26448 training step(s), loss on training batch is 0.00329859.
After 26449 training step(s), loss on training batch is 0.00300929.
After 26450 training step(s), loss on training batch is 0.00347091.
After 26451 training step(s), loss on training batch is 0.00350429.
After 26452 training step(s), loss on training batch is 0.00321025.
After 26453 training step(s), loss on training batch is 0.00296431.
After 26454 training step(s), loss on training batch is 0.00318396.
After 26455 training step(s), loss on training batch is 0.00300102.
After 26456 training step(s), loss on training batch is 0.0035177.
After 26457 training step(s), loss on training batch is 0.00318938.
After 26458 training step(s), loss on training batch is 0.00324073.
After 26459 training step(s), loss on training batch is 0.00321929.
After 26460 training step(s), loss on training batch is 0.003363.
After 26461 training step(s), loss on training batch is 0.00367159.
After 26462 training step(s), loss on training batch is 0.00334938.
After 26463 training step(s), loss on training batch is 0.00352297.
After 26464 training step(s), loss on training batch is 0.00334421.
After 26465 training step(s), loss on training batch is 0.00329972.
After 26466 training step(s), loss on training batch is 0.0034146.
After 26467 training step(s), loss on training batch is 0.0032691.
After 26468 training step(s), loss on training batch is 0.00310775.
After 26469 training step(s), loss on training batch is 0.0034019.
After 26470 training step(s), loss on training batch is 0.00321668.
After 26471 training step(s), loss on training batch is 0.00350983.
After 26472 training step(s), loss on training batch is 0.00312686.
After 26473 training step(s), loss on training batch is 0.00303811.
After 26474 training step(s), loss on training batch is 0.00332209.
After 26475 training step(s), loss on training batch is 0.0031906.
After 26476 training step(s), loss on training batch is 0.00348311.
After 26477 training step(s), loss on training batch is 0.00340718.
After 26478 training step(s), loss on training batch is 0.00336223.
After 26479 training step(s), loss on training batch is 0.00301423.
After 26480 training step(s), loss on training batch is 0.00311323.
After 26481 training step(s), loss on training batch is 0.00339104.
After 26482 training step(s), loss on training batch is 0.00338927.
After 26483 training step(s), loss on training batch is 0.00364349.
After 26484 training step(s), loss on training batch is 0.0031309.
After 26485 training step(s), loss on training batch is 0.00310943.
After 26486 training step(s), loss on training batch is 0.00324076.
After 26487 training step(s), loss on training batch is 0.00348849.
After 26488 training step(s), loss on training batch is 0.00357182.
After 26489 training step(s), loss on training batch is 0.00323199.
After 26490 training step(s), loss on training batch is 0.0031744.
After 26491 training step(s), loss on training batch is 0.0036713.
After 26492 training step(s), loss on training batch is 0.00317911.
After 26493 training step(s), loss on training batch is 0.00346265.
After 26494 training step(s), loss on training batch is 0.00343364.
After 26495 training step(s), loss on training batch is 0.00381562.
After 26496 training step(s), loss on training batch is 0.00402996.
After 26497 training step(s), loss on training batch is 0.00324105.
After 26498 training step(s), loss on training batch is 0.00338067.
After 26499 training step(s), loss on training batch is 0.00340094.
After 26500 training step(s), loss on training batch is 0.0032534.
After 26501 training step(s), loss on training batch is 0.00307523.
After 26502 training step(s), loss on training batch is 0.00379907.
After 26503 training step(s), loss on training batch is 0.00334089.
After 26504 training step(s), loss on training batch is 0.00341113.
After 26505 training step(s), loss on training batch is 0.00308943.
After 26506 training step(s), loss on training batch is 0.0031982.
After 26507 training step(s), loss on training batch is 0.00311185.
After 26508 training step(s), loss on training batch is 0.00321892.
After 26509 training step(s), loss on training batch is 0.00311677.
After 26510 training step(s), loss on training batch is 0.0031334.
After 26511 training step(s), loss on training batch is 0.00316706.
After 26512 training step(s), loss on training batch is 0.00397423.
After 26513 training step(s), loss on training batch is 0.0039188.
After 26514 training step(s), loss on training batch is 0.00301581.
After 26515 training step(s), loss on training batch is 0.00320289.
After 26516 training step(s), loss on training batch is 0.00391784.
After 26517 training step(s), loss on training batch is 0.00339514.
After 26518 training step(s), loss on training batch is 0.00302971.
After 26519 training step(s), loss on training batch is 0.00362807.
After 26520 training step(s), loss on training batch is 0.00304566.
After 26521 training step(s), loss on training batch is 0.0033359.
After 26522 training step(s), loss on training batch is 0.00333135.
After 26523 training step(s), loss on training batch is 0.00328643.
After 26524 training step(s), loss on training batch is 0.00405239.
After 26525 training step(s), loss on training batch is 0.00314604.
After 26526 training step(s), loss on training batch is 0.00351147.
After 26527 training step(s), loss on training batch is 0.00341847.
After 26528 training step(s), loss on training batch is 0.00392575.
After 26529 training step(s), loss on training batch is 0.00331454.
After 26530 training step(s), loss on training batch is 0.0034642.
After 26531 training step(s), loss on training batch is 0.00326576.
After 26532 training step(s), loss on training batch is 0.00349135.
After 26533 training step(s), loss on training batch is 0.00307664.
After 26534 training step(s), loss on training batch is 0.00311436.
After 26535 training step(s), loss on training batch is 0.00337983.
After 26536 training step(s), loss on training batch is 0.00307104.
After 26537 training step(s), loss on training batch is 0.00332343.
After 26538 training step(s), loss on training batch is 0.00315822.
After 26539 training step(s), loss on training batch is 0.00325341.
After 26540 training step(s), loss on training batch is 0.00326891.
After 26541 training step(s), loss on training batch is 0.00341481.
After 26542 training step(s), loss on training batch is 0.00319777.
After 26543 training step(s), loss on training batch is 0.00321753.
After 26544 training step(s), loss on training batch is 0.00344041.
After 26545 training step(s), loss on training batch is 0.00307876.
After 26546 training step(s), loss on training batch is 0.00334712.
After 26547 training step(s), loss on training batch is 0.00354316.
After 26548 training step(s), loss on training batch is 0.0033055.
After 26549 training step(s), loss on training batch is 0.00337013.
After 26550 training step(s), loss on training batch is 0.00406776.
After 26551 training step(s), loss on training batch is 0.00345875.
After 26552 training step(s), loss on training batch is 0.00350484.
After 26553 training step(s), loss on training batch is 0.0031645.
After 26554 training step(s), loss on training batch is 0.00360901.
After 26555 training step(s), loss on training batch is 0.00337068.
After 26556 training step(s), loss on training batch is 0.00319901.
After 26557 training step(s), loss on training batch is 0.0030592.
After 26558 training step(s), loss on training batch is 0.00321496.
After 26559 training step(s), loss on training batch is 0.00332998.
After 26560 training step(s), loss on training batch is 0.003556.
After 26561 training step(s), loss on training batch is 0.00310601.
After 26562 training step(s), loss on training batch is 0.00351629.
After 26563 training step(s), loss on training batch is 0.00339921.
After 26564 training step(s), loss on training batch is 0.00320059.
After 26565 training step(s), loss on training batch is 0.00309877.
After 26566 training step(s), loss on training batch is 0.00324057.
After 26567 training step(s), loss on training batch is 0.00334314.
After 26568 training step(s), loss on training batch is 0.00336811.
After 26569 training step(s), loss on training batch is 0.00296583.
After 26570 training step(s), loss on training batch is 0.00325828.
After 26571 training step(s), loss on training batch is 0.00340176.
After 26572 training step(s), loss on training batch is 0.00299486.
After 26573 training step(s), loss on training batch is 0.00339667.
After 26574 training step(s), loss on training batch is 0.00336892.
After 26575 training step(s), loss on training batch is 0.0034376.
After 26576 training step(s), loss on training batch is 0.00341389.
After 26577 training step(s), loss on training batch is 0.00312653.
After 26578 training step(s), loss on training batch is 0.00379648.
After 26579 training step(s), loss on training batch is 0.00399285.
After 26580 training step(s), loss on training batch is 0.00339661.
After 26581 training step(s), loss on training batch is 0.00338678.
After 26582 training step(s), loss on training batch is 0.00325778.
After 26583 training step(s), loss on training batch is 0.00330279.
After 26584 training step(s), loss on training batch is 0.00308506.
After 26585 training step(s), loss on training batch is 0.00424135.
After 26586 training step(s), loss on training batch is 0.00323734.
After 26587 training step(s), loss on training batch is 0.00333767.
After 26588 training step(s), loss on training batch is 0.00352207.
After 26589 training step(s), loss on training batch is 0.00324918.
After 26590 training step(s), loss on training batch is 0.00338181.
After 26591 training step(s), loss on training batch is 0.00320348.
After 26592 training step(s), loss on training batch is 0.00341666.
After 26593 training step(s), loss on training batch is 0.00338057.
After 26594 training step(s), loss on training batch is 0.00355433.
After 26595 training step(s), loss on training batch is 0.00324334.
After 26596 training step(s), loss on training batch is 0.00338572.
After 26597 training step(s), loss on training batch is 0.00323845.
After 26598 training step(s), loss on training batch is 0.00313357.
After 26599 training step(s), loss on training batch is 0.00376211.
After 26600 training step(s), loss on training batch is 0.00336365.
After 26601 training step(s), loss on training batch is 0.00329496.
After 26602 training step(s), loss on training batch is 0.00350672.
After 26603 training step(s), loss on training batch is 0.00303727.
After 26604 training step(s), loss on training batch is 0.00304759.
After 26605 training step(s), loss on training batch is 0.00304219.
After 26606 training step(s), loss on training batch is 0.00307929.
After 26607 training step(s), loss on training batch is 0.00351847.
After 26608 training step(s), loss on training batch is 0.00333461.
After 26609 training step(s), loss on training batch is 0.00349427.
After 26610 training step(s), loss on training batch is 0.00350394.
After 26611 training step(s), loss on training batch is 0.00357298.
After 26612 training step(s), loss on training batch is 0.00320589.
After 26613 training step(s), loss on training batch is 0.00308141.
After 26614 training step(s), loss on training batch is 0.00352803.
After 26615 training step(s), loss on training batch is 0.00312682.
After 26616 training step(s), loss on training batch is 0.00312937.
After 26617 training step(s), loss on training batch is 0.00353998.
After 26618 training step(s), loss on training batch is 0.00310378.
After 26619 training step(s), loss on training batch is 0.0033328.
After 26620 training step(s), loss on training batch is 0.00335664.
After 26621 training step(s), loss on training batch is 0.00338606.
After 26622 training step(s), loss on training batch is 0.00373487.
After 26623 training step(s), loss on training batch is 0.00347369.
After 26624 training step(s), loss on training batch is 0.00372658.
After 26625 training step(s), loss on training batch is 0.00347423.
After 26626 training step(s), loss on training batch is 0.0032206.
After 26627 training step(s), loss on training batch is 0.00313135.
After 26628 training step(s), loss on training batch is 0.00331128.
After 26629 training step(s), loss on training batch is 0.00324117.
After 26630 training step(s), loss on training batch is 0.00314384.
After 26631 training step(s), loss on training batch is 0.0030891.
After 26632 training step(s), loss on training batch is 0.00349104.
After 26633 training step(s), loss on training batch is 0.00331535.
After 26634 training step(s), loss on training batch is 0.00337007.
After 26635 training step(s), loss on training batch is 0.00330305.
After 26636 training step(s), loss on training batch is 0.00371178.
After 26637 training step(s), loss on training batch is 0.00301871.
After 26638 training step(s), loss on training batch is 0.00336887.
After 26639 training step(s), loss on training batch is 0.00329279.
After 26640 training step(s), loss on training batch is 0.00313691.
After 26641 training step(s), loss on training batch is 0.00313466.
After 26642 training step(s), loss on training batch is 0.00309954.
After 26643 training step(s), loss on training batch is 0.00354957.
After 26644 training step(s), loss on training batch is 0.00322122.
After 26645 training step(s), loss on training batch is 0.00340269.
After 26646 training step(s), loss on training batch is 0.00359855.
After 26647 training step(s), loss on training batch is 0.00349484.
After 26648 training step(s), loss on training batch is 0.00305444.
After 26649 training step(s), loss on training batch is 0.00313954.
After 26650 training step(s), loss on training batch is 0.00362101.
After 26651 training step(s), loss on training batch is 0.00362946.
After 26652 training step(s), loss on training batch is 0.00316738.
After 26653 training step(s), loss on training batch is 0.00313316.
After 26654 training step(s), loss on training batch is 0.00313094.
After 26655 training step(s), loss on training batch is 0.00380556.
After 26656 training step(s), loss on training batch is 0.00366462.
After 26657 training step(s), loss on training batch is 0.00312373.
After 26658 training step(s), loss on training batch is 0.00309119.
After 26659 training step(s), loss on training batch is 0.00351965.
After 26660 training step(s), loss on training batch is 0.00318321.
After 26661 training step(s), loss on training batch is 0.00363735.
After 26662 training step(s), loss on training batch is 0.0038357.
After 26663 training step(s), loss on training batch is 0.00388647.
After 26664 training step(s), loss on training batch is 0.00400108.
After 26665 training step(s), loss on training batch is 0.00309062.
After 26666 training step(s), loss on training batch is 0.00341011.
After 26667 training step(s), loss on training batch is 0.00307478.
After 26668 training step(s), loss on training batch is 0.00305878.
After 26669 training step(s), loss on training batch is 0.0032398.
After 26670 training step(s), loss on training batch is 0.00356654.
After 26671 training step(s), loss on training batch is 0.00381217.
After 26672 training step(s), loss on training batch is 0.0031192.
After 26673 training step(s), loss on training batch is 0.00325095.
After 26674 training step(s), loss on training batch is 0.00572662.
After 26675 training step(s), loss on training batch is 0.00346003.
After 26676 training step(s), loss on training batch is 0.00321357.
After 26677 training step(s), loss on training batch is 0.00403092.
After 26678 training step(s), loss on training batch is 0.00350412.
After 26679 training step(s), loss on training batch is 0.00326465.
After 26680 training step(s), loss on training batch is 0.00306164.
After 26681 training step(s), loss on training batch is 0.0037415.
After 26682 training step(s), loss on training batch is 0.00330756.
After 26683 training step(s), loss on training batch is 0.0034054.
After 26684 training step(s), loss on training batch is 0.00313714.
After 26685 training step(s), loss on training batch is 0.00329282.
After 26686 training step(s), loss on training batch is 0.00306214.
After 26687 training step(s), loss on training batch is 0.00315671.
After 26688 training step(s), loss on training batch is 0.00304823.
After 26689 training step(s), loss on training batch is 0.00400718.
After 26690 training step(s), loss on training batch is 0.00311504.
After 26691 training step(s), loss on training batch is 0.00333477.
After 26692 training step(s), loss on training batch is 0.0031636.
After 26693 training step(s), loss on training batch is 0.00299195.
After 26694 training step(s), loss on training batch is 0.00312771.
After 26695 training step(s), loss on training batch is 0.00327879.
After 26696 training step(s), loss on training batch is 0.00340813.
After 26697 training step(s), loss on training batch is 0.00378954.
After 26698 training step(s), loss on training batch is 0.00328242.
After 26699 training step(s), loss on training batch is 0.00300501.
After 26700 training step(s), loss on training batch is 0.00303183.
After 26701 training step(s), loss on training batch is 0.00321962.
After 26702 training step(s), loss on training batch is 0.00334327.
After 26703 training step(s), loss on training batch is 0.00324003.
After 26704 training step(s), loss on training batch is 0.00316948.
After 26705 training step(s), loss on training batch is 0.00312786.
After 26706 training step(s), loss on training batch is 0.00312211.
After 26707 training step(s), loss on training batch is 0.00308976.
After 26708 training step(s), loss on training batch is 0.00338901.
After 26709 training step(s), loss on training batch is 0.00332619.
After 26710 training step(s), loss on training batch is 0.00331484.
After 26711 training step(s), loss on training batch is 0.00341582.
After 26712 training step(s), loss on training batch is 0.00309267.
After 26713 training step(s), loss on training batch is 0.00323745.
After 26714 training step(s), loss on training batch is 0.00334645.
After 26715 training step(s), loss on training batch is 0.00329028.
After 26716 training step(s), loss on training batch is 0.00344656.
After 26717 training step(s), loss on training batch is 0.00344316.
After 26718 training step(s), loss on training batch is 0.0034565.
After 26719 training step(s), loss on training batch is 0.00326508.
After 26720 training step(s), loss on training batch is 0.00312517.
After 26721 training step(s), loss on training batch is 0.00322415.
After 26722 training step(s), loss on training batch is 0.00324714.
After 26723 training step(s), loss on training batch is 0.00332275.
After 26724 training step(s), loss on training batch is 0.00312653.
After 26725 training step(s), loss on training batch is 0.00340924.
After 26726 training step(s), loss on training batch is 0.00416818.
After 26727 training step(s), loss on training batch is 0.00379361.
After 26728 training step(s), loss on training batch is 0.00321664.
After 26729 training step(s), loss on training batch is 0.00330837.
After 26730 training step(s), loss on training batch is 0.00322389.
After 26731 training step(s), loss on training batch is 0.0036626.
After 26732 training step(s), loss on training batch is 0.00314875.
After 26733 training step(s), loss on training batch is 0.00300452.
After 26734 training step(s), loss on training batch is 0.00324859.
After 26735 training step(s), loss on training batch is 0.00328061.
After 26736 training step(s), loss on training batch is 0.0032233.
After 26737 training step(s), loss on training batch is 0.00327599.
After 26738 training step(s), loss on training batch is 0.0032375.
After 26739 training step(s), loss on training batch is 0.00343341.
After 26740 training step(s), loss on training batch is 0.00332436.
After 26741 training step(s), loss on training batch is 0.00319184.
After 26742 training step(s), loss on training batch is 0.00297381.
After 26743 training step(s), loss on training batch is 0.00308821.
After 26744 training step(s), loss on training batch is 0.00351687.
After 26745 training step(s), loss on training batch is 0.00380369.
After 26746 training step(s), loss on training batch is 0.00353063.
After 26747 training step(s), loss on training batch is 0.00329035.
After 26748 training step(s), loss on training batch is 0.00357089.
After 26749 training step(s), loss on training batch is 0.00343679.
After 26750 training step(s), loss on training batch is 0.00326772.
After 26751 training step(s), loss on training batch is 0.00326947.
After 26752 training step(s), loss on training batch is 0.00334398.
After 26753 training step(s), loss on training batch is 0.00340129.
After 26754 training step(s), loss on training batch is 0.00339109.
After 26755 training step(s), loss on training batch is 0.00319908.
After 26756 training step(s), loss on training batch is 0.00369181.
After 26757 training step(s), loss on training batch is 0.00322021.
After 26758 training step(s), loss on training batch is 0.00346674.
After 26759 training step(s), loss on training batch is 0.00311372.
After 26760 training step(s), loss on training batch is 0.0032408.
After 26761 training step(s), loss on training batch is 0.00303394.
After 26762 training step(s), loss on training batch is 0.00345387.
After 26763 training step(s), loss on training batch is 0.00307489.
After 26764 training step(s), loss on training batch is 0.00334209.
After 26765 training step(s), loss on training batch is 0.00330109.
After 26766 training step(s), loss on training batch is 0.00359464.
After 26767 training step(s), loss on training batch is 0.00318178.
After 26768 training step(s), loss on training batch is 0.00339438.
After 26769 training step(s), loss on training batch is 0.00322988.
After 26770 training step(s), loss on training batch is 0.00309466.
After 26771 training step(s), loss on training batch is 0.00337827.
After 26772 training step(s), loss on training batch is 0.00320104.
After 26773 training step(s), loss on training batch is 0.00307947.
After 26774 training step(s), loss on training batch is 0.00350577.
After 26775 training step(s), loss on training batch is 0.00329913.
After 26776 training step(s), loss on training batch is 0.00377656.
After 26777 training step(s), loss on training batch is 0.00331007.
After 26778 training step(s), loss on training batch is 0.00358694.
After 26779 training step(s), loss on training batch is 0.00377867.
After 26780 training step(s), loss on training batch is 0.00303322.
After 26781 training step(s), loss on training batch is 0.00390568.
After 26782 training step(s), loss on training batch is 0.00325964.
After 26783 training step(s), loss on training batch is 0.00338544.
After 26784 training step(s), loss on training batch is 0.00317291.
After 26785 training step(s), loss on training batch is 0.00314246.
After 26786 training step(s), loss on training batch is 0.00315521.
After 26787 training step(s), loss on training batch is 0.00312159.
After 26788 training step(s), loss on training batch is 0.00331823.
After 26789 training step(s), loss on training batch is 0.0032556.
After 26790 training step(s), loss on training batch is 0.00338421.
After 26791 training step(s), loss on training batch is 0.00323139.
After 26792 training step(s), loss on training batch is 0.00311831.
After 26793 training step(s), loss on training batch is 0.00329465.
After 26794 training step(s), loss on training batch is 0.00323828.
After 26795 training step(s), loss on training batch is 0.0031633.
After 26796 training step(s), loss on training batch is 0.00368244.
After 26797 training step(s), loss on training batch is 0.00326135.
After 26798 training step(s), loss on training batch is 0.00334726.
After 26799 training step(s), loss on training batch is 0.00349985.
After 26800 training step(s), loss on training batch is 0.00339245.
After 26801 training step(s), loss on training batch is 0.00401246.
After 26802 training step(s), loss on training batch is 0.00351548.
After 26803 training step(s), loss on training batch is 0.00370617.
After 26804 training step(s), loss on training batch is 0.00301787.
After 26805 training step(s), loss on training batch is 0.00358913.
After 26806 training step(s), loss on training batch is 0.00370407.
After 26807 training step(s), loss on training batch is 0.00356981.
After 26808 training step(s), loss on training batch is 0.00299584.
After 26809 training step(s), loss on training batch is 0.0033569.
After 26810 training step(s), loss on training batch is 0.00300626.
After 26811 training step(s), loss on training batch is 0.00316402.
After 26812 training step(s), loss on training batch is 0.00340066.
After 26813 training step(s), loss on training batch is 0.00384641.
After 26814 training step(s), loss on training batch is 0.00324002.
After 26815 training step(s), loss on training batch is 0.00303095.
After 26816 training step(s), loss on training batch is 0.00347437.
After 26817 training step(s), loss on training batch is 0.00379521.
After 26818 training step(s), loss on training batch is 0.00326991.
After 26819 training step(s), loss on training batch is 0.00323433.
After 26820 training step(s), loss on training batch is 0.00328908.
After 26821 training step(s), loss on training batch is 0.00343723.
After 26822 training step(s), loss on training batch is 0.00325155.
After 26823 training step(s), loss on training batch is 0.00343046.
After 26824 training step(s), loss on training batch is 0.00322947.
After 26825 training step(s), loss on training batch is 0.00397672.
After 26826 training step(s), loss on training batch is 0.00345417.
After 26827 training step(s), loss on training batch is 0.0032076.
After 26828 training step(s), loss on training batch is 0.00337373.
After 26829 training step(s), loss on training batch is 0.00367285.
After 26830 training step(s), loss on training batch is 0.00322517.
After 26831 training step(s), loss on training batch is 0.00337315.
After 26832 training step(s), loss on training batch is 0.00335447.
After 26833 training step(s), loss on training batch is 0.00329311.
After 26834 training step(s), loss on training batch is 0.00335041.
After 26835 training step(s), loss on training batch is 0.00327521.
After 26836 training step(s), loss on training batch is 0.00615281.
After 26837 training step(s), loss on training batch is 0.0033809.
After 26838 training step(s), loss on training batch is 0.00378479.
After 26839 training step(s), loss on training batch is 0.00321438.
After 26840 training step(s), loss on training batch is 0.00308527.
After 26841 training step(s), loss on training batch is 0.00361522.
After 26842 training step(s), loss on training batch is 0.00306199.
After 26843 training step(s), loss on training batch is 0.00321584.
After 26844 training step(s), loss on training batch is 0.00346141.
After 26845 training step(s), loss on training batch is 0.00378847.
After 26846 training step(s), loss on training batch is 0.00349263.
After 26847 training step(s), loss on training batch is 0.00316767.
After 26848 training step(s), loss on training batch is 0.00315355.
After 26849 training step(s), loss on training batch is 0.00317887.
After 26850 training step(s), loss on training batch is 0.00303632.
After 26851 training step(s), loss on training batch is 0.00333393.
After 26852 training step(s), loss on training batch is 0.00333371.
After 26853 training step(s), loss on training batch is 0.00346482.
After 26854 training step(s), loss on training batch is 0.00317573.
After 26855 training step(s), loss on training batch is 0.00333887.
After 26856 training step(s), loss on training batch is 0.00337322.
After 26857 training step(s), loss on training batch is 0.00331268.
After 26858 training step(s), loss on training batch is 0.00342995.
After 26859 training step(s), loss on training batch is 0.00323796.
After 26860 training step(s), loss on training batch is 0.00305266.
After 26861 training step(s), loss on training batch is 0.00344521.
After 26862 training step(s), loss on training batch is 0.00318178.
After 26863 training step(s), loss on training batch is 0.00307031.
After 26864 training step(s), loss on training batch is 0.00309234.
After 26865 training step(s), loss on training batch is 0.00378229.
After 26866 training step(s), loss on training batch is 0.00315239.
After 26867 training step(s), loss on training batch is 0.00329894.
After 26868 training step(s), loss on training batch is 0.00319024.
After 26869 training step(s), loss on training batch is 0.00332253.
After 26870 training step(s), loss on training batch is 0.00304655.
After 26871 training step(s), loss on training batch is 0.00323043.
After 26872 training step(s), loss on training batch is 0.00361615.
After 26873 training step(s), loss on training batch is 0.00332399.
After 26874 training step(s), loss on training batch is 0.00348641.
After 26875 training step(s), loss on training batch is 0.00356896.
After 26876 training step(s), loss on training batch is 0.00322923.
After 26877 training step(s), loss on training batch is 0.00312276.
After 26878 training step(s), loss on training batch is 0.0034454.
After 26879 training step(s), loss on training batch is 0.00308656.
After 26880 training step(s), loss on training batch is 0.00307315.
After 26881 training step(s), loss on training batch is 0.00315255.
After 26882 training step(s), loss on training batch is 0.00307919.
After 26883 training step(s), loss on training batch is 0.0034353.
After 26884 training step(s), loss on training batch is 0.00314156.
After 26885 training step(s), loss on training batch is 0.00332329.
After 26886 training step(s), loss on training batch is 0.00367362.
After 26887 training step(s), loss on training batch is 0.00330129.
After 26888 training step(s), loss on training batch is 0.0036731.
After 26889 training step(s), loss on training batch is 0.00365151.
After 26890 training step(s), loss on training batch is 0.00312408.
After 26891 training step(s), loss on training batch is 0.00354036.
After 26892 training step(s), loss on training batch is 0.00319955.
After 26893 training step(s), loss on training batch is 0.00317301.
After 26894 training step(s), loss on training batch is 0.00322854.
After 26895 training step(s), loss on training batch is 0.00364619.
After 26896 training step(s), loss on training batch is 0.00334175.
After 26897 training step(s), loss on training batch is 0.0034993.
After 26898 training step(s), loss on training batch is 0.00338429.
After 26899 training step(s), loss on training batch is 0.00333372.
After 26900 training step(s), loss on training batch is 0.00343072.
After 26901 training step(s), loss on training batch is 0.00323702.
After 26902 training step(s), loss on training batch is 0.00352796.
After 26903 training step(s), loss on training batch is 0.00324428.
After 26904 training step(s), loss on training batch is 0.00450913.
After 26905 training step(s), loss on training batch is 0.00299879.
After 26906 training step(s), loss on training batch is 0.0030799.
After 26907 training step(s), loss on training batch is 0.00318741.
After 26908 training step(s), loss on training batch is 0.00336892.
After 26909 training step(s), loss on training batch is 0.00328563.
After 26910 training step(s), loss on training batch is 0.0031959.
After 26911 training step(s), loss on training batch is 0.00402589.
After 26912 training step(s), loss on training batch is 0.00339002.
After 26913 training step(s), loss on training batch is 0.00347814.
After 26914 training step(s), loss on training batch is 0.00324542.
After 26915 training step(s), loss on training batch is 0.00328998.
After 26916 training step(s), loss on training batch is 0.00401034.
After 26917 training step(s), loss on training batch is 0.00339258.
After 26918 training step(s), loss on training batch is 0.00306915.
After 26919 training step(s), loss on training batch is 0.00298368.
After 26920 training step(s), loss on training batch is 0.00319399.
After 26921 training step(s), loss on training batch is 0.00338094.
After 26922 training step(s), loss on training batch is 0.00349577.
After 26923 training step(s), loss on training batch is 0.00335646.
After 26924 training step(s), loss on training batch is 0.00313894.
After 26925 training step(s), loss on training batch is 0.00331907.
After 26926 training step(s), loss on training batch is 0.00314413.
After 26927 training step(s), loss on training batch is 0.00293322.
After 26928 training step(s), loss on training batch is 0.00316386.
After 26929 training step(s), loss on training batch is 0.00342194.
After 26930 training step(s), loss on training batch is 0.00392883.
After 26931 training step(s), loss on training batch is 0.00384047.
After 26932 training step(s), loss on training batch is 0.00312982.
After 26933 training step(s), loss on training batch is 0.00335022.
After 26934 training step(s), loss on training batch is 0.00303507.
After 26935 training step(s), loss on training batch is 0.00330048.
After 26936 training step(s), loss on training batch is 0.00305739.
After 26937 training step(s), loss on training batch is 0.00346869.
After 26938 training step(s), loss on training batch is 0.00344262.
After 26939 training step(s), loss on training batch is 0.0033047.
After 26940 training step(s), loss on training batch is 0.00322061.
After 26941 training step(s), loss on training batch is 0.00349796.
After 26942 training step(s), loss on training batch is 0.00337073.
After 26943 training step(s), loss on training batch is 0.0036732.
After 26944 training step(s), loss on training batch is 0.00355201.
After 26945 training step(s), loss on training batch is 0.00310001.
After 26946 training step(s), loss on training batch is 0.00315982.
After 26947 training step(s), loss on training batch is 0.00353376.
After 26948 training step(s), loss on training batch is 0.00317234.
After 26949 training step(s), loss on training batch is 0.0038247.
After 26950 training step(s), loss on training batch is 0.0031072.
After 26951 training step(s), loss on training batch is 0.00324075.
After 26952 training step(s), loss on training batch is 0.00293684.
After 26953 training step(s), loss on training batch is 0.00324318.
After 26954 training step(s), loss on training batch is 0.00338013.
After 26955 training step(s), loss on training batch is 0.00332734.
After 26956 training step(s), loss on training batch is 0.0032025.
After 26957 training step(s), loss on training batch is 0.00331098.
After 26958 training step(s), loss on training batch is 0.00329641.
After 26959 training step(s), loss on training batch is 0.00336583.
After 26960 training step(s), loss on training batch is 0.00333723.
After 26961 training step(s), loss on training batch is 0.00353336.
After 26962 training step(s), loss on training batch is 0.00341246.
After 26963 training step(s), loss on training batch is 0.00345911.
After 26964 training step(s), loss on training batch is 0.00317268.
After 26965 training step(s), loss on training batch is 0.00302721.
After 26966 training step(s), loss on training batch is 0.00308543.
After 26967 training step(s), loss on training batch is 0.00323191.
After 26968 training step(s), loss on training batch is 0.00312733.
After 26969 training step(s), loss on training batch is 0.00322137.
After 26970 training step(s), loss on training batch is 0.00312811.
After 26971 training step(s), loss on training batch is 0.00339381.
After 26972 training step(s), loss on training batch is 0.00337788.
After 26973 training step(s), loss on training batch is 0.00321744.
After 26974 training step(s), loss on training batch is 0.00318246.
After 26975 training step(s), loss on training batch is 0.00323921.
After 26976 training step(s), loss on training batch is 0.00311635.
After 26977 training step(s), loss on training batch is 0.00331058.
After 26978 training step(s), loss on training batch is 0.00313628.
After 26979 training step(s), loss on training batch is 0.00335457.
After 26980 training step(s), loss on training batch is 0.00306618.
After 26981 training step(s), loss on training batch is 0.00305134.
After 26982 training step(s), loss on training batch is 0.0034741.
After 26983 training step(s), loss on training batch is 0.00366646.
After 26984 training step(s), loss on training batch is 0.00331159.
After 26985 training step(s), loss on training batch is 0.00343835.
After 26986 training step(s), loss on training batch is 0.00328139.
After 26987 training step(s), loss on training batch is 0.00330386.
After 26988 training step(s), loss on training batch is 0.00330837.
After 26989 training step(s), loss on training batch is 0.00308103.
After 26990 training step(s), loss on training batch is 0.00336142.
After 26991 training step(s), loss on training batch is 0.0031768.
After 26992 training step(s), loss on training batch is 0.00337946.
After 26993 training step(s), loss on training batch is 0.00297867.
After 26994 training step(s), loss on training batch is 0.00349534.
After 26995 training step(s), loss on training batch is 0.00355504.
After 26996 training step(s), loss on training batch is 0.00295592.
After 26997 training step(s), loss on training batch is 0.0033354.
After 26998 training step(s), loss on training batch is 0.00306319.
After 26999 training step(s), loss on training batch is 0.00311567.
After 27000 training step(s), loss on training batch is 0.00329115.
After 27001 training step(s), loss on training batch is 0.00351668.
After 27002 training step(s), loss on training batch is 0.00313365.
After 27003 training step(s), loss on training batch is 0.0030724.
After 27004 training step(s), loss on training batch is 0.00380213.
After 27005 training step(s), loss on training batch is 0.00310589.
After 27006 training step(s), loss on training batch is 0.0031436.
After 27007 training step(s), loss on training batch is 0.00325457.
After 27008 training step(s), loss on training batch is 0.00320592.
After 27009 training step(s), loss on training batch is 0.00315731.
After 27010 training step(s), loss on training batch is 0.00319497.
After 27011 training step(s), loss on training batch is 0.0033058.
After 27012 training step(s), loss on training batch is 0.00295075.
After 27013 training step(s), loss on training batch is 0.00351578.
After 27014 training step(s), loss on training batch is 0.00346051.
After 27015 training step(s), loss on training batch is 0.00331169.
After 27016 training step(s), loss on training batch is 0.00362145.
After 27017 training step(s), loss on training batch is 0.00341.
After 27018 training step(s), loss on training batch is 0.00351429.
After 27019 training step(s), loss on training batch is 0.00294632.
After 27020 training step(s), loss on training batch is 0.00325864.
After 27021 training step(s), loss on training batch is 0.00314606.
After 27022 training step(s), loss on training batch is 0.00331635.
After 27023 training step(s), loss on training batch is 0.00303639.
After 27024 training step(s), loss on training batch is 0.00321974.
After 27025 training step(s), loss on training batch is 0.0030984.
After 27026 training step(s), loss on training batch is 0.00350098.
After 27027 training step(s), loss on training batch is 0.00310998.
After 27028 training step(s), loss on training batch is 0.00314316.
After 27029 training step(s), loss on training batch is 0.00329644.
After 27030 training step(s), loss on training batch is 0.00431431.
After 27031 training step(s), loss on training batch is 0.00395572.
After 27032 training step(s), loss on training batch is 0.00312906.
After 27033 training step(s), loss on training batch is 0.00341003.
After 27034 training step(s), loss on training batch is 0.00308375.
After 27035 training step(s), loss on training batch is 0.00349205.
After 27036 training step(s), loss on training batch is 0.00338566.
After 27037 training step(s), loss on training batch is 0.00314513.
After 27038 training step(s), loss on training batch is 0.00332496.
After 27039 training step(s), loss on training batch is 0.0031994.
After 27040 training step(s), loss on training batch is 0.00329278.
After 27041 training step(s), loss on training batch is 0.00347924.
After 27042 training step(s), loss on training batch is 0.00368023.
After 27043 training step(s), loss on training batch is 0.00328417.
After 27044 training step(s), loss on training batch is 0.00371108.
After 27045 training step(s), loss on training batch is 0.00296011.
After 27046 training step(s), loss on training batch is 0.00384551.
After 27047 training step(s), loss on training batch is 0.00329383.
After 27048 training step(s), loss on training batch is 0.00379484.
After 27049 training step(s), loss on training batch is 0.00305453.
After 27050 training step(s), loss on training batch is 0.00346239.
After 27051 training step(s), loss on training batch is 0.00310301.
After 27052 training step(s), loss on training batch is 0.00322173.
After 27053 training step(s), loss on training batch is 0.00358874.
After 27054 training step(s), loss on training batch is 0.00382887.
After 27055 training step(s), loss on training batch is 0.00340088.
After 27056 training step(s), loss on training batch is 0.00366933.
After 27057 training step(s), loss on training batch is 0.00338592.
After 27058 training step(s), loss on training batch is 0.00367018.
After 27059 training step(s), loss on training batch is 0.0036465.
After 27060 training step(s), loss on training batch is 0.00367749.
After 27061 training step(s), loss on training batch is 0.00323316.
After 27062 training step(s), loss on training batch is 0.00364429.
After 27063 training step(s), loss on training batch is 0.00339877.
After 27064 training step(s), loss on training batch is 0.00292041.
After 27065 training step(s), loss on training batch is 0.00330702.
After 27066 training step(s), loss on training batch is 0.00323314.
After 27067 training step(s), loss on training batch is 0.00329435.
After 27068 training step(s), loss on training batch is 0.00306208.
After 27069 training step(s), loss on training batch is 0.00328542.
After 27070 training step(s), loss on training batch is 0.0030451.
After 27071 training step(s), loss on training batch is 0.00362935.
After 27072 training step(s), loss on training batch is 0.00337424.
After 27073 training step(s), loss on training batch is 0.00330061.
After 27074 training step(s), loss on training batch is 0.0039698.
After 27075 training step(s), loss on training batch is 0.00414848.
After 27076 training step(s), loss on training batch is 0.0030881.
After 27077 training step(s), loss on training batch is 0.00331141.
After 27078 training step(s), loss on training batch is 0.00331194.
After 27079 training step(s), loss on training batch is 0.00314222.
After 27080 training step(s), loss on training batch is 0.00335649.
After 27081 training step(s), loss on training batch is 0.00364167.
After 27082 training step(s), loss on training batch is 0.00367885.
After 27083 training step(s), loss on training batch is 0.00337296.
After 27084 training step(s), loss on training batch is 0.0033677.
After 27085 training step(s), loss on training batch is 0.00315802.
After 27086 training step(s), loss on training batch is 0.00390635.
After 27087 training step(s), loss on training batch is 0.00328214.
After 27088 training step(s), loss on training batch is 0.00348743.
After 27089 training step(s), loss on training batch is 0.00335214.
After 27090 training step(s), loss on training batch is 0.00337389.
After 27091 training step(s), loss on training batch is 0.00316825.
After 27092 training step(s), loss on training batch is 0.0034905.
After 27093 training step(s), loss on training batch is 0.0034044.
After 27094 training step(s), loss on training batch is 0.00326045.
After 27095 training step(s), loss on training batch is 0.00307951.
After 27096 training step(s), loss on training batch is 0.00351974.
After 27097 training step(s), loss on training batch is 0.00333237.
After 27098 training step(s), loss on training batch is 0.00312061.
After 27099 training step(s), loss on training batch is 0.00358869.
After 27100 training step(s), loss on training batch is 0.00303063.
After 27101 training step(s), loss on training batch is 0.00351235.
After 27102 training step(s), loss on training batch is 0.00385465.
After 27103 training step(s), loss on training batch is 0.00322797.
After 27104 training step(s), loss on training batch is 0.00339084.
After 27105 training step(s), loss on training batch is 0.00344722.
After 27106 training step(s), loss on training batch is 0.00403368.
After 27107 training step(s), loss on training batch is 0.00303667.
After 27108 training step(s), loss on training batch is 0.00307506.
After 27109 training step(s), loss on training batch is 0.00331452.
After 27110 training step(s), loss on training batch is 0.00318668.
After 27111 training step(s), loss on training batch is 0.00377987.
After 27112 training step(s), loss on training batch is 0.00337983.
After 27113 training step(s), loss on training batch is 0.00362423.
After 27114 training step(s), loss on training batch is 0.00317842.
After 27115 training step(s), loss on training batch is 0.00313128.
After 27116 training step(s), loss on training batch is 0.0031165.
After 27117 training step(s), loss on training batch is 0.00330876.
After 27118 training step(s), loss on training batch is 0.00319514.
After 27119 training step(s), loss on training batch is 0.00353104.
After 27120 training step(s), loss on training batch is 0.003499.
After 27121 training step(s), loss on training batch is 0.00390302.
After 27122 training step(s), loss on training batch is 0.00332449.
After 27123 training step(s), loss on training batch is 0.00346531.
After 27124 training step(s), loss on training batch is 0.00326717.
After 27125 training step(s), loss on training batch is 0.00335526.
After 27126 training step(s), loss on training batch is 0.00340063.
After 27127 training step(s), loss on training batch is 0.00319222.
After 27128 training step(s), loss on training batch is 0.00312045.
After 27129 training step(s), loss on training batch is 0.00319567.
After 27130 training step(s), loss on training batch is 0.00477194.
After 27131 training step(s), loss on training batch is 0.00361477.
After 27132 training step(s), loss on training batch is 0.00304278.
After 27133 training step(s), loss on training batch is 0.00306467.
After 27134 training step(s), loss on training batch is 0.00340882.
After 27135 training step(s), loss on training batch is 0.00316836.
After 27136 training step(s), loss on training batch is 0.00332152.
After 27137 training step(s), loss on training batch is 0.00323448.
After 27138 training step(s), loss on training batch is 0.00324068.
After 27139 training step(s), loss on training batch is 0.00325499.
After 27140 training step(s), loss on training batch is 0.0034018.
After 27141 training step(s), loss on training batch is 0.0034832.
After 27142 training step(s), loss on training batch is 0.00316718.
After 27143 training step(s), loss on training batch is 0.00340342.
After 27144 training step(s), loss on training batch is 0.00336341.
After 27145 training step(s), loss on training batch is 0.00376327.
After 27146 training step(s), loss on training batch is 0.00334097.
After 27147 training step(s), loss on training batch is 0.00345975.
After 27148 training step(s), loss on training batch is 0.00322484.
After 27149 training step(s), loss on training batch is 0.00366294.
After 27150 training step(s), loss on training batch is 0.00323374.
After 27151 training step(s), loss on training batch is 0.00315948.
After 27152 training step(s), loss on training batch is 0.00329619.
After 27153 training step(s), loss on training batch is 0.0032626.
After 27154 training step(s), loss on training batch is 0.00327482.
After 27155 training step(s), loss on training batch is 0.00380094.
After 27156 training step(s), loss on training batch is 0.00320151.
After 27157 training step(s), loss on training batch is 0.0034219.
After 27158 training step(s), loss on training batch is 0.00311291.
After 27159 training step(s), loss on training batch is 0.00322529.
After 27160 training step(s), loss on training batch is 0.00333079.
After 27161 training step(s), loss on training batch is 0.00320256.
After 27162 training step(s), loss on training batch is 0.0029177.
After 27163 training step(s), loss on training batch is 0.00342067.
After 27164 training step(s), loss on training batch is 0.00312533.
After 27165 training step(s), loss on training batch is 0.00321003.
After 27166 training step(s), loss on training batch is 0.00328367.
After 27167 training step(s), loss on training batch is 0.00365815.
After 27168 training step(s), loss on training batch is 0.00308447.
After 27169 training step(s), loss on training batch is 0.00330798.
After 27170 training step(s), loss on training batch is 0.00346976.
After 27171 training step(s), loss on training batch is 0.0031429.
After 27172 training step(s), loss on training batch is 0.00307663.
After 27173 training step(s), loss on training batch is 0.00320555.
After 27174 training step(s), loss on training batch is 0.00320351.
After 27175 training step(s), loss on training batch is 0.00297017.
After 27176 training step(s), loss on training batch is 0.00314613.
After 27177 training step(s), loss on training batch is 0.00377471.
After 27178 training step(s), loss on training batch is 0.00324484.
After 27179 training step(s), loss on training batch is 0.00311056.
After 27180 training step(s), loss on training batch is 0.00318364.
After 27181 training step(s), loss on training batch is 0.00369468.
After 27182 training step(s), loss on training batch is 0.00385363.
After 27183 training step(s), loss on training batch is 0.00322959.
After 27184 training step(s), loss on training batch is 0.00347397.
After 27185 training step(s), loss on training batch is 0.00362942.
After 27186 training step(s), loss on training batch is 0.00317783.
After 27187 training step(s), loss on training batch is 0.00403523.
After 27188 training step(s), loss on training batch is 0.00321324.
After 27189 training step(s), loss on training batch is 0.0029748.
After 27190 training step(s), loss on training batch is 0.00311761.
After 27191 training step(s), loss on training batch is 0.00295093.
After 27192 training step(s), loss on training batch is 0.00309029.
After 27193 training step(s), loss on training batch is 0.00321973.
After 27194 training step(s), loss on training batch is 0.00315521.
After 27195 training step(s), loss on training batch is 0.00324828.
After 27196 training step(s), loss on training batch is 0.00354559.
After 27197 training step(s), loss on training batch is 0.00332522.
After 27198 training step(s), loss on training batch is 0.00313966.
After 27199 training step(s), loss on training batch is 0.00334032.
After 27200 training step(s), loss on training batch is 0.00390697.
After 27201 training step(s), loss on training batch is 0.00304003.
After 27202 training step(s), loss on training batch is 0.00313659.
After 27203 training step(s), loss on training batch is 0.00312666.
After 27204 training step(s), loss on training batch is 0.00363677.
After 27205 training step(s), loss on training batch is 0.0032542.
After 27206 training step(s), loss on training batch is 0.00317634.
After 27207 training step(s), loss on training batch is 0.00315255.
After 27208 training step(s), loss on training batch is 0.00360342.
After 27209 training step(s), loss on training batch is 0.00344705.
After 27210 training step(s), loss on training batch is 0.00308766.
After 27211 training step(s), loss on training batch is 0.00345558.
After 27212 training step(s), loss on training batch is 0.00312553.
After 27213 training step(s), loss on training batch is 0.00327475.
After 27214 training step(s), loss on training batch is 0.0032071.
After 27215 training step(s), loss on training batch is 0.00349071.
After 27216 training step(s), loss on training batch is 0.00339319.
After 27217 training step(s), loss on training batch is 0.00298283.
After 27218 training step(s), loss on training batch is 0.00298918.
After 27219 training step(s), loss on training batch is 0.00311822.
After 27220 training step(s), loss on training batch is 0.00315359.
After 27221 training step(s), loss on training batch is 0.00335117.
After 27222 training step(s), loss on training batch is 0.00321935.
After 27223 training step(s), loss on training batch is 0.00375773.
After 27224 training step(s), loss on training batch is 0.00298509.
After 27225 training step(s), loss on training batch is 0.00345409.
After 27226 training step(s), loss on training batch is 0.00345312.
After 27227 training step(s), loss on training batch is 0.00315173.
After 27228 training step(s), loss on training batch is 0.00311947.
After 27229 training step(s), loss on training batch is 0.00298474.
After 27230 training step(s), loss on training batch is 0.00333429.
After 27231 training step(s), loss on training batch is 0.00313604.
After 27232 training step(s), loss on training batch is 0.00311776.
After 27233 training step(s), loss on training batch is 0.00343629.
After 27234 training step(s), loss on training batch is 0.00355989.
After 27235 training step(s), loss on training batch is 0.00315551.
After 27236 training step(s), loss on training batch is 0.00313654.
After 27237 training step(s), loss on training batch is 0.00335234.
After 27238 training step(s), loss on training batch is 0.00370397.
After 27239 training step(s), loss on training batch is 0.00345762.
After 27240 training step(s), loss on training batch is 0.00386696.
After 27241 training step(s), loss on training batch is 0.00367605.
After 27242 training step(s), loss on training batch is 0.00321339.
After 27243 training step(s), loss on training batch is 0.00321474.
After 27244 training step(s), loss on training batch is 0.00308747.
After 27245 training step(s), loss on training batch is 0.00363288.
After 27246 training step(s), loss on training batch is 0.00326533.
After 27247 training step(s), loss on training batch is 0.00320445.
After 27248 training step(s), loss on training batch is 0.00342765.
After 27249 training step(s), loss on training batch is 0.00348973.
After 27250 training step(s), loss on training batch is 0.00331013.
After 27251 training step(s), loss on training batch is 0.00346153.
After 27252 training step(s), loss on training batch is 0.00306986.
After 27253 training step(s), loss on training batch is 0.00318389.
After 27254 training step(s), loss on training batch is 0.00313623.
After 27255 training step(s), loss on training batch is 0.00354168.
After 27256 training step(s), loss on training batch is 0.00347858.
After 27257 training step(s), loss on training batch is 0.00314382.
After 27258 training step(s), loss on training batch is 0.00301566.
After 27259 training step(s), loss on training batch is 0.00321512.
After 27260 training step(s), loss on training batch is 0.00373071.
After 27261 training step(s), loss on training batch is 0.00320498.
After 27262 training step(s), loss on training batch is 0.00329244.
After 27263 training step(s), loss on training batch is 0.00337547.
After 27264 training step(s), loss on training batch is 0.00341884.
After 27265 training step(s), loss on training batch is 0.0035013.
After 27266 training step(s), loss on training batch is 0.00396179.
After 27267 training step(s), loss on training batch is 0.00330346.
After 27268 training step(s), loss on training batch is 0.00334503.
After 27269 training step(s), loss on training batch is 0.0031641.
After 27270 training step(s), loss on training batch is 0.00300186.
After 27271 training step(s), loss on training batch is 0.00315563.
After 27272 training step(s), loss on training batch is 0.00318741.
After 27273 training step(s), loss on training batch is 0.00304336.
After 27274 training step(s), loss on training batch is 0.00331236.
After 27275 training step(s), loss on training batch is 0.00292302.
After 27276 training step(s), loss on training batch is 0.00316964.
After 27277 training step(s), loss on training batch is 0.00342145.
After 27278 training step(s), loss on training batch is 0.00356357.
After 27279 training step(s), loss on training batch is 0.00336168.
After 27280 training step(s), loss on training batch is 0.00332098.
After 27281 training step(s), loss on training batch is 0.00313704.
After 27282 training step(s), loss on training batch is 0.00308094.
After 27283 training step(s), loss on training batch is 0.00298697.
After 27284 training step(s), loss on training batch is 0.00353912.
After 27285 training step(s), loss on training batch is 0.00339202.
After 27286 training step(s), loss on training batch is 0.00324912.
After 27287 training step(s), loss on training batch is 0.00334149.
After 27288 training step(s), loss on training batch is 0.00302678.
After 27289 training step(s), loss on training batch is 0.00371495.
After 27290 training step(s), loss on training batch is 0.00324622.
After 27291 training step(s), loss on training batch is 0.00332179.
After 27292 training step(s), loss on training batch is 0.00335608.
After 27293 training step(s), loss on training batch is 0.00309055.
After 27294 training step(s), loss on training batch is 0.00314725.
After 27295 training step(s), loss on training batch is 0.00299676.
After 27296 training step(s), loss on training batch is 0.00307874.
After 27297 training step(s), loss on training batch is 0.00309458.
After 27298 training step(s), loss on training batch is 0.0032048.
After 27299 training step(s), loss on training batch is 0.00355598.
After 27300 training step(s), loss on training batch is 0.00302975.
After 27301 training step(s), loss on training batch is 0.00316709.
After 27302 training step(s), loss on training batch is 0.00333113.
After 27303 training step(s), loss on training batch is 0.0031291.
After 27304 training step(s), loss on training batch is 0.00311557.
After 27305 training step(s), loss on training batch is 0.00355522.
After 27306 training step(s), loss on training batch is 0.00331055.
After 27307 training step(s), loss on training batch is 0.00329758.
After 27308 training step(s), loss on training batch is 0.00356306.
After 27309 training step(s), loss on training batch is 0.00318518.
After 27310 training step(s), loss on training batch is 0.00319211.
After 27311 training step(s), loss on training batch is 0.0037301.
After 27312 training step(s), loss on training batch is 0.00328758.
After 27313 training step(s), loss on training batch is 0.00336956.
After 27314 training step(s), loss on training batch is 0.00303367.
After 27315 training step(s), loss on training batch is 0.00316202.
After 27316 training step(s), loss on training batch is 0.00324297.
After 27317 training step(s), loss on training batch is 0.00307903.
After 27318 training step(s), loss on training batch is 0.00347696.
After 27319 training step(s), loss on training batch is 0.00300207.
After 27320 training step(s), loss on training batch is 0.00323406.
After 27321 training step(s), loss on training batch is 0.0032855.
After 27322 training step(s), loss on training batch is 0.00334442.
After 27323 training step(s), loss on training batch is 0.00317639.
After 27324 training step(s), loss on training batch is 0.00349729.
After 27325 training step(s), loss on training batch is 0.00326651.
After 27326 training step(s), loss on training batch is 0.00317697.
After 27327 training step(s), loss on training batch is 0.00304072.
After 27328 training step(s), loss on training batch is 0.00319504.
After 27329 training step(s), loss on training batch is 0.00328567.
After 27330 training step(s), loss on training batch is 0.0033808.
After 27331 training step(s), loss on training batch is 0.00309553.
After 27332 training step(s), loss on training batch is 0.00320504.
After 27333 training step(s), loss on training batch is 0.00335097.
After 27334 training step(s), loss on training batch is 0.00318969.
After 27335 training step(s), loss on training batch is 0.00362553.
After 27336 training step(s), loss on training batch is 0.00295146.
After 27337 training step(s), loss on training batch is 0.00366847.
After 27338 training step(s), loss on training batch is 0.00300501.
After 27339 training step(s), loss on training batch is 0.00447056.
After 27340 training step(s), loss on training batch is 0.00316132.
After 27341 training step(s), loss on training batch is 0.0033913.
After 27342 training step(s), loss on training batch is 0.0029894.
After 27343 training step(s), loss on training batch is 0.00328409.
After 27344 training step(s), loss on training batch is 0.00343145.
After 27345 training step(s), loss on training batch is 0.00377887.
After 27346 training step(s), loss on training batch is 0.00301177.
After 27347 training step(s), loss on training batch is 0.00313217.
After 27348 training step(s), loss on training batch is 0.00333596.
After 27349 training step(s), loss on training batch is 0.00307392.
After 27350 training step(s), loss on training batch is 0.00522877.
After 27351 training step(s), loss on training batch is 0.00332099.
After 27352 training step(s), loss on training batch is 0.00302738.
After 27353 training step(s), loss on training batch is 0.00349319.
After 27354 training step(s), loss on training batch is 0.00344466.
After 27355 training step(s), loss on training batch is 0.00314038.
After 27356 training step(s), loss on training batch is 0.00338137.
After 27357 training step(s), loss on training batch is 0.00324792.
After 27358 training step(s), loss on training batch is 0.00322871.
After 27359 training step(s), loss on training batch is 0.00291559.
After 27360 training step(s), loss on training batch is 0.00389158.
After 27361 training step(s), loss on training batch is 0.00374679.
After 27362 training step(s), loss on training batch is 0.00317502.
After 27363 training step(s), loss on training batch is 0.00317028.
After 27364 training step(s), loss on training batch is 0.0030214.
After 27365 training step(s), loss on training batch is 0.00291443.
After 27366 training step(s), loss on training batch is 0.00338078.
After 27367 training step(s), loss on training batch is 0.00330326.
After 27368 training step(s), loss on training batch is 0.00376622.
After 27369 training step(s), loss on training batch is 0.00328278.
After 27370 training step(s), loss on training batch is 0.00322834.
After 27371 training step(s), loss on training batch is 0.00344755.
After 27372 training step(s), loss on training batch is 0.00375875.
After 27373 training step(s), loss on training batch is 0.00379516.
After 27374 training step(s), loss on training batch is 0.00311654.
After 27375 training step(s), loss on training batch is 0.00308914.
After 27376 training step(s), loss on training batch is 0.00341185.
After 27377 training step(s), loss on training batch is 0.00325814.
After 27378 training step(s), loss on training batch is 0.00307888.
After 27379 training step(s), loss on training batch is 0.00347734.
After 27380 training step(s), loss on training batch is 0.00362129.
After 27381 training step(s), loss on training batch is 0.00384591.
After 27382 training step(s), loss on training batch is 0.00317149.
After 27383 training step(s), loss on training batch is 0.00337606.
After 27384 training step(s), loss on training batch is 0.00324116.
After 27385 training step(s), loss on training batch is 0.00347104.
After 27386 training step(s), loss on training batch is 0.00314199.
After 27387 training step(s), loss on training batch is 0.00343554.
After 27388 training step(s), loss on training batch is 0.00317273.
After 27389 training step(s), loss on training batch is 0.00329381.
After 27390 training step(s), loss on training batch is 0.00312215.
After 27391 training step(s), loss on training batch is 0.00293532.
After 27392 training step(s), loss on training batch is 0.00317596.
After 27393 training step(s), loss on training batch is 0.00306188.
After 27394 training step(s), loss on training batch is 0.00327265.
After 27395 training step(s), loss on training batch is 0.00329215.
After 27396 training step(s), loss on training batch is 0.0029741.
After 27397 training step(s), loss on training batch is 0.00322664.
After 27398 training step(s), loss on training batch is 0.00327904.
After 27399 training step(s), loss on training batch is 0.00379195.
After 27400 training step(s), loss on training batch is 0.00357846.
After 27401 training step(s), loss on training batch is 0.0032182.
After 27402 training step(s), loss on training batch is 0.00421008.
After 27403 training step(s), loss on training batch is 0.00303794.
After 27404 training step(s), loss on training batch is 0.00318825.
After 27405 training step(s), loss on training batch is 0.00316549.
After 27406 training step(s), loss on training batch is 0.00344687.
After 27407 training step(s), loss on training batch is 0.00326865.
After 27408 training step(s), loss on training batch is 0.00348888.
After 27409 training step(s), loss on training batch is 0.00346421.
After 27410 training step(s), loss on training batch is 0.0033686.
After 27411 training step(s), loss on training batch is 0.00359902.
After 27412 training step(s), loss on training batch is 0.00308749.
After 27413 training step(s), loss on training batch is 0.00295017.
After 27414 training step(s), loss on training batch is 0.00323786.
After 27415 training step(s), loss on training batch is 0.00314177.
After 27416 training step(s), loss on training batch is 0.00315578.
After 27417 training step(s), loss on training batch is 0.00330037.
After 27418 training step(s), loss on training batch is 0.00313128.
After 27419 training step(s), loss on training batch is 0.00301077.
After 27420 training step(s), loss on training batch is 0.00301857.
After 27421 training step(s), loss on training batch is 0.00325301.
After 27422 training step(s), loss on training batch is 0.00339013.
After 27423 training step(s), loss on training batch is 0.00392075.
After 27424 training step(s), loss on training batch is 0.00345966.
After 27425 training step(s), loss on training batch is 0.00323999.
After 27426 training step(s), loss on training batch is 0.00358389.
After 27427 training step(s), loss on training batch is 0.00335699.
After 27428 training step(s), loss on training batch is 0.00315558.
After 27429 training step(s), loss on training batch is 0.00383177.
After 27430 training step(s), loss on training batch is 0.0030727.
After 27431 training step(s), loss on training batch is 0.00344599.
After 27432 training step(s), loss on training batch is 0.00310749.
After 27433 training step(s), loss on training batch is 0.00335772.
After 27434 training step(s), loss on training batch is 0.00311323.
After 27435 training step(s), loss on training batch is 0.00346656.
After 27436 training step(s), loss on training batch is 0.00439886.
After 27437 training step(s), loss on training batch is 0.00358239.
After 27438 training step(s), loss on training batch is 0.00330769.
After 27439 training step(s), loss on training batch is 0.00355802.
After 27440 training step(s), loss on training batch is 0.00317237.
After 27441 training step(s), loss on training batch is 0.00328864.
After 27442 training step(s), loss on training batch is 0.003166.
After 27443 training step(s), loss on training batch is 0.00341233.
After 27444 training step(s), loss on training batch is 0.0040296.
After 27445 training step(s), loss on training batch is 0.00343444.
After 27446 training step(s), loss on training batch is 0.0031174.
After 27447 training step(s), loss on training batch is 0.00311892.
After 27448 training step(s), loss on training batch is 0.003097.
After 27449 training step(s), loss on training batch is 0.00331064.
After 27450 training step(s), loss on training batch is 0.00394826.
After 27451 training step(s), loss on training batch is 0.00318911.
After 27452 training step(s), loss on training batch is 0.00324156.
After 27453 training step(s), loss on training batch is 0.00353972.
After 27454 training step(s), loss on training batch is 0.00330309.
After 27455 training step(s), loss on training batch is 0.00313419.
After 27456 training step(s), loss on training batch is 0.00319108.
After 27457 training step(s), loss on training batch is 0.0032718.
After 27458 training step(s), loss on training batch is 0.0031868.
After 27459 training step(s), loss on training batch is 0.00326329.
After 27460 training step(s), loss on training batch is 0.00342271.
After 27461 training step(s), loss on training batch is 0.00355064.
After 27462 training step(s), loss on training batch is 0.00345231.
After 27463 training step(s), loss on training batch is 0.00362731.
After 27464 training step(s), loss on training batch is 0.00320368.
After 27465 training step(s), loss on training batch is 0.00377348.
After 27466 training step(s), loss on training batch is 0.0034054.
After 27467 training step(s), loss on training batch is 0.00322967.
After 27468 training step(s), loss on training batch is 0.00344354.
After 27469 training step(s), loss on training batch is 0.00348086.
After 27470 training step(s), loss on training batch is 0.00355244.
After 27471 training step(s), loss on training batch is 0.00365477.
After 27472 training step(s), loss on training batch is 0.00378104.
After 27473 training step(s), loss on training batch is 0.00316132.
After 27474 training step(s), loss on training batch is 0.00319461.
After 27475 training step(s), loss on training batch is 0.00322941.
After 27476 training step(s), loss on training batch is 0.00404188.
After 27477 training step(s), loss on training batch is 0.0036591.
After 27478 training step(s), loss on training batch is 0.00334554.
After 27479 training step(s), loss on training batch is 0.00380978.
After 27480 training step(s), loss on training batch is 0.00326565.
After 27481 training step(s), loss on training batch is 0.0039839.
After 27482 training step(s), loss on training batch is 0.00318488.
After 27483 training step(s), loss on training batch is 0.00338486.
After 27484 training step(s), loss on training batch is 0.00336991.
After 27485 training step(s), loss on training batch is 0.00307116.
After 27486 training step(s), loss on training batch is 0.00332877.
After 27487 training step(s), loss on training batch is 0.00296746.
After 27488 training step(s), loss on training batch is 0.00324629.
After 27489 training step(s), loss on training batch is 0.00341282.
After 27490 training step(s), loss on training batch is 0.00324715.
After 27491 training step(s), loss on training batch is 0.0033621.
After 27492 training step(s), loss on training batch is 0.00399646.
After 27493 training step(s), loss on training batch is 0.00344178.
After 27494 training step(s), loss on training batch is 0.00340849.
After 27495 training step(s), loss on training batch is 0.0029863.
After 27496 training step(s), loss on training batch is 0.00304409.
After 27497 training step(s), loss on training batch is 0.00305609.
After 27498 training step(s), loss on training batch is 0.0029788.
After 27499 training step(s), loss on training batch is 0.00332383.
After 27500 training step(s), loss on training batch is 0.00346245.
After 27501 training step(s), loss on training batch is 0.00344152.
After 27502 training step(s), loss on training batch is 0.00320871.
After 27503 training step(s), loss on training batch is 0.00324478.
After 27504 training step(s), loss on training batch is 0.00294051.
After 27505 training step(s), loss on training batch is 0.00327774.
After 27506 training step(s), loss on training batch is 0.00316513.
After 27507 training step(s), loss on training batch is 0.00326219.
After 27508 training step(s), loss on training batch is 0.00314773.
After 27509 training step(s), loss on training batch is 0.00327377.
After 27510 training step(s), loss on training batch is 0.0032808.
After 27511 training step(s), loss on training batch is 0.0035511.
After 27512 training step(s), loss on training batch is 0.00325203.
After 27513 training step(s), loss on training batch is 0.00312182.
After 27514 training step(s), loss on training batch is 0.00291726.
After 27515 training step(s), loss on training batch is 0.00315212.
After 27516 training step(s), loss on training batch is 0.00329443.
After 27517 training step(s), loss on training batch is 0.00314423.
After 27518 training step(s), loss on training batch is 0.0029795.
After 27519 training step(s), loss on training batch is 0.00338895.
After 27520 training step(s), loss on training batch is 0.00325391.
After 27521 training step(s), loss on training batch is 0.00336525.
After 27522 training step(s), loss on training batch is 0.00312137.
After 27523 training step(s), loss on training batch is 0.00292473.
After 27524 training step(s), loss on training batch is 0.00346668.
After 27525 training step(s), loss on training batch is 0.0033198.
After 27526 training step(s), loss on training batch is 0.00308388.
After 27527 training step(s), loss on training batch is 0.00329745.
After 27528 training step(s), loss on training batch is 0.00323914.
After 27529 training step(s), loss on training batch is 0.00355426.
After 27530 training step(s), loss on training batch is 0.00343892.
After 27531 training step(s), loss on training batch is 0.00320467.
After 27532 training step(s), loss on training batch is 0.00311789.
After 27533 training step(s), loss on training batch is 0.00307635.
After 27534 training step(s), loss on training batch is 0.00374581.
After 27535 training step(s), loss on training batch is 0.00293923.
After 27536 training step(s), loss on training batch is 0.0031147.
After 27537 training step(s), loss on training batch is 0.00333352.
After 27538 training step(s), loss on training batch is 0.00327785.
After 27539 training step(s), loss on training batch is 0.00316397.
After 27540 training step(s), loss on training batch is 0.00311653.
After 27541 training step(s), loss on training batch is 0.00319768.
After 27542 training step(s), loss on training batch is 0.00359596.
After 27543 training step(s), loss on training batch is 0.00329592.
After 27544 training step(s), loss on training batch is 0.0035829.
After 27545 training step(s), loss on training batch is 0.00330653.
After 27546 training step(s), loss on training batch is 0.00318401.
After 27547 training step(s), loss on training batch is 0.00323616.
After 27548 training step(s), loss on training batch is 0.00336355.
After 27549 training step(s), loss on training batch is 0.00303476.
After 27550 training step(s), loss on training batch is 0.00360322.
After 27551 training step(s), loss on training batch is 0.00351765.
After 27552 training step(s), loss on training batch is 0.00313051.
After 27553 training step(s), loss on training batch is 0.00306846.
After 27554 training step(s), loss on training batch is 0.00306767.
After 27555 training step(s), loss on training batch is 0.00404548.
After 27556 training step(s), loss on training batch is 0.00317037.
After 27557 training step(s), loss on training batch is 0.00346872.
After 27558 training step(s), loss on training batch is 0.00332347.
After 27559 training step(s), loss on training batch is 0.00329196.
After 27560 training step(s), loss on training batch is 0.00354473.
After 27561 training step(s), loss on training batch is 0.0032361.
After 27562 training step(s), loss on training batch is 0.00324089.
After 27563 training step(s), loss on training batch is 0.00325983.
After 27564 training step(s), loss on training batch is 0.00328163.
After 27565 training step(s), loss on training batch is 0.00329461.
After 27566 training step(s), loss on training batch is 0.00327077.
After 27567 training step(s), loss on training batch is 0.00319328.
After 27568 training step(s), loss on training batch is 0.00354531.
After 27569 training step(s), loss on training batch is 0.00323196.
After 27570 training step(s), loss on training batch is 0.00327089.
After 27571 training step(s), loss on training batch is 0.00298374.
After 27572 training step(s), loss on training batch is 0.00317581.
After 27573 training step(s), loss on training batch is 0.00320602.
After 27574 training step(s), loss on training batch is 0.003049.
After 27575 training step(s), loss on training batch is 0.00323257.
After 27576 training step(s), loss on training batch is 0.00351863.
After 27577 training step(s), loss on training batch is 0.00307247.
After 27578 training step(s), loss on training batch is 0.00328937.
After 27579 training step(s), loss on training batch is 0.00326993.
After 27580 training step(s), loss on training batch is 0.00324952.
After 27581 training step(s), loss on training batch is 0.00300392.
After 27582 training step(s), loss on training batch is 0.00304291.
After 27583 training step(s), loss on training batch is 0.00325421.
After 27584 training step(s), loss on training batch is 0.00372783.
After 27585 training step(s), loss on training batch is 0.00289268.
After 27586 training step(s), loss on training batch is 0.00319407.
After 27587 training step(s), loss on training batch is 0.00320717.
After 27588 training step(s), loss on training batch is 0.00324356.
After 27589 training step(s), loss on training batch is 0.00372207.
After 27590 training step(s), loss on training batch is 0.00331696.
After 27591 training step(s), loss on training batch is 0.00330376.
After 27592 training step(s), loss on training batch is 0.00390622.
After 27593 training step(s), loss on training batch is 0.00323242.
After 27594 training step(s), loss on training batch is 0.00316576.
After 27595 training step(s), loss on training batch is 0.00316846.
After 27596 training step(s), loss on training batch is 0.00327476.
After 27597 training step(s), loss on training batch is 0.00330434.
After 27598 training step(s), loss on training batch is 0.00324747.
After 27599 training step(s), loss on training batch is 0.00341918.
After 27600 training step(s), loss on training batch is 0.00350238.
After 27601 training step(s), loss on training batch is 0.0031717.
After 27602 training step(s), loss on training batch is 0.00298213.
After 27603 training step(s), loss on training batch is 0.00329199.
After 27604 training step(s), loss on training batch is 0.00336429.
After 27605 training step(s), loss on training batch is 0.00319662.
After 27606 training step(s), loss on training batch is 0.00321635.
After 27607 training step(s), loss on training batch is 0.00319904.
After 27608 training step(s), loss on training batch is 0.00356257.
After 27609 training step(s), loss on training batch is 0.00321473.
After 27610 training step(s), loss on training batch is 0.00344403.
After 27611 training step(s), loss on training batch is 0.00339612.
After 27612 training step(s), loss on training batch is 0.00306287.
After 27613 training step(s), loss on training batch is 0.00315271.
After 27614 training step(s), loss on training batch is 0.0033685.
After 27615 training step(s), loss on training batch is 0.00305125.
After 27616 training step(s), loss on training batch is 0.00379785.
After 27617 training step(s), loss on training batch is 0.00332881.
After 27618 training step(s), loss on training batch is 0.00431404.
After 27619 training step(s), loss on training batch is 0.00320277.
After 27620 training step(s), loss on training batch is 0.00305752.
After 27621 training step(s), loss on training batch is 0.00305383.
After 27622 training step(s), loss on training batch is 0.00403823.
After 27623 training step(s), loss on training batch is 0.00386794.
After 27624 training step(s), loss on training batch is 0.00309756.
After 27625 training step(s), loss on training batch is 0.00329507.
After 27626 training step(s), loss on training batch is 0.00344019.
After 27627 training step(s), loss on training batch is 0.00392219.
After 27628 training step(s), loss on training batch is 0.00322076.
After 27629 training step(s), loss on training batch is 0.00312957.
After 27630 training step(s), loss on training batch is 0.00339449.
After 27631 training step(s), loss on training batch is 0.00328557.
After 27632 training step(s), loss on training batch is 0.00360621.
After 27633 training step(s), loss on training batch is 0.00315736.
After 27634 training step(s), loss on training batch is 0.003062.
After 27635 training step(s), loss on training batch is 0.00307933.
After 27636 training step(s), loss on training batch is 0.0029356.
After 27637 training step(s), loss on training batch is 0.00337461.
After 27638 training step(s), loss on training batch is 0.0033338.
After 27639 training step(s), loss on training batch is 0.00329612.
After 27640 training step(s), loss on training batch is 0.00337305.
After 27641 training step(s), loss on training batch is 0.00332286.
After 27642 training step(s), loss on training batch is 0.00324061.
After 27643 training step(s), loss on training batch is 0.0032793.
After 27644 training step(s), loss on training batch is 0.00306603.
After 27645 training step(s), loss on training batch is 0.00343197.
After 27646 training step(s), loss on training batch is 0.00335898.
After 27647 training step(s), loss on training batch is 0.00317247.
After 27648 training step(s), loss on training batch is 0.00314717.
After 27649 training step(s), loss on training batch is 0.003051.
After 27650 training step(s), loss on training batch is 0.00327278.
After 27651 training step(s), loss on training batch is 0.00329541.
After 27652 training step(s), loss on training batch is 0.00317572.
After 27653 training step(s), loss on training batch is 0.00340165.
After 27654 training step(s), loss on training batch is 0.00374336.
After 27655 training step(s), loss on training batch is 0.00307204.
After 27656 training step(s), loss on training batch is 0.00332531.
After 27657 training step(s), loss on training batch is 0.00325672.
After 27658 training step(s), loss on training batch is 0.00404339.
After 27659 training step(s), loss on training batch is 0.00294494.
After 27660 training step(s), loss on training batch is 0.00308008.
After 27661 training step(s), loss on training batch is 0.0034939.
After 27662 training step(s), loss on training batch is 0.00329365.
After 27663 training step(s), loss on training batch is 0.00315462.
After 27664 training step(s), loss on training batch is 0.00332562.
After 27665 training step(s), loss on training batch is 0.00368218.
After 27666 training step(s), loss on training batch is 0.00332963.
After 27667 training step(s), loss on training batch is 0.00375069.
After 27668 training step(s), loss on training batch is 0.00408946.
After 27669 training step(s), loss on training batch is 0.00355134.
After 27670 training step(s), loss on training batch is 0.00353809.
After 27671 training step(s), loss on training batch is 0.00334743.
After 27672 training step(s), loss on training batch is 0.00331888.
After 27673 training step(s), loss on training batch is 0.00333539.
After 27674 training step(s), loss on training batch is 0.00308809.
After 27675 training step(s), loss on training batch is 0.00399789.
After 27676 training step(s), loss on training batch is 0.00309224.
After 27677 training step(s), loss on training batch is 0.00294169.
After 27678 training step(s), loss on training batch is 0.00316864.
After 27679 training step(s), loss on training batch is 0.00320513.
After 27680 training step(s), loss on training batch is 0.00330391.
After 27681 training step(s), loss on training batch is 0.00339593.
After 27682 training step(s), loss on training batch is 0.00304189.
After 27683 training step(s), loss on training batch is 0.00305746.
After 27684 training step(s), loss on training batch is 0.00328256.
After 27685 training step(s), loss on training batch is 0.00352273.
After 27686 training step(s), loss on training batch is 0.00319237.
After 27687 training step(s), loss on training batch is 0.00322056.
After 27688 training step(s), loss on training batch is 0.00316584.
After 27689 training step(s), loss on training batch is 0.00381427.
After 27690 training step(s), loss on training batch is 0.0030959.
After 27691 training step(s), loss on training batch is 0.00313518.
After 27692 training step(s), loss on training batch is 0.00318268.
After 27693 training step(s), loss on training batch is 0.00333215.
After 27694 training step(s), loss on training batch is 0.00301226.
After 27695 training step(s), loss on training batch is 0.00327618.
After 27696 training step(s), loss on training batch is 0.00335378.
After 27697 training step(s), loss on training batch is 0.00304375.
After 27698 training step(s), loss on training batch is 0.00436368.
After 27699 training step(s), loss on training batch is 0.00336566.
After 27700 training step(s), loss on training batch is 0.00304421.
After 27701 training step(s), loss on training batch is 0.00340859.
After 27702 training step(s), loss on training batch is 0.00394328.
After 27703 training step(s), loss on training batch is 0.00315152.
After 27704 training step(s), loss on training batch is 0.00329751.
After 27705 training step(s), loss on training batch is 0.00323164.
After 27706 training step(s), loss on training batch is 0.00321466.
After 27707 training step(s), loss on training batch is 0.00306868.
After 27708 training step(s), loss on training batch is 0.00316.
After 27709 training step(s), loss on training batch is 0.00335127.
After 27710 training step(s), loss on training batch is 0.00333964.
After 27711 training step(s), loss on training batch is 0.00355169.
After 27712 training step(s), loss on training batch is 0.0033343.
After 27713 training step(s), loss on training batch is 0.00380467.
After 27714 training step(s), loss on training batch is 0.00320065.
After 27715 training step(s), loss on training batch is 0.00346647.
After 27716 training step(s), loss on training batch is 0.00323283.
After 27717 training step(s), loss on training batch is 0.00339148.
After 27718 training step(s), loss on training batch is 0.00341265.
After 27719 training step(s), loss on training batch is 0.00374518.
After 27720 training step(s), loss on training batch is 0.00346869.
After 27721 training step(s), loss on training batch is 0.00320793.
After 27722 training step(s), loss on training batch is 0.00303683.
After 27723 training step(s), loss on training batch is 0.00325071.
After 27724 training step(s), loss on training batch is 0.00334505.
After 27725 training step(s), loss on training batch is 0.0030629.
After 27726 training step(s), loss on training batch is 0.00294904.
After 27727 training step(s), loss on training batch is 0.00321681.
After 27728 training step(s), loss on training batch is 0.00307652.
After 27729 training step(s), loss on training batch is 0.00338665.
After 27730 training step(s), loss on training batch is 0.00333027.
After 27731 training step(s), loss on training batch is 0.004254.
After 27732 training step(s), loss on training batch is 0.00330233.
After 27733 training step(s), loss on training batch is 0.00362472.
After 27734 training step(s), loss on training batch is 0.00321304.
After 27735 training step(s), loss on training batch is 0.00381891.
After 27736 training step(s), loss on training batch is 0.00344372.
After 27737 training step(s), loss on training batch is 0.00367921.
After 27738 training step(s), loss on training batch is 0.00343163.
After 27739 training step(s), loss on training batch is 0.00324117.
After 27740 training step(s), loss on training batch is 0.00307993.
After 27741 training step(s), loss on training batch is 0.00318115.
After 27742 training step(s), loss on training batch is 0.00304436.
After 27743 training step(s), loss on training batch is 0.00343865.
After 27744 training step(s), loss on training batch is 0.00316728.
After 27745 training step(s), loss on training batch is 0.00369439.
After 27746 training step(s), loss on training batch is 0.0040266.
After 27747 training step(s), loss on training batch is 0.00375628.
After 27748 training step(s), loss on training batch is 0.00352786.
After 27749 training step(s), loss on training batch is 0.00323767.
After 27750 training step(s), loss on training batch is 0.00303334.
After 27751 training step(s), loss on training batch is 0.00311741.
After 27752 training step(s), loss on training batch is 0.00305128.
After 27753 training step(s), loss on training batch is 0.00316035.
After 27754 training step(s), loss on training batch is 0.00321581.
After 27755 training step(s), loss on training batch is 0.00319418.
After 27756 training step(s), loss on training batch is 0.00306608.
After 27757 training step(s), loss on training batch is 0.00321216.
After 27758 training step(s), loss on training batch is 0.00320386.
After 27759 training step(s), loss on training batch is 0.00295065.
After 27760 training step(s), loss on training batch is 0.00297745.
After 27761 training step(s), loss on training batch is 0.00329254.
After 27762 training step(s), loss on training batch is 0.00328371.
After 27763 training step(s), loss on training batch is 0.00348047.
After 27764 training step(s), loss on training batch is 0.00369297.
After 27765 training step(s), loss on training batch is 0.00297679.
After 27766 training step(s), loss on training batch is 0.00306533.
After 27767 training step(s), loss on training batch is 0.00326652.
After 27768 training step(s), loss on training batch is 0.00311337.
After 27769 training step(s), loss on training batch is 0.00326831.
After 27770 training step(s), loss on training batch is 0.0032154.
After 27771 training step(s), loss on training batch is 0.0032162.
After 27772 training step(s), loss on training batch is 0.0033558.
After 27773 training step(s), loss on training batch is 0.00316408.
After 27774 training step(s), loss on training batch is 0.00375561.
After 27775 training step(s), loss on training batch is 0.00334846.
After 27776 training step(s), loss on training batch is 0.0032842.
After 27777 training step(s), loss on training batch is 0.00300854.
After 27778 training step(s), loss on training batch is 0.00316956.
After 27779 training step(s), loss on training batch is 0.00334586.
After 27780 training step(s), loss on training batch is 0.00321349.
After 27781 training step(s), loss on training batch is 0.00316369.
After 27782 training step(s), loss on training batch is 0.00312365.
After 27783 training step(s), loss on training batch is 0.00335008.
After 27784 training step(s), loss on training batch is 0.00344925.
After 27785 training step(s), loss on training batch is 0.00330167.
After 27786 training step(s), loss on training batch is 0.00320752.
After 27787 training step(s), loss on training batch is 0.00339395.
After 27788 training step(s), loss on training batch is 0.00316792.
After 27789 training step(s), loss on training batch is 0.00335569.
After 27790 training step(s), loss on training batch is 0.00352854.
After 27791 training step(s), loss on training batch is 0.00320747.
After 27792 training step(s), loss on training batch is 0.00309547.
After 27793 training step(s), loss on training batch is 0.00329245.
After 27794 training step(s), loss on training batch is 0.00337197.
After 27795 training step(s), loss on training batch is 0.00341437.
After 27796 training step(s), loss on training batch is 0.00334775.
After 27797 training step(s), loss on training batch is 0.00315742.
After 27798 training step(s), loss on training batch is 0.00302585.
After 27799 training step(s), loss on training batch is 0.00332593.
After 27800 training step(s), loss on training batch is 0.00327306.
After 27801 training step(s), loss on training batch is 0.00346746.
After 27802 training step(s), loss on training batch is 0.00321888.
After 27803 training step(s), loss on training batch is 0.00323646.
After 27804 training step(s), loss on training batch is 0.00352092.
After 27805 training step(s), loss on training batch is 0.00322234.
After 27806 training step(s), loss on training batch is 0.00354163.
After 27807 training step(s), loss on training batch is 0.00307782.
After 27808 training step(s), loss on training batch is 0.00334445.
After 27809 training step(s), loss on training batch is 0.00306057.
After 27810 training step(s), loss on training batch is 0.00339098.
After 27811 training step(s), loss on training batch is 0.00310476.
After 27812 training step(s), loss on training batch is 0.00298385.
After 27813 training step(s), loss on training batch is 0.00321237.
After 27814 training step(s), loss on training batch is 0.00315836.
After 27815 training step(s), loss on training batch is 0.00317501.
After 27816 training step(s), loss on training batch is 0.00328.
After 27817 training step(s), loss on training batch is 0.00314658.
After 27818 training step(s), loss on training batch is 0.00323218.
After 27819 training step(s), loss on training batch is 0.00322111.
After 27820 training step(s), loss on training batch is 0.00330934.
After 27821 training step(s), loss on training batch is 0.00317797.
After 27822 training step(s), loss on training batch is 0.00357856.
After 27823 training step(s), loss on training batch is 0.0032646.
After 27824 training step(s), loss on training batch is 0.00389714.
After 27825 training step(s), loss on training batch is 0.00318719.
After 27826 training step(s), loss on training batch is 0.00357342.
After 27827 training step(s), loss on training batch is 0.0032625.
After 27828 training step(s), loss on training batch is 0.00343746.
After 27829 training step(s), loss on training batch is 0.00296492.
After 27830 training step(s), loss on training batch is 0.00323182.
After 27831 training step(s), loss on training batch is 0.00326196.
After 27832 training step(s), loss on training batch is 0.00315079.
After 27833 training step(s), loss on training batch is 0.00314207.
After 27834 training step(s), loss on training batch is 0.00300965.
After 27835 training step(s), loss on training batch is 0.00333929.
After 27836 training step(s), loss on training batch is 0.00316585.
After 27837 training step(s), loss on training batch is 0.00342156.
After 27838 training step(s), loss on training batch is 0.00327895.
After 27839 training step(s), loss on training batch is 0.00313303.
After 27840 training step(s), loss on training batch is 0.00358544.
After 27841 training step(s), loss on training batch is 0.00328926.
After 27842 training step(s), loss on training batch is 0.00313968.
After 27843 training step(s), loss on training batch is 0.0031823.
After 27844 training step(s), loss on training batch is 0.00324906.
After 27845 training step(s), loss on training batch is 0.00330566.
After 27846 training step(s), loss on training batch is 0.00332021.
After 27847 training step(s), loss on training batch is 0.00410525.
After 27848 training step(s), loss on training batch is 0.00328418.
After 27849 training step(s), loss on training batch is 0.00326212.
After 27850 training step(s), loss on training batch is 0.00330127.
After 27851 training step(s), loss on training batch is 0.00301518.
After 27852 training step(s), loss on training batch is 0.00307313.
After 27853 training step(s), loss on training batch is 0.00326718.
After 27854 training step(s), loss on training batch is 0.00333807.
After 27855 training step(s), loss on training batch is 0.00309821.
After 27856 training step(s), loss on training batch is 0.0032582.
After 27857 training step(s), loss on training batch is 0.00299052.
After 27858 training step(s), loss on training batch is 0.00311769.
After 27859 training step(s), loss on training batch is 0.00319434.
After 27860 training step(s), loss on training batch is 0.00311262.
After 27861 training step(s), loss on training batch is 0.0033156.
After 27862 training step(s), loss on training batch is 0.00370357.
After 27863 training step(s), loss on training batch is 0.0037592.
After 27864 training step(s), loss on training batch is 0.00337572.
After 27865 training step(s), loss on training batch is 0.00325688.
After 27866 training step(s), loss on training batch is 0.00467531.
After 27867 training step(s), loss on training batch is 0.0032332.
After 27868 training step(s), loss on training batch is 0.00331117.
After 27869 training step(s), loss on training batch is 0.00327061.
After 27870 training step(s), loss on training batch is 0.00338873.
After 27871 training step(s), loss on training batch is 0.00328066.
After 27872 training step(s), loss on training batch is 0.00318261.
After 27873 training step(s), loss on training batch is 0.0034337.
After 27874 training step(s), loss on training batch is 0.00323575.
After 27875 training step(s), loss on training batch is 0.0029656.
After 27876 training step(s), loss on training batch is 0.00345339.
After 27877 training step(s), loss on training batch is 0.00331234.
After 27878 training step(s), loss on training batch is 0.00347562.
After 27879 training step(s), loss on training batch is 0.00308133.
After 27880 training step(s), loss on training batch is 0.00357815.
After 27881 training step(s), loss on training batch is 0.00337086.
After 27882 training step(s), loss on training batch is 0.00328162.
After 27883 training step(s), loss on training batch is 0.00319226.
After 27884 training step(s), loss on training batch is 0.00337243.
After 27885 training step(s), loss on training batch is 0.00365094.
After 27886 training step(s), loss on training batch is 0.0041678.
After 27887 training step(s), loss on training batch is 0.00310025.
After 27888 training step(s), loss on training batch is 0.0031842.
After 27889 training step(s), loss on training batch is 0.0031492.
After 27890 training step(s), loss on training batch is 0.00326783.
After 27891 training step(s), loss on training batch is 0.00317393.
After 27892 training step(s), loss on training batch is 0.00321564.
After 27893 training step(s), loss on training batch is 0.00303882.
After 27894 training step(s), loss on training batch is 0.0032729.
After 27895 training step(s), loss on training batch is 0.00307373.
After 27896 training step(s), loss on training batch is 0.00334113.
After 27897 training step(s), loss on training batch is 0.00307732.
After 27898 training step(s), loss on training batch is 0.00340998.
After 27899 training step(s), loss on training batch is 0.00324247.
After 27900 training step(s), loss on training batch is 0.00361682.
After 27901 training step(s), loss on training batch is 0.0035877.
After 27902 training step(s), loss on training batch is 0.00328779.
After 27903 training step(s), loss on training batch is 0.00317245.
After 27904 training step(s), loss on training batch is 0.00319592.
After 27905 training step(s), loss on training batch is 0.00323937.
After 27906 training step(s), loss on training batch is 0.0031049.
After 27907 training step(s), loss on training batch is 0.00329481.
After 27908 training step(s), loss on training batch is 0.00318812.
After 27909 training step(s), loss on training batch is 0.00350352.
After 27910 training step(s), loss on training batch is 0.00289274.
After 27911 training step(s), loss on training batch is 0.00302273.
After 27912 training step(s), loss on training batch is 0.0033775.
After 27913 training step(s), loss on training batch is 0.00313319.
After 27914 training step(s), loss on training batch is 0.00327328.
After 27915 training step(s), loss on training batch is 0.0030082.
After 27916 training step(s), loss on training batch is 0.00295873.
After 27917 training step(s), loss on training batch is 0.00318068.
After 27918 training step(s), loss on training batch is 0.0032267.
After 27919 training step(s), loss on training batch is 0.00318594.
After 27920 training step(s), loss on training batch is 0.00368206.
After 27921 training step(s), loss on training batch is 0.00306243.
After 27922 training step(s), loss on training batch is 0.00330432.
After 27923 training step(s), loss on training batch is 0.00322293.
After 27924 training step(s), loss on training batch is 0.00315052.
After 27925 training step(s), loss on training batch is 0.00311789.
After 27926 training step(s), loss on training batch is 0.00328086.
After 27927 training step(s), loss on training batch is 0.00334979.
After 27928 training step(s), loss on training batch is 0.00300127.
After 27929 training step(s), loss on training batch is 0.00318723.
After 27930 training step(s), loss on training batch is 0.00338606.
After 27931 training step(s), loss on training batch is 0.00311215.
After 27932 training step(s), loss on training batch is 0.00344752.
After 27933 training step(s), loss on training batch is 0.00411375.
After 27934 training step(s), loss on training batch is 0.00303623.
After 27935 training step(s), loss on training batch is 0.00326528.
After 27936 training step(s), loss on training batch is 0.00307583.
After 27937 training step(s), loss on training batch is 0.00330755.
After 27938 training step(s), loss on training batch is 0.00304826.
After 27939 training step(s), loss on training batch is 0.00311247.
After 27940 training step(s), loss on training batch is 0.00302821.
After 27941 training step(s), loss on training batch is 0.00360009.
After 27942 training step(s), loss on training batch is 0.00329626.
After 27943 training step(s), loss on training batch is 0.00298398.
After 27944 training step(s), loss on training batch is 0.00310893.
After 27945 training step(s), loss on training batch is 0.00379046.
After 27946 training step(s), loss on training batch is 0.00336826.
After 27947 training step(s), loss on training batch is 0.00342783.
After 27948 training step(s), loss on training batch is 0.00334674.
After 27949 training step(s), loss on training batch is 0.00317379.
After 27950 training step(s), loss on training batch is 0.00324476.
After 27951 training step(s), loss on training batch is 0.0031947.
After 27952 training step(s), loss on training batch is 0.00334585.
After 27953 training step(s), loss on training batch is 0.00321877.
After 27954 training step(s), loss on training batch is 0.00308902.
After 27955 training step(s), loss on training batch is 0.00345059.
After 27956 training step(s), loss on training batch is 0.00345633.
After 27957 training step(s), loss on training batch is 0.00308709.
After 27958 training step(s), loss on training batch is 0.00315247.
After 27959 training step(s), loss on training batch is 0.00309382.
After 27960 training step(s), loss on training batch is 0.00315322.
After 27961 training step(s), loss on training batch is 0.00392179.
After 27962 training step(s), loss on training batch is 0.00348468.
After 27963 training step(s), loss on training batch is 0.00314893.
After 27964 training step(s), loss on training batch is 0.00316206.
After 27965 training step(s), loss on training batch is 0.00328734.
After 27966 training step(s), loss on training batch is 0.00345326.
After 27967 training step(s), loss on training batch is 0.00351061.
After 27968 training step(s), loss on training batch is 0.00306295.
After 27969 training step(s), loss on training batch is 0.00311352.
After 27970 training step(s), loss on training batch is 0.0031386.
After 27971 training step(s), loss on training batch is 0.0037631.
After 27972 training step(s), loss on training batch is 0.00306928.
After 27973 training step(s), loss on training batch is 0.00330381.
After 27974 training step(s), loss on training batch is 0.00311894.
After 27975 training step(s), loss on training batch is 0.00334308.
After 27976 training step(s), loss on training batch is 0.00294565.
After 27977 training step(s), loss on training batch is 0.00307986.
After 27978 training step(s), loss on training batch is 0.00304061.
After 27979 training step(s), loss on training batch is 0.0030303.
After 27980 training step(s), loss on training batch is 0.00330878.
After 27981 training step(s), loss on training batch is 0.00339826.
After 27982 training step(s), loss on training batch is 0.00332974.
After 27983 training step(s), loss on training batch is 0.00349126.
After 27984 training step(s), loss on training batch is 0.0033746.
After 27985 training step(s), loss on training batch is 0.00319333.
After 27986 training step(s), loss on training batch is 0.00349835.
After 27987 training step(s), loss on training batch is 0.00371517.
After 27988 training step(s), loss on training batch is 0.00343677.
After 27989 training step(s), loss on training batch is 0.00330763.
After 27990 training step(s), loss on training batch is 0.00320745.
After 27991 training step(s), loss on training batch is 0.0042421.
After 27992 training step(s), loss on training batch is 0.00295655.
After 27993 training step(s), loss on training batch is 0.0034332.
After 27994 training step(s), loss on training batch is 0.00303901.
After 27995 training step(s), loss on training batch is 0.00299947.
After 27996 training step(s), loss on training batch is 0.00310855.
After 27997 training step(s), loss on training batch is 0.00301098.
After 27998 training step(s), loss on training batch is 0.00298672.
After 27999 training step(s), loss on training batch is 0.00319193.
After 28000 training step(s), loss on training batch is 0.00323707.
After 28001 training step(s), loss on training batch is 0.00323436.
After 28002 training step(s), loss on training batch is 0.00303436.
After 28003 training step(s), loss on training batch is 0.00364378.
After 28004 training step(s), loss on training batch is 0.00358658.
After 28005 training step(s), loss on training batch is 0.00382722.
After 28006 training step(s), loss on training batch is 0.00294372.
After 28007 training step(s), loss on training batch is 0.00314016.
After 28008 training step(s), loss on training batch is 0.00300576.
After 28009 training step(s), loss on training batch is 0.00331398.
After 28010 training step(s), loss on training batch is 0.00339068.
After 28011 training step(s), loss on training batch is 0.00309783.
After 28012 training step(s), loss on training batch is 0.00328254.
After 28013 training step(s), loss on training batch is 0.00318859.
After 28014 training step(s), loss on training batch is 0.00314533.
After 28015 training step(s), loss on training batch is 0.00343268.
After 28016 training step(s), loss on training batch is 0.00378549.
After 28017 training step(s), loss on training batch is 0.00315494.
After 28018 training step(s), loss on training batch is 0.00358257.
After 28019 training step(s), loss on training batch is 0.00318293.
After 28020 training step(s), loss on training batch is 0.00300899.
After 28021 training step(s), loss on training batch is 0.00330023.
After 28022 training step(s), loss on training batch is 0.00347505.
After 28023 training step(s), loss on training batch is 0.00325508.
After 28024 training step(s), loss on training batch is 0.00302919.
After 28025 training step(s), loss on training batch is 0.00363621.
After 28026 training step(s), loss on training batch is 0.00289803.
After 28027 training step(s), loss on training batch is 0.00322713.
After 28028 training step(s), loss on training batch is 0.00352791.
After 28029 training step(s), loss on training batch is 0.00388598.
After 28030 training step(s), loss on training batch is 0.00315428.
After 28031 training step(s), loss on training batch is 0.00307389.
After 28032 training step(s), loss on training batch is 0.00331092.
After 28033 training step(s), loss on training batch is 0.00333665.
After 28034 training step(s), loss on training batch is 0.00364161.
After 28035 training step(s), loss on training batch is 0.00384307.
After 28036 training step(s), loss on training batch is 0.00314692.
After 28037 training step(s), loss on training batch is 0.00329109.
After 28038 training step(s), loss on training batch is 0.0032018.
After 28039 training step(s), loss on training batch is 0.00307897.
After 28040 training step(s), loss on training batch is 0.00306878.
After 28041 training step(s), loss on training batch is 0.0029013.
After 28042 training step(s), loss on training batch is 0.00316008.
After 28043 training step(s), loss on training batch is 0.00333115.
After 28044 training step(s), loss on training batch is 0.0032589.
After 28045 training step(s), loss on training batch is 0.00359499.
After 28046 training step(s), loss on training batch is 0.00465653.
After 28047 training step(s), loss on training batch is 0.00352374.
After 28048 training step(s), loss on training batch is 0.00300203.
After 28049 training step(s), loss on training batch is 0.00350177.
After 28050 training step(s), loss on training batch is 0.00361188.
After 28051 training step(s), loss on training batch is 0.00329374.
After 28052 training step(s), loss on training batch is 0.00339799.
After 28053 training step(s), loss on training batch is 0.0031875.
After 28054 training step(s), loss on training batch is 0.00308741.
After 28055 training step(s), loss on training batch is 0.00331981.
After 28056 training step(s), loss on training batch is 0.00329996.
After 28057 training step(s), loss on training batch is 0.00305752.
After 28058 training step(s), loss on training batch is 0.00305238.
After 28059 training step(s), loss on training batch is 0.00349712.
After 28060 training step(s), loss on training batch is 0.00318324.
After 28061 training step(s), loss on training batch is 0.00345266.
After 28062 training step(s), loss on training batch is 0.00327734.
After 28063 training step(s), loss on training batch is 0.00300196.
After 28064 training step(s), loss on training batch is 0.00308659.
After 28065 training step(s), loss on training batch is 0.00346494.
After 28066 training step(s), loss on training batch is 0.00320868.
After 28067 training step(s), loss on training batch is 0.00325017.
After 28068 training step(s), loss on training batch is 0.00310877.
After 28069 training step(s), loss on training batch is 0.00300461.
After 28070 training step(s), loss on training batch is 0.00342635.
After 28071 training step(s), loss on training batch is 0.00302291.
After 28072 training step(s), loss on training batch is 0.00336453.
After 28073 training step(s), loss on training batch is 0.0033401.
After 28074 training step(s), loss on training batch is 0.00318461.
After 28075 training step(s), loss on training batch is 0.00309938.
After 28076 training step(s), loss on training batch is 0.00318083.
After 28077 training step(s), loss on training batch is 0.00312637.
After 28078 training step(s), loss on training batch is 0.00337.
After 28079 training step(s), loss on training batch is 0.00327692.
After 28080 training step(s), loss on training batch is 0.00336861.
After 28081 training step(s), loss on training batch is 0.00316063.
After 28082 training step(s), loss on training batch is 0.00304482.
After 28083 training step(s), loss on training batch is 0.00369357.
After 28084 training step(s), loss on training batch is 0.00331997.
After 28085 training step(s), loss on training batch is 0.00340326.
After 28086 training step(s), loss on training batch is 0.00295834.
After 28087 training step(s), loss on training batch is 0.00342671.
After 28088 training step(s), loss on training batch is 0.00334375.
After 28089 training step(s), loss on training batch is 0.00353424.
After 28090 training step(s), loss on training batch is 0.00329381.
After 28091 training step(s), loss on training batch is 0.00330007.
After 28092 training step(s), loss on training batch is 0.0030936.
After 28093 training step(s), loss on training batch is 0.00303823.
After 28094 training step(s), loss on training batch is 0.00338204.
After 28095 training step(s), loss on training batch is 0.00402846.
After 28096 training step(s), loss on training batch is 0.00305642.
After 28097 training step(s), loss on training batch is 0.00326928.
After 28098 training step(s), loss on training batch is 0.00303723.
After 28099 training step(s), loss on training batch is 0.00382361.
After 28100 training step(s), loss on training batch is 0.00318166.
After 28101 training step(s), loss on training batch is 0.00339683.
After 28102 training step(s), loss on training batch is 0.00336435.
After 28103 training step(s), loss on training batch is 0.00331364.
After 28104 training step(s), loss on training batch is 0.00311134.
After 28105 training step(s), loss on training batch is 0.00313672.
After 28106 training step(s), loss on training batch is 0.0032998.
After 28107 training step(s), loss on training batch is 0.00318142.
After 28108 training step(s), loss on training batch is 0.00323546.
After 28109 training step(s), loss on training batch is 0.00299015.
After 28110 training step(s), loss on training batch is 0.00337212.
After 28111 training step(s), loss on training batch is 0.00335814.
After 28112 training step(s), loss on training batch is 0.00332035.
After 28113 training step(s), loss on training batch is 0.00354533.
After 28114 training step(s), loss on training batch is 0.00347107.
After 28115 training step(s), loss on training batch is 0.0030184.
After 28116 training step(s), loss on training batch is 0.00316029.
After 28117 training step(s), loss on training batch is 0.00322265.
After 28118 training step(s), loss on training batch is 0.00314758.
After 28119 training step(s), loss on training batch is 0.00300787.
After 28120 training step(s), loss on training batch is 0.00329031.
After 28121 training step(s), loss on training batch is 0.00326677.
After 28122 training step(s), loss on training batch is 0.00339079.
After 28123 training step(s), loss on training batch is 0.00316775.
After 28124 training step(s), loss on training batch is 0.00333719.
After 28125 training step(s), loss on training batch is 0.00306243.
After 28126 training step(s), loss on training batch is 0.0034572.
After 28127 training step(s), loss on training batch is 0.00314772.
After 28128 training step(s), loss on training batch is 0.00357823.
After 28129 training step(s), loss on training batch is 0.00323085.
After 28130 training step(s), loss on training batch is 0.00302911.
After 28131 training step(s), loss on training batch is 0.00331003.
After 28132 training step(s), loss on training batch is 0.00341807.
After 28133 training step(s), loss on training batch is 0.00340991.
After 28134 training step(s), loss on training batch is 0.00321996.
After 28135 training step(s), loss on training batch is 0.00329615.
After 28136 training step(s), loss on training batch is 0.00303444.
After 28137 training step(s), loss on training batch is 0.0032778.
After 28138 training step(s), loss on training batch is 0.00352659.
After 28139 training step(s), loss on training batch is 0.00300871.
After 28140 training step(s), loss on training batch is 0.00326081.
After 28141 training step(s), loss on training batch is 0.00316772.
After 28142 training step(s), loss on training batch is 0.00303332.
After 28143 training step(s), loss on training batch is 0.00312502.
After 28144 training step(s), loss on training batch is 0.00310532.
After 28145 training step(s), loss on training batch is 0.00375364.
After 28146 training step(s), loss on training batch is 0.00347593.
After 28147 training step(s), loss on training batch is 0.00334941.
After 28148 training step(s), loss on training batch is 0.00368555.
After 28149 training step(s), loss on training batch is 0.00315379.
After 28150 training step(s), loss on training batch is 0.00322394.
After 28151 training step(s), loss on training batch is 0.00306581.
After 28152 training step(s), loss on training batch is 0.00328498.
After 28153 training step(s), loss on training batch is 0.00323803.
After 28154 training step(s), loss on training batch is 0.00315815.
After 28155 training step(s), loss on training batch is 0.00353053.
After 28156 training step(s), loss on training batch is 0.00294221.
After 28157 training step(s), loss on training batch is 0.00320749.
After 28158 training step(s), loss on training batch is 0.00309604.
After 28159 training step(s), loss on training batch is 0.00307008.
After 28160 training step(s), loss on training batch is 0.00311435.
After 28161 training step(s), loss on training batch is 0.00333583.
After 28162 training step(s), loss on training batch is 0.00316075.
After 28163 training step(s), loss on training batch is 0.00315199.
After 28164 training step(s), loss on training batch is 0.00308128.
After 28165 training step(s), loss on training batch is 0.00295728.
After 28166 training step(s), loss on training batch is 0.00329591.
After 28167 training step(s), loss on training batch is 0.00360425.
After 28168 training step(s), loss on training batch is 0.0032506.
After 28169 training step(s), loss on training batch is 0.003432.
After 28170 training step(s), loss on training batch is 0.00307709.
After 28171 training step(s), loss on training batch is 0.00334333.
After 28172 training step(s), loss on training batch is 0.00323096.
After 28173 training step(s), loss on training batch is 0.00356722.
After 28174 training step(s), loss on training batch is 0.00342409.
After 28175 training step(s), loss on training batch is 0.00349724.
After 28176 training step(s), loss on training batch is 0.0031427.
After 28177 training step(s), loss on training batch is 0.00324885.
After 28178 training step(s), loss on training batch is 0.0032074.
After 28179 training step(s), loss on training batch is 0.00379721.
After 28180 training step(s), loss on training batch is 0.00303771.
After 28181 training step(s), loss on training batch is 0.00361494.
After 28182 training step(s), loss on training batch is 0.00369772.
After 28183 training step(s), loss on training batch is 0.00345836.
After 28184 training step(s), loss on training batch is 0.00344129.
After 28185 training step(s), loss on training batch is 0.00348146.
After 28186 training step(s), loss on training batch is 0.0031858.
After 28187 training step(s), loss on training batch is 0.00312297.
After 28188 training step(s), loss on training batch is 0.0032708.
After 28189 training step(s), loss on training batch is 0.00304519.
After 28190 training step(s), loss on training batch is 0.00330207.
After 28191 training step(s), loss on training batch is 0.00304831.
After 28192 training step(s), loss on training batch is 0.00341664.
After 28193 training step(s), loss on training batch is 0.00333906.
After 28194 training step(s), loss on training batch is 0.00315175.
After 28195 training step(s), loss on training batch is 0.00415308.
After 28196 training step(s), loss on training batch is 0.00307567.
After 28197 training step(s), loss on training batch is 0.00315202.
After 28198 training step(s), loss on training batch is 0.00350157.
After 28199 training step(s), loss on training batch is 0.00381899.
After 28200 training step(s), loss on training batch is 0.00349921.
After 28201 training step(s), loss on training batch is 0.0032258.
After 28202 training step(s), loss on training batch is 0.00293626.
After 28203 training step(s), loss on training batch is 0.00306367.
After 28204 training step(s), loss on training batch is 0.00307004.
After 28205 training step(s), loss on training batch is 0.00411078.
After 28206 training step(s), loss on training batch is 0.00318211.
After 28207 training step(s), loss on training batch is 0.00349152.
After 28208 training step(s), loss on training batch is 0.00308838.
After 28209 training step(s), loss on training batch is 0.00336526.
After 28210 training step(s), loss on training batch is 0.00298542.
After 28211 training step(s), loss on training batch is 0.00330706.
After 28212 training step(s), loss on training batch is 0.00303032.
After 28213 training step(s), loss on training batch is 0.00310519.
After 28214 training step(s), loss on training batch is 0.00319846.
After 28215 training step(s), loss on training batch is 0.00347396.
After 28216 training step(s), loss on training batch is 0.00327554.
After 28217 training step(s), loss on training batch is 0.00299611.
After 28218 training step(s), loss on training batch is 0.00321847.
After 28219 training step(s), loss on training batch is 0.00335967.
After 28220 training step(s), loss on training batch is 0.00314778.
After 28221 training step(s), loss on training batch is 0.00378755.
After 28222 training step(s), loss on training batch is 0.00342518.
After 28223 training step(s), loss on training batch is 0.00307632.
After 28224 training step(s), loss on training batch is 0.00330777.
After 28225 training step(s), loss on training batch is 0.00303343.
After 28226 training step(s), loss on training batch is 0.00316885.
After 28227 training step(s), loss on training batch is 0.00306796.
After 28228 training step(s), loss on training batch is 0.00305721.
After 28229 training step(s), loss on training batch is 0.00312164.
After 28230 training step(s), loss on training batch is 0.00304344.
After 28231 training step(s), loss on training batch is 0.00322406.
After 28232 training step(s), loss on training batch is 0.00356258.
After 28233 training step(s), loss on training batch is 0.00327792.
After 28234 training step(s), loss on training batch is 0.00311529.
After 28235 training step(s), loss on training batch is 0.0032899.
After 28236 training step(s), loss on training batch is 0.00371516.
After 28237 training step(s), loss on training batch is 0.00312376.
After 28238 training step(s), loss on training batch is 0.00348532.
After 28239 training step(s), loss on training batch is 0.00302464.
After 28240 training step(s), loss on training batch is 0.00354652.
After 28241 training step(s), loss on training batch is 0.00302747.
After 28242 training step(s), loss on training batch is 0.00345473.
After 28243 training step(s), loss on training batch is 0.00319757.
After 28244 training step(s), loss on training batch is 0.00293843.
After 28245 training step(s), loss on training batch is 0.00308388.
After 28246 training step(s), loss on training batch is 0.00309589.
After 28247 training step(s), loss on training batch is 0.00413812.
After 28248 training step(s), loss on training batch is 0.00313623.
After 28249 training step(s), loss on training batch is 0.00299232.
After 28250 training step(s), loss on training batch is 0.00319666.
After 28251 training step(s), loss on training batch is 0.00325651.
After 28252 training step(s), loss on training batch is 0.0032073.
After 28253 training step(s), loss on training batch is 0.00364835.
After 28254 training step(s), loss on training batch is 0.00320478.
After 28255 training step(s), loss on training batch is 0.00289928.
After 28256 training step(s), loss on training batch is 0.00337694.
After 28257 training step(s), loss on training batch is 0.00331258.
After 28258 training step(s), loss on training batch is 0.00298543.
After 28259 training step(s), loss on training batch is 0.0032832.
After 28260 training step(s), loss on training batch is 0.00321537.
After 28261 training step(s), loss on training batch is 0.00294096.
After 28262 training step(s), loss on training batch is 0.00324168.
After 28263 training step(s), loss on training batch is 0.00291273.
After 28264 training step(s), loss on training batch is 0.00293199.
After 28265 training step(s), loss on training batch is 0.00331096.
After 28266 training step(s), loss on training batch is 0.00300861.
After 28267 training step(s), loss on training batch is 0.00316346.
After 28268 training step(s), loss on training batch is 0.00297436.
After 28269 training step(s), loss on training batch is 0.00331931.
After 28270 training step(s), loss on training batch is 0.00307186.
After 28271 training step(s), loss on training batch is 0.00368467.
After 28272 training step(s), loss on training batch is 0.00310503.
After 28273 training step(s), loss on training batch is 0.00325218.
After 28274 training step(s), loss on training batch is 0.00331058.
After 28275 training step(s), loss on training batch is 0.0030804.
After 28276 training step(s), loss on training batch is 0.00357255.
After 28277 training step(s), loss on training batch is 0.00356589.
After 28278 training step(s), loss on training batch is 0.00300127.
After 28279 training step(s), loss on training batch is 0.00346392.
After 28280 training step(s), loss on training batch is 0.00336511.
After 28281 training step(s), loss on training batch is 0.00324395.
After 28282 training step(s), loss on training batch is 0.00328504.
After 28283 training step(s), loss on training batch is 0.0033596.
After 28284 training step(s), loss on training batch is 0.00334424.
After 28285 training step(s), loss on training batch is 0.00332969.
After 28286 training step(s), loss on training batch is 0.00330104.
After 28287 training step(s), loss on training batch is 0.00334705.
After 28288 training step(s), loss on training batch is 0.00357627.
After 28289 training step(s), loss on training batch is 0.00352221.
After 28290 training step(s), loss on training batch is 0.00301831.
After 28291 training step(s), loss on training batch is 0.00310284.
After 28292 training step(s), loss on training batch is 0.00299086.
After 28293 training step(s), loss on training batch is 0.00335997.
After 28294 training step(s), loss on training batch is 0.00360379.
After 28295 training step(s), loss on training batch is 0.00302674.
After 28296 training step(s), loss on training batch is 0.00336619.
After 28297 training step(s), loss on training batch is 0.00313329.
After 28298 training step(s), loss on training batch is 0.00362147.
After 28299 training step(s), loss on training batch is 0.00317906.
After 28300 training step(s), loss on training batch is 0.00325625.
After 28301 training step(s), loss on training batch is 0.00308933.
After 28302 training step(s), loss on training batch is 0.00330867.
After 28303 training step(s), loss on training batch is 0.00307305.
After 28304 training step(s), loss on training batch is 0.00316741.
After 28305 training step(s), loss on training batch is 0.0030469.
After 28306 training step(s), loss on training batch is 0.00330154.
After 28307 training step(s), loss on training batch is 0.00359488.
After 28308 training step(s), loss on training batch is 0.0035136.
After 28309 training step(s), loss on training batch is 0.00332159.
After 28310 training step(s), loss on training batch is 0.00320546.
After 28311 training step(s), loss on training batch is 0.00313595.
After 28312 training step(s), loss on training batch is 0.00345453.
After 28313 training step(s), loss on training batch is 0.00302344.
After 28314 training step(s), loss on training batch is 0.00326732.
After 28315 training step(s), loss on training batch is 0.00359234.
After 28316 training step(s), loss on training batch is 0.00333545.
After 28317 training step(s), loss on training batch is 0.00328881.
After 28318 training step(s), loss on training batch is 0.00346334.
After 28319 training step(s), loss on training batch is 0.00332453.
After 28320 training step(s), loss on training batch is 0.00366305.
After 28321 training step(s), loss on training batch is 0.00318965.
After 28322 training step(s), loss on training batch is 0.00368611.
After 28323 training step(s), loss on training batch is 0.0030477.
After 28324 training step(s), loss on training batch is 0.00342087.
After 28325 training step(s), loss on training batch is 0.00475074.
After 28326 training step(s), loss on training batch is 0.00311926.
After 28327 training step(s), loss on training batch is 0.00338898.
After 28328 training step(s), loss on training batch is 0.00333448.
After 28329 training step(s), loss on training batch is 0.00346904.
After 28330 training step(s), loss on training batch is 0.00359706.
After 28331 training step(s), loss on training batch is 0.00335365.
After 28332 training step(s), loss on training batch is 0.00301838.
After 28333 training step(s), loss on training batch is 0.00310014.
After 28334 training step(s), loss on training batch is 0.0031234.
After 28335 training step(s), loss on training batch is 0.0031326.
After 28336 training step(s), loss on training batch is 0.00366237.
After 28337 training step(s), loss on training batch is 0.00322107.
After 28338 training step(s), loss on training batch is 0.00309579.
After 28339 training step(s), loss on training batch is 0.00302722.
After 28340 training step(s), loss on training batch is 0.00302454.
After 28341 training step(s), loss on training batch is 0.003188.
After 28342 training step(s), loss on training batch is 0.00317389.
After 28343 training step(s), loss on training batch is 0.00320392.
After 28344 training step(s), loss on training batch is 0.00319028.
After 28345 training step(s), loss on training batch is 0.00334726.
After 28346 training step(s), loss on training batch is 0.00334813.
After 28347 training step(s), loss on training batch is 0.00360073.
After 28348 training step(s), loss on training batch is 0.00292433.
After 28349 training step(s), loss on training batch is 0.00341763.
After 28350 training step(s), loss on training batch is 0.00294725.
After 28351 training step(s), loss on training batch is 0.00330901.
After 28352 training step(s), loss on training batch is 0.00351298.
After 28353 training step(s), loss on training batch is 0.00312148.
After 28354 training step(s), loss on training batch is 0.00323161.
After 28355 training step(s), loss on training batch is 0.00344588.
After 28356 training step(s), loss on training batch is 0.00306968.
After 28357 training step(s), loss on training batch is 0.00289629.
After 28358 training step(s), loss on training batch is 0.00310434.
After 28359 training step(s), loss on training batch is 0.0034113.
After 28360 training step(s), loss on training batch is 0.00300903.
After 28361 training step(s), loss on training batch is 0.00348823.
After 28362 training step(s), loss on training batch is 0.00313214.
After 28363 training step(s), loss on training batch is 0.0030455.
After 28364 training step(s), loss on training batch is 0.00390733.
After 28365 training step(s), loss on training batch is 0.00364302.
After 28366 training step(s), loss on training batch is 0.00356053.
After 28367 training step(s), loss on training batch is 0.00332725.
After 28368 training step(s), loss on training batch is 0.00314488.
After 28369 training step(s), loss on training batch is 0.00346683.
After 28370 training step(s), loss on training batch is 0.00305633.
After 28371 training step(s), loss on training batch is 0.00302721.
After 28372 training step(s), loss on training batch is 0.00310236.
After 28373 training step(s), loss on training batch is 0.00380676.
After 28374 training step(s), loss on training batch is 0.00325108.
After 28375 training step(s), loss on training batch is 0.00340925.
After 28376 training step(s), loss on training batch is 0.00295915.
After 28377 training step(s), loss on training batch is 0.00293174.
After 28378 training step(s), loss on training batch is 0.00299774.
After 28379 training step(s), loss on training batch is 0.00338992.
After 28380 training step(s), loss on training batch is 0.00322668.
After 28381 training step(s), loss on training batch is 0.00417503.
After 28382 training step(s), loss on training batch is 0.00296466.
After 28383 training step(s), loss on training batch is 0.0033886.
After 28384 training step(s), loss on training batch is 0.00328698.
After 28385 training step(s), loss on training batch is 0.0031599.
After 28386 training step(s), loss on training batch is 0.00314817.
After 28387 training step(s), loss on training batch is 0.00310254.
After 28388 training step(s), loss on training batch is 0.00367362.
After 28389 training step(s), loss on training batch is 0.00306879.
After 28390 training step(s), loss on training batch is 0.00293815.
After 28391 training step(s), loss on training batch is 0.00303293.
After 28392 training step(s), loss on training batch is 0.00333013.
After 28393 training step(s), loss on training batch is 0.00312037.
After 28394 training step(s), loss on training batch is 0.00320413.
After 28395 training step(s), loss on training batch is 0.003128.
After 28396 training step(s), loss on training batch is 0.00299848.
After 28397 training step(s), loss on training batch is 0.00309525.
After 28398 training step(s), loss on training batch is 0.00309187.
After 28399 training step(s), loss on training batch is 0.00344039.
After 28400 training step(s), loss on training batch is 0.00351027.
After 28401 training step(s), loss on training batch is 0.00313577.
After 28402 training step(s), loss on training batch is 0.00306576.
After 28403 training step(s), loss on training batch is 0.00334495.
After 28404 training step(s), loss on training batch is 0.0034515.
After 28405 training step(s), loss on training batch is 0.00303141.
After 28406 training step(s), loss on training batch is 0.0029207.
After 28407 training step(s), loss on training batch is 0.00344021.
After 28408 training step(s), loss on training batch is 0.00305461.
After 28409 training step(s), loss on training batch is 0.00313695.
After 28410 training step(s), loss on training batch is 0.00299682.
After 28411 training step(s), loss on training batch is 0.00330929.
After 28412 training step(s), loss on training batch is 0.00341122.
After 28413 training step(s), loss on training batch is 0.0030363.
After 28414 training step(s), loss on training batch is 0.00350683.
After 28415 training step(s), loss on training batch is 0.00303509.
After 28416 training step(s), loss on training batch is 0.0036962.
After 28417 training step(s), loss on training batch is 0.00318686.
After 28418 training step(s), loss on training batch is 0.00326607.
After 28419 training step(s), loss on training batch is 0.00302913.
After 28420 training step(s), loss on training batch is 0.0033425.
After 28421 training step(s), loss on training batch is 0.00345855.
After 28422 training step(s), loss on training batch is 0.0031132.
After 28423 training step(s), loss on training batch is 0.00312988.
After 28424 training step(s), loss on training batch is 0.0032444.
After 28425 training step(s), loss on training batch is 0.00330882.
After 28426 training step(s), loss on training batch is 0.00354662.
After 28427 training step(s), loss on training batch is 0.00386686.
After 28428 training step(s), loss on training batch is 0.0033086.
After 28429 training step(s), loss on training batch is 0.0038161.
After 28430 training step(s), loss on training batch is 0.00297575.
After 28431 training step(s), loss on training batch is 0.00308566.
After 28432 training step(s), loss on training batch is 0.0031614.
After 28433 training step(s), loss on training batch is 0.00327777.
After 28434 training step(s), loss on training batch is 0.00307969.
After 28435 training step(s), loss on training batch is 0.00323244.
After 28436 training step(s), loss on training batch is 0.00306279.
After 28437 training step(s), loss on training batch is 0.00361649.
After 28438 training step(s), loss on training batch is 0.00300755.
After 28439 training step(s), loss on training batch is 0.0030984.
After 28440 training step(s), loss on training batch is 0.00331744.
After 28441 training step(s), loss on training batch is 0.00327649.
After 28442 training step(s), loss on training batch is 0.00308088.
After 28443 training step(s), loss on training batch is 0.00326609.
After 28444 training step(s), loss on training batch is 0.00301217.
After 28445 training step(s), loss on training batch is 0.00357552.
After 28446 training step(s), loss on training batch is 0.00344991.
After 28447 training step(s), loss on training batch is 0.00305479.
After 28448 training step(s), loss on training batch is 0.00329748.
After 28449 training step(s), loss on training batch is 0.00316496.
After 28450 training step(s), loss on training batch is 0.00339919.
After 28451 training step(s), loss on training batch is 0.00301774.
After 28452 training step(s), loss on training batch is 0.0034106.
After 28453 training step(s), loss on training batch is 0.00298334.
After 28454 training step(s), loss on training batch is 0.00317858.
After 28455 training step(s), loss on training batch is 0.00327766.
After 28456 training step(s), loss on training batch is 0.00303953.
After 28457 training step(s), loss on training batch is 0.00293938.
After 28458 training step(s), loss on training batch is 0.00338739.
After 28459 training step(s), loss on training batch is 0.00310174.
After 28460 training step(s), loss on training batch is 0.00326605.
After 28461 training step(s), loss on training batch is 0.00317799.
After 28462 training step(s), loss on training batch is 0.00365118.
After 28463 training step(s), loss on training batch is 0.00326707.
After 28464 training step(s), loss on training batch is 0.00345041.
After 28465 training step(s), loss on training batch is 0.00353882.
After 28466 training step(s), loss on training batch is 0.00336466.
After 28467 training step(s), loss on training batch is 0.00333874.
After 28468 training step(s), loss on training batch is 0.00313595.
After 28469 training step(s), loss on training batch is 0.00313137.
After 28470 training step(s), loss on training batch is 0.00446987.
After 28471 training step(s), loss on training batch is 0.0034122.
After 28472 training step(s), loss on training batch is 0.00358741.
After 28473 training step(s), loss on training batch is 0.00329404.
After 28474 training step(s), loss on training batch is 0.0031085.
After 28475 training step(s), loss on training batch is 0.00324798.
After 28476 training step(s), loss on training batch is 0.0032363.
After 28477 training step(s), loss on training batch is 0.00296646.
After 28478 training step(s), loss on training batch is 0.00366045.
After 28479 training step(s), loss on training batch is 0.00299866.
After 28480 training step(s), loss on training batch is 0.00313523.
After 28481 training step(s), loss on training batch is 0.00311221.
After 28482 training step(s), loss on training batch is 0.00294468.
After 28483 training step(s), loss on training batch is 0.00313123.
After 28484 training step(s), loss on training batch is 0.00303137.
After 28485 training step(s), loss on training batch is 0.00333436.
After 28486 training step(s), loss on training batch is 0.00327479.
After 28487 training step(s), loss on training batch is 0.00343881.
After 28488 training step(s), loss on training batch is 0.00339257.
After 28489 training step(s), loss on training batch is 0.00347314.
After 28490 training step(s), loss on training batch is 0.00357948.
After 28491 training step(s), loss on training batch is 0.00339765.
After 28492 training step(s), loss on training batch is 0.00333258.
After 28493 training step(s), loss on training batch is 0.00301164.
After 28494 training step(s), loss on training batch is 0.00304028.
After 28495 training step(s), loss on training batch is 0.00355019.
After 28496 training step(s), loss on training batch is 0.00317853.
After 28497 training step(s), loss on training batch is 0.00314492.
After 28498 training step(s), loss on training batch is 0.00363041.
After 28499 training step(s), loss on training batch is 0.00319175.
After 28500 training step(s), loss on training batch is 0.00298889.
After 28501 training step(s), loss on training batch is 0.00313118.
After 28502 training step(s), loss on training batch is 0.00298.
After 28503 training step(s), loss on training batch is 0.00323064.
After 28504 training step(s), loss on training batch is 0.0029186.
After 28505 training step(s), loss on training batch is 0.00345189.
After 28506 training step(s), loss on training batch is 0.00333076.
After 28507 training step(s), loss on training batch is 0.00321636.
After 28508 training step(s), loss on training batch is 0.00322772.
After 28509 training step(s), loss on training batch is 0.00337769.
After 28510 training step(s), loss on training batch is 0.00333868.
After 28511 training step(s), loss on training batch is 0.00330564.
After 28512 training step(s), loss on training batch is 0.00350815.
After 28513 training step(s), loss on training batch is 0.00358431.
After 28514 training step(s), loss on training batch is 0.00312045.
After 28515 training step(s), loss on training batch is 0.00297199.
After 28516 training step(s), loss on training batch is 0.0031986.
After 28517 training step(s), loss on training batch is 0.0031425.
After 28518 training step(s), loss on training batch is 0.00305978.
After 28519 training step(s), loss on training batch is 0.00321017.
After 28520 training step(s), loss on training batch is 0.00294956.
After 28521 training step(s), loss on training batch is 0.00357525.
After 28522 training step(s), loss on training batch is 0.00337608.
After 28523 training step(s), loss on training batch is 0.00329932.
After 28524 training step(s), loss on training batch is 0.00309022.
After 28525 training step(s), loss on training batch is 0.00316043.
After 28526 training step(s), loss on training batch is 0.00302578.
After 28527 training step(s), loss on training batch is 0.00344409.
After 28528 training step(s), loss on training batch is 0.00371457.
After 28529 training step(s), loss on training batch is 0.00312434.
After 28530 training step(s), loss on training batch is 0.00313593.
After 28531 training step(s), loss on training batch is 0.00325414.
After 28532 training step(s), loss on training batch is 0.00385519.
After 28533 training step(s), loss on training batch is 0.00293247.
After 28534 training step(s), loss on training batch is 0.00333513.
After 28535 training step(s), loss on training batch is 0.00363164.
After 28536 training step(s), loss on training batch is 0.00297563.
After 28537 training step(s), loss on training batch is 0.00315112.
After 28538 training step(s), loss on training batch is 0.00383157.
After 28539 training step(s), loss on training batch is 0.00354742.
After 28540 training step(s), loss on training batch is 0.00426209.
After 28541 training step(s), loss on training batch is 0.00303898.
After 28542 training step(s), loss on training batch is 0.00318261.
After 28543 training step(s), loss on training batch is 0.00309119.
After 28544 training step(s), loss on training batch is 0.00323142.
After 28545 training step(s), loss on training batch is 0.00321287.
After 28546 training step(s), loss on training batch is 0.00299917.
After 28547 training step(s), loss on training batch is 0.0035014.
After 28548 training step(s), loss on training batch is 0.00332613.
After 28549 training step(s), loss on training batch is 0.00314515.
After 28550 training step(s), loss on training batch is 0.00330819.
After 28551 training step(s), loss on training batch is 0.00304948.
After 28552 training step(s), loss on training batch is 0.00322874.
After 28553 training step(s), loss on training batch is 0.00310591.
After 28554 training step(s), loss on training batch is 0.00345543.
After 28555 training step(s), loss on training batch is 0.00309719.
After 28556 training step(s), loss on training batch is 0.00300449.
After 28557 training step(s), loss on training batch is 0.00301341.
After 28558 training step(s), loss on training batch is 0.00296553.
After 28559 training step(s), loss on training batch is 0.00327442.
After 28560 training step(s), loss on training batch is 0.0034493.
After 28561 training step(s), loss on training batch is 0.00321225.
After 28562 training step(s), loss on training batch is 0.00332926.
After 28563 training step(s), loss on training batch is 0.00296855.
After 28564 training step(s), loss on training batch is 0.00348429.
After 28565 training step(s), loss on training batch is 0.00321301.
After 28566 training step(s), loss on training batch is 0.00352864.
After 28567 training step(s), loss on training batch is 0.00316028.
After 28568 training step(s), loss on training batch is 0.00330695.
After 28569 training step(s), loss on training batch is 0.00314175.
After 28570 training step(s), loss on training batch is 0.0031214.
After 28571 training step(s), loss on training batch is 0.00312886.
After 28572 training step(s), loss on training batch is 0.00311851.
After 28573 training step(s), loss on training batch is 0.00330129.
After 28574 training step(s), loss on training batch is 0.00351834.
After 28575 training step(s), loss on training batch is 0.00315347.
After 28576 training step(s), loss on training batch is 0.0032172.
After 28577 training step(s), loss on training batch is 0.0035378.
After 28578 training step(s), loss on training batch is 0.00330913.
After 28579 training step(s), loss on training batch is 0.00311256.
After 28580 training step(s), loss on training batch is 0.00336403.
After 28581 training step(s), loss on training batch is 0.00339242.
After 28582 training step(s), loss on training batch is 0.00308068.
After 28583 training step(s), loss on training batch is 0.00293271.
After 28584 training step(s), loss on training batch is 0.0031972.
After 28585 training step(s), loss on training batch is 0.00362238.
After 28586 training step(s), loss on training batch is 0.00310504.
After 28587 training step(s), loss on training batch is 0.00353439.
After 28588 training step(s), loss on training batch is 0.00316132.
After 28589 training step(s), loss on training batch is 0.00343639.
After 28590 training step(s), loss on training batch is 0.00303415.
After 28591 training step(s), loss on training batch is 0.00309149.
After 28592 training step(s), loss on training batch is 0.00308787.
After 28593 training step(s), loss on training batch is 0.00321119.
After 28594 training step(s), loss on training batch is 0.00364737.
After 28595 training step(s), loss on training batch is 0.00329139.
After 28596 training step(s), loss on training batch is 0.00340967.
After 28597 training step(s), loss on training batch is 0.00384381.
After 28598 training step(s), loss on training batch is 0.00306026.
After 28599 training step(s), loss on training batch is 0.00435895.
After 28600 training step(s), loss on training batch is 0.00333474.
After 28601 training step(s), loss on training batch is 0.00310641.
After 28602 training step(s), loss on training batch is 0.00293946.
After 28603 training step(s), loss on training batch is 0.00326677.
After 28604 training step(s), loss on training batch is 0.00314752.
After 28605 training step(s), loss on training batch is 0.00299006.
After 28606 training step(s), loss on training batch is 0.00349522.
After 28607 training step(s), loss on training batch is 0.00343987.
After 28608 training step(s), loss on training batch is 0.0033063.
After 28609 training step(s), loss on training batch is 0.0031903.
After 28610 training step(s), loss on training batch is 0.00319982.
After 28611 training step(s), loss on training batch is 0.00349375.
After 28612 training step(s), loss on training batch is 0.00364501.
After 28613 training step(s), loss on training batch is 0.00311729.
After 28614 training step(s), loss on training batch is 0.00313455.
After 28615 training step(s), loss on training batch is 0.00339266.
After 28616 training step(s), loss on training batch is 0.00320517.
After 28617 training step(s), loss on training batch is 0.00327578.
After 28618 training step(s), loss on training batch is 0.00372005.
After 28619 training step(s), loss on training batch is 0.00329428.
After 28620 training step(s), loss on training batch is 0.00303147.
After 28621 training step(s), loss on training batch is 0.00305432.
After 28622 training step(s), loss on training batch is 0.00313431.
After 28623 training step(s), loss on training batch is 0.00309393.
After 28624 training step(s), loss on training batch is 0.00306549.
After 28625 training step(s), loss on training batch is 0.00318978.
After 28626 training step(s), loss on training batch is 0.00345178.
After 28627 training step(s), loss on training batch is 0.00359031.
After 28628 training step(s), loss on training batch is 0.00308656.
After 28629 training step(s), loss on training batch is 0.00293478.
After 28630 training step(s), loss on training batch is 0.00308145.
After 28631 training step(s), loss on training batch is 0.00352391.
After 28632 training step(s), loss on training batch is 0.00303139.
After 28633 training step(s), loss on training batch is 0.00336842.
After 28634 training step(s), loss on training batch is 0.00353719.
After 28635 training step(s), loss on training batch is 0.00322264.
After 28636 training step(s), loss on training batch is 0.00321688.
After 28637 training step(s), loss on training batch is 0.00290305.
After 28638 training step(s), loss on training batch is 0.00362215.
After 28639 training step(s), loss on training batch is 0.00330719.
After 28640 training step(s), loss on training batch is 0.00348379.
After 28641 training step(s), loss on training batch is 0.00304554.
After 28642 training step(s), loss on training batch is 0.00303555.
After 28643 training step(s), loss on training batch is 0.00353844.
After 28644 training step(s), loss on training batch is 0.00312618.
After 28645 training step(s), loss on training batch is 0.00316804.
After 28646 training step(s), loss on training batch is 0.00342843.
After 28647 training step(s), loss on training batch is 0.00350441.
After 28648 training step(s), loss on training batch is 0.00323026.
After 28649 training step(s), loss on training batch is 0.00358388.
After 28650 training step(s), loss on training batch is 0.00338783.
After 28651 training step(s), loss on training batch is 0.00303439.
After 28652 training step(s), loss on training batch is 0.0031446.
After 28653 training step(s), loss on training batch is 0.0031842.
After 28654 training step(s), loss on training batch is 0.00343279.
After 28655 training step(s), loss on training batch is 0.0031009.
After 28656 training step(s), loss on training batch is 0.00342008.
After 28657 training step(s), loss on training batch is 0.00323054.
After 28658 training step(s), loss on training batch is 0.0029073.
After 28659 training step(s), loss on training batch is 0.00373959.
After 28660 training step(s), loss on training batch is 0.00310191.
After 28661 training step(s), loss on training batch is 0.00310989.
After 28662 training step(s), loss on training batch is 0.00286325.
After 28663 training step(s), loss on training batch is 0.00340152.
After 28664 training step(s), loss on training batch is 0.00315741.
After 28665 training step(s), loss on training batch is 0.00316393.
After 28666 training step(s), loss on training batch is 0.00301481.
After 28667 training step(s), loss on training batch is 0.00335676.
After 28668 training step(s), loss on training batch is 0.00316951.
After 28669 training step(s), loss on training batch is 0.00339779.
After 28670 training step(s), loss on training batch is 0.00300036.
After 28671 training step(s), loss on training batch is 0.00323915.
After 28672 training step(s), loss on training batch is 0.00325219.
After 28673 training step(s), loss on training batch is 0.00311037.
After 28674 training step(s), loss on training batch is 0.00322986.
After 28675 training step(s), loss on training batch is 0.00332891.
After 28676 training step(s), loss on training batch is 0.00340858.
After 28677 training step(s), loss on training batch is 0.00328741.
After 28678 training step(s), loss on training batch is 0.00365767.
After 28679 training step(s), loss on training batch is 0.00298973.
After 28680 training step(s), loss on training batch is 0.00319305.
After 28681 training step(s), loss on training batch is 0.00353005.
After 28682 training step(s), loss on training batch is 0.00320949.
After 28683 training step(s), loss on training batch is 0.00339896.
After 28684 training step(s), loss on training batch is 0.00311766.
After 28685 training step(s), loss on training batch is 0.00305861.
After 28686 training step(s), loss on training batch is 0.00307859.
After 28687 training step(s), loss on training batch is 0.0031541.
After 28688 training step(s), loss on training batch is 0.0034716.
After 28689 training step(s), loss on training batch is 0.00318487.
After 28690 training step(s), loss on training batch is 0.00322127.
After 28691 training step(s), loss on training batch is 0.00309269.
After 28692 training step(s), loss on training batch is 0.00312428.
After 28693 training step(s), loss on training batch is 0.00311678.
After 28694 training step(s), loss on training batch is 0.0033168.
After 28695 training step(s), loss on training batch is 0.00307424.
After 28696 training step(s), loss on training batch is 0.00299695.
After 28697 training step(s), loss on training batch is 0.00304728.
After 28698 training step(s), loss on training batch is 0.00322775.
After 28699 training step(s), loss on training batch is 0.00322969.
After 28700 training step(s), loss on training batch is 0.00335291.
After 28701 training step(s), loss on training batch is 0.00350434.
After 28702 training step(s), loss on training batch is 0.00328253.
After 28703 training step(s), loss on training batch is 0.00325505.
After 28704 training step(s), loss on training batch is 0.00328806.
After 28705 training step(s), loss on training batch is 0.00307351.
After 28706 training step(s), loss on training batch is 0.00301426.
After 28707 training step(s), loss on training batch is 0.00308785.
After 28708 training step(s), loss on training batch is 0.00316064.
After 28709 training step(s), loss on training batch is 0.00300109.
After 28710 training step(s), loss on training batch is 0.00317921.
After 28711 training step(s), loss on training batch is 0.00324747.
After 28712 training step(s), loss on training batch is 0.00293902.
After 28713 training step(s), loss on training batch is 0.00309004.
After 28714 training step(s), loss on training batch is 0.0034095.
After 28715 training step(s), loss on training batch is 0.00303213.
After 28716 training step(s), loss on training batch is 0.00316952.
After 28717 training step(s), loss on training batch is 0.00309994.
After 28718 training step(s), loss on training batch is 0.00315733.
After 28719 training step(s), loss on training batch is 0.00347201.
After 28720 training step(s), loss on training batch is 0.00298459.
After 28721 training step(s), loss on training batch is 0.00336012.
After 28722 training step(s), loss on training batch is 0.00349271.
After 28723 training step(s), loss on training batch is 0.00310291.
After 28724 training step(s), loss on training batch is 0.00294806.
After 28725 training step(s), loss on training batch is 0.00304736.
After 28726 training step(s), loss on training batch is 0.00314006.
After 28727 training step(s), loss on training batch is 0.00314542.
After 28728 training step(s), loss on training batch is 0.00317058.
After 28729 training step(s), loss on training batch is 0.00316397.
After 28730 training step(s), loss on training batch is 0.00319339.
After 28731 training step(s), loss on training batch is 0.00293098.
After 28732 training step(s), loss on training batch is 0.00321716.
After 28733 training step(s), loss on training batch is 0.00299967.
After 28734 training step(s), loss on training batch is 0.00308961.
After 28735 training step(s), loss on training batch is 0.00349445.
After 28736 training step(s), loss on training batch is 0.00313336.
After 28737 training step(s), loss on training batch is 0.00292497.
After 28738 training step(s), loss on training batch is 0.00322523.
After 28739 training step(s), loss on training batch is 0.00321903.
After 28740 training step(s), loss on training batch is 0.00300293.
After 28741 training step(s), loss on training batch is 0.00337037.
After 28742 training step(s), loss on training batch is 0.00302924.
After 28743 training step(s), loss on training batch is 0.00340923.
After 28744 training step(s), loss on training batch is 0.00331936.
After 28745 training step(s), loss on training batch is 0.00332006.
After 28746 training step(s), loss on training batch is 0.00305524.
After 28747 training step(s), loss on training batch is 0.00327847.
After 28748 training step(s), loss on training batch is 0.00338907.
After 28749 training step(s), loss on training batch is 0.00305611.
After 28750 training step(s), loss on training batch is 0.00318786.
After 28751 training step(s), loss on training batch is 0.00340257.
After 28752 training step(s), loss on training batch is 0.00299082.
After 28753 training step(s), loss on training batch is 0.00343044.
After 28754 training step(s), loss on training batch is 0.00473309.
After 28755 training step(s), loss on training batch is 0.003074.
After 28756 training step(s), loss on training batch is 0.00359015.
After 28757 training step(s), loss on training batch is 0.00329685.
After 28758 training step(s), loss on training batch is 0.00371731.
After 28759 training step(s), loss on training batch is 0.00316925.
After 28760 training step(s), loss on training batch is 0.0030571.
After 28761 training step(s), loss on training batch is 0.00303306.
After 28762 training step(s), loss on training batch is 0.00301526.
After 28763 training step(s), loss on training batch is 0.00345517.
After 28764 training step(s), loss on training batch is 0.00356072.
After 28765 training step(s), loss on training batch is 0.00343629.
After 28766 training step(s), loss on training batch is 0.00344129.
After 28767 training step(s), loss on training batch is 0.00303458.
After 28768 training step(s), loss on training batch is 0.00333757.
After 28769 training step(s), loss on training batch is 0.00341752.
After 28770 training step(s), loss on training batch is 0.00317792.
After 28771 training step(s), loss on training batch is 0.00302615.
After 28772 training step(s), loss on training batch is 0.00301198.
After 28773 training step(s), loss on training batch is 0.00330804.
After 28774 training step(s), loss on training batch is 0.00319502.
After 28775 training step(s), loss on training batch is 0.00319795.
After 28776 training step(s), loss on training batch is 0.00316066.
After 28777 training step(s), loss on training batch is 0.0031141.
After 28778 training step(s), loss on training batch is 0.00295484.
After 28779 training step(s), loss on training batch is 0.00315352.
After 28780 training step(s), loss on training batch is 0.00317504.
After 28781 training step(s), loss on training batch is 0.00306204.
After 28782 training step(s), loss on training batch is 0.00312374.
After 28783 training step(s), loss on training batch is 0.00363262.
After 28784 training step(s), loss on training batch is 0.00346388.
After 28785 training step(s), loss on training batch is 0.00329372.
After 28786 training step(s), loss on training batch is 0.00343323.
After 28787 training step(s), loss on training batch is 0.00300042.
After 28788 training step(s), loss on training batch is 0.00365954.
After 28789 training step(s), loss on training batch is 0.00312777.
After 28790 training step(s), loss on training batch is 0.00310665.
After 28791 training step(s), loss on training batch is 0.00327159.
After 28792 training step(s), loss on training batch is 0.00312965.
After 28793 training step(s), loss on training batch is 0.00331637.
After 28794 training step(s), loss on training batch is 0.00332076.
After 28795 training step(s), loss on training batch is 0.00320847.
After 28796 training step(s), loss on training batch is 0.003429.
After 28797 training step(s), loss on training batch is 0.00301642.
After 28798 training step(s), loss on training batch is 0.00291339.
After 28799 training step(s), loss on training batch is 0.00392821.
After 28800 training step(s), loss on training batch is 0.00319058.
After 28801 training step(s), loss on training batch is 0.00310183.
After 28802 training step(s), loss on training batch is 0.00314314.
After 28803 training step(s), loss on training batch is 0.00348825.
After 28804 training step(s), loss on training batch is 0.00357605.
After 28805 training step(s), loss on training batch is 0.003325.
After 28806 training step(s), loss on training batch is 0.00314293.
After 28807 training step(s), loss on training batch is 0.00301678.
After 28808 training step(s), loss on training batch is 0.00344928.
After 28809 training step(s), loss on training batch is 0.00353756.
After 28810 training step(s), loss on training batch is 0.0043877.
After 28811 training step(s), loss on training batch is 0.00339345.
After 28812 training step(s), loss on training batch is 0.00284939.
After 28813 training step(s), loss on training batch is 0.00326035.
After 28814 training step(s), loss on training batch is 0.00440694.
After 28815 training step(s), loss on training batch is 0.0030754.
After 28816 training step(s), loss on training batch is 0.00321282.
After 28817 training step(s), loss on training batch is 0.00368302.
After 28818 training step(s), loss on training batch is 0.00312798.
After 28819 training step(s), loss on training batch is 0.00339889.
After 28820 training step(s), loss on training batch is 0.00361924.
After 28821 training step(s), loss on training batch is 0.00321306.
After 28822 training step(s), loss on training batch is 0.00299244.
After 28823 training step(s), loss on training batch is 0.00286907.
After 28824 training step(s), loss on training batch is 0.00368078.
After 28825 training step(s), loss on training batch is 0.00297437.
After 28826 training step(s), loss on training batch is 0.00299934.
After 28827 training step(s), loss on training batch is 0.00331672.
After 28828 training step(s), loss on training batch is 0.00365833.
After 28829 training step(s), loss on training batch is 0.00341839.
After 28830 training step(s), loss on training batch is 0.00321824.
After 28831 training step(s), loss on training batch is 0.00400788.
After 28832 training step(s), loss on training batch is 0.00297165.
After 28833 training step(s), loss on training batch is 0.0033777.
After 28834 training step(s), loss on training batch is 0.00419729.
After 28835 training step(s), loss on training batch is 0.00322895.
After 28836 training step(s), loss on training batch is 0.00297078.
After 28837 training step(s), loss on training batch is 0.00312243.
After 28838 training step(s), loss on training batch is 0.00314074.
After 28839 training step(s), loss on training batch is 0.00305139.
After 28840 training step(s), loss on training batch is 0.00391962.
After 28841 training step(s), loss on training batch is 0.00339759.
After 28842 training step(s), loss on training batch is 0.00315877.
After 28843 training step(s), loss on training batch is 0.00318954.
After 28844 training step(s), loss on training batch is 0.00321802.
After 28845 training step(s), loss on training batch is 0.00322563.
After 28846 training step(s), loss on training batch is 0.00365354.
After 28847 training step(s), loss on training batch is 0.00306998.
After 28848 training step(s), loss on training batch is 0.00360257.
After 28849 training step(s), loss on training batch is 0.00291389.
After 28850 training step(s), loss on training batch is 0.00283749.
After 28851 training step(s), loss on training batch is 0.00333708.
After 28852 training step(s), loss on training batch is 0.00296521.
After 28853 training step(s), loss on training batch is 0.00299712.
After 28854 training step(s), loss on training batch is 0.00313759.
After 28855 training step(s), loss on training batch is 0.00311435.
After 28856 training step(s), loss on training batch is 0.00356557.
After 28857 training step(s), loss on training batch is 0.00327736.
After 28858 training step(s), loss on training batch is 0.00329558.
After 28859 training step(s), loss on training batch is 0.00342083.
After 28860 training step(s), loss on training batch is 0.00318282.
After 28861 training step(s), loss on training batch is 0.00355568.
After 28862 training step(s), loss on training batch is 0.00327623.
After 28863 training step(s), loss on training batch is 0.00297034.
After 28864 training step(s), loss on training batch is 0.00316831.
After 28865 training step(s), loss on training batch is 0.00325783.
After 28866 training step(s), loss on training batch is 0.0035406.
After 28867 training step(s), loss on training batch is 0.00321982.
After 28868 training step(s), loss on training batch is 0.0035996.
After 28869 training step(s), loss on training batch is 0.00301278.
After 28870 training step(s), loss on training batch is 0.00324232.
After 28871 training step(s), loss on training batch is 0.003147.
After 28872 training step(s), loss on training batch is 0.00336604.
After 28873 training step(s), loss on training batch is 0.00304271.
After 28874 training step(s), loss on training batch is 0.00305075.
After 28875 training step(s), loss on training batch is 0.00324676.
After 28876 training step(s), loss on training batch is 0.00370704.
After 28877 training step(s), loss on training batch is 0.00338862.
After 28878 training step(s), loss on training batch is 0.00286491.
After 28879 training step(s), loss on training batch is 0.00320979.
After 28880 training step(s), loss on training batch is 0.00304076.
After 28881 training step(s), loss on training batch is 0.00326372.
After 28882 training step(s), loss on training batch is 0.00339003.
After 28883 training step(s), loss on training batch is 0.00331099.
After 28884 training step(s), loss on training batch is 0.00320695.
After 28885 training step(s), loss on training batch is 0.00293959.
After 28886 training step(s), loss on training batch is 0.00340697.
After 28887 training step(s), loss on training batch is 0.0031299.
After 28888 training step(s), loss on training batch is 0.00294582.
After 28889 training step(s), loss on training batch is 0.0030875.
After 28890 training step(s), loss on training batch is 0.00361718.
After 28891 training step(s), loss on training batch is 0.00316803.
After 28892 training step(s), loss on training batch is 0.0029555.
After 28893 training step(s), loss on training batch is 0.00330487.
After 28894 training step(s), loss on training batch is 0.00320023.
After 28895 training step(s), loss on training batch is 0.00309504.
After 28896 training step(s), loss on training batch is 0.00328367.
After 28897 training step(s), loss on training batch is 0.00305368.
After 28898 training step(s), loss on training batch is 0.00312644.
After 28899 training step(s), loss on training batch is 0.00392466.
After 28900 training step(s), loss on training batch is 0.00310306.
After 28901 training step(s), loss on training batch is 0.00433811.
After 28902 training step(s), loss on training batch is 0.00305634.
After 28903 training step(s), loss on training batch is 0.00344943.
After 28904 training step(s), loss on training batch is 0.00309951.
After 28905 training step(s), loss on training batch is 0.00362174.
After 28906 training step(s), loss on training batch is 0.00337638.
After 28907 training step(s), loss on training batch is 0.00304903.
After 28908 training step(s), loss on training batch is 0.00297393.
After 28909 training step(s), loss on training batch is 0.00320253.
After 28910 training step(s), loss on training batch is 0.00320938.
After 28911 training step(s), loss on training batch is 0.00313893.
After 28912 training step(s), loss on training batch is 0.00336785.
After 28913 training step(s), loss on training batch is 0.00330148.
After 28914 training step(s), loss on training batch is 0.00322844.
After 28915 training step(s), loss on training batch is 0.00303785.
After 28916 training step(s), loss on training batch is 0.00330602.
After 28917 training step(s), loss on training batch is 0.00300155.
After 28918 training step(s), loss on training batch is 0.00331209.
After 28919 training step(s), loss on training batch is 0.00316885.
After 28920 training step(s), loss on training batch is 0.00334886.
After 28921 training step(s), loss on training batch is 0.00328655.
After 28922 training step(s), loss on training batch is 0.00355934.
After 28923 training step(s), loss on training batch is 0.00336196.
After 28924 training step(s), loss on training batch is 0.00321908.
After 28925 training step(s), loss on training batch is 0.00298216.
After 28926 training step(s), loss on training batch is 0.0032814.
After 28927 training step(s), loss on training batch is 0.00338337.
After 28928 training step(s), loss on training batch is 0.00316967.
After 28929 training step(s), loss on training batch is 0.00308339.
After 28930 training step(s), loss on training batch is 0.00317684.
After 28931 training step(s), loss on training batch is 0.00307864.
After 28932 training step(s), loss on training batch is 0.0032171.
After 28933 training step(s), loss on training batch is 0.00306995.
After 28934 training step(s), loss on training batch is 0.00301045.
After 28935 training step(s), loss on training batch is 0.00318121.
After 28936 training step(s), loss on training batch is 0.00285755.
After 28937 training step(s), loss on training batch is 0.00300919.
After 28938 training step(s), loss on training batch is 0.00309776.
After 28939 training step(s), loss on training batch is 0.0029933.
After 28940 training step(s), loss on training batch is 0.00298507.
After 28941 training step(s), loss on training batch is 0.00313828.
After 28942 training step(s), loss on training batch is 0.00312485.
After 28943 training step(s), loss on training batch is 0.00299754.
After 28944 training step(s), loss on training batch is 0.00332626.
After 28945 training step(s), loss on training batch is 0.00310772.
After 28946 training step(s), loss on training batch is 0.00309019.
After 28947 training step(s), loss on training batch is 0.00342624.
After 28948 training step(s), loss on training batch is 0.00348883.
After 28949 training step(s), loss on training batch is 0.00318033.
After 28950 training step(s), loss on training batch is 0.0033566.
After 28951 training step(s), loss on training batch is 0.00329916.
After 28952 training step(s), loss on training batch is 0.00324079.
After 28953 training step(s), loss on training batch is 0.00328931.
After 28954 training step(s), loss on training batch is 0.00304696.
After 28955 training step(s), loss on training batch is 0.00336242.
After 28956 training step(s), loss on training batch is 0.00292278.
After 28957 training step(s), loss on training batch is 0.00309676.
After 28958 training step(s), loss on training batch is 0.0030676.
After 28959 training step(s), loss on training batch is 0.00294708.
After 28960 training step(s), loss on training batch is 0.0031064.
After 28961 training step(s), loss on training batch is 0.00330659.
After 28962 training step(s), loss on training batch is 0.00331496.
After 28963 training step(s), loss on training batch is 0.00322828.
After 28964 training step(s), loss on training batch is 0.00312303.
After 28965 training step(s), loss on training batch is 0.00320672.
After 28966 training step(s), loss on training batch is 0.0033191.
After 28967 training step(s), loss on training batch is 0.00350993.
After 28968 training step(s), loss on training batch is 0.00331445.
After 28969 training step(s), loss on training batch is 0.00290398.
After 28970 training step(s), loss on training batch is 0.00303636.
After 28971 training step(s), loss on training batch is 0.00392672.
After 28972 training step(s), loss on training batch is 0.00343598.
After 28973 training step(s), loss on training batch is 0.00411376.
After 28974 training step(s), loss on training batch is 0.00351322.
After 28975 training step(s), loss on training batch is 0.00335186.
After 28976 training step(s), loss on training batch is 0.00330771.
After 28977 training step(s), loss on training batch is 0.00347701.
After 28978 training step(s), loss on training batch is 0.0030347.
After 28979 training step(s), loss on training batch is 0.00308995.
After 28980 training step(s), loss on training batch is 0.00321594.
After 28981 training step(s), loss on training batch is 0.00318343.
After 28982 training step(s), loss on training batch is 0.00307735.
After 28983 training step(s), loss on training batch is 0.00334428.
After 28984 training step(s), loss on training batch is 0.00321883.
After 28985 training step(s), loss on training batch is 0.00353419.
After 28986 training step(s), loss on training batch is 0.00323133.
After 28987 training step(s), loss on training batch is 0.00287833.
After 28988 training step(s), loss on training batch is 0.00377349.
After 28989 training step(s), loss on training batch is 0.00310976.
After 28990 training step(s), loss on training batch is 0.0034075.
After 28991 training step(s), loss on training batch is 0.00331426.
After 28992 training step(s), loss on training batch is 0.0031292.
After 28993 training step(s), loss on training batch is 0.0030353.
After 28994 training step(s), loss on training batch is 0.00290629.
After 28995 training step(s), loss on training batch is 0.00342824.
After 28996 training step(s), loss on training batch is 0.00324287.
After 28997 training step(s), loss on training batch is 0.00310378.
After 28998 training step(s), loss on training batch is 0.00298908.
After 28999 training step(s), loss on training batch is 0.00324662.
After 29000 training step(s), loss on training batch is 0.00375803.
After 29001 training step(s), loss on training batch is 0.00317036.
After 29002 training step(s), loss on training batch is 0.00297372.
After 29003 training step(s), loss on training batch is 0.00313654.
After 29004 training step(s), loss on training batch is 0.00331877.
After 29005 training step(s), loss on training batch is 0.00329607.
After 29006 training step(s), loss on training batch is 0.00333399.
After 29007 training step(s), loss on training batch is 0.00338311.
After 29008 training step(s), loss on training batch is 0.00292982.
After 29009 training step(s), loss on training batch is 0.00322831.
After 29010 training step(s), loss on training batch is 0.0033156.
After 29011 training step(s), loss on training batch is 0.00333325.
After 29012 training step(s), loss on training batch is 0.00314298.
After 29013 training step(s), loss on training batch is 0.00307202.
After 29014 training step(s), loss on training batch is 0.00311463.
After 29015 training step(s), loss on training batch is 0.00321617.
After 29016 training step(s), loss on training batch is 0.00316946.
After 29017 training step(s), loss on training batch is 0.00288232.
After 29018 training step(s), loss on training batch is 0.00296816.
After 29019 training step(s), loss on training batch is 0.00349427.
After 29020 training step(s), loss on training batch is 0.00430068.
After 29021 training step(s), loss on training batch is 0.00340117.
After 29022 training step(s), loss on training batch is 0.0031119.
After 29023 training step(s), loss on training batch is 0.0037045.
After 29024 training step(s), loss on training batch is 0.0031183.
After 29025 training step(s), loss on training batch is 0.00313256.
After 29026 training step(s), loss on training batch is 0.0031038.
After 29027 training step(s), loss on training batch is 0.00341058.
After 29028 training step(s), loss on training batch is 0.0032859.
After 29029 training step(s), loss on training batch is 0.00330129.
After 29030 training step(s), loss on training batch is 0.00319991.
After 29031 training step(s), loss on training batch is 0.00322514.
After 29032 training step(s), loss on training batch is 0.00306376.
After 29033 training step(s), loss on training batch is 0.00391099.
After 29034 training step(s), loss on training batch is 0.00305327.
After 29035 training step(s), loss on training batch is 0.0030631.
After 29036 training step(s), loss on training batch is 0.00348535.
After 29037 training step(s), loss on training batch is 0.00296567.
After 29038 training step(s), loss on training batch is 0.00342091.
After 29039 training step(s), loss on training batch is 0.00313051.
After 29040 training step(s), loss on training batch is 0.00317023.
After 29041 training step(s), loss on training batch is 0.0037651.
After 29042 training step(s), loss on training batch is 0.00312918.
After 29043 training step(s), loss on training batch is 0.0035439.
After 29044 training step(s), loss on training batch is 0.00310889.
After 29045 training step(s), loss on training batch is 0.00331599.
After 29046 training step(s), loss on training batch is 0.00321128.
After 29047 training step(s), loss on training batch is 0.00328242.
After 29048 training step(s), loss on training batch is 0.00334447.
After 29049 training step(s), loss on training batch is 0.00301267.
After 29050 training step(s), loss on training batch is 0.0030638.
After 29051 training step(s), loss on training batch is 0.00317844.
After 29052 training step(s), loss on training batch is 0.0032232.
After 29053 training step(s), loss on training batch is 0.00314437.
After 29054 training step(s), loss on training batch is 0.0032314.
After 29055 training step(s), loss on training batch is 0.00315607.
After 29056 training step(s), loss on training batch is 0.00303694.
After 29057 training step(s), loss on training batch is 0.00317992.
After 29058 training step(s), loss on training batch is 0.00328007.
After 29059 training step(s), loss on training batch is 0.00345006.
After 29060 training step(s), loss on training batch is 0.00387437.
After 29061 training step(s), loss on training batch is 0.00297179.
After 29062 training step(s), loss on training batch is 0.00328344.
After 29063 training step(s), loss on training batch is 0.00311168.
After 29064 training step(s), loss on training batch is 0.00310106.
After 29065 training step(s), loss on training batch is 0.00333563.
After 29066 training step(s), loss on training batch is 0.00297467.
After 29067 training step(s), loss on training batch is 0.00306178.
After 29068 training step(s), loss on training batch is 0.00335238.
After 29069 training step(s), loss on training batch is 0.00300079.
After 29070 training step(s), loss on training batch is 0.00283842.
After 29071 training step(s), loss on training batch is 0.00332096.
After 29072 training step(s), loss on training batch is 0.00355092.
After 29073 training step(s), loss on training batch is 0.00324137.
After 29074 training step(s), loss on training batch is 0.00352919.
After 29075 training step(s), loss on training batch is 0.00294651.
After 29076 training step(s), loss on training batch is 0.00332081.
After 29077 training step(s), loss on training batch is 0.00307778.
After 29078 training step(s), loss on training batch is 0.00324858.
After 29079 training step(s), loss on training batch is 0.00326729.
After 29080 training step(s), loss on training batch is 0.00319959.
After 29081 training step(s), loss on training batch is 0.00324417.
After 29082 training step(s), loss on training batch is 0.00345515.
After 29083 training step(s), loss on training batch is 0.00346027.
After 29084 training step(s), loss on training batch is 0.00301467.
After 29085 training step(s), loss on training batch is 0.00327585.
After 29086 training step(s), loss on training batch is 0.00339601.
After 29087 training step(s), loss on training batch is 0.00341842.
After 29088 training step(s), loss on training batch is 0.00305167.
After 29089 training step(s), loss on training batch is 0.00326341.
After 29090 training step(s), loss on training batch is 0.0033146.
After 29091 training step(s), loss on training batch is 0.00318929.
After 29092 training step(s), loss on training batch is 0.00311154.
After 29093 training step(s), loss on training batch is 0.00388584.
After 29094 training step(s), loss on training batch is 0.0033514.
After 29095 training step(s), loss on training batch is 0.00325655.
After 29096 training step(s), loss on training batch is 0.00344416.
After 29097 training step(s), loss on training batch is 0.00312328.
After 29098 training step(s), loss on training batch is 0.00331966.
After 29099 training step(s), loss on training batch is 0.00329004.
After 29100 training step(s), loss on training batch is 0.00342796.
After 29101 training step(s), loss on training batch is 0.0033258.
After 29102 training step(s), loss on training batch is 0.00329187.
After 29103 training step(s), loss on training batch is 0.00319272.
After 29104 training step(s), loss on training batch is 0.00326186.
After 29105 training step(s), loss on training batch is 0.00298208.
After 29106 training step(s), loss on training batch is 0.00325507.
After 29107 training step(s), loss on training batch is 0.00301834.
After 29108 training step(s), loss on training batch is 0.00311519.
After 29109 training step(s), loss on training batch is 0.00333278.
After 29110 training step(s), loss on training batch is 0.00302317.
After 29111 training step(s), loss on training batch is 0.00338359.
After 29112 training step(s), loss on training batch is 0.0034636.
After 29113 training step(s), loss on training batch is 0.00338394.
After 29114 training step(s), loss on training batch is 0.00293032.
After 29115 training step(s), loss on training batch is 0.0033.
After 29116 training step(s), loss on training batch is 0.00387315.
After 29117 training step(s), loss on training batch is 0.00303495.
After 29118 training step(s), loss on training batch is 0.00313874.
After 29119 training step(s), loss on training batch is 0.00311139.
After 29120 training step(s), loss on training batch is 0.0029343.
After 29121 training step(s), loss on training batch is 0.00323032.
After 29122 training step(s), loss on training batch is 0.0033858.
After 29123 training step(s), loss on training batch is 0.00305508.
After 29124 training step(s), loss on training batch is 0.00304683.
After 29125 training step(s), loss on training batch is 0.0031362.
After 29126 training step(s), loss on training batch is 0.00364304.
After 29127 training step(s), loss on training batch is 0.00377788.
After 29128 training step(s), loss on training batch is 0.00311139.
After 29129 training step(s), loss on training batch is 0.00363308.
After 29130 training step(s), loss on training batch is 0.00326025.
After 29131 training step(s), loss on training batch is 0.00341222.
After 29132 training step(s), loss on training batch is 0.00302744.
After 29133 training step(s), loss on training batch is 0.0032936.
After 29134 training step(s), loss on training batch is 0.00316089.
After 29135 training step(s), loss on training batch is 0.00322045.
After 29136 training step(s), loss on training batch is 0.00355994.
After 29137 training step(s), loss on training batch is 0.00312367.
After 29138 training step(s), loss on training batch is 0.00304288.
After 29139 training step(s), loss on training batch is 0.00313789.
After 29140 training step(s), loss on training batch is 0.00301596.
After 29141 training step(s), loss on training batch is 0.00289444.
After 29142 training step(s), loss on training batch is 0.0037256.
After 29143 training step(s), loss on training batch is 0.00372158.
After 29144 training step(s), loss on training batch is 0.00304654.
After 29145 training step(s), loss on training batch is 0.00303138.
After 29146 training step(s), loss on training batch is 0.00317456.
After 29147 training step(s), loss on training batch is 0.00319895.
After 29148 training step(s), loss on training batch is 0.00300664.
After 29149 training step(s), loss on training batch is 0.00301796.
After 29150 training step(s), loss on training batch is 0.00326806.
After 29151 training step(s), loss on training batch is 0.00329709.
After 29152 training step(s), loss on training batch is 0.00314584.
After 29153 training step(s), loss on training batch is 0.00344868.
After 29154 training step(s), loss on training batch is 0.00338885.
After 29155 training step(s), loss on training batch is 0.00363202.
After 29156 training step(s), loss on training batch is 0.00317777.
After 29157 training step(s), loss on training batch is 0.00301704.
After 29158 training step(s), loss on training batch is 0.00350698.
After 29159 training step(s), loss on training batch is 0.00299184.
After 29160 training step(s), loss on training batch is 0.00332437.
After 29161 training step(s), loss on training batch is 0.00308578.
After 29162 training step(s), loss on training batch is 0.00329953.
After 29163 training step(s), loss on training batch is 0.00295749.
After 29164 training step(s), loss on training batch is 0.00332073.
After 29165 training step(s), loss on training batch is 0.0032798.
After 29166 training step(s), loss on training batch is 0.00332433.
After 29167 training step(s), loss on training batch is 0.00359893.
After 29168 training step(s), loss on training batch is 0.00397327.
After 29169 training step(s), loss on training batch is 0.00317105.
After 29170 training step(s), loss on training batch is 0.00316863.
After 29171 training step(s), loss on training batch is 0.00305375.
After 29172 training step(s), loss on training batch is 0.00339914.
After 29173 training step(s), loss on training batch is 0.002925.
After 29174 training step(s), loss on training batch is 0.00307649.
After 29175 training step(s), loss on training batch is 0.0033748.
After 29176 training step(s), loss on training batch is 0.00314646.
After 29177 training step(s), loss on training batch is 0.00348208.
After 29178 training step(s), loss on training batch is 0.00323617.
After 29179 training step(s), loss on training batch is 0.00308511.
After 29180 training step(s), loss on training batch is 0.00375512.
After 29181 training step(s), loss on training batch is 0.00303934.
After 29182 training step(s), loss on training batch is 0.00320057.
After 29183 training step(s), loss on training batch is 0.00308625.
After 29184 training step(s), loss on training batch is 0.00333506.
After 29185 training step(s), loss on training batch is 0.00311339.
After 29186 training step(s), loss on training batch is 0.00311704.
After 29187 training step(s), loss on training batch is 0.00316922.
After 29188 training step(s), loss on training batch is 0.00332677.
After 29189 training step(s), loss on training batch is 0.00361967.
After 29190 training step(s), loss on training batch is 0.00336688.
After 29191 training step(s), loss on training batch is 0.00444353.
After 29192 training step(s), loss on training batch is 0.00323744.
After 29193 training step(s), loss on training batch is 0.0030245.
After 29194 training step(s), loss on training batch is 0.00331334.
After 29195 training step(s), loss on training batch is 0.00304827.
After 29196 training step(s), loss on training batch is 0.00303465.
After 29197 training step(s), loss on training batch is 0.00315621.
After 29198 training step(s), loss on training batch is 0.0031541.
After 29199 training step(s), loss on training batch is 0.00337973.
After 29200 training step(s), loss on training batch is 0.00335616.
After 29201 training step(s), loss on training batch is 0.00337347.
After 29202 training step(s), loss on training batch is 0.00298312.
After 29203 training step(s), loss on training batch is 0.00301023.
After 29204 training step(s), loss on training batch is 0.00335879.
After 29205 training step(s), loss on training batch is 0.00313994.
After 29206 training step(s), loss on training batch is 0.00318362.
After 29207 training step(s), loss on training batch is 0.00320619.
After 29208 training step(s), loss on training batch is 0.00380347.
After 29209 training step(s), loss on training batch is 0.00292057.
After 29210 training step(s), loss on training batch is 0.00298393.
After 29211 training step(s), loss on training batch is 0.00301056.
After 29212 training step(s), loss on training batch is 0.00318973.
After 29213 training step(s), loss on training batch is 0.00301144.
After 29214 training step(s), loss on training batch is 0.00313815.
After 29215 training step(s), loss on training batch is 0.0031345.
After 29216 training step(s), loss on training batch is 0.00339889.
After 29217 training step(s), loss on training batch is 0.00300332.
After 29218 training step(s), loss on training batch is 0.00308795.
After 29219 training step(s), loss on training batch is 0.00322758.
After 29220 training step(s), loss on training batch is 0.00333621.
After 29221 training step(s), loss on training batch is 0.00317443.
After 29222 training step(s), loss on training batch is 0.00299669.
After 29223 training step(s), loss on training batch is 0.00316262.
After 29224 training step(s), loss on training batch is 0.00352813.
After 29225 training step(s), loss on training batch is 0.00325764.
After 29226 training step(s), loss on training batch is 0.00321026.
After 29227 training step(s), loss on training batch is 0.00302708.
After 29228 training step(s), loss on training batch is 0.00287983.
After 29229 training step(s), loss on training batch is 0.00351242.
After 29230 training step(s), loss on training batch is 0.00312167.
After 29231 training step(s), loss on training batch is 0.00331569.
After 29232 training step(s), loss on training batch is 0.00328743.
After 29233 training step(s), loss on training batch is 0.00333315.
After 29234 training step(s), loss on training batch is 0.00297684.
After 29235 training step(s), loss on training batch is 0.00319888.
After 29236 training step(s), loss on training batch is 0.00294178.
After 29237 training step(s), loss on training batch is 0.00330146.
After 29238 training step(s), loss on training batch is 0.00381773.
After 29239 training step(s), loss on training batch is 0.00309592.
After 29240 training step(s), loss on training batch is 0.00304017.
After 29241 training step(s), loss on training batch is 0.00293077.
After 29242 training step(s), loss on training batch is 0.00337564.
After 29243 training step(s), loss on training batch is 0.00362281.
After 29244 training step(s), loss on training batch is 0.0031507.
After 29245 training step(s), loss on training batch is 0.00294846.
After 29246 training step(s), loss on training batch is 0.00318235.
After 29247 training step(s), loss on training batch is 0.00308584.
After 29248 training step(s), loss on training batch is 0.00335102.
After 29249 training step(s), loss on training batch is 0.00305875.
After 29250 training step(s), loss on training batch is 0.00302454.
After 29251 training step(s), loss on training batch is 0.00298191.
After 29252 training step(s), loss on training batch is 0.00304705.
After 29253 training step(s), loss on training batch is 0.00308716.
After 29254 training step(s), loss on training batch is 0.00314781.
After 29255 training step(s), loss on training batch is 0.00302506.
After 29256 training step(s), loss on training batch is 0.00298901.
After 29257 training step(s), loss on training batch is 0.00336403.
After 29258 training step(s), loss on training batch is 0.00319586.
After 29259 training step(s), loss on training batch is 0.00325667.
After 29260 training step(s), loss on training batch is 0.00326545.
After 29261 training step(s), loss on training batch is 0.00306897.
After 29262 training step(s), loss on training batch is 0.0033897.
After 29263 training step(s), loss on training batch is 0.00302377.
After 29264 training step(s), loss on training batch is 0.00345778.
After 29265 training step(s), loss on training batch is 0.00319021.
After 29266 training step(s), loss on training batch is 0.00348065.
After 29267 training step(s), loss on training batch is 0.00313189.
After 29268 training step(s), loss on training batch is 0.00325942.
After 29269 training step(s), loss on training batch is 0.0033391.
After 29270 training step(s), loss on training batch is 0.00354315.
After 29271 training step(s), loss on training batch is 0.00305809.
After 29272 training step(s), loss on training batch is 0.00311943.
After 29273 training step(s), loss on training batch is 0.00310682.
After 29274 training step(s), loss on training batch is 0.00325846.
After 29275 training step(s), loss on training batch is 0.00313116.
After 29276 training step(s), loss on training batch is 0.00292388.
After 29277 training step(s), loss on training batch is 0.0030163.
After 29278 training step(s), loss on training batch is 0.00331524.
After 29279 training step(s), loss on training batch is 0.00306253.
After 29280 training step(s), loss on training batch is 0.00351675.
After 29281 training step(s), loss on training batch is 0.00299221.
After 29282 training step(s), loss on training batch is 0.00311603.
After 29283 training step(s), loss on training batch is 0.00302614.
After 29284 training step(s), loss on training batch is 0.0033746.
After 29285 training step(s), loss on training batch is 0.00300027.
After 29286 training step(s), loss on training batch is 0.00313155.
After 29287 training step(s), loss on training batch is 0.00324882.
After 29288 training step(s), loss on training batch is 0.00319016.
After 29289 training step(s), loss on training batch is 0.00321257.
After 29290 training step(s), loss on training batch is 0.00297148.
After 29291 training step(s), loss on training batch is 0.00287576.
After 29292 training step(s), loss on training batch is 0.00326181.
After 29293 training step(s), loss on training batch is 0.00323387.
After 29294 training step(s), loss on training batch is 0.00297599.
After 29295 training step(s), loss on training batch is 0.00304614.
After 29296 training step(s), loss on training batch is 0.00297748.
After 29297 training step(s), loss on training batch is 0.00320323.
After 29298 training step(s), loss on training batch is 0.00319972.
After 29299 training step(s), loss on training batch is 0.00286918.
After 29300 training step(s), loss on training batch is 0.00319297.
After 29301 training step(s), loss on training batch is 0.00357671.
After 29302 training step(s), loss on training batch is 0.00339633.
After 29303 training step(s), loss on training batch is 0.00405745.
After 29304 training step(s), loss on training batch is 0.00324557.
After 29305 training step(s), loss on training batch is 0.00340656.
After 29306 training step(s), loss on training batch is 0.00299908.
After 29307 training step(s), loss on training batch is 0.00341378.
After 29308 training step(s), loss on training batch is 0.00345662.
After 29309 training step(s), loss on training batch is 0.00300424.
After 29310 training step(s), loss on training batch is 0.00365872.
After 29311 training step(s), loss on training batch is 0.00331159.
After 29312 training step(s), loss on training batch is 0.00349908.
After 29313 training step(s), loss on training batch is 0.00302038.
After 29314 training step(s), loss on training batch is 0.00337956.
After 29315 training step(s), loss on training batch is 0.00305812.
After 29316 training step(s), loss on training batch is 0.0031982.
After 29317 training step(s), loss on training batch is 0.00329729.
After 29318 training step(s), loss on training batch is 0.00303903.
After 29319 training step(s), loss on training batch is 0.0030452.
After 29320 training step(s), loss on training batch is 0.00311902.
After 29321 training step(s), loss on training batch is 0.0029245.
After 29322 training step(s), loss on training batch is 0.00364736.
After 29323 training step(s), loss on training batch is 0.0028699.
After 29324 training step(s), loss on training batch is 0.00306611.
After 29325 training step(s), loss on training batch is 0.00301832.
After 29326 training step(s), loss on training batch is 0.00311515.
After 29327 training step(s), loss on training batch is 0.00321836.
After 29328 training step(s), loss on training batch is 0.00330541.
After 29329 training step(s), loss on training batch is 0.00314527.
After 29330 training step(s), loss on training batch is 0.00323309.
After 29331 training step(s), loss on training batch is 0.0030692.
After 29332 training step(s), loss on training batch is 0.00321706.
After 29333 training step(s), loss on training batch is 0.00325369.
After 29334 training step(s), loss on training batch is 0.00318775.
After 29335 training step(s), loss on training batch is 0.00322463.
After 29336 training step(s), loss on training batch is 0.00299336.
After 29337 training step(s), loss on training batch is 0.00338598.
After 29338 training step(s), loss on training batch is 0.00420071.
After 29339 training step(s), loss on training batch is 0.00337816.
After 29340 training step(s), loss on training batch is 0.00334501.
After 29341 training step(s), loss on training batch is 0.00330609.
After 29342 training step(s), loss on training batch is 0.00295179.
After 29343 training step(s), loss on training batch is 0.00358861.
After 29344 training step(s), loss on training batch is 0.00332498.
After 29345 training step(s), loss on training batch is 0.00294681.
After 29346 training step(s), loss on training batch is 0.00361901.
After 29347 training step(s), loss on training batch is 0.00400952.
After 29348 training step(s), loss on training batch is 0.00341469.
After 29349 training step(s), loss on training batch is 0.00299159.
After 29350 training step(s), loss on training batch is 0.00298043.
After 29351 training step(s), loss on training batch is 0.00304539.
After 29352 training step(s), loss on training batch is 0.00323704.
After 29353 training step(s), loss on training batch is 0.00296413.
After 29354 training step(s), loss on training batch is 0.00383279.
After 29355 training step(s), loss on training batch is 0.00321342.
After 29356 training step(s), loss on training batch is 0.0034541.
After 29357 training step(s), loss on training batch is 0.0030296.
After 29358 training step(s), loss on training batch is 0.00373497.
After 29359 training step(s), loss on training batch is 0.00343823.
After 29360 training step(s), loss on training batch is 0.0034922.
After 29361 training step(s), loss on training batch is 0.00317639.
After 29362 training step(s), loss on training batch is 0.0032926.
After 29363 training step(s), loss on training batch is 0.00331793.
After 29364 training step(s), loss on training batch is 0.0030531.
After 29365 training step(s), loss on training batch is 0.00329622.
After 29366 training step(s), loss on training batch is 0.00315271.
After 29367 training step(s), loss on training batch is 0.00287607.
After 29368 training step(s), loss on training batch is 0.00302058.
After 29369 training step(s), loss on training batch is 0.00299221.
After 29370 training step(s), loss on training batch is 0.00313679.
After 29371 training step(s), loss on training batch is 0.00299856.
After 29372 training step(s), loss on training batch is 0.00301758.
After 29373 training step(s), loss on training batch is 0.00319777.
After 29374 training step(s), loss on training batch is 0.00293926.
After 29375 training step(s), loss on training batch is 0.00324939.
After 29376 training step(s), loss on training batch is 0.00344051.
After 29377 training step(s), loss on training batch is 0.00319588.
After 29378 training step(s), loss on training batch is 0.00315226.
After 29379 training step(s), loss on training batch is 0.0034024.
After 29380 training step(s), loss on training batch is 0.00313057.
After 29381 training step(s), loss on training batch is 0.00311822.
After 29382 training step(s), loss on training batch is 0.00355967.
After 29383 training step(s), loss on training batch is 0.00304444.
After 29384 training step(s), loss on training batch is 0.00338671.
After 29385 training step(s), loss on training batch is 0.00302274.
After 29386 training step(s), loss on training batch is 0.00328057.
After 29387 training step(s), loss on training batch is 0.00336617.
After 29388 training step(s), loss on training batch is 0.00335822.
After 29389 training step(s), loss on training batch is 0.00316027.
After 29390 training step(s), loss on training batch is 0.00362334.
After 29391 training step(s), loss on training batch is 0.00324533.
After 29392 training step(s), loss on training batch is 0.00302418.
After 29393 training step(s), loss on training batch is 0.00321652.
After 29394 training step(s), loss on training batch is 0.00317307.
After 29395 training step(s), loss on training batch is 0.00333543.
After 29396 training step(s), loss on training batch is 0.00322162.
After 29397 training step(s), loss on training batch is 0.00298034.
After 29398 training step(s), loss on training batch is 0.00284631.
After 29399 training step(s), loss on training batch is 0.00343683.
After 29400 training step(s), loss on training batch is 0.00331637.
After 29401 training step(s), loss on training batch is 0.00305544.
After 29402 training step(s), loss on training batch is 0.00355852.
After 29403 training step(s), loss on training batch is 0.00310812.
After 29404 training step(s), loss on training batch is 0.00289846.
After 29405 training step(s), loss on training batch is 0.00375564.
After 29406 training step(s), loss on training batch is 0.00303441.
After 29407 training step(s), loss on training batch is 0.00317291.
After 29408 training step(s), loss on training batch is 0.00299829.
After 29409 training step(s), loss on training batch is 0.0034269.
After 29410 training step(s), loss on training batch is 0.00339594.
After 29411 training step(s), loss on training batch is 0.00341374.
After 29412 training step(s), loss on training batch is 0.00311237.
After 29413 training step(s), loss on training batch is 0.00296806.
After 29414 training step(s), loss on training batch is 0.00309125.
After 29415 training step(s), loss on training batch is 0.00327254.
After 29416 training step(s), loss on training batch is 0.00302705.
After 29417 training step(s), loss on training batch is 0.00314884.
After 29418 training step(s), loss on training batch is 0.00306692.
After 29419 training step(s), loss on training batch is 0.00330275.
After 29420 training step(s), loss on training batch is 0.00300198.
After 29421 training step(s), loss on training batch is 0.00303414.
After 29422 training step(s), loss on training batch is 0.00290697.
After 29423 training step(s), loss on training batch is 0.00342786.
After 29424 training step(s), loss on training batch is 0.00331754.
After 29425 training step(s), loss on training batch is 0.00319721.
After 29426 training step(s), loss on training batch is 0.00295033.
After 29427 training step(s), loss on training batch is 0.0030253.
After 29428 training step(s), loss on training batch is 0.00345031.
After 29429 training step(s), loss on training batch is 0.00304694.
After 29430 training step(s), loss on training batch is 0.00309695.
After 29431 training step(s), loss on training batch is 0.00338655.
After 29432 training step(s), loss on training batch is 0.00350122.
After 29433 training step(s), loss on training batch is 0.003249.
After 29434 training step(s), loss on training batch is 0.003634.
After 29435 training step(s), loss on training batch is 0.00322544.
After 29436 training step(s), loss on training batch is 0.003042.
After 29437 training step(s), loss on training batch is 0.00336296.
After 29438 training step(s), loss on training batch is 0.00324617.
After 29439 training step(s), loss on training batch is 0.00299737.
After 29440 training step(s), loss on training batch is 0.00301308.
After 29441 training step(s), loss on training batch is 0.0034431.
After 29442 training step(s), loss on training batch is 0.00296651.
After 29443 training step(s), loss on training batch is 0.00319642.
After 29444 training step(s), loss on training batch is 0.00302013.
After 29445 training step(s), loss on training batch is 0.0033947.
After 29446 training step(s), loss on training batch is 0.003087.
After 29447 training step(s), loss on training batch is 0.00288726.
After 29448 training step(s), loss on training batch is 0.00359818.
After 29449 training step(s), loss on training batch is 0.00329664.
After 29450 training step(s), loss on training batch is 0.00324171.
After 29451 training step(s), loss on training batch is 0.00304523.
After 29452 training step(s), loss on training batch is 0.00309047.
After 29453 training step(s), loss on training batch is 0.00354447.
After 29454 training step(s), loss on training batch is 0.00340561.
After 29455 training step(s), loss on training batch is 0.00295854.
After 29456 training step(s), loss on training batch is 0.00346129.
After 29457 training step(s), loss on training batch is 0.00299047.
After 29458 training step(s), loss on training batch is 0.00300013.
After 29459 training step(s), loss on training batch is 0.00318539.
After 29460 training step(s), loss on training batch is 0.00296835.
After 29461 training step(s), loss on training batch is 0.00312334.
After 29462 training step(s), loss on training batch is 0.00317932.
After 29463 training step(s), loss on training batch is 0.0036736.
After 29464 training step(s), loss on training batch is 0.00307436.
After 29465 training step(s), loss on training batch is 0.00334524.
After 29466 training step(s), loss on training batch is 0.00320636.
After 29467 training step(s), loss on training batch is 0.00339648.
After 29468 training step(s), loss on training batch is 0.00324357.
After 29469 training step(s), loss on training batch is 0.00315154.
After 29470 training step(s), loss on training batch is 0.00316204.
After 29471 training step(s), loss on training batch is 0.00323755.
After 29472 training step(s), loss on training batch is 0.00333439.
After 29473 training step(s), loss on training batch is 0.00331968.
After 29474 training step(s), loss on training batch is 0.00310446.
After 29475 training step(s), loss on training batch is 0.00325738.
After 29476 training step(s), loss on training batch is 0.00329442.
After 29477 training step(s), loss on training batch is 0.0030907.
After 29478 training step(s), loss on training batch is 0.0031849.
After 29479 training step(s), loss on training batch is 0.00320031.
After 29480 training step(s), loss on training batch is 0.00335797.
After 29481 training step(s), loss on training batch is 0.00303256.
After 29482 training step(s), loss on training batch is 0.00390311.
After 29483 training step(s), loss on training batch is 0.00314706.
After 29484 training step(s), loss on training batch is 0.0032342.
After 29485 training step(s), loss on training batch is 0.00313706.
After 29486 training step(s), loss on training batch is 0.00325986.
After 29487 training step(s), loss on training batch is 0.00318822.
After 29488 training step(s), loss on training batch is 0.0031218.
After 29489 training step(s), loss on training batch is 0.00336826.
After 29490 training step(s), loss on training batch is 0.0037107.
After 29491 training step(s), loss on training batch is 0.00300001.
After 29492 training step(s), loss on training batch is 0.00382465.
After 29493 training step(s), loss on training batch is 0.00292397.
After 29494 training step(s), loss on training batch is 0.00317311.
After 29495 training step(s), loss on training batch is 0.00356747.
After 29496 training step(s), loss on training batch is 0.00347034.
After 29497 training step(s), loss on training batch is 0.00341589.
After 29498 training step(s), loss on training batch is 0.00297038.
After 29499 training step(s), loss on training batch is 0.00332183.
After 29500 training step(s), loss on training batch is 0.00328618.
After 29501 training step(s), loss on training batch is 0.00298173.
After 29502 training step(s), loss on training batch is 0.00310214.
After 29503 training step(s), loss on training batch is 0.00321756.
After 29504 training step(s), loss on training batch is 0.00436534.
After 29505 training step(s), loss on training batch is 0.00361391.
After 29506 training step(s), loss on training batch is 0.00304467.
After 29507 training step(s), loss on training batch is 0.00326076.
After 29508 training step(s), loss on training batch is 0.00310914.
After 29509 training step(s), loss on training batch is 0.00303741.
After 29510 training step(s), loss on training batch is 0.00296122.
After 29511 training step(s), loss on training batch is 0.00327481.
After 29512 training step(s), loss on training batch is 0.00311676.
After 29513 training step(s), loss on training batch is 0.00328758.
After 29514 training step(s), loss on training batch is 0.00321931.
After 29515 training step(s), loss on training batch is 0.00334423.
After 29516 training step(s), loss on training batch is 0.00330299.
After 29517 training step(s), loss on training batch is 0.00285392.
After 29518 training step(s), loss on training batch is 0.00458216.
After 29519 training step(s), loss on training batch is 0.00309691.
After 29520 training step(s), loss on training batch is 0.00364967.
After 29521 training step(s), loss on training batch is 0.00304479.
After 29522 training step(s), loss on training batch is 0.00290997.
After 29523 training step(s), loss on training batch is 0.00322008.
After 29524 training step(s), loss on training batch is 0.00298796.
After 29525 training step(s), loss on training batch is 0.00307864.
After 29526 training step(s), loss on training batch is 0.0034284.
After 29527 training step(s), loss on training batch is 0.00407196.
After 29528 training step(s), loss on training batch is 0.00299489.
After 29529 training step(s), loss on training batch is 0.00334026.
After 29530 training step(s), loss on training batch is 0.0035297.
After 29531 training step(s), loss on training batch is 0.00308886.
After 29532 training step(s), loss on training batch is 0.00325911.
After 29533 training step(s), loss on training batch is 0.0030181.
After 29534 training step(s), loss on training batch is 0.00305643.
After 29535 training step(s), loss on training batch is 0.00313017.
After 29536 training step(s), loss on training batch is 0.00303846.
After 29537 training step(s), loss on training batch is 0.00289488.
After 29538 training step(s), loss on training batch is 0.00298464.
After 29539 training step(s), loss on training batch is 0.00313897.
After 29540 training step(s), loss on training batch is 0.00307007.
After 29541 training step(s), loss on training batch is 0.00295551.
After 29542 training step(s), loss on training batch is 0.00297468.
After 29543 training step(s), loss on training batch is 0.00323562.
After 29544 training step(s), loss on training batch is 0.00300117.
After 29545 training step(s), loss on training batch is 0.00337998.
After 29546 training step(s), loss on training batch is 0.0029551.
After 29547 training step(s), loss on training batch is 0.0032008.
After 29548 training step(s), loss on training batch is 0.00323266.
After 29549 training step(s), loss on training batch is 0.00363044.
After 29550 training step(s), loss on training batch is 0.00311405.
After 29551 training step(s), loss on training batch is 0.0031336.
After 29552 training step(s), loss on training batch is 0.00311194.
After 29553 training step(s), loss on training batch is 0.00345374.
After 29554 training step(s), loss on training batch is 0.00301002.
After 29555 training step(s), loss on training batch is 0.00297342.
After 29556 training step(s), loss on training batch is 0.00296867.
After 29557 training step(s), loss on training batch is 0.00314533.
After 29558 training step(s), loss on training batch is 0.00335334.
After 29559 training step(s), loss on training batch is 0.00323349.
After 29560 training step(s), loss on training batch is 0.00311583.
After 29561 training step(s), loss on training batch is 0.00331554.
After 29562 training step(s), loss on training batch is 0.00317093.
After 29563 training step(s), loss on training batch is 0.0033154.
After 29564 training step(s), loss on training batch is 0.00311881.
After 29565 training step(s), loss on training batch is 0.00308894.
After 29566 training step(s), loss on training batch is 0.00323173.
After 29567 training step(s), loss on training batch is 0.00353363.
After 29568 training step(s), loss on training batch is 0.00313442.
After 29569 training step(s), loss on training batch is 0.00342153.
After 29570 training step(s), loss on training batch is 0.00328394.
After 29571 training step(s), loss on training batch is 0.00313996.
After 29572 training step(s), loss on training batch is 0.00374819.
After 29573 training step(s), loss on training batch is 0.00307415.
After 29574 training step(s), loss on training batch is 0.00328038.
After 29575 training step(s), loss on training batch is 0.00304197.
After 29576 training step(s), loss on training batch is 0.00319384.
After 29577 training step(s), loss on training batch is 0.00342118.
After 29578 training step(s), loss on training batch is 0.00311384.
After 29579 training step(s), loss on training batch is 0.00358268.
After 29580 training step(s), loss on training batch is 0.00312742.
After 29581 training step(s), loss on training batch is 0.00328412.
After 29582 training step(s), loss on training batch is 0.00332017.
After 29583 training step(s), loss on training batch is 0.00366109.
After 29584 training step(s), loss on training batch is 0.00362144.
After 29585 training step(s), loss on training batch is 0.00297428.
After 29586 training step(s), loss on training batch is 0.00298885.
After 29587 training step(s), loss on training batch is 0.00327363.
After 29588 training step(s), loss on training batch is 0.00329737.
After 29589 training step(s), loss on training batch is 0.00309142.
After 29590 training step(s), loss on training batch is 0.00384363.
After 29591 training step(s), loss on training batch is 0.00362073.
After 29592 training step(s), loss on training batch is 0.0034063.
After 29593 training step(s), loss on training batch is 0.00311838.
After 29594 training step(s), loss on training batch is 0.0029166.
After 29595 training step(s), loss on training batch is 0.00351485.
After 29596 training step(s), loss on training batch is 0.00323069.
After 29597 training step(s), loss on training batch is 0.00335596.
After 29598 training step(s), loss on training batch is 0.00352823.
After 29599 training step(s), loss on training batch is 0.00303456.
After 29600 training step(s), loss on training batch is 0.00442255.
After 29601 training step(s), loss on training batch is 0.00327574.
After 29602 training step(s), loss on training batch is 0.00330986.
After 29603 training step(s), loss on training batch is 0.00317256.
After 29604 training step(s), loss on training batch is 0.00343276.
After 29605 training step(s), loss on training batch is 0.00313068.
After 29606 training step(s), loss on training batch is 0.00352124.
After 29607 training step(s), loss on training batch is 0.00327247.
After 29608 training step(s), loss on training batch is 0.00332622.
After 29609 training step(s), loss on training batch is 0.0034382.
After 29610 training step(s), loss on training batch is 0.00293309.
After 29611 training step(s), loss on training batch is 0.0032884.
After 29612 training step(s), loss on training batch is 0.00305083.
After 29613 training step(s), loss on training batch is 0.00317613.
After 29614 training step(s), loss on training batch is 0.00310243.
After 29615 training step(s), loss on training batch is 0.00295596.
After 29616 training step(s), loss on training batch is 0.00344001.
After 29617 training step(s), loss on training batch is 0.00311356.
After 29618 training step(s), loss on training batch is 0.00310949.
After 29619 training step(s), loss on training batch is 0.00294663.
After 29620 training step(s), loss on training batch is 0.00377403.
After 29621 training step(s), loss on training batch is 0.00341066.
After 29622 training step(s), loss on training batch is 0.00317114.
After 29623 training step(s), loss on training batch is 0.00298116.
After 29624 training step(s), loss on training batch is 0.00326269.
After 29625 training step(s), loss on training batch is 0.00312306.
After 29626 training step(s), loss on training batch is 0.00287971.
After 29627 training step(s), loss on training batch is 0.00342113.
After 29628 training step(s), loss on training batch is 0.00318125.
After 29629 training step(s), loss on training batch is 0.002878.
After 29630 training step(s), loss on training batch is 0.00292475.
After 29631 training step(s), loss on training batch is 0.00307975.
After 29632 training step(s), loss on training batch is 0.00326963.
After 29633 training step(s), loss on training batch is 0.00353557.
After 29634 training step(s), loss on training batch is 0.00311095.
After 29635 training step(s), loss on training batch is 0.00303896.
After 29636 training step(s), loss on training batch is 0.00321394.
After 29637 training step(s), loss on training batch is 0.00292693.
After 29638 training step(s), loss on training batch is 0.00308909.
After 29639 training step(s), loss on training batch is 0.00320761.
After 29640 training step(s), loss on training batch is 0.00321008.
After 29641 training step(s), loss on training batch is 0.00356235.
After 29642 training step(s), loss on training batch is 0.00329976.
After 29643 training step(s), loss on training batch is 0.00324469.
After 29644 training step(s), loss on training batch is 0.0031458.
After 29645 training step(s), loss on training batch is 0.00332318.
After 29646 training step(s), loss on training batch is 0.0032442.
After 29647 training step(s), loss on training batch is 0.00303248.
After 29648 training step(s), loss on training batch is 0.00313942.
After 29649 training step(s), loss on training batch is 0.00326028.
After 29650 training step(s), loss on training batch is 0.00298071.
After 29651 training step(s), loss on training batch is 0.00296732.
After 29652 training step(s), loss on training batch is 0.00289818.
After 29653 training step(s), loss on training batch is 0.00295267.
After 29654 training step(s), loss on training batch is 0.00370344.
After 29655 training step(s), loss on training batch is 0.00305124.
After 29656 training step(s), loss on training batch is 0.00291468.
After 29657 training step(s), loss on training batch is 0.00320209.
After 29658 training step(s), loss on training batch is 0.0029685.
After 29659 training step(s), loss on training batch is 0.00312475.
After 29660 training step(s), loss on training batch is 0.00315677.
After 29661 training step(s), loss on training batch is 0.0032256.
After 29662 training step(s), loss on training batch is 0.00327811.
After 29663 training step(s), loss on training batch is 0.00325114.
After 29664 training step(s), loss on training batch is 0.00328034.
After 29665 training step(s), loss on training batch is 0.00310597.
After 29666 training step(s), loss on training batch is 0.00303788.
After 29667 training step(s), loss on training batch is 0.0028643.
After 29668 training step(s), loss on training batch is 0.00319611.
After 29669 training step(s), loss on training batch is 0.0030462.
After 29670 training step(s), loss on training batch is 0.00309093.
After 29671 training step(s), loss on training batch is 0.00323488.
After 29672 training step(s), loss on training batch is 0.00302868.
After 29673 training step(s), loss on training batch is 0.00285079.
After 29674 training step(s), loss on training batch is 0.00315835.
After 29675 training step(s), loss on training batch is 0.00315715.
After 29676 training step(s), loss on training batch is 0.00315743.
After 29677 training step(s), loss on training batch is 0.00312114.
After 29678 training step(s), loss on training batch is 0.00349057.
After 29679 training step(s), loss on training batch is 0.00296435.
After 29680 training step(s), loss on training batch is 0.00393362.
After 29681 training step(s), loss on training batch is 0.00320369.
After 29682 training step(s), loss on training batch is 0.00308014.
After 29683 training step(s), loss on training batch is 0.00297099.
After 29684 training step(s), loss on training batch is 0.00288297.
After 29685 training step(s), loss on training batch is 0.00320059.
After 29686 training step(s), loss on training batch is 0.00289385.
After 29687 training step(s), loss on training batch is 0.00319457.
After 29688 training step(s), loss on training batch is 0.00303142.
After 29689 training step(s), loss on training batch is 0.00333224.
After 29690 training step(s), loss on training batch is 0.00308502.
After 29691 training step(s), loss on training batch is 0.00295184.
After 29692 training step(s), loss on training batch is 0.00355497.
After 29693 training step(s), loss on training batch is 0.00291207.
After 29694 training step(s), loss on training batch is 0.00321742.
After 29695 training step(s), loss on training batch is 0.00309764.
After 29696 training step(s), loss on training batch is 0.00302653.
After 29697 training step(s), loss on training batch is 0.00306683.
After 29698 training step(s), loss on training batch is 0.00343026.
After 29699 training step(s), loss on training batch is 0.00305568.
After 29700 training step(s), loss on training batch is 0.00311148.
After 29701 training step(s), loss on training batch is 0.00319849.
After 29702 training step(s), loss on training batch is 0.0032096.
After 29703 training step(s), loss on training batch is 0.00291129.
After 29704 training step(s), loss on training batch is 0.00290439.
After 29705 training step(s), loss on training batch is 0.00367786.
After 29706 training step(s), loss on training batch is 0.00303208.
After 29707 training step(s), loss on training batch is 0.00324325.
After 29708 training step(s), loss on training batch is 0.00325636.
After 29709 training step(s), loss on training batch is 0.00325831.
After 29710 training step(s), loss on training batch is 0.00322436.
After 29711 training step(s), loss on training batch is 0.0035533.
After 29712 training step(s), loss on training batch is 0.00311191.
After 29713 training step(s), loss on training batch is 0.00305442.
After 29714 training step(s), loss on training batch is 0.00298649.
After 29715 training step(s), loss on training batch is 0.0029494.
After 29716 training step(s), loss on training batch is 0.00312724.
After 29717 training step(s), loss on training batch is 0.00305964.
After 29718 training step(s), loss on training batch is 0.00314006.
After 29719 training step(s), loss on training batch is 0.00320508.
After 29720 training step(s), loss on training batch is 0.00323117.
After 29721 training step(s), loss on training batch is 0.00331205.
After 29722 training step(s), loss on training batch is 0.00329009.
After 29723 training step(s), loss on training batch is 0.00306359.
After 29724 training step(s), loss on training batch is 0.00310242.
After 29725 training step(s), loss on training batch is 0.00310188.
After 29726 training step(s), loss on training batch is 0.0031031.
After 29727 training step(s), loss on training batch is 0.00325644.
After 29728 training step(s), loss on training batch is 0.00341784.
After 29729 training step(s), loss on training batch is 0.0030622.
After 29730 training step(s), loss on training batch is 0.00304054.
After 29731 training step(s), loss on training batch is 0.00293281.
After 29732 training step(s), loss on training batch is 0.002852.
After 29733 training step(s), loss on training batch is 0.00345341.
After 29734 training step(s), loss on training batch is 0.00330178.
After 29735 training step(s), loss on training batch is 0.0035007.
After 29736 training step(s), loss on training batch is 0.00320176.
After 29737 training step(s), loss on training batch is 0.00346707.
After 29738 training step(s), loss on training batch is 0.00325287.
After 29739 training step(s), loss on training batch is 0.0030799.
After 29740 training step(s), loss on training batch is 0.00298654.
After 29741 training step(s), loss on training batch is 0.0032219.
After 29742 training step(s), loss on training batch is 0.00341987.
After 29743 training step(s), loss on training batch is 0.00319925.
After 29744 training step(s), loss on training batch is 0.00317573.
After 29745 training step(s), loss on training batch is 0.00368145.
After 29746 training step(s), loss on training batch is 0.00296806.
After 29747 training step(s), loss on training batch is 0.00306512.
After 29748 training step(s), loss on training batch is 0.00301885.
After 29749 training step(s), loss on training batch is 0.00327295.
After 29750 training step(s), loss on training batch is 0.00283683.
After 29751 training step(s), loss on training batch is 0.00300459.
After 29752 training step(s), loss on training batch is 0.00313506.
After 29753 training step(s), loss on training batch is 0.00321523.
After 29754 training step(s), loss on training batch is 0.00311986.
After 29755 training step(s), loss on training batch is 0.00290082.
After 29756 training step(s), loss on training batch is 0.00295441.
After 29757 training step(s), loss on training batch is 0.00324048.
After 29758 training step(s), loss on training batch is 0.00299565.
After 29759 training step(s), loss on training batch is 0.00329355.
After 29760 training step(s), loss on training batch is 0.00305763.
After 29761 training step(s), loss on training batch is 0.00337061.
After 29762 training step(s), loss on training batch is 0.00308747.
After 29763 training step(s), loss on training batch is 0.00288943.
After 29764 training step(s), loss on training batch is 0.00292534.
After 29765 training step(s), loss on training batch is 0.00330885.
After 29766 training step(s), loss on training batch is 0.00312173.
After 29767 training step(s), loss on training batch is 0.00322285.
After 29768 training step(s), loss on training batch is 0.00293254.
After 29769 training step(s), loss on training batch is 0.00326784.
After 29770 training step(s), loss on training batch is 0.00347582.
After 29771 training step(s), loss on training batch is 0.00321869.
After 29772 training step(s), loss on training batch is 0.00320855.
After 29773 training step(s), loss on training batch is 0.00296239.
After 29774 training step(s), loss on training batch is 0.00329386.
After 29775 training step(s), loss on training batch is 0.00355773.
After 29776 training step(s), loss on training batch is 0.0031657.
After 29777 training step(s), loss on training batch is 0.00320192.
After 29778 training step(s), loss on training batch is 0.00307545.
After 29779 training step(s), loss on training batch is 0.00317003.
After 29780 training step(s), loss on training batch is 0.0029171.
After 29781 training step(s), loss on training batch is 0.00342071.
After 29782 training step(s), loss on training batch is 0.00311063.
After 29783 training step(s), loss on training batch is 0.00293309.
After 29784 training step(s), loss on training batch is 0.00302852.
After 29785 training step(s), loss on training batch is 0.00337962.
After 29786 training step(s), loss on training batch is 0.00302835.
After 29787 training step(s), loss on training batch is 0.0029308.
After 29788 training step(s), loss on training batch is 0.0029656.
After 29789 training step(s), loss on training batch is 0.00309317.
After 29790 training step(s), loss on training batch is 0.0029284.
After 29791 training step(s), loss on training batch is 0.00296074.
After 29792 training step(s), loss on training batch is 0.00296505.
After 29793 training step(s), loss on training batch is 0.00293884.
After 29794 training step(s), loss on training batch is 0.00320744.
After 29795 training step(s), loss on training batch is 0.00325716.
After 29796 training step(s), loss on training batch is 0.00340669.
After 29797 training step(s), loss on training batch is 0.00380267.
After 29798 training step(s), loss on training batch is 0.00385853.
After 29799 training step(s), loss on training batch is 0.00302592.
After 29800 training step(s), loss on training batch is 0.0036853.
After 29801 training step(s), loss on training batch is 0.00321256.
After 29802 training step(s), loss on training batch is 0.00310881.
After 29803 training step(s), loss on training batch is 0.00301191.
After 29804 training step(s), loss on training batch is 0.00304055.
After 29805 training step(s), loss on training batch is 0.00307663.
After 29806 training step(s), loss on training batch is 0.00320688.
After 29807 training step(s), loss on training batch is 0.00304694.
After 29808 training step(s), loss on training batch is 0.00305693.
After 29809 training step(s), loss on training batch is 0.00302751.
After 29810 training step(s), loss on training batch is 0.00344981.
After 29811 training step(s), loss on training batch is 0.00311191.
After 29812 training step(s), loss on training batch is 0.00313105.
After 29813 training step(s), loss on training batch is 0.00281429.
After 29814 training step(s), loss on training batch is 0.00333115.
After 29815 training step(s), loss on training batch is 0.00320939.
After 29816 training step(s), loss on training batch is 0.00287911.
After 29817 training step(s), loss on training batch is 0.00338973.
After 29818 training step(s), loss on training batch is 0.00339043.
After 29819 training step(s), loss on training batch is 0.00356897.
After 29820 training step(s), loss on training batch is 0.00313954.
After 29821 training step(s), loss on training batch is 0.00444767.
After 29822 training step(s), loss on training batch is 0.00314665.
After 29823 training step(s), loss on training batch is 0.00316453.
After 29824 training step(s), loss on training batch is 0.00298393.
After 29825 training step(s), loss on training batch is 0.00314176.
After 29826 training step(s), loss on training batch is 0.00342173.
After 29827 training step(s), loss on training batch is 0.00299478.
After 29828 training step(s), loss on training batch is 0.00301495.
After 29829 training step(s), loss on training batch is 0.00404888.
After 29830 training step(s), loss on training batch is 0.00320167.
After 29831 training step(s), loss on training batch is 0.00310724.
After 29832 training step(s), loss on training batch is 0.00316565.
After 29833 training step(s), loss on training batch is 0.00316099.
After 29834 training step(s), loss on training batch is 0.00311408.
After 29835 training step(s), loss on training batch is 0.00323803.
After 29836 training step(s), loss on training batch is 0.00307418.
After 29837 training step(s), loss on training batch is 0.00326315.
After 29838 training step(s), loss on training batch is 0.00297751.
After 29839 training step(s), loss on training batch is 0.00322237.
After 29840 training step(s), loss on training batch is 0.0029847.
After 29841 training step(s), loss on training batch is 0.00349507.
After 29842 training step(s), loss on training batch is 0.00292729.
After 29843 training step(s), loss on training batch is 0.00326965.
After 29844 training step(s), loss on training batch is 0.0034422.
After 29845 training step(s), loss on training batch is 0.00342424.
After 29846 training step(s), loss on training batch is 0.00341862.
After 29847 training step(s), loss on training batch is 0.00315921.
After 29848 training step(s), loss on training batch is 0.00303895.
After 29849 training step(s), loss on training batch is 0.00361946.
After 29850 training step(s), loss on training batch is 0.00349216.
After 29851 training step(s), loss on training batch is 0.00289501.
After 29852 training step(s), loss on training batch is 0.00327653.
After 29853 training step(s), loss on training batch is 0.00332897.
After 29854 training step(s), loss on training batch is 0.00313307.
After 29855 training step(s), loss on training batch is 0.00318327.
After 29856 training step(s), loss on training batch is 0.00313075.
After 29857 training step(s), loss on training batch is 0.00285812.
After 29858 training step(s), loss on training batch is 0.00306131.
After 29859 training step(s), loss on training batch is 0.00302154.
After 29860 training step(s), loss on training batch is 0.00329519.
After 29861 training step(s), loss on training batch is 0.00308285.
After 29862 training step(s), loss on training batch is 0.00332182.
After 29863 training step(s), loss on training batch is 0.00300259.
After 29864 training step(s), loss on training batch is 0.00341554.
After 29865 training step(s), loss on training batch is 0.00341726.
After 29866 training step(s), loss on training batch is 0.00306682.
After 29867 training step(s), loss on training batch is 0.00302864.
After 29868 training step(s), loss on training batch is 0.0033894.
After 29869 training step(s), loss on training batch is 0.00350088.
After 29870 training step(s), loss on training batch is 0.00324193.
After 29871 training step(s), loss on training batch is 0.00300289.
After 29872 training step(s), loss on training batch is 0.00358258.
After 29873 training step(s), loss on training batch is 0.00311521.
After 29874 training step(s), loss on training batch is 0.00333894.
After 29875 training step(s), loss on training batch is 0.00326291.
After 29876 training step(s), loss on training batch is 0.00292285.
After 29877 training step(s), loss on training batch is 0.00309012.
After 29878 training step(s), loss on training batch is 0.00316924.
After 29879 training step(s), loss on training batch is 0.00302298.
After 29880 training step(s), loss on training batch is 0.0036402.
After 29881 training step(s), loss on training batch is 0.00298941.
After 29882 training step(s), loss on training batch is 0.00330912.
After 29883 training step(s), loss on training batch is 0.00293225.
After 29884 training step(s), loss on training batch is 0.00314065.
After 29885 training step(s), loss on training batch is 0.00309851.
After 29886 training step(s), loss on training batch is 0.00407386.
After 29887 training step(s), loss on training batch is 0.00354162.
After 29888 training step(s), loss on training batch is 0.00333454.
After 29889 training step(s), loss on training batch is 0.00353428.
After 29890 training step(s), loss on training batch is 0.00323499.
After 29891 training step(s), loss on training batch is 0.00319365.
After 29892 training step(s), loss on training batch is 0.00326126.
After 29893 training step(s), loss on training batch is 0.00311285.
After 29894 training step(s), loss on training batch is 0.00292146.
After 29895 training step(s), loss on training batch is 0.00341977.
After 29896 training step(s), loss on training batch is 0.00311146.
After 29897 training step(s), loss on training batch is 0.00309037.
After 29898 training step(s), loss on training batch is 0.00313541.
After 29899 training step(s), loss on training batch is 0.00299104.
After 29900 training step(s), loss on training batch is 0.00371693.
After 29901 training step(s), loss on training batch is 0.00431222.
After 29902 training step(s), loss on training batch is 0.00331928.
After 29903 training step(s), loss on training batch is 0.00322874.
After 29904 training step(s), loss on training batch is 0.00308255.
After 29905 training step(s), loss on training batch is 0.00303722.
After 29906 training step(s), loss on training batch is 0.00292828.
After 29907 training step(s), loss on training batch is 0.00317016.
After 29908 training step(s), loss on training batch is 0.00317789.
After 29909 training step(s), loss on training batch is 0.00296837.
After 29910 training step(s), loss on training batch is 0.00327277.
After 29911 training step(s), loss on training batch is 0.00306115.
After 29912 training step(s), loss on training batch is 0.00328311.
After 29913 training step(s), loss on training batch is 0.00355994.
After 29914 training step(s), loss on training batch is 0.00334489.
After 29915 training step(s), loss on training batch is 0.00305718.
After 29916 training step(s), loss on training batch is 0.00340405.
After 29917 training step(s), loss on training batch is 0.00344631.
After 29918 training step(s), loss on training batch is 0.00321844.
After 29919 training step(s), loss on training batch is 0.00312194.
After 29920 training step(s), loss on training batch is 0.00331747.
After 29921 training step(s), loss on training batch is 0.00351823.
After 29922 training step(s), loss on training batch is 0.00296422.
After 29923 training step(s), loss on training batch is 0.00314809.
After 29924 training step(s), loss on training batch is 0.00341653.
After 29925 training step(s), loss on training batch is 0.0030577.
After 29926 training step(s), loss on training batch is 0.00348171.
After 29927 training step(s), loss on training batch is 0.00360763.
After 29928 training step(s), loss on training batch is 0.00306822.
After 29929 training step(s), loss on training batch is 0.0031192.
After 29930 training step(s), loss on training batch is 0.00318659.
After 29931 training step(s), loss on training batch is 0.00304424.
After 29932 training step(s), loss on training batch is 0.00324951.
After 29933 training step(s), loss on training batch is 0.00333933.
After 29934 training step(s), loss on training batch is 0.00337381.
After 29935 training step(s), loss on training batch is 0.00379128.
After 29936 training step(s), loss on training batch is 0.00316207.
After 29937 training step(s), loss on training batch is 0.00287173.
After 29938 training step(s), loss on training batch is 0.00315403.
After 29939 training step(s), loss on training batch is 0.00334644.
After 29940 training step(s), loss on training batch is 0.00332742.
After 29941 training step(s), loss on training batch is 0.0032831.
After 29942 training step(s), loss on training batch is 0.00340291.
After 29943 training step(s), loss on training batch is 0.00326001.
After 29944 training step(s), loss on training batch is 0.00298541.
After 29945 training step(s), loss on training batch is 0.00294989.
After 29946 training step(s), loss on training batch is 0.00302803.
After 29947 training step(s), loss on training batch is 0.00372486.
After 29948 training step(s), loss on training batch is 0.00321321.
After 29949 training step(s), loss on training batch is 0.00328049.
After 29950 training step(s), loss on training batch is 0.00297372.
After 29951 training step(s), loss on training batch is 0.00314996.
After 29952 training step(s), loss on training batch is 0.00313626.
After 29953 training step(s), loss on training batch is 0.00304356.
After 29954 training step(s), loss on training batch is 0.00312732.
After 29955 training step(s), loss on training batch is 0.00300399.
After 29956 training step(s), loss on training batch is 0.00320101.
After 29957 training step(s), loss on training batch is 0.00309617.
After 29958 training step(s), loss on training batch is 0.00337996.
After 29959 training step(s), loss on training batch is 0.00306632.
After 29960 training step(s), loss on training batch is 0.00306249.
After 29961 training step(s), loss on training batch is 0.00326692.
After 29962 training step(s), loss on training batch is 0.00328141.
After 29963 training step(s), loss on training batch is 0.00293509.
After 29964 training step(s), loss on training batch is 0.0031523.
After 29965 training step(s), loss on training batch is 0.00286776.
After 29966 training step(s), loss on training batch is 0.00282722.
After 29967 training step(s), loss on training batch is 0.00311129.
After 29968 training step(s), loss on training batch is 0.00318032.
After 29969 training step(s), loss on training batch is 0.00380699.
After 29970 training step(s), loss on training batch is 0.00305777.
After 29971 training step(s), loss on training batch is 0.00309787.
After 29972 training step(s), loss on training batch is 0.00291788.
After 29973 training step(s), loss on training batch is 0.00290997.
After 29974 training step(s), loss on training batch is 0.00314102.
After 29975 training step(s), loss on training batch is 0.00349207.
After 29976 training step(s), loss on training batch is 0.00315745.
After 29977 training step(s), loss on training batch is 0.00337832.
After 29978 training step(s), loss on training batch is 0.00353631.
After 29979 training step(s), loss on training batch is 0.00291291.
After 29980 training step(s), loss on training batch is 0.00295745.
After 29981 training step(s), loss on training batch is 0.00291132.
After 29982 training step(s), loss on training batch is 0.00293607.
After 29983 training step(s), loss on training batch is 0.00352206.
After 29984 training step(s), loss on training batch is 0.00336246.
After 29985 training step(s), loss on training batch is 0.00303941.
After 29986 training step(s), loss on training batch is 0.00317496.
After 29987 training step(s), loss on training batch is 0.0030625.
After 29988 training step(s), loss on training batch is 0.0030098.
After 29989 training step(s), loss on training batch is 0.00318766.
After 29990 training step(s), loss on training batch is 0.00315478.
After 29991 training step(s), loss on training batch is 0.00332267.
After 29992 training step(s), loss on training batch is 0.00306639.
After 29993 training step(s), loss on training batch is 0.00287775.
After 29994 training step(s), loss on training batch is 0.00291761.
After 29995 training step(s), loss on training batch is 0.00303761.
After 29996 training step(s), loss on training batch is 0.00294153.
After 29997 training step(s), loss on training batch is 0.00298929.
After 29998 training step(s), loss on training batch is 0.00375614.
After 29999 training step(s), loss on training batch is 0.00336585.
After 30000 training step(s), loss on training batch is 0.00321682.