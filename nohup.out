2018-02-08 20:50:21.484836: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-08 20:50:21.484873: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-08 20:50:21.484893: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-02-08 20:50:21.484897: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-08 20:50:21.484901: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2018-02-08 20:50:21.644045: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-02-08 20:50:21.644365: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties: 
name: GeForce GTX 1080
major: 6 minor: 1 memoryClockRate (GHz) 1.8475
pciBusID 0000:01:00.0
Total memory: 7.92GiB
Free memory: 7.49GiB
2018-02-08 20:50:21.750752: W tensorflow/stream_executor/cuda/cuda_driver.cc:523] A non-primary context 0x5575c45ff0e0 exists before initializing the StreamExecutor. We haven't verified StreamExecutor works with that.
2018-02-08 20:50:21.750963: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-02-08 20:50:21.751323: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 1 with properties: 
name: GeForce GTX 1080
major: 6 minor: 1 memoryClockRate (GHz) 1.8475
pciBusID 0000:0a:00.0
Total memory: 7.92GiB
Free memory: 7.80GiB
2018-02-08 20:50:21.751679: I tensorflow/core/common_runtime/gpu/gpu_device.cc:847] Peer access not supported between device ordinals 0 and 1
2018-02-08 20:50:21.751695: I tensorflow/core/common_runtime/gpu/gpu_device.cc:847] Peer access not supported between device ordinals 1 and 0
2018-02-08 20:50:21.751710: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0 1 
2018-02-08 20:50:21.751715: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y N 
2018-02-08 20:50:21.751719: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 1:   N Y 
2018-02-08 20:50:21.751725: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0)
2018-02-08 20:50:21.751730: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:1) -> (device: 1, name: GeForce GTX 1080, pci bus id: 0000:0a:00.0)
After 1 training step(s), loss on training batch is 0.13743.
After 2 training step(s), loss on training batch is 0.242546.
After 3 training step(s), loss on training batch is 0.24428.
After 4 training step(s), loss on training batch is 0.216287.
After 5 training step(s), loss on training batch is 0.0246847.
After 6 training step(s), loss on training batch is 0.0250852.
After 7 training step(s), loss on training batch is 0.0224259.
After 8 training step(s), loss on training batch is 0.0240051.
After 9 training step(s), loss on training batch is 0.0238148.
After 10 training step(s), loss on training batch is 0.0223875.
After 11 training step(s), loss on training batch is 0.0229876.
After 12 training step(s), loss on training batch is 0.0238686.
After 13 training step(s), loss on training batch is 0.0234227.
After 14 training step(s), loss on training batch is 0.0238693.
After 15 training step(s), loss on training batch is 0.0235069.
After 16 training step(s), loss on training batch is 0.0225934.
After 17 training step(s), loss on training batch is 0.0230748.
After 18 training step(s), loss on training batch is 0.0229743.
After 19 training step(s), loss on training batch is 0.0227689.
After 20 training step(s), loss on training batch is 0.0219628.
After 21 training step(s), loss on training batch is 0.0209956.
After 22 training step(s), loss on training batch is 0.0223048.
After 23 training step(s), loss on training batch is 0.0218947.
After 24 training step(s), loss on training batch is 0.0211073.
After 25 training step(s), loss on training batch is 0.0217046.
After 26 training step(s), loss on training batch is 0.022059.
After 27 training step(s), loss on training batch is 0.0232177.
After 28 training step(s), loss on training batch is 0.0225998.
After 29 training step(s), loss on training batch is 0.0218479.
After 30 training step(s), loss on training batch is 0.0208913.
After 31 training step(s), loss on training batch is 0.020844.
After 32 training step(s), loss on training batch is 0.0209602.
After 33 training step(s), loss on training batch is 0.0208066.
After 34 training step(s), loss on training batch is 0.020413.
After 35 training step(s), loss on training batch is 0.0186151.
After 36 training step(s), loss on training batch is 0.0189396.
After 37 training step(s), loss on training batch is 0.0191691.
After 38 training step(s), loss on training batch is 0.0199353.
After 39 training step(s), loss on training batch is 0.0186206.
After 40 training step(s), loss on training batch is 0.019753.
After 41 training step(s), loss on training batch is 0.0182024.
After 42 training step(s), loss on training batch is 0.0188137.
After 43 training step(s), loss on training batch is 0.0182865.
After 44 training step(s), loss on training batch is 0.017718.
After 45 training step(s), loss on training batch is 0.016843.
After 46 training step(s), loss on training batch is 0.0188599.
After 47 training step(s), loss on training batch is 0.0177453.
After 48 training step(s), loss on training batch is 0.0173389.
After 49 training step(s), loss on training batch is 0.0174532.
After 50 training step(s), loss on training batch is 0.0176825.
After 51 training step(s), loss on training batch is 0.0177266.
After 52 training step(s), loss on training batch is 0.0177465.
After 53 training step(s), loss on training batch is 0.0173822.
After 54 training step(s), loss on training batch is 0.0164572.
After 55 training step(s), loss on training batch is 0.0167255.
After 56 training step(s), loss on training batch is 0.0170093.
After 57 training step(s), loss on training batch is 0.0172513.
After 58 training step(s), loss on training batch is 0.0164236.
After 59 training step(s), loss on training batch is 0.0165.
After 60 training step(s), loss on training batch is 0.0162765.
After 61 training step(s), loss on training batch is 0.0162743.
After 62 training step(s), loss on training batch is 0.0153746.
After 63 training step(s), loss on training batch is 0.0161136.
After 64 training step(s), loss on training batch is 0.0156475.
After 65 training step(s), loss on training batch is 0.0153596.
After 66 training step(s), loss on training batch is 0.0152128.
After 67 training step(s), loss on training batch is 0.0139334.
After 68 training step(s), loss on training batch is 0.0149974.
After 69 training step(s), loss on training batch is 0.0143688.
After 70 training step(s), loss on training batch is 0.0148835.
After 71 training step(s), loss on training batch is 0.0143228.
After 72 training step(s), loss on training batch is 0.0138767.
After 73 training step(s), loss on training batch is 0.0146226.
After 74 training step(s), loss on training batch is 0.0151465.
After 75 training step(s), loss on training batch is 0.014745.
After 76 training step(s), loss on training batch is 0.0137264.
After 77 training step(s), loss on training batch is 0.0155248.
After 78 training step(s), loss on training batch is 0.0141984.
After 79 training step(s), loss on training batch is 0.0133302.
After 80 training step(s), loss on training batch is 0.0131673.
After 81 training step(s), loss on training batch is 0.0127607.
After 82 training step(s), loss on training batch is 0.0137638.
After 83 training step(s), loss on training batch is 0.0116183.
After 84 training step(s), loss on training batch is 0.0126086.
After 85 training step(s), loss on training batch is 0.0126391.
After 86 training step(s), loss on training batch is 0.0125127.
After 87 training step(s), loss on training batch is 0.0130287.
After 88 training step(s), loss on training batch is 0.012122.
After 89 training step(s), loss on training batch is 0.0124402.
After 90 training step(s), loss on training batch is 0.0104419.
After 91 training step(s), loss on training batch is 0.011398.
After 92 training step(s), loss on training batch is 0.012365.
After 93 training step(s), loss on training batch is 0.0104296.
After 94 training step(s), loss on training batch is 0.0109003.
After 95 training step(s), loss on training batch is 0.0111255.
After 96 training step(s), loss on training batch is 0.0109183.
After 97 training step(s), loss on training batch is 0.0108288.
After 98 training step(s), loss on training batch is 0.010018.
After 99 training step(s), loss on training batch is 0.0104391.
After 100 training step(s), loss on training batch is 0.0112614.
After 101 training step(s), loss on training batch is 0.0118506.
After 102 training step(s), loss on training batch is 0.01578.
After 103 training step(s), loss on training batch is 0.0135167.
After 104 training step(s), loss on training batch is 0.0133983.
After 105 training step(s), loss on training batch is 0.0176493.
After 106 training step(s), loss on training batch is 0.0178635.
After 107 training step(s), loss on training batch is 0.0194537.
After 108 training step(s), loss on training batch is 0.0172821.
After 109 training step(s), loss on training batch is 0.0143249.
After 110 training step(s), loss on training batch is 0.0135019.
After 111 training step(s), loss on training batch is 0.0126985.
After 112 training step(s), loss on training batch is 0.0136817.
After 113 training step(s), loss on training batch is 0.0122075.
After 114 training step(s), loss on training batch is 0.0135342.
After 115 training step(s), loss on training batch is 0.0148983.
After 116 training step(s), loss on training batch is 0.0152461.
After 117 training step(s), loss on training batch is 0.0150691.
After 118 training step(s), loss on training batch is 0.0161553.
After 119 training step(s), loss on training batch is 0.0141305.
After 120 training step(s), loss on training batch is 0.016104.
After 121 training step(s), loss on training batch is 0.0181817.
After 122 training step(s), loss on training batch is 0.0137116.
After 123 training step(s), loss on training batch is 0.0134393.
After 124 training step(s), loss on training batch is 0.0102655.
After 125 training step(s), loss on training batch is 0.0109592.
After 126 training step(s), loss on training batch is 0.0107918.
After 127 training step(s), loss on training batch is 0.0149083.
After 128 training step(s), loss on training batch is 0.0150199.
After 129 training step(s), loss on training batch is 0.0134024.
After 130 training step(s), loss on training batch is 0.0110379.
After 131 training step(s), loss on training batch is 0.0128798.
After 132 training step(s), loss on training batch is 0.0122986.
After 133 training step(s), loss on training batch is 0.0104308.
After 134 training step(s), loss on training batch is 0.00988358.
After 135 training step(s), loss on training batch is 0.0104802.
After 136 training step(s), loss on training batch is 0.0114498.
After 137 training step(s), loss on training batch is 0.0137635.
After 138 training step(s), loss on training batch is 0.0209502.
After 139 training step(s), loss on training batch is 0.0176007.
After 140 training step(s), loss on training batch is 0.0205093.
After 141 training step(s), loss on training batch is 0.0118218.
After 142 training step(s), loss on training batch is 0.0133362.
After 143 training step(s), loss on training batch is 0.0132586.
After 144 training step(s), loss on training batch is 0.0139965.
After 145 training step(s), loss on training batch is 0.0103617.
After 146 training step(s), loss on training batch is 0.00978742.
After 147 training step(s), loss on training batch is 0.0100065.
After 148 training step(s), loss on training batch is 0.00904261.
After 149 training step(s), loss on training batch is 0.0100913.
After 150 training step(s), loss on training batch is 0.012566.
After 151 training step(s), loss on training batch is 0.0123634.
After 152 training step(s), loss on training batch is 0.0110848.
After 153 training step(s), loss on training batch is 0.00982521.
After 154 training step(s), loss on training batch is 0.00920297.
After 155 training step(s), loss on training batch is 0.00959307.
After 156 training step(s), loss on training batch is 0.0103443.
After 157 training step(s), loss on training batch is 0.0119309.
After 158 training step(s), loss on training batch is 0.00946699.
After 159 training step(s), loss on training batch is 0.00905975.
After 160 training step(s), loss on training batch is 0.00917082.
After 161 training step(s), loss on training batch is 0.00767856.
After 162 training step(s), loss on training batch is 0.00761489.
After 163 training step(s), loss on training batch is 0.00847523.
After 164 training step(s), loss on training batch is 0.0114953.
After 165 training step(s), loss on training batch is 0.0108285.
After 166 training step(s), loss on training batch is 0.0100444.
After 167 training step(s), loss on training batch is 0.00954265.
After 168 training step(s), loss on training batch is 0.00716713.
After 169 training step(s), loss on training batch is 0.0101214.
After 170 training step(s), loss on training batch is 0.0083963.
After 171 training step(s), loss on training batch is 0.00760838.
After 172 training step(s), loss on training batch is 0.00767288.
After 173 training step(s), loss on training batch is 0.00759781.
After 174 training step(s), loss on training batch is 0.00700601.
After 175 training step(s), loss on training batch is 0.00969227.
After 176 training step(s), loss on training batch is 0.00963095.
After 177 training step(s), loss on training batch is 0.00921192.
After 178 training step(s), loss on training batch is 0.00716172.
After 179 training step(s), loss on training batch is 0.00586899.
After 180 training step(s), loss on training batch is 0.00981118.
After 181 training step(s), loss on training batch is 0.00941513.
After 182 training step(s), loss on training batch is 0.00914314.
After 183 training step(s), loss on training batch is 0.00768259.
After 184 training step(s), loss on training batch is 0.0079896.
After 185 training step(s), loss on training batch is 0.0085945.
After 186 training step(s), loss on training batch is 0.00791741.
After 187 training step(s), loss on training batch is 0.00859953.
After 188 training step(s), loss on training batch is 0.00765377.
After 189 training step(s), loss on training batch is 0.00696224.
After 190 training step(s), loss on training batch is 0.00692562.
After 191 training step(s), loss on training batch is 0.00716304.
After 192 training step(s), loss on training batch is 0.00782612.
After 193 training step(s), loss on training batch is 0.00662349.
After 194 training step(s), loss on training batch is 0.00618836.
After 195 training step(s), loss on training batch is 0.00797483.
After 196 training step(s), loss on training batch is 0.0078089.
After 197 training step(s), loss on training batch is 0.00637644.
After 198 training step(s), loss on training batch is 0.00938031.
After 199 training step(s), loss on training batch is 0.0116546.
After 200 training step(s), loss on training batch is 0.0136025.
After 201 training step(s), loss on training batch is 0.00952765.
After 202 training step(s), loss on training batch is 0.01236.
After 203 training step(s), loss on training batch is 0.00585466.
After 204 training step(s), loss on training batch is 0.00506571.
After 205 training step(s), loss on training batch is 0.0059834.
After 206 training step(s), loss on training batch is 0.00766475.
After 207 training step(s), loss on training batch is 0.00730016.
After 208 training step(s), loss on training batch is 0.00748436.
After 209 training step(s), loss on training batch is 0.00541875.
After 210 training step(s), loss on training batch is 0.00744601.
After 211 training step(s), loss on training batch is 0.00658768.
After 212 training step(s), loss on training batch is 0.00672583.
After 213 training step(s), loss on training batch is 0.00592045.
After 214 training step(s), loss on training batch is 0.00591119.
After 215 training step(s), loss on training batch is 0.00571376.
After 216 training step(s), loss on training batch is 0.0057769.
After 217 training step(s), loss on training batch is 0.00615662.
After 218 training step(s), loss on training batch is 0.00750986.
After 219 training step(s), loss on training batch is 0.00845254.
After 220 training step(s), loss on training batch is 0.00541491.
After 221 training step(s), loss on training batch is 0.00951611.
After 222 training step(s), loss on training batch is 0.00677374.
After 223 training step(s), loss on training batch is 0.00581426.
After 224 training step(s), loss on training batch is 0.00549851.
After 225 training step(s), loss on training batch is 0.00642424.
After 226 training step(s), loss on training batch is 0.00934519.
After 227 training step(s), loss on training batch is 0.00688214.
After 228 training step(s), loss on training batch is 0.00742723.
After 229 training step(s), loss on training batch is 0.00583056.
After 230 training step(s), loss on training batch is 0.00654447.
After 231 training step(s), loss on training batch is 0.00573005.
After 232 training step(s), loss on training batch is 0.00425903.
After 233 training step(s), loss on training batch is 0.00528529.
After 234 training step(s), loss on training batch is 0.00460388.
After 235 training step(s), loss on training batch is 0.00475985.
After 236 training step(s), loss on training batch is 0.00519838.
After 237 training step(s), loss on training batch is 0.00774353.
After 238 training step(s), loss on training batch is 0.00560657.
After 239 training step(s), loss on training batch is 0.00599091.
After 240 training step(s), loss on training batch is 0.0064623.
After 241 training step(s), loss on training batch is 0.00885947.
After 242 training step(s), loss on training batch is 0.00542685.
After 243 training step(s), loss on training batch is 0.00547388.
After 244 training step(s), loss on training batch is 0.00501624.
After 245 training step(s), loss on training batch is 0.00558213.
After 246 training step(s), loss on training batch is 0.00537548.
After 247 training step(s), loss on training batch is 0.00501748.
After 248 training step(s), loss on training batch is 0.00675373.
After 249 training step(s), loss on training batch is 0.00622036.
After 250 training step(s), loss on training batch is 0.00582623.
After 251 training step(s), loss on training batch is 0.00658168.
After 252 training step(s), loss on training batch is 0.00739025.
After 253 training step(s), loss on training batch is 0.00972643.
After 254 training step(s), loss on training batch is 0.0068799.
After 255 training step(s), loss on training batch is 0.00857798.
After 256 training step(s), loss on training batch is 0.0060964.
After 257 training step(s), loss on training batch is 0.00606799.
After 258 training step(s), loss on training batch is 0.00566907.
After 259 training step(s), loss on training batch is 0.00470215.
After 260 training step(s), loss on training batch is 0.00505095.
After 261 training step(s), loss on training batch is 0.00396087.
After 262 training step(s), loss on training batch is 0.00393876.
After 263 training step(s), loss on training batch is 0.00420981.
After 264 training step(s), loss on training batch is 0.00395197.
After 265 training step(s), loss on training batch is 0.00522892.
After 266 training step(s), loss on training batch is 0.00717274.
After 267 training step(s), loss on training batch is 0.00767415.
After 268 training step(s), loss on training batch is 0.00599058.
After 269 training step(s), loss on training batch is 0.00567432.
After 270 training step(s), loss on training batch is 0.00717724.
After 271 training step(s), loss on training batch is 0.00575656.
After 272 training step(s), loss on training batch is 0.00620841.
After 273 training step(s), loss on training batch is 0.00717073.
After 274 training step(s), loss on training batch is 0.0070549.
After 275 training step(s), loss on training batch is 0.00583589.
After 276 training step(s), loss on training batch is 0.00571146.
After 277 training step(s), loss on training batch is 0.00852766.
After 278 training step(s), loss on training batch is 0.00851818.
After 279 training step(s), loss on training batch is 0.00743386.
After 280 training step(s), loss on training batch is 0.00689459.
After 281 training step(s), loss on training batch is 0.00507506.
After 282 training step(s), loss on training batch is 0.005325.
After 283 training step(s), loss on training batch is 0.00601125.
After 284 training step(s), loss on training batch is 0.00490611.
After 285 training step(s), loss on training batch is 0.00484152.
After 286 training step(s), loss on training batch is 0.00556038.
After 287 training step(s), loss on training batch is 0.00515422.
After 288 training step(s), loss on training batch is 0.00610844.
After 289 training step(s), loss on training batch is 0.006235.
After 290 training step(s), loss on training batch is 0.00480516.
After 291 training step(s), loss on training batch is 0.00536582.
After 292 training step(s), loss on training batch is 0.00669183.
After 293 training step(s), loss on training batch is 0.00490768.
After 294 training step(s), loss on training batch is 0.0062843.
After 295 training step(s), loss on training batch is 0.00584731.
After 296 training step(s), loss on training batch is 0.00563494.
After 297 training step(s), loss on training batch is 0.00619707.
After 298 training step(s), loss on training batch is 0.00575211.
After 299 training step(s), loss on training batch is 0.00517063.
After 300 training step(s), loss on training batch is 0.00510002.
After 301 training step(s), loss on training batch is 0.00442263.
After 302 training step(s), loss on training batch is 0.00588533.
After 303 training step(s), loss on training batch is 0.0056619.
After 304 training step(s), loss on training batch is 0.00897756.
After 305 training step(s), loss on training batch is 0.0125509.
After 306 training step(s), loss on training batch is 0.0247232.
After 307 training step(s), loss on training batch is 0.0943568.
After 308 training step(s), loss on training batch is 0.263277.
After 309 training step(s), loss on training batch is 0.0249962.
After 310 training step(s), loss on training batch is 0.025322.
After 311 training step(s), loss on training batch is 0.0256175.
After 312 training step(s), loss on training batch is 0.0268101.
After 313 training step(s), loss on training batch is 0.0270237.
After 314 training step(s), loss on training batch is 0.0246671.
After 315 training step(s), loss on training batch is 0.0261416.
After 316 training step(s), loss on training batch is 0.0212629.
After 317 training step(s), loss on training batch is 0.0216919.
After 318 training step(s), loss on training batch is 0.0218007.
After 319 training step(s), loss on training batch is 0.0278333.
After 320 training step(s), loss on training batch is 0.0289658.
After 321 training step(s), loss on training batch is 0.0293933.
After 322 training step(s), loss on training batch is 0.0288144.
After 323 training step(s), loss on training batch is 0.0277985.
After 324 training step(s), loss on training batch is 0.0255606.
After 325 training step(s), loss on training batch is 0.0251269.
After 326 training step(s), loss on training batch is 0.0248989.
After 327 training step(s), loss on training batch is 0.0275001.
After 328 training step(s), loss on training batch is 0.0255818.
After 329 training step(s), loss on training batch is 0.0247621.
After 330 training step(s), loss on training batch is 0.025976.
After 331 training step(s), loss on training batch is 0.027662.
After 332 training step(s), loss on training batch is 0.0260581.
After 333 training step(s), loss on training batch is 0.0257966.
After 334 training step(s), loss on training batch is 0.0249324.
After 335 training step(s), loss on training batch is 0.0256932.
After 336 training step(s), loss on training batch is 0.0257227.
After 337 training step(s), loss on training batch is 0.0255044.
After 338 training step(s), loss on training batch is 0.0250026.
After 339 training step(s), loss on training batch is 0.02641.
After 340 training step(s), loss on training batch is 0.0264019.
After 341 training step(s), loss on training batch is 0.0249862.
After 342 training step(s), loss on training batch is 0.0246899.
After 343 training step(s), loss on training batch is 0.0238575.
After 344 training step(s), loss on training batch is 0.0240476.
After 345 training step(s), loss on training batch is 0.0253574.
After 346 training step(s), loss on training batch is 0.0275635.
After 347 training step(s), loss on training batch is 0.0251162.
After 348 training step(s), loss on training batch is 0.0247104.
After 349 training step(s), loss on training batch is 0.0226166.
After 350 training step(s), loss on training batch is 0.0224746.
After 351 training step(s), loss on training batch is 0.023107.
After 352 training step(s), loss on training batch is 0.0245571.
After 353 training step(s), loss on training batch is 0.0240048.
After 354 training step(s), loss on training batch is 0.0246563.
After 355 training step(s), loss on training batch is 0.0223287.
After 356 training step(s), loss on training batch is 0.0214333.
After 357 training step(s), loss on training batch is 0.0223177.
After 358 training step(s), loss on training batch is 0.0223288.
After 359 training step(s), loss on training batch is 0.0250925.
After 360 training step(s), loss on training batch is 0.0229323.
After 361 training step(s), loss on training batch is 0.0242926.
After 362 training step(s), loss on training batch is 0.0206509.
After 363 training step(s), loss on training batch is 0.0240618.
After 364 training step(s), loss on training batch is 0.0239433.
After 365 training step(s), loss on training batch is 0.0239681.
After 366 training step(s), loss on training batch is 0.0236001.
After 367 training step(s), loss on training batch is 0.0235095.
After 368 training step(s), loss on training batch is 0.0215291.
After 369 training step(s), loss on training batch is 0.0208814.
After 370 training step(s), loss on training batch is 0.021575.
After 371 training step(s), loss on training batch is 0.0214659.
After 372 training step(s), loss on training batch is 0.0230126.
After 373 training step(s), loss on training batch is 0.0229724.
After 374 training step(s), loss on training batch is 0.0224685.
After 375 training step(s), loss on training batch is 0.0245072.
After 376 training step(s), loss on training batch is 0.0238929.
After 377 training step(s), loss on training batch is 0.0218975.
After 378 training step(s), loss on training batch is 0.0221863.
After 379 training step(s), loss on training batch is 0.0214164.
After 380 training step(s), loss on training batch is 0.0202038.
After 381 training step(s), loss on training batch is 0.0208697.
After 382 training step(s), loss on training batch is 0.0206827.
After 383 training step(s), loss on training batch is 0.0230437.
After 384 training step(s), loss on training batch is 0.0211808.
After 385 training step(s), loss on training batch is 0.0221412.
After 386 training step(s), loss on training batch is 0.0237468.
After 387 training step(s), loss on training batch is 0.0220078.
After 388 training step(s), loss on training batch is 0.0203642.
After 389 training step(s), loss on training batch is 0.0205858.
After 390 training step(s), loss on training batch is 0.0207431.
After 391 training step(s), loss on training batch is 0.0205552.
After 392 training step(s), loss on training batch is 0.0194872.
After 393 training step(s), loss on training batch is 0.0203759.
After 394 training step(s), loss on training batch is 0.0202507.
After 395 training step(s), loss on training batch is 0.0186083.
After 396 training step(s), loss on training batch is 0.0202586.
After 397 training step(s), loss on training batch is 0.0203598.
After 398 training step(s), loss on training batch is 0.0204444.
After 399 training step(s), loss on training batch is 0.0197582.
After 400 training step(s), loss on training batch is 0.0212686.
After 401 training step(s), loss on training batch is 0.0213591.
After 402 training step(s), loss on training batch is 0.0217133.
After 403 training step(s), loss on training batch is 0.0207049.
After 404 training step(s), loss on training batch is 0.0205127.
After 405 training step(s), loss on training batch is 0.0203134.
After 406 training step(s), loss on training batch is 0.021663.
After 407 training step(s), loss on training batch is 0.0187136.
After 408 training step(s), loss on training batch is 0.0195207.
After 409 training step(s), loss on training batch is 0.0201161.
After 410 training step(s), loss on training batch is 0.0190271.
After 411 training step(s), loss on training batch is 0.0190841.
After 412 training step(s), loss on training batch is 0.0198874.
After 413 training step(s), loss on training batch is 0.0196649.
After 414 training step(s), loss on training batch is 0.020269.
After 415 training step(s), loss on training batch is 0.0198825.
After 416 training step(s), loss on training batch is 0.0187466.
After 417 training step(s), loss on training batch is 0.0200563.
After 418 training step(s), loss on training batch is 0.0192327.
After 419 training step(s), loss on training batch is 0.0192545.
After 420 training step(s), loss on training batch is 0.0184801.
After 421 training step(s), loss on training batch is 0.0174423.
After 422 training step(s), loss on training batch is 0.0189289.
After 423 training step(s), loss on training batch is 0.0183804.
After 424 training step(s), loss on training batch is 0.0175484.
After 425 training step(s), loss on training batch is 0.0182506.
After 426 training step(s), loss on training batch is 0.0178799.
After 427 training step(s), loss on training batch is 0.0189436.
After 428 training step(s), loss on training batch is 0.0183542.
After 429 training step(s), loss on training batch is 0.0175162.
After 430 training step(s), loss on training batch is 0.0169462.
After 431 training step(s), loss on training batch is 0.0168223.
After 432 training step(s), loss on training batch is 0.0174975.
After 433 training step(s), loss on training batch is 0.0174972.
After 434 training step(s), loss on training batch is 0.016662.
After 435 training step(s), loss on training batch is 0.0149531.
After 436 training step(s), loss on training batch is 0.0158371.
After 437 training step(s), loss on training batch is 0.015837.
After 438 training step(s), loss on training batch is 0.0160847.
After 439 training step(s), loss on training batch is 0.0148268.
After 440 training step(s), loss on training batch is 0.0165609.
After 441 training step(s), loss on training batch is 0.0144822.
After 442 training step(s), loss on training batch is 0.0147675.
After 443 training step(s), loss on training batch is 0.0144049.
After 444 training step(s), loss on training batch is 0.0141862.
After 445 training step(s), loss on training batch is 0.0132737.
After 446 training step(s), loss on training batch is 0.0158399.
After 447 training step(s), loss on training batch is 0.0143771.
After 448 training step(s), loss on training batch is 0.014748.
After 449 training step(s), loss on training batch is 0.0136066.
After 450 training step(s), loss on training batch is 0.0143343.
After 451 training step(s), loss on training batch is 0.0140531.
After 452 training step(s), loss on training batch is 0.0144083.
After 453 training step(s), loss on training batch is 0.013901.
After 454 training step(s), loss on training batch is 0.0126492.
After 455 training step(s), loss on training batch is 0.0128318.
After 456 training step(s), loss on training batch is 0.0139574.
After 457 training step(s), loss on training batch is 0.0139276.
After 458 training step(s), loss on training batch is 0.013469.
After 459 training step(s), loss on training batch is 0.013728.
After 460 training step(s), loss on training batch is 0.0149653.
After 461 training step(s), loss on training batch is 0.0128943.
After 462 training step(s), loss on training batch is 0.0120717.
After 463 training step(s), loss on training batch is 0.0127486.
After 464 training step(s), loss on training batch is 0.0123275.
After 465 training step(s), loss on training batch is 0.012341.
After 466 training step(s), loss on training batch is 0.0126296.
After 467 training step(s), loss on training batch is 0.0125535.
After 468 training step(s), loss on training batch is 0.0123955.
After 469 training step(s), loss on training batch is 0.0119955.
After 470 training step(s), loss on training batch is 0.0123254.
After 471 training step(s), loss on training batch is 0.011546.
After 472 training step(s), loss on training batch is 0.0112846.
After 473 training step(s), loss on training batch is 0.0142001.
After 474 training step(s), loss on training batch is 0.012852.
After 475 training step(s), loss on training batch is 0.0132226.
After 476 training step(s), loss on training batch is 0.012664.
After 477 training step(s), loss on training batch is 0.0189919.
After 478 training step(s), loss on training batch is 0.0121345.
After 479 training step(s), loss on training batch is 0.0111893.
After 480 training step(s), loss on training batch is 0.0110023.
After 481 training step(s), loss on training batch is 0.0132397.
After 482 training step(s), loss on training batch is 0.0133375.
After 483 training step(s), loss on training batch is 0.0115487.
After 484 training step(s), loss on training batch is 0.0115025.
After 485 training step(s), loss on training batch is 0.0110872.
After 486 training step(s), loss on training batch is 0.0102123.
After 487 training step(s), loss on training batch is 0.0108486.
After 488 training step(s), loss on training batch is 0.0106312.
After 489 training step(s), loss on training batch is 0.0113867.
After 490 training step(s), loss on training batch is 0.00978412.
After 491 training step(s), loss on training batch is 0.011921.
After 492 training step(s), loss on training batch is 0.0105578.
After 493 training step(s), loss on training batch is 0.0086404.
After 494 training step(s), loss on training batch is 0.00861458.
After 495 training step(s), loss on training batch is 0.00944265.
After 496 training step(s), loss on training batch is 0.00880123.
After 497 training step(s), loss on training batch is 0.00872715.
After 498 training step(s), loss on training batch is 0.00785541.
After 499 training step(s), loss on training batch is 0.00899605.
After 500 training step(s), loss on training batch is 0.010116.
After 501 training step(s), loss on training batch is 0.00984254.
After 502 training step(s), loss on training batch is 0.0100503.
After 503 training step(s), loss on training batch is 0.00858399.
After 504 training step(s), loss on training batch is 0.00792096.
After 505 training step(s), loss on training batch is 0.00831476.
After 506 training step(s), loss on training batch is 0.0090996.
After 507 training step(s), loss on training batch is 0.0107972.
After 508 training step(s), loss on training batch is 0.0102084.
After 509 training step(s), loss on training batch is 0.0154775.
After 510 training step(s), loss on training batch is 0.00916253.
After 511 training step(s), loss on training batch is 0.0108136.
After 512 training step(s), loss on training batch is 0.0106334.
After 513 training step(s), loss on training batch is 0.0129553.
After 514 training step(s), loss on training batch is 0.0123162.
After 515 training step(s), loss on training batch is 0.0146864.
After 516 training step(s), loss on training batch is 0.0120137.
After 517 training step(s), loss on training batch is 0.0105828.
After 518 training step(s), loss on training batch is 0.0113696.
After 519 training step(s), loss on training batch is 0.0135986.
After 520 training step(s), loss on training batch is 0.0106065.
After 521 training step(s), loss on training batch is 0.0136717.
After 522 training step(s), loss on training batch is 0.00673593.
After 523 training step(s), loss on training batch is 0.00702511.
After 524 training step(s), loss on training batch is 0.00594761.
After 525 training step(s), loss on training batch is 0.00535277.
After 526 training step(s), loss on training batch is 0.00751079.
After 527 training step(s), loss on training batch is 0.00853628.
After 528 training step(s), loss on training batch is 0.0117371.
After 529 training step(s), loss on training batch is 0.0118666.
After 530 training step(s), loss on training batch is 0.00923371.
After 531 training step(s), loss on training batch is 0.00808901.
After 532 training step(s), loss on training batch is 0.0066978.
After 533 training step(s), loss on training batch is 0.00651163.
After 534 training step(s), loss on training batch is 0.00530699.
After 535 training step(s), loss on training batch is 0.00535453.
After 536 training step(s), loss on training batch is 0.00506218.
After 537 training step(s), loss on training batch is 0.00939508.
After 538 training step(s), loss on training batch is 0.00764518.
After 539 training step(s), loss on training batch is 0.0122778.
After 540 training step(s), loss on training batch is 0.0100815.
After 541 training step(s), loss on training batch is 0.0184481.
After 542 training step(s), loss on training batch is 0.00975112.
After 543 training step(s), loss on training batch is 0.0140291.
After 544 training step(s), loss on training batch is 0.0140109.
After 545 training step(s), loss on training batch is 0.0184921.
After 546 training step(s), loss on training batch is 0.00848836.
After 547 training step(s), loss on training batch is 0.00886764.
After 548 training step(s), loss on training batch is 0.0071812.
After 549 training step(s), loss on training batch is 0.00842937.
After 550 training step(s), loss on training batch is 0.0113986.
After 551 training step(s), loss on training batch is 0.0119082.
After 552 training step(s), loss on training batch is 0.00845554.
After 553 training step(s), loss on training batch is 0.00778476.
After 554 training step(s), loss on training batch is 0.00534802.
After 555 training step(s), loss on training batch is 0.00586098.
After 556 training step(s), loss on training batch is 0.00604484.
After 557 training step(s), loss on training batch is 0.00813908.
After 558 training step(s), loss on training batch is 0.00523337.
After 559 training step(s), loss on training batch is 0.00470621.
After 560 training step(s), loss on training batch is 0.00449565.
After 561 training step(s), loss on training batch is 0.00416387.
After 562 training step(s), loss on training batch is 0.0039037.
After 563 training step(s), loss on training batch is 0.00330366.
After 564 training step(s), loss on training batch is 0.00990799.
After 565 training step(s), loss on training batch is 0.00909226.
After 566 training step(s), loss on training batch is 0.0133659.
After 567 training step(s), loss on training batch is 0.00962314.
After 568 training step(s), loss on training batch is 0.00834794.
After 569 training step(s), loss on training batch is 0.00706756.
After 570 training step(s), loss on training batch is 0.00526846.
After 571 training step(s), loss on training batch is 0.00388607.
After 572 training step(s), loss on training batch is 0.00399232.
After 573 training step(s), loss on training batch is 0.004121.
After 574 training step(s), loss on training batch is 0.00353905.
After 575 training step(s), loss on training batch is 0.00581362.
After 576 training step(s), loss on training batch is 0.00557399.
After 577 training step(s), loss on training batch is 0.00745848.
After 578 training step(s), loss on training batch is 0.00457288.
After 579 training step(s), loss on training batch is 0.00252423.
After 580 training step(s), loss on training batch is 0.0106389.
After 581 training step(s), loss on training batch is 0.00797287.
After 582 training step(s), loss on training batch is 0.00875992.
After 583 training step(s), loss on training batch is 0.00721291.
After 584 training step(s), loss on training batch is 0.00759039.
After 585 training step(s), loss on training batch is 0.00689419.
After 586 training step(s), loss on training batch is 0.00583238.
After 587 training step(s), loss on training batch is 0.00742622.
After 588 training step(s), loss on training batch is 0.00649664.
After 589 training step(s), loss on training batch is 0.0047654.
After 590 training step(s), loss on training batch is 0.00553301.
After 591 training step(s), loss on training batch is 0.00589271.
After 592 training step(s), loss on training batch is 0.00728612.
After 593 training step(s), loss on training batch is 0.0062265.
After 594 training step(s), loss on training batch is 0.00616097.
After 595 training step(s), loss on training batch is 0.00631856.
After 596 training step(s), loss on training batch is 0.00755717.
After 597 training step(s), loss on training batch is 0.0060785.
After 598 training step(s), loss on training batch is 0.0139016.
After 599 training step(s), loss on training batch is 0.0125289.
After 600 training step(s), loss on training batch is 0.0250836.
After 601 training step(s), loss on training batch is 0.0153029.
After 602 training step(s), loss on training batch is 0.0129421.
After 603 training step(s), loss on training batch is 0.00462511.
After 604 training step(s), loss on training batch is 0.00433061.
After 605 training step(s), loss on training batch is 0.00510435.
After 606 training step(s), loss on training batch is 0.00676796.
After 607 training step(s), loss on training batch is 0.00557392.
After 608 training step(s), loss on training batch is 0.00659062.
After 609 training step(s), loss on training batch is 0.00376399.
After 610 training step(s), loss on training batch is 0.00497717.
After 611 training step(s), loss on training batch is 0.00604193.
After 612 training step(s), loss on training batch is 0.00414425.
After 613 training step(s), loss on training batch is 0.00449062.
After 614 training step(s), loss on training batch is 0.00348318.
After 615 training step(s), loss on training batch is 0.00385768.
After 616 training step(s), loss on training batch is 0.0032294.
After 617 training step(s), loss on training batch is 0.0035746.
After 618 training step(s), loss on training batch is 0.00687499.
After 619 training step(s), loss on training batch is 0.0078559.
After 620 training step(s), loss on training batch is 0.00345072.
After 621 training step(s), loss on training batch is 0.00712912.
After 622 training step(s), loss on training batch is 0.00423839.
After 623 training step(s), loss on training batch is 0.0036751.
After 624 training step(s), loss on training batch is 0.00313927.
After 625 training step(s), loss on training batch is 0.00359094.
After 626 training step(s), loss on training batch is 0.00676911.
After 627 training step(s), loss on training batch is 0.0043629.
After 628 training step(s), loss on training batch is 0.00401766.
After 629 training step(s), loss on training batch is 0.00325035.
After 630 training step(s), loss on training batch is 0.0048731.
After 631 training step(s), loss on training batch is 0.00397294.
After 632 training step(s), loss on training batch is 0.00311575.
After 633 training step(s), loss on training batch is 0.00486122.
After 634 training step(s), loss on training batch is 0.00386597.
After 635 training step(s), loss on training batch is 0.00268078.
After 636 training step(s), loss on training batch is 0.00331962.
After 637 training step(s), loss on training batch is 0.00503255.
After 638 training step(s), loss on training batch is 0.0033589.
After 639 training step(s), loss on training batch is 0.00428839.
After 640 training step(s), loss on training batch is 0.00397033.
After 641 training step(s), loss on training batch is 0.00433272.
After 642 training step(s), loss on training batch is 0.00295772.
After 643 training step(s), loss on training batch is 0.00324101.
After 644 training step(s), loss on training batch is 0.00310205.
After 645 training step(s), loss on training batch is 0.00315726.
After 646 training step(s), loss on training batch is 0.00343744.
After 647 training step(s), loss on training batch is 0.00352584.
After 648 training step(s), loss on training batch is 0.00419772.
After 649 training step(s), loss on training batch is 0.00416885.
After 650 training step(s), loss on training batch is 0.00358912.
After 651 training step(s), loss on training batch is 0.00437852.
After 652 training step(s), loss on training batch is 0.00409938.
After 653 training step(s), loss on training batch is 0.00371612.
After 654 training step(s), loss on training batch is 0.00339468.
After 655 training step(s), loss on training batch is 0.00554305.
After 656 training step(s), loss on training batch is 0.00408909.
After 657 training step(s), loss on training batch is 0.00294444.
After 658 training step(s), loss on training batch is 0.00390932.
After 659 training step(s), loss on training batch is 0.00323203.
After 660 training step(s), loss on training batch is 0.00358669.
After 661 training step(s), loss on training batch is 0.00269485.
After 662 training step(s), loss on training batch is 0.00239881.
After 663 training step(s), loss on training batch is 0.00277141.
After 664 training step(s), loss on training batch is 0.00303046.
After 665 training step(s), loss on training batch is 0.00361406.
After 666 training step(s), loss on training batch is 0.0049599.
After 667 training step(s), loss on training batch is 0.00636822.
After 668 training step(s), loss on training batch is 0.00455037.
After 669 training step(s), loss on training batch is 0.00474113.
After 670 training step(s), loss on training batch is 0.00601598.
After 671 training step(s), loss on training batch is 0.00473665.
After 672 training step(s), loss on training batch is 0.00493658.
After 673 training step(s), loss on training batch is 0.00700639.
After 674 training step(s), loss on training batch is 0.00589902.
After 675 training step(s), loss on training batch is 0.00494953.
After 676 training step(s), loss on training batch is 0.00346512.
After 677 training step(s), loss on training batch is 0.00577207.
After 678 training step(s), loss on training batch is 0.0050723.
After 679 training step(s), loss on training batch is 0.00453509.
After 680 training step(s), loss on training batch is 0.00502415.
After 681 training step(s), loss on training batch is 0.00342807.
After 682 training step(s), loss on training batch is 0.00403588.
After 683 training step(s), loss on training batch is 0.00427094.
After 684 training step(s), loss on training batch is 0.00350824.
After 685 training step(s), loss on training batch is 0.00353398.
After 686 training step(s), loss on training batch is 0.00400252.
After 687 training step(s), loss on training batch is 0.00353371.
After 688 training step(s), loss on training batch is 0.00415214.
After 689 training step(s), loss on training batch is 0.00469544.
After 690 training step(s), loss on training batch is 0.00297641.
After 691 training step(s), loss on training batch is 0.00451108.
After 692 training step(s), loss on training batch is 0.00582118.
After 693 training step(s), loss on training batch is 0.00362367.
After 694 training step(s), loss on training batch is 0.00616491.
After 695 training step(s), loss on training batch is 0.00460162.
After 696 training step(s), loss on training batch is 0.00474869.
After 697 training step(s), loss on training batch is 0.00534274.
After 698 training step(s), loss on training batch is 0.0045567.
After 699 training step(s), loss on training batch is 0.00433704.
After 700 training step(s), loss on training batch is 0.00374816.
After 701 training step(s), loss on training batch is 0.00269496.
After 702 training step(s), loss on training batch is 0.00706996.
After 703 training step(s), loss on training batch is 0.00742813.
After 704 training step(s), loss on training batch is 0.0149654.
After 705 training step(s), loss on training batch is 0.0191061.
After 706 training step(s), loss on training batch is 0.0538285.
After 707 training step(s), loss on training batch is 0.183142.
After 708 training step(s), loss on training batch is 0.0296601.
After 709 training step(s), loss on training batch is 0.022942.
After 710 training step(s), loss on training batch is 0.0227689.
After 711 training step(s), loss on training batch is 0.0212252.
After 712 training step(s), loss on training batch is 0.0212758.
After 713 training step(s), loss on training batch is 0.0203613.
After 714 training step(s), loss on training batch is 0.0177215.
After 715 training step(s), loss on training batch is 0.018275.
After 716 training step(s), loss on training batch is 0.0154901.
After 717 training step(s), loss on training batch is 0.0151825.
After 718 training step(s), loss on training batch is 0.0140317.
After 719 training step(s), loss on training batch is 0.0182183.
After 720 training step(s), loss on training batch is 0.0183835.
After 721 training step(s), loss on training batch is 0.0183136.
After 722 training step(s), loss on training batch is 0.0172797.
After 723 training step(s), loss on training batch is 0.0165747.
After 724 training step(s), loss on training batch is 0.0141407.
After 725 training step(s), loss on training batch is 0.0132868.
After 726 training step(s), loss on training batch is 0.0127508.
After 727 training step(s), loss on training batch is 0.0136649.
After 728 training step(s), loss on training batch is 0.0140117.
After 729 training step(s), loss on training batch is 0.0114756.
After 730 training step(s), loss on training batch is 0.0155681.
After 731 training step(s), loss on training batch is 0.0149803.
After 732 training step(s), loss on training batch is 0.0128876.
After 733 training step(s), loss on training batch is 0.012998.
After 734 training step(s), loss on training batch is 0.0113647.
After 735 training step(s), loss on training batch is 0.0110679.
After 736 training step(s), loss on training batch is 0.00988564.
After 737 training step(s), loss on training batch is 0.0108212.
After 738 training step(s), loss on training batch is 0.0105935.
After 739 training step(s), loss on training batch is 0.0107564.
After 740 training step(s), loss on training batch is 0.0101837.
After 741 training step(s), loss on training batch is 0.00942455.
After 742 training step(s), loss on training batch is 0.00816501.
After 743 training step(s), loss on training batch is 0.00990476.
After 744 training step(s), loss on training batch is 0.0104131.
After 745 training step(s), loss on training batch is 0.0095815.
After 746 training step(s), loss on training batch is 0.0121481.
After 747 training step(s), loss on training batch is 0.0101698.
After 748 training step(s), loss on training batch is 0.00910557.
After 749 training step(s), loss on training batch is 0.00915326.
After 750 training step(s), loss on training batch is 0.0102438.
After 751 training step(s), loss on training batch is 0.0088775.
After 752 training step(s), loss on training batch is 0.00991686.
After 753 training step(s), loss on training batch is 0.00951632.
After 754 training step(s), loss on training batch is 0.00791855.
After 755 training step(s), loss on training batch is 0.0088883.
After 756 training step(s), loss on training batch is 0.00786004.
After 757 training step(s), loss on training batch is 0.00640649.
After 758 training step(s), loss on training batch is 0.00859314.
After 759 training step(s), loss on training batch is 0.00952058.
After 760 training step(s), loss on training batch is 0.00890195.
After 761 training step(s), loss on training batch is 0.00938617.
After 762 training step(s), loss on training batch is 0.00473506.
After 763 training step(s), loss on training batch is 0.00652202.
After 764 training step(s), loss on training batch is 0.00928747.
After 765 training step(s), loss on training batch is 0.00785527.
After 766 training step(s), loss on training batch is 0.00604534.
After 767 training step(s), loss on training batch is 0.00817882.
After 768 training step(s), loss on training batch is 0.0102948.
After 769 training step(s), loss on training batch is 0.00792207.
After 770 training step(s), loss on training batch is 0.00697614.
After 771 training step(s), loss on training batch is 0.00339364.
After 772 training step(s), loss on training batch is 0.00659943.
After 773 training step(s), loss on training batch is 0.00580723.
After 774 training step(s), loss on training batch is 0.0048378.
After 775 training step(s), loss on training batch is 0.00892587.
After 776 training step(s), loss on training batch is 0.00733886.
After 777 training step(s), loss on training batch is 0.00618262.
After 778 training step(s), loss on training batch is 0.00586847.
After 779 training step(s), loss on training batch is 0.00569176.
After 780 training step(s), loss on training batch is 0.00696024.
After 781 training step(s), loss on training batch is 0.0067268.
After 782 training step(s), loss on training batch is 0.00643305.
After 783 training step(s), loss on training batch is 0.00824691.
After 784 training step(s), loss on training batch is 0.00933749.
After 785 training step(s), loss on training batch is 0.00716143.
After 786 training step(s), loss on training batch is 0.0114966.
After 787 training step(s), loss on training batch is 0.0067346.
After 788 training step(s), loss on training batch is 0.00496192.
After 789 training step(s), loss on training batch is 0.00695752.
After 790 training step(s), loss on training batch is 0.00561838.
After 791 training step(s), loss on training batch is 0.00537426.
After 792 training step(s), loss on training batch is 0.00489801.
After 793 training step(s), loss on training batch is 0.00553352.
After 794 training step(s), loss on training batch is 0.00299606.
After 795 training step(s), loss on training batch is 0.00777087.
After 796 training step(s), loss on training batch is 0.00683919.
After 797 training step(s), loss on training batch is 0.00437747.
After 798 training step(s), loss on training batch is 0.00581174.
After 799 training step(s), loss on training batch is 0.00561297.
After 800 training step(s), loss on training batch is 0.00634484.
After 801 training step(s), loss on training batch is 0.00477528.
After 802 training step(s), loss on training batch is 0.00583665.
After 803 training step(s), loss on training batch is 0.00358078.
After 804 training step(s), loss on training batch is 0.00612441.
After 805 training step(s), loss on training batch is 0.00679271.
After 806 training step(s), loss on training batch is 0.00686022.
After 807 training step(s), loss on training batch is 0.00344404.
After 808 training step(s), loss on training batch is 0.00470814.
After 809 training step(s), loss on training batch is 0.00638453.
After 810 training step(s), loss on training batch is 0.00709307.
After 811 training step(s), loss on training batch is 0.00487333.
After 812 training step(s), loss on training batch is 0.00335144.
After 813 training step(s), loss on training batch is 0.00340676.
After 814 training step(s), loss on training batch is 0.0075299.
After 815 training step(s), loss on training batch is 0.00758639.
After 816 training step(s), loss on training batch is 0.00591665.
After 817 training step(s), loss on training batch is 0.00922218.
After 818 training step(s), loss on training batch is 0.00501563.
After 819 training step(s), loss on training batch is 0.00552449.
After 820 training step(s), loss on training batch is 0.00340396.
After 821 training step(s), loss on training batch is 0.00373394.
After 822 training step(s), loss on training batch is 0.0068365.
After 823 training step(s), loss on training batch is 0.00373268.
After 824 training step(s), loss on training batch is 0.00295822.
After 825 training step(s), loss on training batch is 0.00413182.
After 826 training step(s), loss on training batch is 0.00425461.
After 827 training step(s), loss on training batch is 0.0072094.
After 828 training step(s), loss on training batch is 0.00741731.
After 829 training step(s), loss on training batch is 0.00546205.
After 830 training step(s), loss on training batch is 0.00589044.
After 831 training step(s), loss on training batch is 0.00535878.
After 832 training step(s), loss on training batch is 0.00751947.
After 833 training step(s), loss on training batch is 0.00762196.
After 834 training step(s), loss on training batch is 0.00535175.
After 835 training step(s), loss on training batch is 0.00436293.
After 836 training step(s), loss on training batch is 0.00579008.
After 837 training step(s), loss on training batch is 0.00497826.
After 838 training step(s), loss on training batch is 0.00380759.
After 839 training step(s), loss on training batch is 0.00310049.
After 840 training step(s), loss on training batch is 0.00685345.
After 841 training step(s), loss on training batch is 0.0040664.
After 842 training step(s), loss on training batch is 0.00390413.
After 843 training step(s), loss on training batch is 0.00342539.
After 844 training step(s), loss on training batch is 0.00479371.
After 845 training step(s), loss on training batch is 0.00343313.
After 846 training step(s), loss on training batch is 0.00554571.
After 847 training step(s), loss on training batch is 0.00482405.
After 848 training step(s), loss on training batch is 0.00326457.
After 849 training step(s), loss on training batch is 0.00375841.
After 850 training step(s), loss on training batch is 0.0042514.
After 851 training step(s), loss on training batch is 0.00380783.
After 852 training step(s), loss on training batch is 0.00398485.
After 853 training step(s), loss on training batch is 0.00375836.
After 854 training step(s), loss on training batch is 0.00244942.
After 855 training step(s), loss on training batch is 0.00310972.
After 856 training step(s), loss on training batch is 0.00543068.
After 857 training step(s), loss on training batch is 0.00504688.
After 858 training step(s), loss on training batch is 0.00564781.
After 859 training step(s), loss on training batch is 0.00454706.
After 860 training step(s), loss on training batch is 0.00402997.
After 861 training step(s), loss on training batch is 0.00399014.
After 862 training step(s), loss on training batch is 0.00372032.
After 863 training step(s), loss on training batch is 0.00415965.
After 864 training step(s), loss on training batch is 0.00423124.
After 865 training step(s), loss on training batch is 0.00602942.
After 866 training step(s), loss on training batch is 0.00510805.
After 867 training step(s), loss on training batch is 0.00342857.
After 868 training step(s), loss on training batch is 0.00803342.
After 869 training step(s), loss on training batch is 0.00811333.
After 870 training step(s), loss on training batch is 0.0078639.
After 871 training step(s), loss on training batch is 0.00813821.
After 872 training step(s), loss on training batch is 0.00407344.
After 873 training step(s), loss on training batch is 0.00687675.
After 874 training step(s), loss on training batch is 0.0057687.
After 875 training step(s), loss on training batch is 0.00631513.
After 876 training step(s), loss on training batch is 0.00562448.
After 877 training step(s), loss on training batch is 0.00686643.
After 878 training step(s), loss on training batch is 0.00526784.
After 879 training step(s), loss on training batch is 0.00477672.
After 880 training step(s), loss on training batch is 0.00364454.
After 881 training step(s), loss on training batch is 0.00852247.
After 882 training step(s), loss on training batch is 0.0076133.
After 883 training step(s), loss on training batch is 0.00473507.
After 884 training step(s), loss on training batch is 0.00516436.
After 885 training step(s), loss on training batch is 0.00464977.
After 886 training step(s), loss on training batch is 0.00439802.
After 887 training step(s), loss on training batch is 0.00507943.
After 888 training step(s), loss on training batch is 0.00533922.
After 889 training step(s), loss on training batch is 0.00600347.
After 890 training step(s), loss on training batch is 0.00360246.
After 891 training step(s), loss on training batch is 0.00358916.
After 892 training step(s), loss on training batch is 0.00458874.
After 893 training step(s), loss on training batch is 0.00384473.
After 894 training step(s), loss on training batch is 0.00348829.
After 895 training step(s), loss on training batch is 0.00460365.
After 896 training step(s), loss on training batch is 0.00364629.
After 897 training step(s), loss on training batch is 0.00386067.
After 898 training step(s), loss on training batch is 0.00292133.
After 899 training step(s), loss on training batch is 0.00260875.
After 900 training step(s), loss on training batch is 0.00560284.
After 901 training step(s), loss on training batch is 0.00486535.
After 902 training step(s), loss on training batch is 0.00319687.
After 903 training step(s), loss on training batch is 0.0035766.
After 904 training step(s), loss on training batch is 0.00358847.
After 905 training step(s), loss on training batch is 0.00414922.
After 906 training step(s), loss on training batch is 0.00405973.
After 907 training step(s), loss on training batch is 0.00342511.
After 908 training step(s), loss on training batch is 0.00183855.
After 909 training step(s), loss on training batch is 0.00324294.
After 910 training step(s), loss on training batch is 0.00226983.
After 911 training step(s), loss on training batch is 0.00275961.
After 912 training step(s), loss on training batch is 0.0052309.
After 913 training step(s), loss on training batch is 0.00495878.
After 914 training step(s), loss on training batch is 0.00549507.
After 915 training step(s), loss on training batch is 0.00713009.
After 916 training step(s), loss on training batch is 0.00922252.
After 917 training step(s), loss on training batch is 0.0196496.
After 918 training step(s), loss on training batch is 0.169724.
After 919 training step(s), loss on training batch is 0.200495.
After 920 training step(s), loss on training batch is 0.077467.
After 921 training step(s), loss on training batch is 0.0813055.
After 922 training step(s), loss on training batch is 0.0304369.
After 923 training step(s), loss on training batch is 0.0205696.
After 924 training step(s), loss on training batch is 0.0183369.
After 925 training step(s), loss on training batch is 0.0146184.
After 926 training step(s), loss on training batch is 0.0154908.
After 927 training step(s), loss on training batch is 0.0143013.
After 928 training step(s), loss on training batch is 0.014071.
After 929 training step(s), loss on training batch is 0.0139246.
After 930 training step(s), loss on training batch is 0.0112893.
After 931 training step(s), loss on training batch is 0.01123.
After 932 training step(s), loss on training batch is 0.0110412.
After 933 training step(s), loss on training batch is 0.0106121.
After 934 training step(s), loss on training batch is 0.00869031.
After 935 training step(s), loss on training batch is 0.00845551.
After 936 training step(s), loss on training batch is 0.00752371.
After 937 training step(s), loss on training batch is 0.00706603.
After 938 training step(s), loss on training batch is 0.00659665.
After 939 training step(s), loss on training batch is 0.00773864.
After 940 training step(s), loss on training batch is 0.00665128.
After 941 training step(s), loss on training batch is 0.00601498.
After 942 training step(s), loss on training batch is 0.00865349.
After 943 training step(s), loss on training batch is 0.011962.
After 944 training step(s), loss on training batch is 0.0113639.
After 945 training step(s), loss on training batch is 0.00594518.
After 946 training step(s), loss on training batch is 0.00914568.
After 947 training step(s), loss on training batch is 0.00768834.
After 948 training step(s), loss on training batch is 0.00560631.
After 949 training step(s), loss on training batch is 0.00542618.
After 950 training step(s), loss on training batch is 0.00988458.
After 951 training step(s), loss on training batch is 0.00877774.
After 952 training step(s), loss on training batch is 0.00483333.
After 953 training step(s), loss on training batch is 0.00368231.
After 954 training step(s), loss on training batch is 0.00412781.
After 955 training step(s), loss on training batch is 0.00495916.
After 956 training step(s), loss on training batch is 0.00483253.
After 957 training step(s), loss on training batch is 0.00575508.
After 958 training step(s), loss on training batch is 0.00348425.
After 959 training step(s), loss on training batch is 0.00312159.
After 960 training step(s), loss on training batch is 0.00314829.
After 961 training step(s), loss on training batch is 0.00273905.
After 962 training step(s), loss on training batch is 0.0029856.
After 963 training step(s), loss on training batch is 0.00228522.
After 964 training step(s), loss on training batch is 0.0123047.
After 965 training step(s), loss on training batch is 0.0113743.
After 966 training step(s), loss on training batch is 0.0184006.
After 967 training step(s), loss on training batch is 0.227027.
After 968 training step(s), loss on training batch is 0.156718.
After 969 training step(s), loss on training batch is 0.01481.
After 970 training step(s), loss on training batch is 0.0122482.
After 971 training step(s), loss on training batch is 0.0131882.
After 972 training step(s), loss on training batch is 0.0117478.
After 973 training step(s), loss on training batch is 0.0115848.
After 974 training step(s), loss on training batch is 0.0105439.
After 975 training step(s), loss on training batch is 0.0133264.
After 976 training step(s), loss on training batch is 0.0122973.
After 977 training step(s), loss on training batch is 0.0117317.
After 978 training step(s), loss on training batch is 0.00940025.
After 979 training step(s), loss on training batch is 0.00753622.
After 980 training step(s), loss on training batch is 0.0113336.
After 981 training step(s), loss on training batch is 0.00984233.
After 982 training step(s), loss on training batch is 0.00856908.
After 983 training step(s), loss on training batch is 0.0103411.
After 984 training step(s), loss on training batch is 0.00990397.
After 985 training step(s), loss on training batch is 0.00956536.
After 986 training step(s), loss on training batch is 0.00781203.
After 987 training step(s), loss on training batch is 0.00878518.
After 988 training step(s), loss on training batch is 0.00733848.
After 989 training step(s), loss on training batch is 0.0061757.
After 990 training step(s), loss on training batch is 0.00716981.
After 991 training step(s), loss on training batch is 0.00680725.
After 992 training step(s), loss on training batch is 0.00845579.
After 993 training step(s), loss on training batch is 0.00730858.
After 994 training step(s), loss on training batch is 0.00741396.
After 995 training step(s), loss on training batch is 0.00811416.
After 996 training step(s), loss on training batch is 0.0094543.
After 997 training step(s), loss on training batch is 0.00560172.
After 998 training step(s), loss on training batch is 0.0113637.
After 999 training step(s), loss on training batch is 0.00793192.
After 1000 training step(s), loss on training batch is 0.00853002.
After 1001 training step(s), loss on training batch is 0.00654653.
After 1002 training step(s), loss on training batch is 0.00714608.
After 1003 training step(s), loss on training batch is 0.005062.
After 1004 training step(s), loss on training batch is 0.00413057.
After 1005 training step(s), loss on training batch is 0.00528625.
After 1006 training step(s), loss on training batch is 0.00722417.
After 1007 training step(s), loss on training batch is 0.00549745.
After 1008 training step(s), loss on training batch is 0.00687808.
After 1009 training step(s), loss on training batch is 0.00337358.
After 1010 training step(s), loss on training batch is 0.00530499.
After 1011 training step(s), loss on training batch is 0.00605894.
After 1012 training step(s), loss on training batch is 0.00427889.
After 1013 training step(s), loss on training batch is 0.00449898.
After 1014 training step(s), loss on training batch is 0.00337171.
After 1015 training step(s), loss on training batch is 0.00383411.
After 1016 training step(s), loss on training batch is 0.00300857.
After 1017 training step(s), loss on training batch is 0.0030545.
After 1018 training step(s), loss on training batch is 0.00714905.
After 1019 training step(s), loss on training batch is 0.00816068.
After 1020 training step(s), loss on training batch is 0.00363892.
After 1021 training step(s), loss on training batch is 0.00773672.
After 1022 training step(s), loss on training batch is 0.00391937.
After 1023 training step(s), loss on training batch is 0.00364812.
After 1024 training step(s), loss on training batch is 0.00273286.
After 1025 training step(s), loss on training batch is 0.00324309.
After 1026 training step(s), loss on training batch is 0.00562051.
After 1027 training step(s), loss on training batch is 0.00294531.
After 1028 training step(s), loss on training batch is 0.00335663.
After 1029 training step(s), loss on training batch is 0.00289428.
After 1030 training step(s), loss on training batch is 0.00433528.
After 1031 training step(s), loss on training batch is 0.00344532.
After 1032 training step(s), loss on training batch is 0.00268298.
After 1033 training step(s), loss on training batch is 0.00492203.
After 1034 training step(s), loss on training batch is 0.00291438.
After 1035 training step(s), loss on training batch is 0.00180384.
After 1036 training step(s), loss on training batch is 0.0032414.
After 1037 training step(s), loss on training batch is 0.00562521.
After 1038 training step(s), loss on training batch is 0.00321005.
After 1039 training step(s), loss on training batch is 0.00368033.
After 1040 training step(s), loss on training batch is 0.00366454.
After 1041 training step(s), loss on training batch is 0.00374118.
After 1042 training step(s), loss on training batch is 0.00245599.
After 1043 training step(s), loss on training batch is 0.00262662.
After 1044 training step(s), loss on training batch is 0.00237685.
After 1045 training step(s), loss on training batch is 0.00249038.
After 1046 training step(s), loss on training batch is 0.0029015.
After 1047 training step(s), loss on training batch is 0.00371087.
After 1048 training step(s), loss on training batch is 0.00392064.
After 1049 training step(s), loss on training batch is 0.00324455.
After 1050 training step(s), loss on training batch is 0.00314003.
After 1051 training step(s), loss on training batch is 0.00319931.
After 1052 training step(s), loss on training batch is 0.00294309.
After 1053 training step(s), loss on training batch is 0.00325137.
After 1054 training step(s), loss on training batch is 0.0026718.
After 1055 training step(s), loss on training batch is 0.0058535.
After 1056 training step(s), loss on training batch is 0.00299415.
After 1057 training step(s), loss on training batch is 0.00230867.
After 1058 training step(s), loss on training batch is 0.00338346.
After 1059 training step(s), loss on training batch is 0.00260856.
After 1060 training step(s), loss on training batch is 0.00308882.
After 1061 training step(s), loss on training batch is 0.00207318.
After 1062 training step(s), loss on training batch is 0.00178197.
After 1063 training step(s), loss on training batch is 0.00207872.
After 1064 training step(s), loss on training batch is 0.00290973.
After 1065 training step(s), loss on training batch is 0.00292744.
After 1066 training step(s), loss on training batch is 0.00403705.
After 1067 training step(s), loss on training batch is 0.00586753.
After 1068 training step(s), loss on training batch is 0.00414123.
After 1069 training step(s), loss on training batch is 0.0046173.
After 1070 training step(s), loss on training batch is 0.00565981.
After 1071 training step(s), loss on training batch is 0.00490856.
After 1072 training step(s), loss on training batch is 0.00490246.
After 1073 training step(s), loss on training batch is 0.00789211.
After 1074 training step(s), loss on training batch is 0.00563431.
After 1075 training step(s), loss on training batch is 0.0050019.
After 1076 training step(s), loss on training batch is 0.00317801.
After 1077 training step(s), loss on training batch is 0.0063036.
After 1078 training step(s), loss on training batch is 0.00458803.
After 1079 training step(s), loss on training batch is 0.00408642.
After 1080 training step(s), loss on training batch is 0.00492668.
After 1081 training step(s), loss on training batch is 0.00318615.
After 1082 training step(s), loss on training batch is 0.00314428.
After 1083 training step(s), loss on training batch is 0.0038172.
After 1084 training step(s), loss on training batch is 0.00310325.
After 1085 training step(s), loss on training batch is 0.00288449.
After 1086 training step(s), loss on training batch is 0.00348975.
After 1087 training step(s), loss on training batch is 0.00298935.
After 1088 training step(s), loss on training batch is 0.00360614.
After 1089 training step(s), loss on training batch is 0.00360558.
After 1090 training step(s), loss on training batch is 0.00312151.
After 1091 training step(s), loss on training batch is 0.00433136.
After 1092 training step(s), loss on training batch is 0.0059614.
After 1093 training step(s), loss on training batch is 0.00338871.
After 1094 training step(s), loss on training batch is 0.00605956.
After 1095 training step(s), loss on training batch is 0.00392024.
After 1096 training step(s), loss on training batch is 0.00391682.
After 1097 training step(s), loss on training batch is 0.00543342.
After 1098 training step(s), loss on training batch is 0.00458743.
After 1099 training step(s), loss on training batch is 0.00435748.
After 1100 training step(s), loss on training batch is 0.00418827.
After 1101 training step(s), loss on training batch is 0.00255716.
After 1102 training step(s), loss on training batch is 0.00642591.
After 1103 training step(s), loss on training batch is 0.00572227.
After 1104 training step(s), loss on training batch is 0.00979163.
After 1105 training step(s), loss on training batch is 0.00683711.
After 1106 training step(s), loss on training batch is 0.00701723.
After 1107 training step(s), loss on training batch is 0.00538978.
After 1108 training step(s), loss on training batch is 0.00655398.
After 1109 training step(s), loss on training batch is 0.00698774.
After 1110 training step(s), loss on training batch is 0.00528375.
After 1111 training step(s), loss on training batch is 0.00539982.
After 1112 training step(s), loss on training batch is 0.00375266.
After 1113 training step(s), loss on training batch is 0.00330325.
After 1114 training step(s), loss on training batch is 0.0057069.
After 1115 training step(s), loss on training batch is 0.00325536.
After 1116 training step(s), loss on training batch is 0.0057146.
After 1117 training step(s), loss on training batch is 0.00338771.
After 1118 training step(s), loss on training batch is 0.00225761.
After 1119 training step(s), loss on training batch is 0.00279474.
After 1120 training step(s), loss on training batch is 0.00352608.
After 1121 training step(s), loss on training batch is 0.00300259.
After 1122 training step(s), loss on training batch is 0.00350665.
After 1123 training step(s), loss on training batch is 0.00448117.
After 1124 training step(s), loss on training batch is 0.00381707.
After 1125 training step(s), loss on training batch is 0.00331627.
After 1126 training step(s), loss on training batch is 0.0037857.
After 1127 training step(s), loss on training batch is 0.00402437.
After 1128 training step(s), loss on training batch is 0.00837555.
After 1129 training step(s), loss on training batch is 0.0042656.
After 1130 training step(s), loss on training batch is 0.0086246.
After 1131 training step(s), loss on training batch is 0.00696693.
After 1132 training step(s), loss on training batch is 0.00463028.
After 1133 training step(s), loss on training batch is 0.00586254.
After 1134 training step(s), loss on training batch is 0.00431401.
After 1135 training step(s), loss on training batch is 0.00359542.
After 1136 training step(s), loss on training batch is 0.00347468.
After 1137 training step(s), loss on training batch is 0.00426952.
After 1138 training step(s), loss on training batch is 0.00486736.
After 1139 training step(s), loss on training batch is 0.00396582.
After 1140 training step(s), loss on training batch is 0.00351681.
After 1141 training step(s), loss on training batch is 0.00328484.
After 1142 training step(s), loss on training batch is 0.00489189.
After 1143 training step(s), loss on training batch is 0.00559142.
After 1144 training step(s), loss on training batch is 0.00479774.
After 1145 training step(s), loss on training batch is 0.00356165.
After 1146 training step(s), loss on training batch is 0.00503312.
After 1147 training step(s), loss on training batch is 0.00411156.
After 1148 training step(s), loss on training batch is 0.00353198.
After 1149 training step(s), loss on training batch is 0.00439349.
After 1150 training step(s), loss on training batch is 0.00368942.
After 1151 training step(s), loss on training batch is 0.00352668.
After 1152 training step(s), loss on training batch is 0.00441617.
After 1153 training step(s), loss on training batch is 0.00444495.
After 1154 training step(s), loss on training batch is 0.00368673.
After 1155 training step(s), loss on training batch is 0.00464811.
After 1156 training step(s), loss on training batch is 0.00321042.
After 1157 training step(s), loss on training batch is 0.00254118.
After 1158 training step(s), loss on training batch is 0.00372534.
After 1159 training step(s), loss on training batch is 0.00409744.
After 1160 training step(s), loss on training batch is 0.00482615.
After 1161 training step(s), loss on training batch is 0.00444274.
After 1162 training step(s), loss on training batch is 0.00252234.
After 1163 training step(s), loss on training batch is 0.00315209.
After 1164 training step(s), loss on training batch is 0.00579532.
After 1165 training step(s), loss on training batch is 0.00413686.
After 1166 training step(s), loss on training batch is 0.00411554.
After 1167 training step(s), loss on training batch is 0.00520281.
After 1168 training step(s), loss on training batch is 0.00523592.
After 1169 training step(s), loss on training batch is 0.00301588.
After 1170 training step(s), loss on training batch is 0.00340264.
After 1171 training step(s), loss on training batch is 0.00177876.
After 1172 training step(s), loss on training batch is 0.00433054.
After 1173 training step(s), loss on training batch is 0.00279611.
After 1174 training step(s), loss on training batch is 0.00219091.
After 1175 training step(s), loss on training batch is 0.00507944.
After 1176 training step(s), loss on training batch is 0.00397198.
After 1177 training step(s), loss on training batch is 0.00370408.
After 1178 training step(s), loss on training batch is 0.00275845.
After 1179 training step(s), loss on training batch is 0.00303642.
After 1180 training step(s), loss on training batch is 0.00282513.
After 1181 training step(s), loss on training batch is 0.00297543.
After 1182 training step(s), loss on training batch is 0.00280255.
After 1183 training step(s), loss on training batch is 0.00371498.
After 1184 training step(s), loss on training batch is 0.00327466.
After 1185 training step(s), loss on training batch is 0.0031881.
After 1186 training step(s), loss on training batch is 0.00611852.
After 1187 training step(s), loss on training batch is 0.00338049.
After 1188 training step(s), loss on training batch is 0.0024159.
After 1189 training step(s), loss on training batch is 0.00395307.
After 1190 training step(s), loss on training batch is 0.00216728.
After 1191 training step(s), loss on training batch is 0.00242857.
After 1192 training step(s), loss on training batch is 0.00249295.
After 1193 training step(s), loss on training batch is 0.00304572.
After 1194 training step(s), loss on training batch is 0.00192469.
After 1195 training step(s), loss on training batch is 0.00347644.
After 1196 training step(s), loss on training batch is 0.00264482.
After 1197 training step(s), loss on training batch is 0.00196965.
After 1198 training step(s), loss on training batch is 0.00317817.
After 1199 training step(s), loss on training batch is 0.00283075.
After 1200 training step(s), loss on training batch is 0.00286546.
After 1201 training step(s), loss on training batch is 0.00336998.
After 1202 training step(s), loss on training batch is 0.00328855.
After 1203 training step(s), loss on training batch is 0.00202863.
After 1204 training step(s), loss on training batch is 0.00346143.
After 1205 training step(s), loss on training batch is 0.00543995.
After 1206 training step(s), loss on training batch is 0.00472961.
After 1207 training step(s), loss on training batch is 0.00242009.
After 1208 training step(s), loss on training batch is 0.00275855.
After 1209 training step(s), loss on training batch is 0.0039149.
After 1210 training step(s), loss on training batch is 0.00542564.
After 1211 training step(s), loss on training batch is 0.00286046.
After 1212 training step(s), loss on training batch is 0.0023099.
After 1213 training step(s), loss on training batch is 0.0025874.
After 1214 training step(s), loss on training batch is 0.00371907.
After 1215 training step(s), loss on training batch is 0.00425015.
After 1216 training step(s), loss on training batch is 0.00349643.
After 1217 training step(s), loss on training batch is 0.00427932.
After 1218 training step(s), loss on training batch is 0.00253683.
After 1219 training step(s), loss on training batch is 0.00265312.
After 1220 training step(s), loss on training batch is 0.00162027.
After 1221 training step(s), loss on training batch is 0.00323433.
After 1222 training step(s), loss on training batch is 0.00428295.
After 1223 training step(s), loss on training batch is 0.00246494.
After 1224 training step(s), loss on training batch is 0.00146056.
After 1225 training step(s), loss on training batch is 0.00211536.
After 1226 training step(s), loss on training batch is 0.00247323.
After 1227 training step(s), loss on training batch is 0.00487308.
After 1228 training step(s), loss on training batch is 0.0050998.
After 1229 training step(s), loss on training batch is 0.00414823.
After 1230 training step(s), loss on training batch is 0.00361643.
After 1231 training step(s), loss on training batch is 0.00445483.
After 1232 training step(s), loss on training batch is 0.00444517.
After 1233 training step(s), loss on training batch is 0.00534178.
After 1234 training step(s), loss on training batch is 0.0038401.
After 1235 training step(s), loss on training batch is 0.0030529.
After 1236 training step(s), loss on training batch is 0.0041034.
After 1237 training step(s), loss on training batch is 0.00361466.
After 1238 training step(s), loss on training batch is 0.00251984.
After 1239 training step(s), loss on training batch is 0.00233896.
After 1240 training step(s), loss on training batch is 0.00591753.
After 1241 training step(s), loss on training batch is 0.00305629.
After 1242 training step(s), loss on training batch is 0.00316162.
After 1243 training step(s), loss on training batch is 0.00284216.
After 1244 training step(s), loss on training batch is 0.00311779.
After 1245 training step(s), loss on training batch is 0.00210237.
After 1246 training step(s), loss on training batch is 0.00404543.
After 1247 training step(s), loss on training batch is 0.00251012.
After 1248 training step(s), loss on training batch is 0.00253852.
After 1249 training step(s), loss on training batch is 0.00277457.
After 1250 training step(s), loss on training batch is 0.00266755.
After 1251 training step(s), loss on training batch is 0.00290196.
After 1252 training step(s), loss on training batch is 0.00318528.
After 1253 training step(s), loss on training batch is 0.00254081.
After 1254 training step(s), loss on training batch is 0.00182804.
After 1255 training step(s), loss on training batch is 0.00256543.
After 1256 training step(s), loss on training batch is 0.00451878.
After 1257 training step(s), loss on training batch is 0.00426244.
After 1258 training step(s), loss on training batch is 0.0045331.
After 1259 training step(s), loss on training batch is 0.0041686.
After 1260 training step(s), loss on training batch is 0.0034297.
After 1261 training step(s), loss on training batch is 0.00272236.
After 1262 training step(s), loss on training batch is 0.00319743.
After 1263 training step(s), loss on training batch is 0.0032521.
After 1264 training step(s), loss on training batch is 0.00377972.
After 1265 training step(s), loss on training batch is 0.00432129.
After 1266 training step(s), loss on training batch is 0.00381695.
After 1267 training step(s), loss on training batch is 0.00339213.
After 1268 training step(s), loss on training batch is 0.00604189.
After 1269 training step(s), loss on training batch is 0.0049234.
After 1270 training step(s), loss on training batch is 0.0047858.
After 1271 training step(s), loss on training batch is 0.00503742.
After 1272 training step(s), loss on training batch is 0.00389417.
After 1273 training step(s), loss on training batch is 0.0052079.
After 1274 training step(s), loss on training batch is 0.00428754.
After 1275 training step(s), loss on training batch is 0.00429167.
After 1276 training step(s), loss on training batch is 0.00409899.
After 1277 training step(s), loss on training batch is 0.00587611.
After 1278 training step(s), loss on training batch is 0.00420298.
After 1279 training step(s), loss on training batch is 0.00396116.
After 1280 training step(s), loss on training batch is 0.00328967.
After 1281 training step(s), loss on training batch is 0.00445387.
After 1282 training step(s), loss on training batch is 0.00379441.
After 1283 training step(s), loss on training batch is 0.0026157.
After 1284 training step(s), loss on training batch is 0.00339454.
After 1285 training step(s), loss on training batch is 0.00331949.
After 1286 training step(s), loss on training batch is 0.00310757.
After 1287 training step(s), loss on training batch is 0.00360501.
After 1288 training step(s), loss on training batch is 0.00357735.
After 1289 training step(s), loss on training batch is 0.00389891.
After 1290 training step(s), loss on training batch is 0.00277177.
After 1291 training step(s), loss on training batch is 0.00284627.
After 1292 training step(s), loss on training batch is 0.00369591.
After 1293 training step(s), loss on training batch is 0.00276741.
After 1294 training step(s), loss on training batch is 0.00261147.
After 1295 training step(s), loss on training batch is 0.00326832.
After 1296 training step(s), loss on training batch is 0.00283624.
After 1297 training step(s), loss on training batch is 0.00243856.
After 1298 training step(s), loss on training batch is 0.00230524.
After 1299 training step(s), loss on training batch is 0.00201521.
After 1300 training step(s), loss on training batch is 0.0043156.
After 1301 training step(s), loss on training batch is 0.00402633.
After 1302 training step(s), loss on training batch is 0.00270721.
After 1303 training step(s), loss on training batch is 0.00269151.
After 1304 training step(s), loss on training batch is 0.00263639.
After 1305 training step(s), loss on training batch is 0.00256192.
After 1306 training step(s), loss on training batch is 0.00306363.
After 1307 training step(s), loss on training batch is 0.00224698.
After 1308 training step(s), loss on training batch is 0.00129528.
After 1309 training step(s), loss on training batch is 0.00173451.
After 1310 training step(s), loss on training batch is 0.00157088.
After 1311 training step(s), loss on training batch is 0.00179967.
After 1312 training step(s), loss on training batch is 0.0030544.
After 1313 training step(s), loss on training batch is 0.0025381.
After 1314 training step(s), loss on training batch is 0.00393316.
After 1315 training step(s), loss on training batch is 0.00517542.
After 1316 training step(s), loss on training batch is 0.00442062.
After 1317 training step(s), loss on training batch is 0.00382033.
After 1318 training step(s), loss on training batch is 0.003532.
After 1319 training step(s), loss on training batch is 0.00207765.
After 1320 training step(s), loss on training batch is 0.00158106.
After 1321 training step(s), loss on training batch is 0.00127046.
After 1322 training step(s), loss on training batch is 0.00121952.
After 1323 training step(s), loss on training batch is 0.00232242.
After 1324 training step(s), loss on training batch is 0.00213794.
After 1325 training step(s), loss on training batch is 0.00164394.
After 1326 training step(s), loss on training batch is 0.00464569.
After 1327 training step(s), loss on training batch is 0.00224501.
After 1328 training step(s), loss on training batch is 0.0105377.
After 1329 training step(s), loss on training batch is 0.00559797.
After 1330 training step(s), loss on training batch is 0.00308354.
After 1331 training step(s), loss on training batch is 0.00264297.
After 1332 training step(s), loss on training batch is 0.00224739.
After 1333 training step(s), loss on training batch is 0.00221138.
After 1334 training step(s), loss on training batch is 0.00190712.
After 1335 training step(s), loss on training batch is 0.00171487.
After 1336 training step(s), loss on training batch is 0.00132256.
After 1337 training step(s), loss on training batch is 0.00137701.
After 1338 training step(s), loss on training batch is 0.0011202.
After 1339 training step(s), loss on training batch is 0.00235425.
After 1340 training step(s), loss on training batch is 0.00143139.
After 1341 training step(s), loss on training batch is 0.00100436.
After 1342 training step(s), loss on training batch is 0.0031757.
After 1343 training step(s), loss on training batch is 0.00432848.
After 1344 training step(s), loss on training batch is 0.00410193.
After 1345 training step(s), loss on training batch is 0.00192345.
After 1346 training step(s), loss on training batch is 0.003528.
After 1347 training step(s), loss on training batch is 0.00243979.
After 1348 training step(s), loss on training batch is 0.0022555.
After 1349 training step(s), loss on training batch is 0.00205865.
After 1350 training step(s), loss on training batch is 0.00382383.
After 1351 training step(s), loss on training batch is 0.00321071.
After 1352 training step(s), loss on training batch is 0.00203822.
After 1353 training step(s), loss on training batch is 0.00144196.
After 1354 training step(s), loss on training batch is 0.00113813.
After 1355 training step(s), loss on training batch is 0.00237266.
After 1356 training step(s), loss on training batch is 0.00224906.
After 1357 training step(s), loss on training batch is 0.00234623.
After 1358 training step(s), loss on training batch is 0.00117597.
After 1359 training step(s), loss on training batch is 0.00112789.
After 1360 training step(s), loss on training batch is 0.00123371.
After 1361 training step(s), loss on training batch is 0.00101418.
After 1362 training step(s), loss on training batch is 0.00115786.
After 1363 training step(s), loss on training batch is 0.000974882.
After 1364 training step(s), loss on training batch is 0.00755997.
After 1365 training step(s), loss on training batch is 0.00391419.
After 1366 training step(s), loss on training batch is 0.00297534.
After 1367 training step(s), loss on training batch is 0.00315595.
After 1368 training step(s), loss on training batch is 0.00142005.
After 1369 training step(s), loss on training batch is 0.00203053.
After 1370 training step(s), loss on training batch is 0.0011877.
After 1371 training step(s), loss on training batch is 0.00160174.
After 1372 training step(s), loss on training batch is 0.00109215.
After 1373 training step(s), loss on training batch is 0.00124212.
After 1374 training step(s), loss on training batch is 0.00106286.
After 1375 training step(s), loss on training batch is 0.00252868.
After 1376 training step(s), loss on training batch is 0.00161802.
After 1377 training step(s), loss on training batch is 0.00337594.
After 1378 training step(s), loss on training batch is 0.00155388.
After 1379 training step(s), loss on training batch is 0.00140734.
After 1380 training step(s), loss on training batch is 0.00618317.
After 1381 training step(s), loss on training batch is 0.0025443.
After 1382 training step(s), loss on training batch is 0.00294667.
After 1383 training step(s), loss on training batch is 0.00353956.
After 1384 training step(s), loss on training batch is 0.00433345.
After 1385 training step(s), loss on training batch is 0.00456298.
After 1386 training step(s), loss on training batch is 0.00345502.
After 1387 training step(s), loss on training batch is 0.00345608.
After 1388 training step(s), loss on training batch is 0.00282602.
After 1389 training step(s), loss on training batch is 0.0026045.
After 1390 training step(s), loss on training batch is 0.00287095.
After 1391 training step(s), loss on training batch is 0.00260953.
After 1392 training step(s), loss on training batch is 0.00359378.
After 1393 training step(s), loss on training batch is 0.00274538.
After 1394 training step(s), loss on training batch is 0.00268954.
After 1395 training step(s), loss on training batch is 0.00359227.
After 1396 training step(s), loss on training batch is 0.00407337.
After 1397 training step(s), loss on training batch is 0.00309915.
After 1398 training step(s), loss on training batch is 0.00567829.
After 1399 training step(s), loss on training batch is 0.00493615.
After 1400 training step(s), loss on training batch is 0.00376737.
After 1401 training step(s), loss on training batch is 0.00325011.
After 1402 training step(s), loss on training batch is 0.00292899.
After 1403 training step(s), loss on training batch is 0.00228632.
After 1404 training step(s), loss on training batch is 0.00212656.
After 1405 training step(s), loss on training batch is 0.00251907.
After 1406 training step(s), loss on training batch is 0.00383842.
After 1407 training step(s), loss on training batch is 0.00319565.
After 1408 training step(s), loss on training batch is 0.00334113.
After 1409 training step(s), loss on training batch is 0.00202601.
After 1410 training step(s), loss on training batch is 0.00337573.
After 1411 training step(s), loss on training batch is 0.00252725.
After 1412 training step(s), loss on training batch is 0.00228835.
After 1413 training step(s), loss on training batch is 0.00242237.
After 1414 training step(s), loss on training batch is 0.00172871.
After 1415 training step(s), loss on training batch is 0.00193682.
After 1416 training step(s), loss on training batch is 0.00176346.
After 1417 training step(s), loss on training batch is 0.00169468.
After 1418 training step(s), loss on training batch is 0.00407817.
After 1419 training step(s), loss on training batch is 0.00493672.
After 1420 training step(s), loss on training batch is 0.00267154.
After 1421 training step(s), loss on training batch is 0.00505742.
After 1422 training step(s), loss on training batch is 0.00294095.
After 1423 training step(s), loss on training batch is 0.00240385.
After 1424 training step(s), loss on training batch is 0.00179882.
After 1425 training step(s), loss on training batch is 0.00237031.
After 1426 training step(s), loss on training batch is 0.00273013.
After 1427 training step(s), loss on training batch is 0.00196579.
After 1428 training step(s), loss on training batch is 0.00206858.
After 1429 training step(s), loss on training batch is 0.00180062.
After 1430 training step(s), loss on training batch is 0.00257532.
After 1431 training step(s), loss on training batch is 0.00235546.
After 1432 training step(s), loss on training batch is 0.00170626.
After 1433 training step(s), loss on training batch is 0.00230116.
After 1434 training step(s), loss on training batch is 0.00168765.
After 1435 training step(s), loss on training batch is 0.00116263.
After 1436 training step(s), loss on training batch is 0.00206261.
After 1437 training step(s), loss on training batch is 0.00461265.
After 1438 training step(s), loss on training batch is 0.0018361.
After 1439 training step(s), loss on training batch is 0.0021769.
After 1440 training step(s), loss on training batch is 0.00244931.
After 1441 training step(s), loss on training batch is 0.00194854.
After 1442 training step(s), loss on training batch is 0.00159049.
After 1443 training step(s), loss on training batch is 0.00169907.
After 1444 training step(s), loss on training batch is 0.00150194.
After 1445 training step(s), loss on training batch is 0.00170141.
After 1446 training step(s), loss on training batch is 0.00189922.
After 1447 training step(s), loss on training batch is 0.00223947.
After 1448 training step(s), loss on training batch is 0.00261552.
After 1449 training step(s), loss on training batch is 0.0021243.
After 1450 training step(s), loss on training batch is 0.00222596.
After 1451 training step(s), loss on training batch is 0.00197344.
After 1452 training step(s), loss on training batch is 0.00177731.
After 1453 training step(s), loss on training batch is 0.00259257.
After 1454 training step(s), loss on training batch is 0.00185903.
After 1455 training step(s), loss on training batch is 0.00359102.
After 1456 training step(s), loss on training batch is 0.00176077.
After 1457 training step(s), loss on training batch is 0.0014379.
After 1458 training step(s), loss on training batch is 0.00250794.
After 1459 training step(s), loss on training batch is 0.00218092.
After 1460 training step(s), loss on training batch is 0.00228455.
After 1461 training step(s), loss on training batch is 0.00148969.
After 1462 training step(s), loss on training batch is 0.0012371.
After 1463 training step(s), loss on training batch is 0.00140922.
After 1464 training step(s), loss on training batch is 0.00166902.
After 1465 training step(s), loss on training batch is 0.00206009.
After 1466 training step(s), loss on training batch is 0.0029799.
After 1467 training step(s), loss on training batch is 0.00403299.
After 1468 training step(s), loss on training batch is 0.00298332.
After 1469 training step(s), loss on training batch is 0.00304575.
After 1470 training step(s), loss on training batch is 0.00349996.
After 1471 training step(s), loss on training batch is 0.00281981.
After 1472 training step(s), loss on training batch is 0.00328723.
After 1473 training step(s), loss on training batch is 0.0052214.
After 1474 training step(s), loss on training batch is 0.00384028.
After 1475 training step(s), loss on training batch is 0.00383263.
After 1476 training step(s), loss on training batch is 0.00271248.
After 1477 training step(s), loss on training batch is 0.00400708.
After 1478 training step(s), loss on training batch is 0.00335175.
After 1479 training step(s), loss on training batch is 0.00253182.
After 1480 training step(s), loss on training batch is 0.00327193.
After 1481 training step(s), loss on training batch is 0.00237442.
After 1482 training step(s), loss on training batch is 0.00217305.
After 1483 training step(s), loss on training batch is 0.00282459.
After 1484 training step(s), loss on training batch is 0.0022816.
After 1485 training step(s), loss on training batch is 0.00207517.
After 1486 training step(s), loss on training batch is 0.00281684.
After 1487 training step(s), loss on training batch is 0.00219319.
After 1488 training step(s), loss on training batch is 0.00293151.
After 1489 training step(s), loss on training batch is 0.00258009.
After 1490 training step(s), loss on training batch is 0.00239995.
After 1491 training step(s), loss on training batch is 0.0027274.
After 1492 training step(s), loss on training batch is 0.00402507.
After 1493 training step(s), loss on training batch is 0.00249403.
After 1494 training step(s), loss on training batch is 0.00361495.
After 1495 training step(s), loss on training batch is 0.00295559.
After 1496 training step(s), loss on training batch is 0.00291471.
After 1497 training step(s), loss on training batch is 0.0037641.
After 1498 training step(s), loss on training batch is 0.00280325.
After 1499 training step(s), loss on training batch is 0.00281229.
After 1500 training step(s), loss on training batch is 0.00260604.
After 1501 training step(s), loss on training batch is 0.0019266.
After 1502 training step(s), loss on training batch is 0.00416577.
After 1503 training step(s), loss on training batch is 0.00327809.
After 1504 training step(s), loss on training batch is 0.00549894.
After 1505 training step(s), loss on training batch is 0.00469637.
After 1506 training step(s), loss on training batch is 0.0040514.
After 1507 training step(s), loss on training batch is 0.00378117.
After 1508 training step(s), loss on training batch is 0.00403615.
After 1509 training step(s), loss on training batch is 0.0045831.
After 1510 training step(s), loss on training batch is 0.003504.
After 1511 training step(s), loss on training batch is 0.00319955.
After 1512 training step(s), loss on training batch is 0.00289353.
After 1513 training step(s), loss on training batch is 0.0024844.
After 1514 training step(s), loss on training batch is 0.00459849.
After 1515 training step(s), loss on training batch is 0.00259122.
After 1516 training step(s), loss on training batch is 0.00268617.
After 1517 training step(s), loss on training batch is 0.00173274.
After 1518 training step(s), loss on training batch is 0.00141046.
After 1519 training step(s), loss on training batch is 0.00182632.
After 1520 training step(s), loss on training batch is 0.00237886.
After 1521 training step(s), loss on training batch is 0.00184105.
After 1522 training step(s), loss on training batch is 0.00201035.
After 1523 training step(s), loss on training batch is 0.00248029.
After 1524 training step(s), loss on training batch is 0.00237065.
After 1525 training step(s), loss on training batch is 0.00241741.
After 1526 training step(s), loss on training batch is 0.00273915.
After 1527 training step(s), loss on training batch is 0.00258555.
After 1528 training step(s), loss on training batch is 0.00483951.
After 1529 training step(s), loss on training batch is 0.00318777.
After 1530 training step(s), loss on training batch is 0.00634483.
After 1531 training step(s), loss on training batch is 0.00527669.
After 1532 training step(s), loss on training batch is 0.00323506.
After 1533 training step(s), loss on training batch is 0.00413312.
After 1534 training step(s), loss on training batch is 0.00301391.
After 1535 training step(s), loss on training batch is 0.00253757.
After 1536 training step(s), loss on training batch is 0.00247079.
After 1537 training step(s), loss on training batch is 0.00301253.
After 1538 training step(s), loss on training batch is 0.00348394.
After 1539 training step(s), loss on training batch is 0.00302288.
After 1540 training step(s), loss on training batch is 0.00269719.
After 1541 training step(s), loss on training batch is 0.00228336.
After 1542 training step(s), loss on training batch is 0.0031938.
After 1543 training step(s), loss on training batch is 0.00264552.
After 1544 training step(s), loss on training batch is 0.0025462.
After 1545 training step(s), loss on training batch is 0.00233873.
After 1546 training step(s), loss on training batch is 0.00326098.
After 1547 training step(s), loss on training batch is 0.00262528.
After 1548 training step(s), loss on training batch is 0.00205306.
After 1549 training step(s), loss on training batch is 0.0019644.
After 1550 training step(s), loss on training batch is 0.00170228.
After 1551 training step(s), loss on training batch is 0.00175032.
After 1552 training step(s), loss on training batch is 0.00226207.
After 1553 training step(s), loss on training batch is 0.00245367.
After 1554 training step(s), loss on training batch is 0.00196457.
After 1555 training step(s), loss on training batch is 0.00286012.
After 1556 training step(s), loss on training batch is 0.00165831.
After 1557 training step(s), loss on training batch is 0.00140443.
After 1558 training step(s), loss on training batch is 0.00171231.
After 1559 training step(s), loss on training batch is 0.00281791.
After 1560 training step(s), loss on training batch is 0.00361348.
After 1561 training step(s), loss on training batch is 0.00281079.
After 1562 training step(s), loss on training batch is 0.0014954.
After 1563 training step(s), loss on training batch is 0.00176224.
After 1564 training step(s), loss on training batch is 0.00339476.
After 1565 training step(s), loss on training batch is 0.00232914.
After 1566 training step(s), loss on training batch is 0.00218897.
After 1567 training step(s), loss on training batch is 0.00280878.
After 1568 training step(s), loss on training batch is 0.00279209.
After 1569 training step(s), loss on training batch is 0.00154555.
After 1570 training step(s), loss on training batch is 0.00186931.
After 1571 training step(s), loss on training batch is 0.00105356.
After 1572 training step(s), loss on training batch is 0.002425.
After 1573 training step(s), loss on training batch is 0.0014204.
After 1574 training step(s), loss on training batch is 0.00125487.
After 1575 training step(s), loss on training batch is 0.0029714.
After 1576 training step(s), loss on training batch is 0.00249539.
After 1577 training step(s), loss on training batch is 0.0022773.
After 1578 training step(s), loss on training batch is 0.00151704.
After 1579 training step(s), loss on training batch is 0.0016196.
After 1580 training step(s), loss on training batch is 0.00141448.
After 1581 training step(s), loss on training batch is 0.00154234.
After 1582 training step(s), loss on training batch is 0.00156288.
After 1583 training step(s), loss on training batch is 0.00198694.
After 1584 training step(s), loss on training batch is 0.00151441.
After 1585 training step(s), loss on training batch is 0.00175454.
After 1586 training step(s), loss on training batch is 0.00347106.
After 1587 training step(s), loss on training batch is 0.00226981.
After 1588 training step(s), loss on training batch is 0.00149174.
After 1589 training step(s), loss on training batch is 0.00175967.
After 1590 training step(s), loss on training batch is 0.00126916.
After 1591 training step(s), loss on training batch is 0.00134614.
After 1592 training step(s), loss on training batch is 0.00135596.
After 1593 training step(s), loss on training batch is 0.00136821.
After 1594 training step(s), loss on training batch is 0.000931481.
After 1595 training step(s), loss on training batch is 0.00260673.
After 1596 training step(s), loss on training batch is 0.00160136.
After 1597 training step(s), loss on training batch is 0.00106865.
After 1598 training step(s), loss on training batch is 0.00140515.
After 1599 training step(s), loss on training batch is 0.00159511.
After 1600 training step(s), loss on training batch is 0.00169389.
After 1601 training step(s), loss on training batch is 0.00204591.
After 1602 training step(s), loss on training batch is 0.00204545.
After 1603 training step(s), loss on training batch is 0.00123003.
After 1604 training step(s), loss on training batch is 0.00220077.
After 1605 training step(s), loss on training batch is 0.0044225.
After 1606 training step(s), loss on training batch is 0.00324997.
After 1607 training step(s), loss on training batch is 0.00175328.
After 1608 training step(s), loss on training batch is 0.00181384.
After 1609 training step(s), loss on training batch is 0.00227697.
After 1610 training step(s), loss on training batch is 0.00341251.
After 1611 training step(s), loss on training batch is 0.00170164.
After 1612 training step(s), loss on training batch is 0.00144913.
After 1613 training step(s), loss on training batch is 0.00153545.
After 1614 training step(s), loss on training batch is 0.00168656.
After 1615 training step(s), loss on training batch is 0.00182875.
After 1616 training step(s), loss on training batch is 0.00182961.
After 1617 training step(s), loss on training batch is 0.00210701.
After 1618 training step(s), loss on training batch is 0.00146782.
After 1619 training step(s), loss on training batch is 0.00158676.
After 1620 training step(s), loss on training batch is 0.00113178.
After 1621 training step(s), loss on training batch is 0.00152284.
After 1622 training step(s), loss on training batch is 0.00202624.
After 1623 training step(s), loss on training batch is 0.00140958.
After 1624 training step(s), loss on training batch is 0.000986399.
After 1625 training step(s), loss on training batch is 0.00122332.
After 1626 training step(s), loss on training batch is 0.00140314.
After 1627 training step(s), loss on training batch is 0.0023269.
After 1628 training step(s), loss on training batch is 0.00362358.
After 1629 training step(s), loss on training batch is 0.00257193.
After 1630 training step(s), loss on training batch is 0.0023349.
After 1631 training step(s), loss on training batch is 0.00237685.
After 1632 training step(s), loss on training batch is 0.00294555.
After 1633 training step(s), loss on training batch is 0.00330763.
After 1634 training step(s), loss on training batch is 0.00242702.
After 1635 training step(s), loss on training batch is 0.00199485.
After 1636 training step(s), loss on training batch is 0.00270286.
After 1637 training step(s), loss on training batch is 0.00255846.
After 1638 training step(s), loss on training batch is 0.00174761.
After 1639 training step(s), loss on training batch is 0.00175305.
After 1640 training step(s), loss on training batch is 0.00435714.
After 1641 training step(s), loss on training batch is 0.0021038.
After 1642 training step(s), loss on training batch is 0.00213298.
After 1643 training step(s), loss on training batch is 0.00203693.
After 1644 training step(s), loss on training batch is 0.00241206.
After 1645 training step(s), loss on training batch is 0.00147085.
After 1646 training step(s), loss on training batch is 0.00323701.
After 1647 training step(s), loss on training batch is 0.00177891.
After 1648 training step(s), loss on training batch is 0.00198988.
After 1649 training step(s), loss on training batch is 0.00202617.
After 1650 training step(s), loss on training batch is 0.00213245.
After 1651 training step(s), loss on training batch is 0.0022203.
After 1652 training step(s), loss on training batch is 0.00224803.
After 1653 training step(s), loss on training batch is 0.0021948.
After 1654 training step(s), loss on training batch is 0.00129132.
After 1655 training step(s), loss on training batch is 0.00200213.
After 1656 training step(s), loss on training batch is 0.00346523.
After 1657 training step(s), loss on training batch is 0.00291349.
After 1658 training step(s), loss on training batch is 0.00319397.
After 1659 training step(s), loss on training batch is 0.00320013.
After 1660 training step(s), loss on training batch is 0.00233887.
After 1661 training step(s), loss on training batch is 0.00209296.
After 1662 training step(s), loss on training batch is 0.0027435.
After 1663 training step(s), loss on training batch is 0.00258621.
After 1664 training step(s), loss on training batch is 0.00265558.
After 1665 training step(s), loss on training batch is 0.00337205.
After 1666 training step(s), loss on training batch is 0.0026726.
After 1667 training step(s), loss on training batch is 0.00235322.
After 1668 training step(s), loss on training batch is 0.00402595.
After 1669 training step(s), loss on training batch is 0.00360428.
After 1670 training step(s), loss on training batch is 0.00347389.
After 1671 training step(s), loss on training batch is 0.00368256.
After 1672 training step(s), loss on training batch is 0.00295286.
After 1673 training step(s), loss on training batch is 0.00384852.
After 1674 training step(s), loss on training batch is 0.00319276.
After 1675 training step(s), loss on training batch is 0.00298527.
After 1676 training step(s), loss on training batch is 0.00280389.
After 1677 training step(s), loss on training batch is 0.00505851.
After 1678 training step(s), loss on training batch is 0.00297027.
After 1679 training step(s), loss on training batch is 0.00280376.
After 1680 training step(s), loss on training batch is 0.00256005.
After 1681 training step(s), loss on training batch is 0.0025771.
After 1682 training step(s), loss on training batch is 0.00255093.
After 1683 training step(s), loss on training batch is 0.00146451.
After 1684 training step(s), loss on training batch is 0.00181428.
After 1685 training step(s), loss on training batch is 0.00206341.
After 1686 training step(s), loss on training batch is 0.00214033.
After 1687 training step(s), loss on training batch is 0.00260477.
After 1688 training step(s), loss on training batch is 0.00228486.
After 1689 training step(s), loss on training batch is 0.00203106.
After 1690 training step(s), loss on training batch is 0.0016643.
After 1691 training step(s), loss on training batch is 0.00168684.
After 1692 training step(s), loss on training batch is 0.00244018.
After 1693 training step(s), loss on training batch is 0.00157701.
After 1694 training step(s), loss on training batch is 0.0015258.
After 1695 training step(s), loss on training batch is 0.00193465.
After 1696 training step(s), loss on training batch is 0.00170086.
After 1697 training step(s), loss on training batch is 0.00141745.
After 1698 training step(s), loss on training batch is 0.0013032.
After 1699 training step(s), loss on training batch is 0.00116644.
After 1700 training step(s), loss on training batch is 0.00223047.
After 1701 training step(s), loss on training batch is 0.00310382.
After 1702 training step(s), loss on training batch is 0.00195689.
After 1703 training step(s), loss on training batch is 0.00188551.
After 1704 training step(s), loss on training batch is 0.00198206.
After 1705 training step(s), loss on training batch is 0.00152089.
After 1706 training step(s), loss on training batch is 0.00191546.
After 1707 training step(s), loss on training batch is 0.00180027.
After 1708 training step(s), loss on training batch is 0.000972749.
After 1709 training step(s), loss on training batch is 0.00104613.
After 1710 training step(s), loss on training batch is 0.00115246.
After 1711 training step(s), loss on training batch is 0.00142202.
After 1712 training step(s), loss on training batch is 0.00187115.
After 1713 training step(s), loss on training batch is 0.0014656.
After 1714 training step(s), loss on training batch is 0.00294865.
After 1715 training step(s), loss on training batch is 0.00412238.
After 1716 training step(s), loss on training batch is 0.00281298.
After 1717 training step(s), loss on training batch is 0.0027514.
After 1718 training step(s), loss on training batch is 0.00260987.
After 1719 training step(s), loss on training batch is 0.00143626.
After 1720 training step(s), loss on training batch is 0.00119032.
After 1721 training step(s), loss on training batch is 0.00105218.
After 1722 training step(s), loss on training batch is 0.000804153.
After 1723 training step(s), loss on training batch is 0.0016879.
After 1724 training step(s), loss on training batch is 0.00122627.
After 1725 training step(s), loss on training batch is 0.00116346.
After 1726 training step(s), loss on training batch is 0.00200291.
After 1727 training step(s), loss on training batch is 0.00192397.
After 1728 training step(s), loss on training batch is 0.0106997.
After 1729 training step(s), loss on training batch is 0.0030726.
After 1730 training step(s), loss on training batch is 0.00190284.
After 1731 training step(s), loss on training batch is 0.0017657.
After 1732 training step(s), loss on training batch is 0.0015259.
After 1733 training step(s), loss on training batch is 0.00132416.
After 1734 training step(s), loss on training batch is 0.00125937.
After 1735 training step(s), loss on training batch is 0.00113958.
After 1736 training step(s), loss on training batch is 0.000948378.
After 1737 training step(s), loss on training batch is 0.000974532.
After 1738 training step(s), loss on training batch is 0.00084727.
After 1739 training step(s), loss on training batch is 0.0013287.
After 1740 training step(s), loss on training batch is 0.0010778.
After 1741 training step(s), loss on training batch is 0.000706248.
After 1742 training step(s), loss on training batch is 0.00155664.
After 1743 training step(s), loss on training batch is 0.00174556.
After 1744 training step(s), loss on training batch is 0.00194143.
After 1745 training step(s), loss on training batch is 0.00112389.
After 1746 training step(s), loss on training batch is 0.00172008.
After 1747 training step(s), loss on training batch is 0.00124573.
After 1748 training step(s), loss on training batch is 0.00117381.
After 1749 training step(s), loss on training batch is 0.0011874.
After 1750 training step(s), loss on training batch is 0.00185519.
After 1751 training step(s), loss on training batch is 0.00166913.
After 1752 training step(s), loss on training batch is 0.00117853.
After 1753 training step(s), loss on training batch is 0.000954708.
After 1754 training step(s), loss on training batch is 0.000739778.
After 1755 training step(s), loss on training batch is 0.00137646.
After 1756 training step(s), loss on training batch is 0.00136429.
After 1757 training step(s), loss on training batch is 0.00153574.
After 1758 training step(s), loss on training batch is 0.000760213.
After 1759 training step(s), loss on training batch is 0.000824568.
After 1760 training step(s), loss on training batch is 0.00100751.
After 1761 training step(s), loss on training batch is 0.000736562.
After 1762 training step(s), loss on training batch is 0.000792563.
After 1763 training step(s), loss on training batch is 0.00074649.
After 1764 training step(s), loss on training batch is 0.00308914.
After 1765 training step(s), loss on training batch is 0.00201329.
After 1766 training step(s), loss on training batch is 0.00135715.
After 1767 training step(s), loss on training batch is 0.00151239.
After 1768 training step(s), loss on training batch is 0.000829307.
After 1769 training step(s), loss on training batch is 0.00120406.
After 1770 training step(s), loss on training batch is 0.000832704.
After 1771 training step(s), loss on training batch is 0.0012407.
After 1772 training step(s), loss on training batch is 0.000768309.
After 1773 training step(s), loss on training batch is 0.000891473.
After 1774 training step(s), loss on training batch is 0.000812742.
After 1775 training step(s), loss on training batch is 0.00141464.
After 1776 training step(s), loss on training batch is 0.00108248.
After 1777 training step(s), loss on training batch is 0.00189669.
After 1778 training step(s), loss on training batch is 0.00102573.
After 1779 training step(s), loss on training batch is 0.000964205.
After 1780 training step(s), loss on training batch is 0.00345027.
After 1781 training step(s), loss on training batch is 0.00125765.
After 1782 training step(s), loss on training batch is 0.00161505.
After 1783 training step(s), loss on training batch is 0.00246204.
After 1784 training step(s), loss on training batch is 0.00318837.
After 1785 training step(s), loss on training batch is 0.00371671.
After 1786 training step(s), loss on training batch is 0.00255291.
After 1787 training step(s), loss on training batch is 0.00223213.
After 1788 training step(s), loss on training batch is 0.0017266.
After 1789 training step(s), loss on training batch is 0.00178394.
After 1790 training step(s), loss on training batch is 0.00190279.
After 1791 training step(s), loss on training batch is 0.00170268.
After 1792 training step(s), loss on training batch is 0.0026444.
After 1793 training step(s), loss on training batch is 0.00163133.
After 1794 training step(s), loss on training batch is 0.00165022.
After 1795 training step(s), loss on training batch is 0.00292754.
After 1796 training step(s), loss on training batch is 0.00286487.
After 1797 training step(s), loss on training batch is 0.00210665.
After 1798 training step(s), loss on training batch is 0.0033892.
After 1799 training step(s), loss on training batch is 0.00376225.
After 1800 training step(s), loss on training batch is 0.00214555.
After 1801 training step(s), loss on training batch is 0.00222308.
After 1802 training step(s), loss on training batch is 0.00205384.
After 1803 training step(s), loss on training batch is 0.00168631.
After 1804 training step(s), loss on training batch is 0.00155797.
After 1805 training step(s), loss on training batch is 0.00162296.
After 1806 training step(s), loss on training batch is 0.0024601.
After 1807 training step(s), loss on training batch is 0.00254809.
After 1808 training step(s), loss on training batch is 0.00254763.
After 1809 training step(s), loss on training batch is 0.00145776.
After 1810 training step(s), loss on training batch is 0.00295226.
After 1811 training step(s), loss on training batch is 0.00190144.
After 1812 training step(s), loss on training batch is 0.00189935.
After 1813 training step(s), loss on training batch is 0.00174923.
After 1814 training step(s), loss on training batch is 0.00126026.
After 1815 training step(s), loss on training batch is 0.00139676.
After 1816 training step(s), loss on training batch is 0.00129675.
After 1817 training step(s), loss on training batch is 0.00137978.
After 1818 training step(s), loss on training batch is 0.00284915.
After 1819 training step(s), loss on training batch is 0.00353135.
After 1820 training step(s), loss on training batch is 0.00214692.
After 1821 training step(s), loss on training batch is 0.00467277.
After 1822 training step(s), loss on training batch is 0.00239295.
After 1823 training step(s), loss on training batch is 0.00211899.
After 1824 training step(s), loss on training batch is 0.00149268.
After 1825 training step(s), loss on training batch is 0.00205324.
After 1826 training step(s), loss on training batch is 0.00162488.
After 1827 training step(s), loss on training batch is 0.00148778.
After 1828 training step(s), loss on training batch is 0.00163856.
After 1829 training step(s), loss on training batch is 0.00137519.
After 1830 training step(s), loss on training batch is 0.00212124.
After 1831 training step(s), loss on training batch is 0.00226694.
After 1832 training step(s), loss on training batch is 0.00145756.
After 1833 training step(s), loss on training batch is 0.00132911.
After 1834 training step(s), loss on training batch is 0.00114805.
After 1835 training step(s), loss on training batch is 0.000854657.
After 1836 training step(s), loss on training batch is 0.0019337.
After 1837 training step(s), loss on training batch is 0.00416948.
After 1838 training step(s), loss on training batch is 0.00150801.
After 1839 training step(s), loss on training batch is 0.00169865.
After 1840 training step(s), loss on training batch is 0.0020152.
After 1841 training step(s), loss on training batch is 0.00159231.
After 1842 training step(s), loss on training batch is 0.00130866.
After 1843 training step(s), loss on training batch is 0.00139492.
After 1844 training step(s), loss on training batch is 0.00125805.
After 1845 training step(s), loss on training batch is 0.00147641.
After 1846 training step(s), loss on training batch is 0.00159585.
After 1847 training step(s), loss on training batch is 0.00148161.
After 1848 training step(s), loss on training batch is 0.00221482.
After 1849 training step(s), loss on training batch is 0.00174856.
After 1850 training step(s), loss on training batch is 0.00189471.
After 1851 training step(s), loss on training batch is 0.00153225.
After 1852 training step(s), loss on training batch is 0.00130243.
After 1853 training step(s), loss on training batch is 0.0023535.
After 1854 training step(s), loss on training batch is 0.00155964.
After 1855 training step(s), loss on training batch is 0.00238682.
After 1856 training step(s), loss on training batch is 0.00135831.
After 1857 training step(s), loss on training batch is 0.0011847.
After 1858 training step(s), loss on training batch is 0.00215652.
After 1859 training step(s), loss on training batch is 0.0020301.
After 1860 training step(s), loss on training batch is 0.00191005.
After 1861 training step(s), loss on training batch is 0.00146169.
After 1862 training step(s), loss on training batch is 0.00126861.
After 1863 training step(s), loss on training batch is 0.00129904.
After 1864 training step(s), loss on training batch is 0.00116668.
After 1865 training step(s), loss on training batch is 0.00175347.
After 1866 training step(s), loss on training batch is 0.00257589.
After 1867 training step(s), loss on training batch is 0.00324231.
After 1868 training step(s), loss on training batch is 0.00246843.
After 1869 training step(s), loss on training batch is 0.00236461.
After 1870 training step(s), loss on training batch is 0.00275537.
After 1871 training step(s), loss on training batch is 0.0021011.
After 1872 training step(s), loss on training batch is 0.00262495.
After 1873 training step(s), loss on training batch is 0.00475439.
After 1874 training step(s), loss on training batch is 0.00306448.
After 1875 training step(s), loss on training batch is 0.00329278.
After 1876 training step(s), loss on training batch is 0.00256499.
After 1877 training step(s), loss on training batch is 0.00275774.
After 1878 training step(s), loss on training batch is 0.00286575.
After 1879 training step(s), loss on training batch is 0.00195047.
After 1880 training step(s), loss on training batch is 0.00240401.
After 1881 training step(s), loss on training batch is 0.00196719.
After 1882 training step(s), loss on training batch is 0.00184176.
After 1883 training step(s), loss on training batch is 0.00237892.
After 1884 training step(s), loss on training batch is 0.00195791.
After 1885 training step(s), loss on training batch is 0.00177455.
After 1886 training step(s), loss on training batch is 0.00261469.
After 1887 training step(s), loss on training batch is 0.0019058.
After 1888 training step(s), loss on training batch is 0.00262284.
After 1889 training step(s), loss on training batch is 0.00218664.
After 1890 training step(s), loss on training batch is 0.0020814.
After 1891 training step(s), loss on training batch is 0.00194502.
After 1892 training step(s), loss on training batch is 0.00302702.
After 1893 training step(s), loss on training batch is 0.00205236.
After 1894 training step(s), loss on training batch is 0.00261489.
After 1895 training step(s), loss on training batch is 0.00256647.
After 1896 training step(s), loss on training batch is 0.00242787.
After 1897 training step(s), loss on training batch is 0.0027516.
After 1898 training step(s), loss on training batch is 0.00212452.
After 1899 training step(s), loss on training batch is 0.00210546.
After 1900 training step(s), loss on training batch is 0.00196668.
After 1901 training step(s), loss on training batch is 0.0015572.
After 1902 training step(s), loss on training batch is 0.00291142.
After 1903 training step(s), loss on training batch is 0.00229392.
After 1904 training step(s), loss on training batch is 0.00459645.
After 1905 training step(s), loss on training batch is 0.00353102.
After 1906 training step(s), loss on training batch is 0.0028964.
After 1907 training step(s), loss on training batch is 0.00357215.
After 1908 training step(s), loss on training batch is 0.00270433.
After 1909 training step(s), loss on training batch is 0.00454528.
After 1910 training step(s), loss on training batch is 0.00329374.
After 1911 training step(s), loss on training batch is 0.00252708.
After 1912 training step(s), loss on training batch is 0.0023559.
After 1913 training step(s), loss on training batch is 0.00207975.
After 1914 training step(s), loss on training batch is 0.00374645.
After 1915 training step(s), loss on training batch is 0.00225196.
After 1916 training step(s), loss on training batch is 0.0014618.
After 1917 training step(s), loss on training batch is 0.00109198.
After 1918 training step(s), loss on training batch is 0.000976746.
After 1919 training step(s), loss on training batch is 0.00137945.
After 1920 training step(s), loss on training batch is 0.00180594.
After 1921 training step(s), loss on training batch is 0.00133486.
After 1922 training step(s), loss on training batch is 0.00136106.
After 1923 training step(s), loss on training batch is 0.00161778.
After 1924 training step(s), loss on training batch is 0.00173707.
After 1925 training step(s), loss on training batch is 0.00191945.
After 1926 training step(s), loss on training batch is 0.0021273.
After 1927 training step(s), loss on training batch is 0.00197267.
After 1928 training step(s), loss on training batch is 0.00333691.
After 1929 training step(s), loss on training batch is 0.00213814.
After 1930 training step(s), loss on training batch is 0.00587008.
After 1931 training step(s), loss on training batch is 0.00466479.
After 1932 training step(s), loss on training batch is 0.00278308.
After 1933 training step(s), loss on training batch is 0.00334994.
After 1934 training step(s), loss on training batch is 0.00221981.
After 1935 training step(s), loss on training batch is 0.00206013.
After 1936 training step(s), loss on training batch is 0.00213569.
After 1937 training step(s), loss on training batch is 0.00213441.
After 1938 training step(s), loss on training batch is 0.00277397.
After 1939 training step(s), loss on training batch is 0.00251867.
After 1940 training step(s), loss on training batch is 0.00230701.
After 1941 training step(s), loss on training batch is 0.00191164.
After 1942 training step(s), loss on training batch is 0.00257142.
After 1943 training step(s), loss on training batch is 0.00165248.
After 1944 training step(s), loss on training batch is 0.00173534.
After 1945 training step(s), loss on training batch is 0.00161174.
After 1946 training step(s), loss on training batch is 0.00228186.
After 1947 training step(s), loss on training batch is 0.00199429.
After 1948 training step(s), loss on training batch is 0.0014562.
After 1949 training step(s), loss on training batch is 0.00130761.
After 1950 training step(s), loss on training batch is 0.0010659.
After 1951 training step(s), loss on training batch is 0.00118427.
After 1952 training step(s), loss on training batch is 0.00148624.
After 1953 training step(s), loss on training batch is 0.00146717.
After 1954 training step(s), loss on training batch is 0.00122038.
After 1955 training step(s), loss on training batch is 0.00214863.
After 1956 training step(s), loss on training batch is 0.00110593.
After 1957 training step(s), loss on training batch is 0.000968667.
After 1958 training step(s), loss on training batch is 0.00112369.
After 1959 training step(s), loss on training batch is 0.00230618.
After 1960 training step(s), loss on training batch is 0.00314172.
After 1961 training step(s), loss on training batch is 0.00209438.
After 1962 training step(s), loss on training batch is 0.00116264.
After 1963 training step(s), loss on training batch is 0.00131104.
After 1964 training step(s), loss on training batch is 0.00248659.
After 1965 training step(s), loss on training batch is 0.00171895.
After 1966 training step(s), loss on training batch is 0.00152157.
After 1967 training step(s), loss on training batch is 0.00163805.
After 1968 training step(s), loss on training batch is 0.00176301.
After 1969 training step(s), loss on training batch is 0.00102293.
After 1970 training step(s), loss on training batch is 0.00119681.
After 1971 training step(s), loss on training batch is 0.000681977.
After 1972 training step(s), loss on training batch is 0.00160147.
After 1973 training step(s), loss on training batch is 0.000903788.
After 1974 training step(s), loss on training batch is 0.000852764.
After 1975 training step(s), loss on training batch is 0.00201664.
After 1976 training step(s), loss on training batch is 0.00173054.
After 1977 training step(s), loss on training batch is 0.00164135.
After 1978 training step(s), loss on training batch is 0.00106178.
After 1979 training step(s), loss on training batch is 0.00109237.
After 1980 training step(s), loss on training batch is 0.000932738.
After 1981 training step(s), loss on training batch is 0.001046.
After 1982 training step(s), loss on training batch is 0.00104741.
After 1983 training step(s), loss on training batch is 0.00133529.
After 1984 training step(s), loss on training batch is 0.00105179.
After 1985 training step(s), loss on training batch is 0.00117222.
After 1986 training step(s), loss on training batch is 0.00227718.
After 1987 training step(s), loss on training batch is 0.0015793.
After 1988 training step(s), loss on training batch is 0.000988145.
After 1989 training step(s), loss on training batch is 0.00106252.
After 1990 training step(s), loss on training batch is 0.000874876.
After 1991 training step(s), loss on training batch is 0.000920828.
After 1992 training step(s), loss on training batch is 0.000979826.
After 1993 training step(s), loss on training batch is 0.000841762.
After 1994 training step(s), loss on training batch is 0.000706002.
After 1995 training step(s), loss on training batch is 0.00170557.
After 1996 training step(s), loss on training batch is 0.00116036.
After 1997 training step(s), loss on training batch is 0.000754791.
After 1998 training step(s), loss on training batch is 0.000775664.
After 1999 training step(s), loss on training batch is 0.00110483.
After 2000 training step(s), loss on training batch is 0.00122881.
After 2001 training step(s), loss on training batch is 0.00153148.
After 2002 training step(s), loss on training batch is 0.00156251.
After 2003 training step(s), loss on training batch is 0.000930198.
After 2004 training step(s), loss on training batch is 0.00161992.
After 2005 training step(s), loss on training batch is 0.00374641.
After 2006 training step(s), loss on training batch is 0.00281467.
After 2007 training step(s), loss on training batch is 0.00154078.
After 2008 training step(s), loss on training batch is 0.00145218.
After 2009 training step(s), loss on training batch is 0.00170075.
After 2010 training step(s), loss on training batch is 0.00243871.
After 2011 training step(s), loss on training batch is 0.0012446.
After 2012 training step(s), loss on training batch is 0.000937817.
After 2013 training step(s), loss on training batch is 0.00109912.
After 2014 training step(s), loss on training batch is 0.00101014.
After 2015 training step(s), loss on training batch is 0.00103087.
After 2016 training step(s), loss on training batch is 0.0010692.
After 2017 training step(s), loss on training batch is 0.00124167.
After 2018 training step(s), loss on training batch is 0.000983695.
After 2019 training step(s), loss on training batch is 0.00109504.
After 2020 training step(s), loss on training batch is 0.00083003.
After 2021 training step(s), loss on training batch is 0.00090174.
After 2022 training step(s), loss on training batch is 0.00125112.
After 2023 training step(s), loss on training batch is 0.000919935.
After 2024 training step(s), loss on training batch is 0.00076347.
After 2025 training step(s), loss on training batch is 0.000844507.
After 2026 training step(s), loss on training batch is 0.00100199.
After 2027 training step(s), loss on training batch is 0.0012659.
After 2028 training step(s), loss on training batch is 0.00291454.
After 2029 training step(s), loss on training batch is 0.00189249.
After 2030 training step(s), loss on training batch is 0.00168386.
After 2031 training step(s), loss on training batch is 0.00169023.
After 2032 training step(s), loss on training batch is 0.0022187.
After 2033 training step(s), loss on training batch is 0.00237579.
After 2034 training step(s), loss on training batch is 0.00180525.
After 2035 training step(s), loss on training batch is 0.0014471.
After 2036 training step(s), loss on training batch is 0.00210306.
After 2037 training step(s), loss on training batch is 0.00203316.
After 2038 training step(s), loss on training batch is 0.00137788.
After 2039 training step(s), loss on training batch is 0.00141627.
After 2040 training step(s), loss on training batch is 0.00356799.
After 2041 training step(s), loss on training batch is 0.001612.
After 2042 training step(s), loss on training batch is 0.00176227.
After 2043 training step(s), loss on training batch is 0.00171694.
After 2044 training step(s), loss on training batch is 0.00195788.
After 2045 training step(s), loss on training batch is 0.00121861.
After 2046 training step(s), loss on training batch is 0.00255726.
After 2047 training step(s), loss on training batch is 0.00144609.
After 2048 training step(s), loss on training batch is 0.00169513.
After 2049 training step(s), loss on training batch is 0.00165697.
After 2050 training step(s), loss on training batch is 0.00175867.
After 2051 training step(s), loss on training batch is 0.00189141.
After 2052 training step(s), loss on training batch is 0.00174584.
After 2053 training step(s), loss on training batch is 0.00188773.
After 2054 training step(s), loss on training batch is 0.00123111.
After 2055 training step(s), loss on training batch is 0.00168658.
After 2056 training step(s), loss on training batch is 0.00292139.
After 2057 training step(s), loss on training batch is 0.00232938.
After 2058 training step(s), loss on training batch is 0.00250665.
After 2059 training step(s), loss on training batch is 0.00257962.
After 2060 training step(s), loss on training batch is 0.00187773.
After 2061 training step(s), loss on training batch is 0.001798.
After 2062 training step(s), loss on training batch is 0.00232668.
After 2063 training step(s), loss on training batch is 0.00220404.
After 2064 training step(s), loss on training batch is 0.00210916.
After 2065 training step(s), loss on training batch is 0.00272488.
After 2066 training step(s), loss on training batch is 0.00219124.
After 2067 training step(s), loss on training batch is 0.00205893.
After 2068 training step(s), loss on training batch is 0.00285851.
After 2069 training step(s), loss on training batch is 0.00309873.
After 2070 training step(s), loss on training batch is 0.00307101.
After 2071 training step(s), loss on training batch is 0.00302756.
After 2072 training step(s), loss on training batch is 0.00255708.
After 2073 training step(s), loss on training batch is 0.00294832.
After 2074 training step(s), loss on training batch is 0.0026137.
After 2075 training step(s), loss on training batch is 0.0024133.
After 2076 training step(s), loss on training batch is 0.00225529.
After 2077 training step(s), loss on training batch is 0.00428402.
After 2078 training step(s), loss on training batch is 0.0024123.
After 2079 training step(s), loss on training batch is 0.00216967.
After 2080 training step(s), loss on training batch is 0.00210644.
After 2081 training step(s), loss on training batch is 0.00179885.
After 2082 training step(s), loss on training batch is 0.00193341.
After 2083 training step(s), loss on training batch is 0.00103278.
After 2084 training step(s), loss on training batch is 0.00123768.
After 2085 training step(s), loss on training batch is 0.00161678.
After 2086 training step(s), loss on training batch is 0.00171574.
After 2087 training step(s), loss on training batch is 0.00212791.
After 2088 training step(s), loss on training batch is 0.0017177.
After 2089 training step(s), loss on training batch is 0.0013219.
After 2090 training step(s), loss on training batch is 0.00121643.
After 2091 training step(s), loss on training batch is 0.00118301.
After 2092 training step(s), loss on training batch is 0.00164895.
After 2093 training step(s), loss on training batch is 0.00109591.
After 2094 training step(s), loss on training batch is 0.00111025.
After 2095 training step(s), loss on training batch is 0.00137736.
After 2096 training step(s), loss on training batch is 0.00121953.
After 2097 training step(s), loss on training batch is 0.000982989.
After 2098 training step(s), loss on training batch is 0.000915966.
After 2099 training step(s), loss on training batch is 0.000866171.
After 2100 training step(s), loss on training batch is 0.00144541.
After 2101 training step(s), loss on training batch is 0.00302288.
After 2102 training step(s), loss on training batch is 0.00156655.
After 2103 training step(s), loss on training batch is 0.00151747.
After 2104 training step(s), loss on training batch is 0.00167709.
After 2105 training step(s), loss on training batch is 0.00124127.
After 2106 training step(s), loss on training batch is 0.00149859.
After 2107 training step(s), loss on training batch is 0.00156558.
After 2108 training step(s), loss on training batch is 0.000824757.
After 2109 training step(s), loss on training batch is 0.000838747.
After 2110 training step(s), loss on training batch is 0.00100869.
After 2111 training step(s), loss on training batch is 0.0012852.
After 2112 training step(s), loss on training batch is 0.00135648.
After 2113 training step(s), loss on training batch is 0.00109452.
After 2114 training step(s), loss on training batch is 0.00242347.
After 2115 training step(s), loss on training batch is 0.0034883.
After 2116 training step(s), loss on training batch is 0.00236772.
After 2117 training step(s), loss on training batch is 0.00205572.
After 2118 training step(s), loss on training batch is 0.00205219.
After 2119 training step(s), loss on training batch is 0.00105268.
After 2120 training step(s), loss on training batch is 0.00106209.
After 2121 training step(s), loss on training batch is 0.00100037.
After 2122 training step(s), loss on training batch is 0.000707709.
After 2123 training step(s), loss on training batch is 0.00136806.
After 2124 training step(s), loss on training batch is 0.000990034.
After 2125 training step(s), loss on training batch is 0.0009383.
After 2126 training step(s), loss on training batch is 0.00102504.
After 2127 training step(s), loss on training batch is 0.00203821.
After 2128 training step(s), loss on training batch is 0.0107114.
After 2129 training step(s), loss on training batch is 0.00402288.
After 2130 training step(s), loss on training batch is 0.00171859.
After 2131 training step(s), loss on training batch is 0.00129847.
After 2132 training step(s), loss on training batch is 0.00113526.
After 2133 training step(s), loss on training batch is 0.00104159.
After 2134 training step(s), loss on training batch is 0.000986936.
After 2135 training step(s), loss on training batch is 0.000789169.
After 2136 training step(s), loss on training batch is 0.000726782.
After 2137 training step(s), loss on training batch is 0.000793022.
After 2138 training step(s), loss on training batch is 0.000724434.
After 2139 training step(s), loss on training batch is 0.00092496.
After 2140 training step(s), loss on training batch is 0.000834086.
After 2141 training step(s), loss on training batch is 0.000595767.
After 2142 training step(s), loss on training batch is 0.000962251.
After 2143 training step(s), loss on training batch is 0.00102498.
After 2144 training step(s), loss on training batch is 0.00126406.
After 2145 training step(s), loss on training batch is 0.000766178.
After 2146 training step(s), loss on training batch is 0.00115747.
After 2147 training step(s), loss on training batch is 0.000847575.
After 2148 training step(s), loss on training batch is 0.000774487.
After 2149 training step(s), loss on training batch is 0.000828785.
After 2150 training step(s), loss on training batch is 0.00117184.
After 2151 training step(s), loss on training batch is 0.00110223.
After 2152 training step(s), loss on training batch is 0.000820148.
After 2153 training step(s), loss on training batch is 0.000748724.
After 2154 training step(s), loss on training batch is 0.000603244.
After 2155 training step(s), loss on training batch is 0.000891175.
After 2156 training step(s), loss on training batch is 0.000988624.
After 2157 training step(s), loss on training batch is 0.00115542.
After 2158 training step(s), loss on training batch is 0.000609765.
After 2159 training step(s), loss on training batch is 0.000684295.
After 2160 training step(s), loss on training batch is 0.000887662.
After 2161 training step(s), loss on training batch is 0.000629085.
After 2162 training step(s), loss on training batch is 0.000621848.
After 2163 training step(s), loss on training batch is 0.000661207.
After 2164 training step(s), loss on training batch is 0.00153984.
After 2165 training step(s), loss on training batch is 0.0010487.
After 2166 training step(s), loss on training batch is 0.000881927.
After 2167 training step(s), loss on training batch is 0.000975564.
After 2168 training step(s), loss on training batch is 0.000612585.
After 2169 training step(s), loss on training batch is 0.000862435.
After 2170 training step(s), loss on training batch is 0.000682744.
After 2171 training step(s), loss on training batch is 0.000995281.
After 2172 training step(s), loss on training batch is 0.000626321.
After 2173 training step(s), loss on training batch is 0.000725326.
After 2174 training step(s), loss on training batch is 0.000704726.
After 2175 training step(s), loss on training batch is 0.000943718.
After 2176 training step(s), loss on training batch is 0.000839447.
After 2177 training step(s), loss on training batch is 0.00132969.
After 2178 training step(s), loss on training batch is 0.000826299.
After 2179 training step(s), loss on training batch is 0.000810697.
After 2180 training step(s), loss on training batch is 0.00186608.
After 2181 training step(s), loss on training batch is 0.000841135.
After 2182 training step(s), loss on training batch is 0.00115335.
After 2183 training step(s), loss on training batch is 0.00186822.
After 2184 training step(s), loss on training batch is 0.00255526.
After 2185 training step(s), loss on training batch is 0.00311346.
After 2186 training step(s), loss on training batch is 0.00201754.
After 2187 training step(s), loss on training batch is 0.00169089.
After 2188 training step(s), loss on training batch is 0.00135361.
After 2189 training step(s), loss on training batch is 0.00152148.
After 2190 training step(s), loss on training batch is 0.00147924.
After 2191 training step(s), loss on training batch is 0.00137718.
After 2192 training step(s), loss on training batch is 0.00207652.
After 2193 training step(s), loss on training batch is 0.00121409.
After 2194 training step(s), loss on training batch is 0.00124584.
After 2195 training step(s), loss on training batch is 0.00249266.
After 2196 training step(s), loss on training batch is 0.00223269.
After 2197 training step(s), loss on training batch is 0.00158862.
After 2198 training step(s), loss on training batch is 0.00249727.
After 2199 training step(s), loss on training batch is 0.00285601.
After 2200 training step(s), loss on training batch is 0.00155343.
After 2201 training step(s), loss on training batch is 0.00175015.
After 2202 training step(s), loss on training batch is 0.00161446.
After 2203 training step(s), loss on training batch is 0.00138842.
After 2204 training step(s), loss on training batch is 0.00128775.
After 2205 training step(s), loss on training batch is 0.00123754.
After 2206 training step(s), loss on training batch is 0.00184393.
After 2207 training step(s), loss on training batch is 0.00219283.
After 2208 training step(s), loss on training batch is 0.00214119.
After 2209 training step(s), loss on training batch is 0.00120858.
After 2210 training step(s), loss on training batch is 0.00260209.
After 2211 training step(s), loss on training batch is 0.00165355.
After 2212 training step(s), loss on training batch is 0.0016335.
After 2213 training step(s), loss on training batch is 0.00141063.
After 2214 training step(s), loss on training batch is 0.00108495.
After 2215 training step(s), loss on training batch is 0.00117261.
After 2216 training step(s), loss on training batch is 0.00115697.
After 2217 training step(s), loss on training batch is 0.00119472.
After 2218 training step(s), loss on training batch is 0.00204.
After 2219 training step(s), loss on training batch is 0.00278266.
After 2220 training step(s), loss on training batch is 0.00186548.
After 2221 training step(s), loss on training batch is 0.00398114.
After 2222 training step(s), loss on training batch is 0.00206665.
After 2223 training step(s), loss on training batch is 0.00184546.
After 2224 training step(s), loss on training batch is 0.00141752.
After 2225 training step(s), loss on training batch is 0.00176784.
After 2226 training step(s), loss on training batch is 0.00133445.
After 2227 training step(s), loss on training batch is 0.00126285.
After 2228 training step(s), loss on training batch is 0.00140408.
After 2229 training step(s), loss on training batch is 0.00124191.
After 2230 training step(s), loss on training batch is 0.00177095.
After 2231 training step(s), loss on training batch is 0.00208408.
After 2232 training step(s), loss on training batch is 0.00122499.
After 2233 training step(s), loss on training batch is 0.00104336.
After 2234 training step(s), loss on training batch is 0.00102146.
After 2235 training step(s), loss on training batch is 0.000762713.
After 2236 training step(s), loss on training batch is 0.00171043.
After 2237 training step(s), loss on training batch is 0.00366571.
After 2238 training step(s), loss on training batch is 0.00119053.
After 2239 training step(s), loss on training batch is 0.00142761.
After 2240 training step(s), loss on training batch is 0.00165657.
After 2241 training step(s), loss on training batch is 0.00133513.
After 2242 training step(s), loss on training batch is 0.00113992.
After 2243 training step(s), loss on training batch is 0.00132611.
After 2244 training step(s), loss on training batch is 0.00121264.
After 2245 training step(s), loss on training batch is 0.00134158.
After 2246 training step(s), loss on training batch is 0.00141332.
After 2247 training step(s), loss on training batch is 0.00123202.
After 2248 training step(s), loss on training batch is 0.00185886.
After 2249 training step(s), loss on training batch is 0.00150419.
After 2250 training step(s), loss on training batch is 0.00165868.
After 2251 training step(s), loss on training batch is 0.00135128.
After 2252 training step(s), loss on training batch is 0.00108074.
After 2253 training step(s), loss on training batch is 0.00213037.
After 2254 training step(s), loss on training batch is 0.00135503.
After 2255 training step(s), loss on training batch is 0.00176628.
After 2256 training step(s), loss on training batch is 0.00113005.
After 2257 training step(s), loss on training batch is 0.00103472.
After 2258 training step(s), loss on training batch is 0.0018612.
After 2259 training step(s), loss on training batch is 0.00210396.
After 2260 training step(s), loss on training batch is 0.00169404.
After 2261 training step(s), loss on training batch is 0.00113607.
After 2262 training step(s), loss on training batch is 0.00103487.
After 2263 training step(s), loss on training batch is 0.00106357.
After 2264 training step(s), loss on training batch is 0.000985811.
After 2265 training step(s), loss on training batch is 0.00162083.
After 2266 training step(s), loss on training batch is 0.00223163.
After 2267 training step(s), loss on training batch is 0.00279912.
After 2268 training step(s), loss on training batch is 0.00218073.
After 2269 training step(s), loss on training batch is 0.00201435.
After 2270 training step(s), loss on training batch is 0.00221617.
After 2271 training step(s), loss on training batch is 0.00184608.
After 2272 training step(s), loss on training batch is 0.0021962.
After 2273 training step(s), loss on training batch is 0.00426401.
After 2274 training step(s), loss on training batch is 0.00248307.
After 2275 training step(s), loss on training batch is 0.00282115.
After 2276 training step(s), loss on training batch is 0.002255.
After 2277 training step(s), loss on training batch is 0.00214659.
After 2278 training step(s), loss on training batch is 0.0025244.
After 2279 training step(s), loss on training batch is 0.00170178.
After 2280 training step(s), loss on training batch is 0.00202598.
After 2281 training step(s), loss on training batch is 0.00172478.
After 2282 training step(s), loss on training batch is 0.00163817.
After 2283 training step(s), loss on training batch is 0.00207055.
After 2284 training step(s), loss on training batch is 0.00174664.
After 2285 training step(s), loss on training batch is 0.0015893.
After 2286 training step(s), loss on training batch is 0.00237086.
After 2287 training step(s), loss on training batch is 0.00171466.
After 2288 training step(s), loss on training batch is 0.00237546.
After 2289 training step(s), loss on training batch is 0.00190331.
After 2290 training step(s), loss on training batch is 0.00184371.
After 2291 training step(s), loss on training batch is 0.00160524.
After 2292 training step(s), loss on training batch is 0.00267997.
After 2293 training step(s), loss on training batch is 0.00179229.
After 2294 training step(s), loss on training batch is 0.00209289.
After 2295 training step(s), loss on training batch is 0.00229504.
After 2296 training step(s), loss on training batch is 0.00214846.
After 2297 training step(s), loss on training batch is 0.00224424.
After 2298 training step(s), loss on training batch is 0.00183081.
After 2299 training step(s), loss on training batch is 0.00181949.
After 2300 training step(s), loss on training batch is 0.00178842.
After 2301 training step(s), loss on training batch is 0.00143295.
After 2302 training step(s), loss on training batch is 0.00229186.
After 2303 training step(s), loss on training batch is 0.00179253.
After 2304 training step(s), loss on training batch is 0.00349051.
After 2305 training step(s), loss on training batch is 0.00275874.
After 2306 training step(s), loss on training batch is 0.00236609.
After 2307 training step(s), loss on training batch is 0.0030216.
After 2308 training step(s), loss on training batch is 0.00220497.
After 2309 training step(s), loss on training batch is 0.00403031.
After 2310 training step(s), loss on training batch is 0.00268265.
After 2311 training step(s), loss on training batch is 0.00213737.
After 2312 training step(s), loss on training batch is 0.00206721.
After 2313 training step(s), loss on training batch is 0.00182207.
After 2314 training step(s), loss on training batch is 0.00321954.
After 2315 training step(s), loss on training batch is 0.00188093.
After 2316 training step(s), loss on training batch is 0.00100543.
After 2317 training step(s), loss on training batch is 0.000834328.
After 2318 training step(s), loss on training batch is 0.000790284.
After 2319 training step(s), loss on training batch is 0.00114526.
After 2320 training step(s), loss on training batch is 0.00149943.
After 2321 training step(s), loss on training batch is 0.00109355.
After 2322 training step(s), loss on training batch is 0.00110944.
After 2323 training step(s), loss on training batch is 0.00124525.
After 2324 training step(s), loss on training batch is 0.00141429.
After 2325 training step(s), loss on training batch is 0.00160974.
After 2326 training step(s), loss on training batch is 0.00174749.
After 2327 training step(s), loss on training batch is 0.00166382.
After 2328 training step(s), loss on training batch is 0.00250852.
After 2329 training step(s), loss on training batch is 0.00160423.
After 2330 training step(s), loss on training batch is 0.00580356.
After 2331 training step(s), loss on training batch is 0.00416017.
After 2332 training step(s), loss on training batch is 0.00253211.
After 2333 training step(s), loss on training batch is 0.00278281.
After 2334 training step(s), loss on training batch is 0.00200897.
After 2335 training step(s), loss on training batch is 0.00181229.
After 2336 training step(s), loss on training batch is 0.00184408.
After 2337 training step(s), loss on training batch is 0.00178246.
After 2338 training step(s), loss on training batch is 0.0024046.
After 2339 training step(s), loss on training batch is 0.00220253.
After 2340 training step(s), loss on training batch is 0.00196213.
After 2341 training step(s), loss on training batch is 0.00166731.
After 2342 training step(s), loss on training batch is 0.00211786.
After 2343 training step(s), loss on training batch is 0.00124683.
After 2344 training step(s), loss on training batch is 0.00144024.
After 2345 training step(s), loss on training batch is 0.00121541.
After 2346 training step(s), loss on training batch is 0.00180626.
After 2347 training step(s), loss on training batch is 0.00150023.
After 2348 training step(s), loss on training batch is 0.00118344.
After 2349 training step(s), loss on training batch is 0.000980968.
After 2350 training step(s), loss on training batch is 0.000801797.
After 2351 training step(s), loss on training batch is 0.000956602.
After 2352 training step(s), loss on training batch is 0.00111649.
After 2353 training step(s), loss on training batch is 0.0011205.
After 2354 training step(s), loss on training batch is 0.00087494.
After 2355 training step(s), loss on training batch is 0.00179934.
After 2356 training step(s), loss on training batch is 0.000859786.
After 2357 training step(s), loss on training batch is 0.000760804.
After 2358 training step(s), loss on training batch is 0.000868912.
After 2359 training step(s), loss on training batch is 0.00215524.
After 2360 training step(s), loss on training batch is 0.00282745.
After 2361 training step(s), loss on training batch is 0.00170507.
After 2362 training step(s), loss on training batch is 0.000950066.
After 2363 training step(s), loss on training batch is 0.00107265.
After 2364 training step(s), loss on training batch is 0.00184476.
After 2365 training step(s), loss on training batch is 0.00137482.
After 2366 training step(s), loss on training batch is 0.00117205.
After 2367 training step(s), loss on training batch is 0.00119389.
After 2368 training step(s), loss on training batch is 0.0013271.
After 2369 training step(s), loss on training batch is 0.000793908.
After 2370 training step(s), loss on training batch is 0.00100532.
After 2371 training step(s), loss on training batch is 0.000585444.
After 2372 training step(s), loss on training batch is 0.00113834.
After 2373 training step(s), loss on training batch is 0.000727897.
After 2374 training step(s), loss on training batch is 0.000681276.
After 2375 training step(s), loss on training batch is 0.00157789.
After 2376 training step(s), loss on training batch is 0.00140084.
After 2377 training step(s), loss on training batch is 0.00126182.
After 2378 training step(s), loss on training batch is 0.000845504.
After 2379 training step(s), loss on training batch is 0.000833302.
After 2380 training step(s), loss on training batch is 0.000706879.
After 2381 training step(s), loss on training batch is 0.000827673.
After 2382 training step(s), loss on training batch is 0.000787688.
After 2383 training step(s), loss on training batch is 0.00104027.
After 2384 training step(s), loss on training batch is 0.000843116.
After 2385 training step(s), loss on training batch is 0.000941784.
After 2386 training step(s), loss on training batch is 0.00157461.
After 2387 training step(s), loss on training batch is 0.00121569.
After 2388 training step(s), loss on training batch is 0.000809327.
After 2389 training step(s), loss on training batch is 0.000811313.
After 2390 training step(s), loss on training batch is 0.000713513.
After 2391 training step(s), loss on training batch is 0.000737456.
After 2392 training step(s), loss on training batch is 0.000804925.
After 2393 training step(s), loss on training batch is 0.000671939.
After 2394 training step(s), loss on training batch is 0.000568638.
After 2395 training step(s), loss on training batch is 0.001428.
After 2396 training step(s), loss on training batch is 0.000942946.
After 2397 training step(s), loss on training batch is 0.000601054.
After 2398 training step(s), loss on training batch is 0.000592192.
After 2399 training step(s), loss on training batch is 0.000842884.
After 2400 training step(s), loss on training batch is 0.000982251.
After 2401 training step(s), loss on training batch is 0.00128812.
After 2402 training step(s), loss on training batch is 0.0013347.
After 2403 training step(s), loss on training batch is 0.000768077.
After 2404 training step(s), loss on training batch is 0.00131981.
After 2405 training step(s), loss on training batch is 0.00322678.
After 2406 training step(s), loss on training batch is 0.00202101.
After 2407 training step(s), loss on training batch is 0.00111608.
After 2408 training step(s), loss on training batch is 0.00128394.
After 2409 training step(s), loss on training batch is 0.00143509.
After 2410 training step(s), loss on training batch is 0.0018463.
After 2411 training step(s), loss on training batch is 0.00101015.
After 2412 training step(s), loss on training batch is 0.000755286.
After 2413 training step(s), loss on training batch is 0.000890484.
After 2414 training step(s), loss on training batch is 0.000792881.
After 2415 training step(s), loss on training batch is 0.000742386.
After 2416 training step(s), loss on training batch is 0.000806669.
After 2417 training step(s), loss on training batch is 0.000914815.
After 2418 training step(s), loss on training batch is 0.000760166.
After 2419 training step(s), loss on training batch is 0.000870717.
After 2420 training step(s), loss on training batch is 0.00072591.
After 2421 training step(s), loss on training batch is 0.000671385.
After 2422 training step(s), loss on training batch is 0.000865468.
After 2423 training step(s), loss on training batch is 0.000718877.
After 2424 training step(s), loss on training batch is 0.000642064.
After 2425 training step(s), loss on training batch is 0.000671152.
After 2426 training step(s), loss on training batch is 0.000836128.
After 2427 training step(s), loss on training batch is 0.000826601.
After 2428 training step(s), loss on training batch is 0.00255464.
After 2429 training step(s), loss on training batch is 0.00153441.
After 2430 training step(s), loss on training batch is 0.00136859.
After 2431 training step(s), loss on training batch is 0.00139363.
After 2432 training step(s), loss on training batch is 0.00180778.
After 2433 training step(s), loss on training batch is 0.00188507.
After 2434 training step(s), loss on training batch is 0.00146991.
After 2435 training step(s), loss on training batch is 0.00119459.
After 2436 training step(s), loss on training batch is 0.00175139.
After 2437 training step(s), loss on training batch is 0.0017307.
After 2438 training step(s), loss on training batch is 0.00116768.
After 2439 training step(s), loss on training batch is 0.00120575.
After 2440 training step(s), loss on training batch is 0.00289912.
After 2441 training step(s), loss on training batch is 0.00135404.
After 2442 training step(s), loss on training batch is 0.00145728.
After 2443 training step(s), loss on training batch is 0.00151074.
After 2444 training step(s), loss on training batch is 0.00159643.
After 2445 training step(s), loss on training batch is 0.00107975.
After 2446 training step(s), loss on training batch is 0.00206453.
After 2447 training step(s), loss on training batch is 0.00124282.
After 2448 training step(s), loss on training batch is 0.00148066.
After 2449 training step(s), loss on training batch is 0.00143235.
After 2450 training step(s), loss on training batch is 0.00151968.
After 2451 training step(s), loss on training batch is 0.00159457.
After 2452 training step(s), loss on training batch is 0.00143745.
After 2453 training step(s), loss on training batch is 0.00168984.
After 2454 training step(s), loss on training batch is 0.00106074.
After 2455 training step(s), loss on training batch is 0.00146572.
After 2456 training step(s), loss on training batch is 0.00255535.
After 2457 training step(s), loss on training batch is 0.00200502.
After 2458 training step(s), loss on training batch is 0.00213171.
After 2459 training step(s), loss on training batch is 0.0021957.
After 2460 training step(s), loss on training batch is 0.00165065.
After 2461 training step(s), loss on training batch is 0.00160781.
After 2462 training step(s), loss on training batch is 0.00206286.
After 2463 training step(s), loss on training batch is 0.00194964.
After 2464 training step(s), loss on training batch is 0.00180651.
After 2465 training step(s), loss on training batch is 0.00239379.
After 2466 training step(s), loss on training batch is 0.00190676.
After 2467 training step(s), loss on training batch is 0.00183092.
After 2468 training step(s), loss on training batch is 0.00225543.
After 2469 training step(s), loss on training batch is 0.00307118.
After 2470 training step(s), loss on training batch is 0.00305632.
After 2471 training step(s), loss on training batch is 0.00266584.
After 2472 training step(s), loss on training batch is 0.00196819.
After 2473 training step(s), loss on training batch is 0.00256953.
After 2474 training step(s), loss on training batch is 0.00224252.
After 2475 training step(s), loss on training batch is 0.00209764.
After 2476 training step(s), loss on training batch is 0.00189461.
After 2477 training step(s), loss on training batch is 0.00365158.
After 2478 training step(s), loss on training batch is 0.00201471.
After 2479 training step(s), loss on training batch is 0.00180486.
After 2480 training step(s), loss on training batch is 0.00181248.
After 2481 training step(s), loss on training batch is 0.00146119.
After 2482 training step(s), loss on training batch is 0.00159215.
After 2483 training step(s), loss on training batch is 0.000824453.
After 2484 training step(s), loss on training batch is 0.000983135.
After 2485 training step(s), loss on training batch is 0.00122029.
After 2486 training step(s), loss on training batch is 0.00142817.
After 2487 training step(s), loss on training batch is 0.00178956.
After 2488 training step(s), loss on training batch is 0.00140486.
After 2489 training step(s), loss on training batch is 0.00101798.
After 2490 training step(s), loss on training batch is 0.000975174.
After 2491 training step(s), loss on training batch is 0.00092384.
After 2492 training step(s), loss on training batch is 0.00126762.
After 2493 training step(s), loss on training batch is 0.000848928.
After 2494 training step(s), loss on training batch is 0.000880438.
After 2495 training step(s), loss on training batch is 0.00109124.
After 2496 training step(s), loss on training batch is 0.000990441.
After 2497 training step(s), loss on training batch is 0.000774725.
After 2498 training step(s), loss on training batch is 0.000731983.
After 2499 training step(s), loss on training batch is 0.000709478.
After 2500 training step(s), loss on training batch is 0.00109838.
After 2501 training step(s), loss on training batch is 0.00267308.
After 2502 training step(s), loss on training batch is 0.00135432.
After 2503 training step(s), loss on training batch is 0.00126533.
After 2504 training step(s), loss on training batch is 0.00145996.
After 2505 training step(s), loss on training batch is 0.00114466.
After 2506 training step(s), loss on training batch is 0.00127458.
After 2507 training step(s), loss on training batch is 0.00141351.
After 2508 training step(s), loss on training batch is 0.000700059.
After 2509 training step(s), loss on training batch is 0.000725417.
After 2510 training step(s), loss on training batch is 0.000889947.
After 2511 training step(s), loss on training batch is 0.0011317.
After 2512 training step(s), loss on training batch is 0.00104727.
After 2513 training step(s), loss on training batch is 0.000940125.
After 2514 training step(s), loss on training batch is 0.00208405.
After 2515 training step(s), loss on training batch is 0.00303954.
After 2516 training step(s), loss on training batch is 0.00205987.
After 2517 training step(s), loss on training batch is 0.00166146.
After 2518 training step(s), loss on training batch is 0.00171588.
After 2519 training step(s), loss on training batch is 0.000827495.
After 2520 training step(s), loss on training batch is 0.000843438.
After 2521 training step(s), loss on training batch is 0.000814209.
After 2522 training step(s), loss on training batch is 0.000609965.
After 2523 training step(s), loss on training batch is 0.00115675.
After 2524 training step(s), loss on training batch is 0.000843255.
After 2525 training step(s), loss on training batch is 0.000818615.
After 2526 training step(s), loss on training batch is 0.000721426.
After 2527 training step(s), loss on training batch is 0.00205204.
After 2528 training step(s), loss on training batch is 0.00978143.
After 2529 training step(s), loss on training batch is 0.00287357.
After 2530 training step(s), loss on training batch is 0.0013767.
After 2531 training step(s), loss on training batch is 0.00105452.
After 2532 training step(s), loss on training batch is 0.000923904.
After 2533 training step(s), loss on training batch is 0.000884929.
After 2534 training step(s), loss on training batch is 0.000830999.
After 2535 training step(s), loss on training batch is 0.000727742.
After 2536 training step(s), loss on training batch is 0.000694992.
After 2537 training step(s), loss on training batch is 0.000765709.
After 2538 training step(s), loss on training batch is 0.00070338.
After 2539 training step(s), loss on training batch is 0.000767509.
After 2540 training step(s), loss on training batch is 0.000740693.
After 2541 training step(s), loss on training batch is 0.000547313.
After 2542 training step(s), loss on training batch is 0.000717898.
After 2543 training step(s), loss on training batch is 0.0007054.
After 2544 training step(s), loss on training batch is 0.00105032.
After 2545 training step(s), loss on training batch is 0.000621418.
After 2546 training step(s), loss on training batch is 0.000918172.
After 2547 training step(s), loss on training batch is 0.000681755.
After 2548 training step(s), loss on training batch is 0.000587144.
After 2549 training step(s), loss on training batch is 0.000676486.
After 2550 training step(s), loss on training batch is 0.00088134.
After 2551 training step(s), loss on training batch is 0.000874632.
After 2552 training step(s), loss on training batch is 0.000658897.
After 2553 training step(s), loss on training batch is 0.000635799.
After 2554 training step(s), loss on training batch is 0.000537383.
After 2555 training step(s), loss on training batch is 0.000695817.
After 2556 training step(s), loss on training batch is 0.000814399.
After 2557 training step(s), loss on training batch is 0.000966854.
After 2558 training step(s), loss on training batch is 0.000531163.
After 2559 training step(s), loss on training batch is 0.000604455.
After 2560 training step(s), loss on training batch is 0.000797419.
After 2561 training step(s), loss on training batch is 0.000574113.
After 2562 training step(s), loss on training batch is 0.000588958.
After 2563 training step(s), loss on training batch is 0.00061216.
After 2564 training step(s), loss on training batch is 0.000987616.
After 2565 training step(s), loss on training batch is 0.000673444.
After 2566 training step(s), loss on training batch is 0.000638984.
After 2567 training step(s), loss on training batch is 0.000795401.
After 2568 training step(s), loss on training batch is 0.000531867.
After 2569 training step(s), loss on training batch is 0.000690637.
After 2570 training step(s), loss on training batch is 0.00059236.
After 2571 training step(s), loss on training batch is 0.000889822.
After 2572 training step(s), loss on training batch is 0.000541836.
After 2573 training step(s), loss on training batch is 0.000615597.
After 2574 training step(s), loss on training batch is 0.000624227.
After 2575 training step(s), loss on training batch is 0.000765233.
After 2576 training step(s), loss on training batch is 0.000706067.
After 2577 training step(s), loss on training batch is 0.00107806.
After 2578 training step(s), loss on training batch is 0.000715972.
After 2579 training step(s), loss on training batch is 0.000693102.
After 2580 training step(s), loss on training batch is 0.0013034.
After 2581 training step(s), loss on training batch is 0.000660626.
After 2582 training step(s), loss on training batch is 0.000883211.
After 2583 training step(s), loss on training batch is 0.0016875.
After 2584 training step(s), loss on training batch is 0.00220682.
After 2585 training step(s), loss on training batch is 0.00286792.
After 2586 training step(s), loss on training batch is 0.00163205.
After 2587 training step(s), loss on training batch is 0.00143143.
After 2588 training step(s), loss on training batch is 0.00114048.
After 2589 training step(s), loss on training batch is 0.00131074.
After 2590 training step(s), loss on training batch is 0.00125648.
After 2591 training step(s), loss on training batch is 0.00122466.
After 2592 training step(s), loss on training batch is 0.001761.
After 2593 training step(s), loss on training batch is 0.00102957.
After 2594 training step(s), loss on training batch is 0.00107197.
After 2595 training step(s), loss on training batch is 0.00219138.
After 2596 training step(s), loss on training batch is 0.00184023.
After 2597 training step(s), loss on training batch is 0.00135673.
After 2598 training step(s), loss on training batch is 0.00179844.
After 2599 training step(s), loss on training batch is 0.00238775.
After 2600 training step(s), loss on training batch is 0.00128308.
After 2601 training step(s), loss on training batch is 0.00145672.
After 2602 training step(s), loss on training batch is 0.00140087.
After 2603 training step(s), loss on training batch is 0.00118219.
After 2604 training step(s), loss on training batch is 0.00109816.
After 2605 training step(s), loss on training batch is 0.00103358.
After 2606 training step(s), loss on training batch is 0.00152126.
After 2607 training step(s), loss on training batch is 0.00193198.
After 2608 training step(s), loss on training batch is 0.00187652.
After 2609 training step(s), loss on training batch is 0.0010595.
After 2610 training step(s), loss on training batch is 0.00231961.
After 2611 training step(s), loss on training batch is 0.00137435.
After 2612 training step(s), loss on training batch is 0.00147794.
After 2613 training step(s), loss on training batch is 0.00121737.
After 2614 training step(s), loss on training batch is 0.000959986.
After 2615 training step(s), loss on training batch is 0.00101893.
After 2616 training step(s), loss on training batch is 0.000993789.
After 2617 training step(s), loss on training batch is 0.00106683.
After 2618 training step(s), loss on training batch is 0.00178304.
After 2619 training step(s), loss on training batch is 0.00237335.
After 2620 training step(s), loss on training batch is 0.00183833.
After 2621 training step(s), loss on training batch is 0.00375871.
After 2622 training step(s), loss on training batch is 0.00165322.
After 2623 training step(s), loss on training batch is 0.00158562.
After 2624 training step(s), loss on training batch is 0.00130692.
After 2625 training step(s), loss on training batch is 0.00173696.
After 2626 training step(s), loss on training batch is 0.00120847.
After 2627 training step(s), loss on training batch is 0.00116142.
After 2628 training step(s), loss on training batch is 0.00124286.
After 2629 training step(s), loss on training batch is 0.00114082.
After 2630 training step(s), loss on training batch is 0.00157838.
After 2631 training step(s), loss on training batch is 0.00192676.
After 2632 training step(s), loss on training batch is 0.0010658.
After 2633 training step(s), loss on training batch is 0.000987207.
After 2634 training step(s), loss on training batch is 0.000927832.
After 2635 training step(s), loss on training batch is 0.00071788.
After 2636 training step(s), loss on training batch is 0.00150126.
After 2637 training step(s), loss on training batch is 0.00319544.
After 2638 training step(s), loss on training batch is 0.0010442.
After 2639 training step(s), loss on training batch is 0.00124802.
After 2640 training step(s), loss on training batch is 0.00145655.
After 2641 training step(s), loss on training batch is 0.00122835.
After 2642 training step(s), loss on training batch is 0.00107746.
After 2643 training step(s), loss on training batch is 0.00126917.
After 2644 training step(s), loss on training batch is 0.00112603.
After 2645 training step(s), loss on training batch is 0.0012348.
After 2646 training step(s), loss on training batch is 0.00129075.
After 2647 training step(s), loss on training batch is 0.00109603.
After 2648 training step(s), loss on training batch is 0.00158344.
After 2649 training step(s), loss on training batch is 0.00133703.
After 2650 training step(s), loss on training batch is 0.00148302.
After 2651 training step(s), loss on training batch is 0.00122544.
After 2652 training step(s), loss on training batch is 0.000958528.
After 2653 training step(s), loss on training batch is 0.00192355.
After 2654 training step(s), loss on training batch is 0.00121965.
After 2655 training step(s), loss on training batch is 0.00145136.
After 2656 training step(s), loss on training batch is 0.00101697.
After 2657 training step(s), loss on training batch is 0.000907061.
After 2658 training step(s), loss on training batch is 0.00168359.
After 2659 training step(s), loss on training batch is 0.00201393.
After 2660 training step(s), loss on training batch is 0.00146477.
After 2661 training step(s), loss on training batch is 0.00103761.
After 2662 training step(s), loss on training batch is 0.000986563.
After 2663 training step(s), loss on training batch is 0.0010453.
After 2664 training step(s), loss on training batch is 0.0009037.
After 2665 training step(s), loss on training batch is 0.00138257.
After 2666 training step(s), loss on training batch is 0.00206836.
After 2667 training step(s), loss on training batch is 0.00248698.
After 2668 training step(s), loss on training batch is 0.00196938.
After 2669 training step(s), loss on training batch is 0.00187202.
After 2670 training step(s), loss on training batch is 0.00183633.
After 2671 training step(s), loss on training batch is 0.00168541.
After 2672 training step(s), loss on training batch is 0.00192484.
After 2673 training step(s), loss on training batch is 0.004226.
After 2674 training step(s), loss on training batch is 0.00227287.
After 2675 training step(s), loss on training batch is 0.00255275.
After 2676 training step(s), loss on training batch is 0.00215591.
After 2677 training step(s), loss on training batch is 0.00186766.
After 2678 training step(s), loss on training batch is 0.00224269.
After 2679 training step(s), loss on training batch is 0.00156142.
After 2680 training step(s), loss on training batch is 0.00164542.
After 2681 training step(s), loss on training batch is 0.00161647.
After 2682 training step(s), loss on training batch is 0.001536.
After 2683 training step(s), loss on training batch is 0.00183944.
After 2684 training step(s), loss on training batch is 0.0015859.
After 2685 training step(s), loss on training batch is 0.00151728.
After 2686 training step(s), loss on training batch is 0.00221573.
After 2687 training step(s), loss on training batch is 0.00156507.
After 2688 training step(s), loss on training batch is 0.00220247.
After 2689 training step(s), loss on training batch is 0.0017306.
After 2690 training step(s), loss on training batch is 0.00176553.
After 2691 training step(s), loss on training batch is 0.00152494.
After 2692 training step(s), loss on training batch is 0.00206512.
After 2693 training step(s), loss on training batch is 0.00159687.
After 2694 training step(s), loss on training batch is 0.00183546.
After 2695 training step(s), loss on training batch is 0.00209953.
After 2696 training step(s), loss on training batch is 0.00192998.
After 2697 training step(s), loss on training batch is 0.00195077.
After 2698 training step(s), loss on training batch is 0.00166348.
After 2699 training step(s), loss on training batch is 0.00155601.
After 2700 training step(s), loss on training batch is 0.00155587.
After 2701 training step(s), loss on training batch is 0.00127018.
After 2702 training step(s), loss on training batch is 0.0020628.
After 2703 training step(s), loss on training batch is 0.00181632.
After 2704 training step(s), loss on training batch is 0.00250821.
After 2705 training step(s), loss on training batch is 0.00218786.
After 2706 training step(s), loss on training batch is 0.00207181.
After 2707 training step(s), loss on training batch is 0.00263184.
After 2708 training step(s), loss on training batch is 0.00192146.
After 2709 training step(s), loss on training batch is 0.00459268.
After 2710 training step(s), loss on training batch is 0.00261191.
After 2711 training step(s), loss on training batch is 0.00207574.
After 2712 training step(s), loss on training batch is 0.00200448.
After 2713 training step(s), loss on training batch is 0.00171018.
After 2714 training step(s), loss on training batch is 0.00283574.
After 2715 training step(s), loss on training batch is 0.00178022.
After 2716 training step(s), loss on training batch is 0.000840716.
After 2717 training step(s), loss on training batch is 0.00069402.
After 2718 training step(s), loss on training batch is 0.00068342.
After 2719 training step(s), loss on training batch is 0.0009754.
After 2720 training step(s), loss on training batch is 0.00128415.
After 2721 training step(s), loss on training batch is 0.000923837.
After 2722 training step(s), loss on training batch is 0.000923306.
After 2723 training step(s), loss on training batch is 0.00103688.
After 2724 training step(s), loss on training batch is 0.00117121.
After 2725 training step(s), loss on training batch is 0.00125493.
After 2726 training step(s), loss on training batch is 0.00151646.
After 2727 training step(s), loss on training batch is 0.00148996.
After 2728 training step(s), loss on training batch is 0.00215023.
After 2729 training step(s), loss on training batch is 0.00136396.
After 2730 training step(s), loss on training batch is 0.00520252.
After 2731 training step(s), loss on training batch is 0.00370774.
After 2732 training step(s), loss on training batch is 0.00214133.
After 2733 training step(s), loss on training batch is 0.0024456.
After 2734 training step(s), loss on training batch is 0.00172137.
After 2735 training step(s), loss on training batch is 0.00159218.
After 2736 training step(s), loss on training batch is 0.00168415.
After 2737 training step(s), loss on training batch is 0.00157554.
After 2738 training step(s), loss on training batch is 0.00209973.
After 2739 training step(s), loss on training batch is 0.00196908.
After 2740 training step(s), loss on training batch is 0.00178952.
After 2741 training step(s), loss on training batch is 0.00156769.
After 2742 training step(s), loss on training batch is 0.00202261.
After 2743 training step(s), loss on training batch is 0.00104536.
After 2744 training step(s), loss on training batch is 0.00120408.
After 2745 training step(s), loss on training batch is 0.000972452.
After 2746 training step(s), loss on training batch is 0.00143456.
After 2747 training step(s), loss on training batch is 0.00127943.
After 2748 training step(s), loss on training batch is 0.00100574.
After 2749 training step(s), loss on training batch is 0.000841625.
After 2750 training step(s), loss on training batch is 0.000675848.
After 2751 training step(s), loss on training batch is 0.000836418.
After 2752 training step(s), loss on training batch is 0.000971847.
After 2753 training step(s), loss on training batch is 0.000880442.
After 2754 training step(s), loss on training batch is 0.000723829.
After 2755 training step(s), loss on training batch is 0.001555.
After 2756 training step(s), loss on training batch is 0.000724606.
After 2757 training step(s), loss on training batch is 0.000650035.
After 2758 training step(s), loss on training batch is 0.000781009.
After 2759 training step(s), loss on training batch is 0.00193707.
After 2760 training step(s), loss on training batch is 0.00254606.
After 2761 training step(s), loss on training batch is 0.00143209.
After 2762 training step(s), loss on training batch is 0.000832681.
After 2763 training step(s), loss on training batch is 0.000959652.
After 2764 training step(s), loss on training batch is 0.00147949.
After 2765 training step(s), loss on training batch is 0.00116322.
After 2766 training step(s), loss on training batch is 0.00101831.
After 2767 training step(s), loss on training batch is 0.000945358.
After 2768 training step(s), loss on training batch is 0.00110002.
After 2769 training step(s), loss on training batch is 0.000682764.
After 2770 training step(s), loss on training batch is 0.000874572.
After 2771 training step(s), loss on training batch is 0.000514198.
After 2772 training step(s), loss on training batch is 0.000936212.
After 2773 training step(s), loss on training batch is 0.000623749.
After 2774 training step(s), loss on training batch is 0.00058102.
After 2775 training step(s), loss on training batch is 0.00132625.
After 2776 training step(s), loss on training batch is 0.00116517.
After 2777 training step(s), loss on training batch is 0.0010616.
After 2778 training step(s), loss on training batch is 0.000701751.
After 2779 training step(s), loss on training batch is 0.000721358.
After 2780 training step(s), loss on training batch is 0.000608541.
After 2781 training step(s), loss on training batch is 0.000724719.
After 2782 training step(s), loss on training batch is 0.000661309.
After 2783 training step(s), loss on training batch is 0.00086579.
After 2784 training step(s), loss on training batch is 0.000759959.
After 2785 training step(s), loss on training batch is 0.000791494.
After 2786 training step(s), loss on training batch is 0.00132071.
After 2787 training step(s), loss on training batch is 0.0010122.
After 2788 training step(s), loss on training batch is 0.00066418.
After 2789 training step(s), loss on training batch is 0.000686304.
After 2790 training step(s), loss on training batch is 0.00061442.
After 2791 training step(s), loss on training batch is 0.000625304.
After 2792 training step(s), loss on training batch is 0.00069113.
After 2793 training step(s), loss on training batch is 0.000582199.
After 2794 training step(s), loss on training batch is 0.000520642.
After 2795 training step(s), loss on training batch is 0.00112192.
After 2796 training step(s), loss on training batch is 0.000826017.
After 2797 training step(s), loss on training batch is 0.000505706.
After 2798 training step(s), loss on training batch is 0.000499826.
After 2799 training step(s), loss on training batch is 0.000714879.
After 2800 training step(s), loss on training batch is 0.000857688.
After 2801 training step(s), loss on training batch is 0.00110777.
After 2802 training step(s), loss on training batch is 0.00118316.
After 2803 training step(s), loss on training batch is 0.000669277.
After 2804 training step(s), loss on training batch is 0.00112684.
After 2805 training step(s), loss on training batch is 0.00290407.
After 2806 training step(s), loss on training batch is 0.00172175.
After 2807 training step(s), loss on training batch is 0.000937148.
After 2808 training step(s), loss on training batch is 0.00110934.
After 2809 training step(s), loss on training batch is 0.00125494.
After 2810 training step(s), loss on training batch is 0.00150153.
After 2811 training step(s), loss on training batch is 0.000868241.
After 2812 training step(s), loss on training batch is 0.000610867.
After 2813 training step(s), loss on training batch is 0.000702456.
After 2814 training step(s), loss on training batch is 0.000660458.
After 2815 training step(s), loss on training batch is 0.000615639.
After 2816 training step(s), loss on training batch is 0.00064622.
After 2817 training step(s), loss on training batch is 0.000743538.
After 2818 training step(s), loss on training batch is 0.000644718.
After 2819 training step(s), loss on training batch is 0.000758936.
After 2820 training step(s), loss on training batch is 0.000653384.
After 2821 training step(s), loss on training batch is 0.000563184.
After 2822 training step(s), loss on training batch is 0.000711012.
After 2823 training step(s), loss on training batch is 0.000596932.
After 2824 training step(s), loss on training batch is 0.000568747.
After 2825 training step(s), loss on training batch is 0.000575984.
After 2826 training step(s), loss on training batch is 0.000738068.
After 2827 training step(s), loss on training batch is 0.000643073.
After 2828 training step(s), loss on training batch is 0.00230264.
After 2829 training step(s), loss on training batch is 0.00131268.
After 2830 training step(s), loss on training batch is 0.00116041.
After 2831 training step(s), loss on training batch is 0.00120601.
After 2832 training step(s), loss on training batch is 0.00150265.
After 2833 training step(s), loss on training batch is 0.00158393.
After 2834 training step(s), loss on training batch is 0.0012548.
After 2835 training step(s), loss on training batch is 0.00102256.
After 2836 training step(s), loss on training batch is 0.00152813.
After 2837 training step(s), loss on training batch is 0.0015113.
After 2838 training step(s), loss on training batch is 0.00101768.
After 2839 training step(s), loss on training batch is 0.00104595.
After 2840 training step(s), loss on training batch is 0.00254405.
After 2841 training step(s), loss on training batch is 0.00116027.
After 2842 training step(s), loss on training batch is 0.00129302.
After 2843 training step(s), loss on training batch is 0.00136329.
After 2844 training step(s), loss on training batch is 0.00142032.
After 2845 training step(s), loss on training batch is 0.000979268.
After 2846 training step(s), loss on training batch is 0.00172586.
After 2847 training step(s), loss on training batch is 0.00110819.
After 2848 training step(s), loss on training batch is 0.0013237.
After 2849 training step(s), loss on training batch is 0.00127061.
After 2850 training step(s), loss on training batch is 0.00134204.
After 2851 training step(s), loss on training batch is 0.00139364.
After 2852 training step(s), loss on training batch is 0.00121463.
After 2853 training step(s), loss on training batch is 0.00151951.
After 2854 training step(s), loss on training batch is 0.000926249.
After 2855 training step(s), loss on training batch is 0.00127858.
After 2856 training step(s), loss on training batch is 0.00223209.
After 2857 training step(s), loss on training batch is 0.00175093.
After 2858 training step(s), loss on training batch is 0.00185474.
After 2859 training step(s), loss on training batch is 0.00192525.
After 2860 training step(s), loss on training batch is 0.00139174.
After 2861 training step(s), loss on training batch is 0.00140274.
After 2862 training step(s), loss on training batch is 0.00188133.
After 2863 training step(s), loss on training batch is 0.00172454.
After 2864 training step(s), loss on training batch is 0.00152567.
After 2865 training step(s), loss on training batch is 0.00224482.
After 2866 training step(s), loss on training batch is 0.00162714.
After 2867 training step(s), loss on training batch is 0.0015418.
After 2868 training step(s), loss on training batch is 0.00200647.
After 2869 training step(s), loss on training batch is 0.00276593.
After 2870 training step(s), loss on training batch is 0.00281215.
After 2871 training step(s), loss on training batch is 0.00234979.
After 2872 training step(s), loss on training batch is 0.00186519.
After 2873 training step(s), loss on training batch is 0.00217691.
After 2874 training step(s), loss on training batch is 0.00190252.
After 2875 training step(s), loss on training batch is 0.00185354.
After 2876 training step(s), loss on training batch is 0.00167858.
After 2877 training step(s), loss on training batch is 0.00326588.
After 2878 training step(s), loss on training batch is 0.00179046.
After 2879 training step(s), loss on training batch is 0.00156488.
After 2880 training step(s), loss on training batch is 0.00158586.
After 2881 training step(s), loss on training batch is 0.00121968.
After 2882 training step(s), loss on training batch is 0.0013634.
After 2883 training step(s), loss on training batch is 0.00068551.
After 2884 training step(s), loss on training batch is 0.000843433.
After 2885 training step(s), loss on training batch is 0.00103711.
After 2886 training step(s), loss on training batch is 0.00123693.
After 2887 training step(s), loss on training batch is 0.00158363.
After 2888 training step(s), loss on training batch is 0.00117492.
After 2889 training step(s), loss on training batch is 0.000831076.
After 2890 training step(s), loss on training batch is 0.000823336.
After 2891 training step(s), loss on training batch is 0.000763937.
After 2892 training step(s), loss on training batch is 0.0010848.
After 2893 training step(s), loss on training batch is 0.000683958.
After 2894 training step(s), loss on training batch is 0.000723968.
After 2895 training step(s), loss on training batch is 0.000923382.
After 2896 training step(s), loss on training batch is 0.000837544.
After 2897 training step(s), loss on training batch is 0.000649112.
After 2898 training step(s), loss on training batch is 0.000621556.
After 2899 training step(s), loss on training batch is 0.000606369.
After 2900 training step(s), loss on training batch is 0.00090207.
After 2901 training step(s), loss on training batch is 0.00242151.
After 2902 training step(s), loss on training batch is 0.00122009.
After 2903 training step(s), loss on training batch is 0.00109435.
After 2904 training step(s), loss on training batch is 0.00114937.
After 2905 training step(s), loss on training batch is 0.00097248.
After 2906 training step(s), loss on training batch is 0.00123431.
After 2907 training step(s), loss on training batch is 0.00140053.
After 2908 training step(s), loss on training batch is 0.000625634.
After 2909 training step(s), loss on training batch is 0.00064673.
After 2910 training step(s), loss on training batch is 0.00077643.
After 2911 training step(s), loss on training batch is 0.000937778.
After 2912 training step(s), loss on training batch is 0.000939402.
After 2913 training step(s), loss on training batch is 0.000847007.
After 2914 training step(s), loss on training batch is 0.00169615.
After 2915 training step(s), loss on training batch is 0.00274444.
After 2916 training step(s), loss on training batch is 0.00185077.
After 2917 training step(s), loss on training batch is 0.00163646.
After 2918 training step(s), loss on training batch is 0.00148087.
After 2919 training step(s), loss on training batch is 0.000725365.
After 2920 training step(s), loss on training batch is 0.000800123.
After 2921 training step(s), loss on training batch is 0.000776299.
After 2922 training step(s), loss on training batch is 0.000585582.
After 2923 training step(s), loss on training batch is 0.0010265.
After 2924 training step(s), loss on training batch is 0.000760398.
After 2925 training step(s), loss on training batch is 0.000728818.
After 2926 training step(s), loss on training batch is 0.000615575.
After 2927 training step(s), loss on training batch is 0.00198831.
After 2928 training step(s), loss on training batch is 0.0083373.
After 2929 training step(s), loss on training batch is 0.0103225.
After 2930 training step(s), loss on training batch is 0.00144841.
After 2931 training step(s), loss on training batch is 0.0010093.
After 2932 training step(s), loss on training batch is 0.000887298.
After 2933 training step(s), loss on training batch is 0.000869348.
After 2934 training step(s), loss on training batch is 0.000854499.
After 2935 training step(s), loss on training batch is 0.000732334.
After 2936 training step(s), loss on training batch is 0.000706978.
After 2937 training step(s), loss on training batch is 0.000833828.
After 2938 training step(s), loss on training batch is 0.000784248.
After 2939 training step(s), loss on training batch is 0.00082971.
After 2940 training step(s), loss on training batch is 0.000871924.
After 2941 training step(s), loss on training batch is 0.00061642.
After 2942 training step(s), loss on training batch is 0.00077135.
After 2943 training step(s), loss on training batch is 0.00071906.
After 2944 training step(s), loss on training batch is 0.00103648.
After 2945 training step(s), loss on training batch is 0.000589304.
After 2946 training step(s), loss on training batch is 0.000809493.
After 2947 training step(s), loss on training batch is 0.000670896.
After 2948 training step(s), loss on training batch is 0.000563529.
After 2949 training step(s), loss on training batch is 0.000655973.
After 2950 training step(s), loss on training batch is 0.000905427.
After 2951 training step(s), loss on training batch is 0.00083729.
After 2952 training step(s), loss on training batch is 0.000670093.
After 2953 training step(s), loss on training batch is 0.000634346.
After 2954 training step(s), loss on training batch is 0.000523896.
After 2955 training step(s), loss on training batch is 0.000609132.
After 2956 training step(s), loss on training batch is 0.000759737.
After 2957 training step(s), loss on training batch is 0.000913915.
After 2958 training step(s), loss on training batch is 0.000514286.
After 2959 training step(s), loss on training batch is 0.000593069.
After 2960 training step(s), loss on training batch is 0.000767978.
After 2961 training step(s), loss on training batch is 0.000549954.
After 2962 training step(s), loss on training batch is 0.000501002.
After 2963 training step(s), loss on training batch is 0.000571583.
After 2964 training step(s), loss on training batch is 0.000836065.
After 2965 training step(s), loss on training batch is 0.000672987.
After 2966 training step(s), loss on training batch is 0.000623212.
After 2967 training step(s), loss on training batch is 0.000701141.
After 2968 training step(s), loss on training batch is 0.000505386.
After 2969 training step(s), loss on training batch is 0.000681847.
After 2970 training step(s), loss on training batch is 0.000566295.
After 2971 training step(s), loss on training batch is 0.000838327.
After 2972 training step(s), loss on training batch is 0.000511383.
After 2973 training step(s), loss on training batch is 0.000563321.
After 2974 training step(s), loss on training batch is 0.000601527.
After 2975 training step(s), loss on training batch is 0.000711145.
After 2976 training step(s), loss on training batch is 0.000669718.
After 2977 training step(s), loss on training batch is 0.000966105.
After 2978 training step(s), loss on training batch is 0.000687669.
After 2979 training step(s), loss on training batch is 0.00058805.
After 2980 training step(s), loss on training batch is 0.001112.
After 2981 training step(s), loss on training batch is 0.000597394.
After 2982 training step(s), loss on training batch is 0.000733175.
After 2983 training step(s), loss on training batch is 0.00154209.
After 2984 training step(s), loss on training batch is 0.00199916.
After 2985 training step(s), loss on training batch is 0.00263799.
After 2986 training step(s), loss on training batch is 0.00146539.
After 2987 training step(s), loss on training batch is 0.00133527.
After 2988 training step(s), loss on training batch is 0.00105988.
After 2989 training step(s), loss on training batch is 0.00121646.
After 2990 training step(s), loss on training batch is 0.00116549.
After 2991 training step(s), loss on training batch is 0.00116541.
After 2992 training step(s), loss on training batch is 0.00160214.
After 2993 training step(s), loss on training batch is 0.000941935.
After 2994 training step(s), loss on training batch is 0.00100379.
After 2995 training step(s), loss on training batch is 0.00195398.
After 2996 training step(s), loss on training batch is 0.00164766.
After 2997 training step(s), loss on training batch is 0.00121287.
After 2998 training step(s), loss on training batch is 0.00149989.
After 2999 training step(s), loss on training batch is 0.00215026.
After 3000 training step(s), loss on training batch is 0.00114082.
After 3001 training step(s), loss on training batch is 0.00131482.
After 3002 training step(s), loss on training batch is 0.00128464.
After 3003 training step(s), loss on training batch is 0.00103733.
After 3004 training step(s), loss on training batch is 0.00100859.
After 3005 training step(s), loss on training batch is 0.000936881.
After 3006 training step(s), loss on training batch is 0.00132119.
After 3007 training step(s), loss on training batch is 0.0017967.
After 3008 training step(s), loss on training batch is 0.00173231.
After 3009 training step(s), loss on training batch is 0.000970131.
After 3010 training step(s), loss on training batch is 0.00219714.
After 3011 training step(s), loss on training batch is 0.00119035.
After 3012 training step(s), loss on training batch is 0.00137322.
After 3013 training step(s), loss on training batch is 0.00110554.
After 3014 training step(s), loss on training batch is 0.000894206.
After 3015 training step(s), loss on training batch is 0.000930319.
After 3016 training step(s), loss on training batch is 0.000881709.
After 3017 training step(s), loss on training batch is 0.000974379.
After 3018 training step(s), loss on training batch is 0.00154716.
After 3019 training step(s), loss on training batch is 0.0022417.
After 3020 training step(s), loss on training batch is 0.0015652.
After 3021 training step(s), loss on training batch is 0.00347423.
After 3022 training step(s), loss on training batch is 0.00144934.
After 3023 training step(s), loss on training batch is 0.00139712.
After 3024 training step(s), loss on training batch is 0.00119319.
After 3025 training step(s), loss on training batch is 0.00160007.
After 3026 training step(s), loss on training batch is 0.00118662.
After 3027 training step(s), loss on training batch is 0.0011285.
After 3028 training step(s), loss on training batch is 0.00114698.
After 3029 training step(s), loss on training batch is 0.00109018.
After 3030 training step(s), loss on training batch is 0.00141269.
After 3031 training step(s), loss on training batch is 0.00165206.
After 3032 training step(s), loss on training batch is 0.000959345.
After 3033 training step(s), loss on training batch is 0.000925094.
After 3034 training step(s), loss on training batch is 0.000889869.
After 3035 training step(s), loss on training batch is 0.000680602.
After 3036 training step(s), loss on training batch is 0.00137481.
After 3037 training step(s), loss on training batch is 0.00299068.
After 3038 training step(s), loss on training batch is 0.00101256.
After 3039 training step(s), loss on training batch is 0.00117506.
After 3040 training step(s), loss on training batch is 0.00123419.
After 3041 training step(s), loss on training batch is 0.00102775.
After 3042 training step(s), loss on training batch is 0.000939689.
After 3043 training step(s), loss on training batch is 0.00111572.
After 3044 training step(s), loss on training batch is 0.00102126.
After 3045 training step(s), loss on training batch is 0.00112625.
After 3046 training step(s), loss on training batch is 0.00121065.
After 3047 training step(s), loss on training batch is 0.000997085.
After 3048 training step(s), loss on training batch is 0.00150348.
After 3049 training step(s), loss on training batch is 0.00123388.
After 3050 training step(s), loss on training batch is 0.00137558.
After 3051 training step(s), loss on training batch is 0.00112287.
After 3052 training step(s), loss on training batch is 0.000855598.
After 3053 training step(s), loss on training batch is 0.0018111.
After 3054 training step(s), loss on training batch is 0.00113671.
After 3055 training step(s), loss on training batch is 0.0012497.
After 3056 training step(s), loss on training batch is 0.000930681.
After 3057 training step(s), loss on training batch is 0.000870138.
After 3058 training step(s), loss on training batch is 0.0015659.
After 3059 training step(s), loss on training batch is 0.00178405.
After 3060 training step(s), loss on training batch is 0.00129342.
After 3061 training step(s), loss on training batch is 0.000968896.
After 3062 training step(s), loss on training batch is 0.000931428.
After 3063 training step(s), loss on training batch is 0.000967279.
After 3064 training step(s), loss on training batch is 0.000787024.
After 3065 training step(s), loss on training batch is 0.0013147.
After 3066 training step(s), loss on training batch is 0.00189155.
After 3067 training step(s), loss on training batch is 0.00229559.
After 3068 training step(s), loss on training batch is 0.00180294.
After 3069 training step(s), loss on training batch is 0.00170452.
After 3070 training step(s), loss on training batch is 0.00170143.
After 3071 training step(s), loss on training batch is 0.00158115.
After 3072 training step(s), loss on training batch is 0.00174835.
After 3073 training step(s), loss on training batch is 0.00389023.
After 3074 training step(s), loss on training batch is 0.00195723.
After 3075 training step(s), loss on training batch is 0.00234835.
After 3076 training step(s), loss on training batch is 0.00192015.
After 3077 training step(s), loss on training batch is 0.00171404.
After 3078 training step(s), loss on training batch is 0.00202542.
After 3079 training step(s), loss on training batch is 0.00145815.
After 3080 training step(s), loss on training batch is 0.00148047.
After 3081 training step(s), loss on training batch is 0.00147267.
After 3082 training step(s), loss on training batch is 0.00142027.
After 3083 training step(s), loss on training batch is 0.00171113.
After 3084 training step(s), loss on training batch is 0.00146159.
After 3085 training step(s), loss on training batch is 0.00141841.
After 3086 training step(s), loss on training batch is 0.00202387.
After 3087 training step(s), loss on training batch is 0.00145717.
After 3088 training step(s), loss on training batch is 0.00204308.
After 3089 training step(s), loss on training batch is 0.00159983.
After 3090 training step(s), loss on training batch is 0.00158688.
After 3091 training step(s), loss on training batch is 0.00138679.
After 3092 training step(s), loss on training batch is 0.00190122.
After 3093 training step(s), loss on training batch is 0.00147985.
After 3094 training step(s), loss on training batch is 0.0016694.
After 3095 training step(s), loss on training batch is 0.00194543.
After 3096 training step(s), loss on training batch is 0.00175312.
After 3097 training step(s), loss on training batch is 0.00173048.
After 3098 training step(s), loss on training batch is 0.00150173.
After 3099 training step(s), loss on training batch is 0.00143558.
After 3100 training step(s), loss on training batch is 0.00143159.
After 3101 training step(s), loss on training batch is 0.00122156.
After 3102 training step(s), loss on training batch is 0.00163155.
After 3103 training step(s), loss on training batch is 0.00133377.
After 3104 training step(s), loss on training batch is 0.00259943.
After 3105 training step(s), loss on training batch is 0.0018508.
After 3106 training step(s), loss on training batch is 0.00186653.
After 3107 training step(s), loss on training batch is 0.00238566.
After 3108 training step(s), loss on training batch is 0.00173343.
After 3109 training step(s), loss on training batch is 0.0047054.
After 3110 training step(s), loss on training batch is 0.00235554.
After 3111 training step(s), loss on training batch is 0.00204116.
After 3112 training step(s), loss on training batch is 0.00201714.
After 3113 training step(s), loss on training batch is 0.00166944.
After 3114 training step(s), loss on training batch is 0.00247595.
After 3115 training step(s), loss on training batch is 0.00162453.
After 3116 training step(s), loss on training batch is 0.000730184.
After 3117 training step(s), loss on training batch is 0.000614592.
After 3118 training step(s), loss on training batch is 0.000621736.
After 3119 training step(s), loss on training batch is 0.000838513.
After 3120 training step(s), loss on training batch is 0.00110016.
After 3121 training step(s), loss on training batch is 0.000811555.
After 3122 training step(s), loss on training batch is 0.000815956.
After 3123 training step(s), loss on training batch is 0.000906206.
After 3124 training step(s), loss on training batch is 0.0010072.
After 3125 training step(s), loss on training batch is 0.00105766.
After 3126 training step(s), loss on training batch is 0.00139754.
After 3127 training step(s), loss on training batch is 0.00138483.
After 3128 training step(s), loss on training batch is 0.00186743.
After 3129 training step(s), loss on training batch is 0.00134483.
After 3130 training step(s), loss on training batch is 0.00416014.
After 3131 training step(s), loss on training batch is 0.00331822.
After 3132 training step(s), loss on training batch is 0.00201.
After 3133 training step(s), loss on training batch is 0.00219271.
After 3134 training step(s), loss on training batch is 0.00162192.
After 3135 training step(s), loss on training batch is 0.00151345.
After 3136 training step(s), loss on training batch is 0.00161933.
After 3137 training step(s), loss on training batch is 0.0014086.
After 3138 training step(s), loss on training batch is 0.00175816.
After 3139 training step(s), loss on training batch is 0.00178232.
After 3140 training step(s), loss on training batch is 0.0016235.
After 3141 training step(s), loss on training batch is 0.0014515.
After 3142 training step(s), loss on training batch is 0.00203532.
After 3143 training step(s), loss on training batch is 0.000883628.
After 3144 training step(s), loss on training batch is 0.00109093.
After 3145 training step(s), loss on training batch is 0.000833827.
After 3146 training step(s), loss on training batch is 0.00121573.
After 3147 training step(s), loss on training batch is 0.0010741.
After 3148 training step(s), loss on training batch is 0.000895431.
After 3149 training step(s), loss on training batch is 0.000734919.
After 3150 training step(s), loss on training batch is 0.000596394.
After 3151 training step(s), loss on training batch is 0.000777558.
After 3152 training step(s), loss on training batch is 0.000834271.
After 3153 training step(s), loss on training batch is 0.000788917.
After 3154 training step(s), loss on training batch is 0.000627025.
After 3155 training step(s), loss on training batch is 0.00130174.
After 3156 training step(s), loss on training batch is 0.000658479.
After 3157 training step(s), loss on training batch is 0.00059395.
After 3158 training step(s), loss on training batch is 0.000633444.
After 3159 training step(s), loss on training batch is 0.00163269.
After 3160 training step(s), loss on training batch is 0.00220113.
After 3161 training step(s), loss on training batch is 0.00127231.
After 3162 training step(s), loss on training batch is 0.000768221.
After 3163 training step(s), loss on training batch is 0.000833097.
After 3164 training step(s), loss on training batch is 0.00125387.
After 3165 training step(s), loss on training batch is 0.0010093.
After 3166 training step(s), loss on training batch is 0.000900455.
After 3167 training step(s), loss on training batch is 0.000823003.
After 3168 training step(s), loss on training batch is 0.000931651.
After 3169 training step(s), loss on training batch is 0.00060263.
After 3170 training step(s), loss on training batch is 0.000668763.
After 3171 training step(s), loss on training batch is 0.000402369.
After 3172 training step(s), loss on training batch is 0.000851385.
After 3173 training step(s), loss on training batch is 0.000543061.
After 3174 training step(s), loss on training batch is 0.000517525.
After 3175 training step(s), loss on training batch is 0.00124854.
After 3176 training step(s), loss on training batch is 0.00102432.
After 3177 training step(s), loss on training batch is 0.000929956.
After 3178 training step(s), loss on training batch is 0.000625438.
After 3179 training step(s), loss on training batch is 0.000643373.
After 3180 training step(s), loss on training batch is 0.000537599.
After 3181 training step(s), loss on training batch is 0.00064275.
After 3182 training step(s), loss on training batch is 0.000590105.
After 3183 training step(s), loss on training batch is 0.000766253.
After 3184 training step(s), loss on training batch is 0.000681077.
After 3185 training step(s), loss on training batch is 0.000716824.
After 3186 training step(s), loss on training batch is 0.00109334.
After 3187 training step(s), loss on training batch is 0.000898117.
After 3188 training step(s), loss on training batch is 0.000582808.
After 3189 training step(s), loss on training batch is 0.000616314.
After 3190 training step(s), loss on training batch is 0.000558087.
After 3191 training step(s), loss on training batch is 0.000555061.
After 3192 training step(s), loss on training batch is 0.000628963.
After 3193 training step(s), loss on training batch is 0.000548121.
After 3194 training step(s), loss on training batch is 0.000489356.
After 3195 training step(s), loss on training batch is 0.0009343.
After 3196 training step(s), loss on training batch is 0.000753059.
After 3197 training step(s), loss on training batch is 0.000455796.
After 3198 training step(s), loss on training batch is 0.000441119.
After 3199 training step(s), loss on training batch is 0.000613737.
After 3200 training step(s), loss on training batch is 0.000761641.
After 3201 training step(s), loss on training batch is 0.00101084.
After 3202 training step(s), loss on training batch is 0.00106566.
After 3203 training step(s), loss on training batch is 0.000624925.
After 3204 training step(s), loss on training batch is 0.000979653.
After 3205 training step(s), loss on training batch is 0.00248789.
After 3206 training step(s), loss on training batch is 0.00150669.
After 3207 training step(s), loss on training batch is 0.000803071.
After 3208 training step(s), loss on training batch is 0.000968008.
After 3209 training step(s), loss on training batch is 0.00112265.
After 3210 training step(s), loss on training batch is 0.0013054.
After 3211 training step(s), loss on training batch is 0.000771349.
After 3212 training step(s), loss on training batch is 0.000576659.
After 3213 training step(s), loss on training batch is 0.000661211.
After 3214 training step(s), loss on training batch is 0.00062901.
After 3215 training step(s), loss on training batch is 0.00057006.
After 3216 training step(s), loss on training batch is 0.000557339.
After 3217 training step(s), loss on training batch is 0.000666739.
After 3218 training step(s), loss on training batch is 0.000562129.
After 3219 training step(s), loss on training batch is 0.000681848.
After 3220 training step(s), loss on training batch is 0.000588857.
After 3221 training step(s), loss on training batch is 0.000506957.
After 3222 training step(s), loss on training batch is 0.000615727.
After 3223 training step(s), loss on training batch is 0.000523966.
After 3224 training step(s), loss on training batch is 0.000514682.
After 3225 training step(s), loss on training batch is 0.000499003.
After 3226 training step(s), loss on training batch is 0.000674457.
After 3227 training step(s), loss on training batch is 0.000531899.
After 3228 training step(s), loss on training batch is 0.00207243.
After 3229 training step(s), loss on training batch is 0.00116286.
After 3230 training step(s), loss on training batch is 0.00103235.
After 3231 training step(s), loss on training batch is 0.00106691.
After 3232 training step(s), loss on training batch is 0.00132788.
After 3233 training step(s), loss on training batch is 0.00139741.
After 3234 training step(s), loss on training batch is 0.00111603.
After 3235 training step(s), loss on training batch is 0.000901373.
After 3236 training step(s), loss on training batch is 0.00133484.
After 3237 training step(s), loss on training batch is 0.00133701.
After 3238 training step(s), loss on training batch is 0.000914704.
After 3239 training step(s), loss on training batch is 0.000918922.
After 3240 training step(s), loss on training batch is 0.00288861.
After 3241 training step(s), loss on training batch is 0.00091674.
After 3242 training step(s), loss on training batch is 0.00103419.
After 3243 training step(s), loss on training batch is 0.00111881.
After 3244 training step(s), loss on training batch is 0.00130731.
After 3245 training step(s), loss on training batch is 0.000835998.
After 3246 training step(s), loss on training batch is 0.00160166.
After 3247 training step(s), loss on training batch is 0.000980109.
After 3248 training step(s), loss on training batch is 0.00117732.
After 3249 training step(s), loss on training batch is 0.00114168.
After 3250 training step(s), loss on training batch is 0.00124501.
After 3251 training step(s), loss on training batch is 0.0012761.
After 3252 training step(s), loss on training batch is 0.00108017.
After 3253 training step(s), loss on training batch is 0.00143654.
After 3254 training step(s), loss on training batch is 0.00083187.
After 3255 training step(s), loss on training batch is 0.00115991.
After 3256 training step(s), loss on training batch is 0.00200524.
After 3257 training step(s), loss on training batch is 0.00158765.
After 3258 training step(s), loss on training batch is 0.00168799.
After 3259 training step(s), loss on training batch is 0.00173665.
After 3260 training step(s), loss on training batch is 0.00137205.
After 3261 training step(s), loss on training batch is 0.00134333.
After 3262 training step(s), loss on training batch is 0.00170783.
After 3263 training step(s), loss on training batch is 0.00157498.
After 3264 training step(s), loss on training batch is 0.00142643.
After 3265 training step(s), loss on training batch is 0.00205674.
After 3266 training step(s), loss on training batch is 0.00148152.
After 3267 training step(s), loss on training batch is 0.0013636.
After 3268 training step(s), loss on training batch is 0.00185021.
After 3269 training step(s), loss on training batch is 0.00271326.
After 3270 training step(s), loss on training batch is 0.00275289.
After 3271 training step(s), loss on training batch is 0.00222316.
After 3272 training step(s), loss on training batch is 0.00183435.
After 3273 training step(s), loss on training batch is 0.00198415.
After 3274 training step(s), loss on training batch is 0.0017414.
After 3275 training step(s), loss on training batch is 0.00168861.
After 3276 training step(s), loss on training batch is 0.00157683.
After 3277 training step(s), loss on training batch is 0.00291095.
After 3278 training step(s), loss on training batch is 0.00162236.
After 3279 training step(s), loss on training batch is 0.00145482.
After 3280 training step(s), loss on training batch is 0.00144364.
After 3281 training step(s), loss on training batch is 0.0011413.
After 3282 training step(s), loss on training batch is 0.00123193.
After 3283 training step(s), loss on training batch is 0.000581693.
After 3284 training step(s), loss on training batch is 0.000753334.
After 3285 training step(s), loss on training batch is 0.000901058.
After 3286 training step(s), loss on training batch is 0.0010918.
After 3287 training step(s), loss on training batch is 0.00138417.
After 3288 training step(s), loss on training batch is 0.00102834.
After 3289 training step(s), loss on training batch is 0.000723767.
After 3290 training step(s), loss on training batch is 0.000742735.
After 3291 training step(s), loss on training batch is 0.000656113.
After 3292 training step(s), loss on training batch is 0.000863802.
After 3293 training step(s), loss on training batch is 0.000596023.
After 3294 training step(s), loss on training batch is 0.000643674.
After 3295 training step(s), loss on training batch is 0.000797248.
After 3296 training step(s), loss on training batch is 0.000739505.
After 3297 training step(s), loss on training batch is 0.000557663.
After 3298 training step(s), loss on training batch is 0.000542861.
After 3299 training step(s), loss on training batch is 0.00053211.
After 3300 training step(s), loss on training batch is 0.000769339.
After 3301 training step(s), loss on training batch is 0.00197048.
After 3302 training step(s), loss on training batch is 0.00110152.
After 3303 training step(s), loss on training batch is 0.00100169.
After 3304 training step(s), loss on training batch is 0.00120224.
After 3305 training step(s), loss on training batch is 0.000975959.
After 3306 training step(s), loss on training batch is 0.00105663.
After 3307 training step(s), loss on training batch is 0.00118614.
After 3308 training step(s), loss on training batch is 0.000610344.
After 3309 training step(s), loss on training batch is 0.000632282.
After 3310 training step(s), loss on training batch is 0.000761997.
After 3311 training step(s), loss on training batch is 0.00092722.
After 3312 training step(s), loss on training batch is 0.000880397.
After 3313 training step(s), loss on training batch is 0.000837532.
After 3314 training step(s), loss on training batch is 0.00153324.
After 3315 training step(s), loss on training batch is 0.00257886.
After 3316 training step(s), loss on training batch is 0.00174281.
After 3317 training step(s), loss on training batch is 0.00158541.
After 3318 training step(s), loss on training batch is 0.00138014.
After 3319 training step(s), loss on training batch is 0.000643703.
After 3320 training step(s), loss on training batch is 0.000718408.
After 3321 training step(s), loss on training batch is 0.000701807.
After 3322 training step(s), loss on training batch is 0.000536964.
After 3323 training step(s), loss on training batch is 0.00093217.
After 3324 training step(s), loss on training batch is 0.000691258.
After 3325 training step(s), loss on training batch is 0.000667559.
After 3326 training step(s), loss on training batch is 0.000572911.
After 3327 training step(s), loss on training batch is 0.00188054.
After 3328 training step(s), loss on training batch is 0.00708745.
After 3329 training step(s), loss on training batch is 0.00178716.
After 3330 training step(s), loss on training batch is 0.00107945.
After 3331 training step(s), loss on training batch is 0.000871955.
After 3332 training step(s), loss on training batch is 0.000738708.
After 3333 training step(s), loss on training batch is 0.000726576.
After 3334 training step(s), loss on training batch is 0.000657641.
After 3335 training step(s), loss on training batch is 0.000586776.
After 3336 training step(s), loss on training batch is 0.000593686.
After 3337 training step(s), loss on training batch is 0.000670695.
After 3338 training step(s), loss on training batch is 0.000646032.
After 3339 training step(s), loss on training batch is 0.000672589.
After 3340 training step(s), loss on training batch is 0.000738573.
After 3341 training step(s), loss on training batch is 0.000525982.
After 3342 training step(s), loss on training batch is 0.000621031.
After 3343 training step(s), loss on training batch is 0.000556875.
After 3344 training step(s), loss on training batch is 0.000805107.
After 3345 training step(s), loss on training batch is 0.000492538.
After 3346 training step(s), loss on training batch is 0.000707263.
After 3347 training step(s), loss on training batch is 0.000554863.
After 3348 training step(s), loss on training batch is 0.000494268.
After 3349 training step(s), loss on training batch is 0.000566987.
After 3350 training step(s), loss on training batch is 0.000717744.
After 3351 training step(s), loss on training batch is 0.00067561.
After 3352 training step(s), loss on training batch is 0.00055302.
After 3353 training step(s), loss on training batch is 0.0005568.
After 3354 training step(s), loss on training batch is 0.000480955.
After 3355 training step(s), loss on training batch is 0.000544424.
After 3356 training step(s), loss on training batch is 0.000658861.
After 3357 training step(s), loss on training batch is 0.000791689.
After 3358 training step(s), loss on training batch is 0.000466207.
After 3359 training step(s), loss on training batch is 0.000542676.
After 3360 training step(s), loss on training batch is 0.000691362.
After 3361 training step(s), loss on training batch is 0.000502175.
After 3362 training step(s), loss on training batch is 0.00045535.
After 3363 training step(s), loss on training batch is 0.000524923.
After 3364 training step(s), loss on training batch is 0.000727778.
After 3365 training step(s), loss on training batch is 0.000586103.
After 3366 training step(s), loss on training batch is 0.000542904.
After 3367 training step(s), loss on training batch is 0.000625277.
After 3368 training step(s), loss on training batch is 0.00047609.
After 3369 training step(s), loss on training batch is 0.000577572.
After 3370 training step(s), loss on training batch is 0.000511252.
After 3371 training step(s), loss on training batch is 0.000701777.
After 3372 training step(s), loss on training batch is 0.000462541.
After 3373 training step(s), loss on training batch is 0.000504379.
After 3374 training step(s), loss on training batch is 0.00052109.
After 3375 training step(s), loss on training batch is 0.000606605.
After 3376 training step(s), loss on training batch is 0.000578502.
After 3377 training step(s), loss on training batch is 0.000885413.
After 3378 training step(s), loss on training batch is 0.000615526.
After 3379 training step(s), loss on training batch is 0.000578568.
After 3380 training step(s), loss on training batch is 0.000824334.
After 3381 training step(s), loss on training batch is 0.000516524.
After 3382 training step(s), loss on training batch is 0.000634252.
After 3383 training step(s), loss on training batch is 0.00128107.
After 3384 training step(s), loss on training batch is 0.0017437.
After 3385 training step(s), loss on training batch is 0.00220599.
After 3386 training step(s), loss on training batch is 0.00126246.
After 3387 training step(s), loss on training batch is 0.00115032.
After 3388 training step(s), loss on training batch is 0.000954982.
After 3389 training step(s), loss on training batch is 0.00112463.
After 3390 training step(s), loss on training batch is 0.00102617.
After 3391 training step(s), loss on training batch is 0.00106347.
After 3392 training step(s), loss on training batch is 0.00139636.
After 3393 training step(s), loss on training batch is 0.000859781.
After 3394 training step(s), loss on training batch is 0.000922369.
After 3395 training step(s), loss on training batch is 0.00169411.
After 3396 training step(s), loss on training batch is 0.00140144.
After 3397 training step(s), loss on training batch is 0.00108311.
After 3398 training step(s), loss on training batch is 0.00125642.
After 3399 training step(s), loss on training batch is 0.00183607.
After 3400 training step(s), loss on training batch is 0.00104958.
After 3401 training step(s), loss on training batch is 0.00113892.
After 3402 training step(s), loss on training batch is 0.00114294.
After 3403 training step(s), loss on training batch is 0.000981016.
After 3404 training step(s), loss on training batch is 0.00094152.
After 3405 training step(s), loss on training batch is 0.000855455.
After 3406 training step(s), loss on training batch is 0.00113813.
After 3407 training step(s), loss on training batch is 0.00159922.
After 3408 training step(s), loss on training batch is 0.00151347.
After 3409 training step(s), loss on training batch is 0.000877264.
After 3410 training step(s), loss on training batch is 0.00189574.
After 3411 training step(s), loss on training batch is 0.00114231.
After 3412 training step(s), loss on training batch is 0.00119536.
After 3413 training step(s), loss on training batch is 0.00098114.
After 3414 training step(s), loss on training batch is 0.00080493.
After 3415 training step(s), loss on training batch is 0.000821383.
After 3416 training step(s), loss on training batch is 0.000790547.
After 3417 training step(s), loss on training batch is 0.000871149.
After 3418 training step(s), loss on training batch is 0.00137161.
After 3419 training step(s), loss on training batch is 0.00188724.
After 3420 training step(s), loss on training batch is 0.00166359.
After 3421 training step(s), loss on training batch is 0.0029326.
After 3422 training step(s), loss on training batch is 0.00131759.
After 3423 training step(s), loss on training batch is 0.00130777.
After 3424 training step(s), loss on training batch is 0.00114456.
After 3425 training step(s), loss on training batch is 0.00148784.
After 3426 training step(s), loss on training batch is 0.00120875.
After 3427 training step(s), loss on training batch is 0.00112331.
After 3428 training step(s), loss on training batch is 0.00109299.
After 3429 training step(s), loss on training batch is 0.00110846.
After 3430 training step(s), loss on training batch is 0.00126692.
After 3431 training step(s), loss on training batch is 0.00148894.
After 3432 training step(s), loss on training batch is 0.000922455.
After 3433 training step(s), loss on training batch is 0.000877953.
After 3434 training step(s), loss on training batch is 0.000852993.
After 3435 training step(s), loss on training batch is 0.000679941.
After 3436 training step(s), loss on training batch is 0.00125347.
After 3437 training step(s), loss on training batch is 0.00246562.
After 3438 training step(s), loss on training batch is 0.000853981.
After 3439 training step(s), loss on training batch is 0.00103514.
After 3440 training step(s), loss on training batch is 0.00120381.
After 3441 training step(s), loss on training batch is 0.00100357.
After 3442 training step(s), loss on training batch is 0.000931158.
After 3443 training step(s), loss on training batch is 0.00109638.
After 3444 training step(s), loss on training batch is 0.000993844.
After 3445 training step(s), loss on training batch is 0.0010712.
After 3446 training step(s), loss on training batch is 0.00115634.
After 3447 training step(s), loss on training batch is 0.000931703.
After 3448 training step(s), loss on training batch is 0.00123877.
After 3449 training step(s), loss on training batch is 0.00112045.
After 3450 training step(s), loss on training batch is 0.00124591.
After 3451 training step(s), loss on training batch is 0.00104064.
After 3452 training step(s), loss on training batch is 0.000815152.
After 3453 training step(s), loss on training batch is 0.00160626.
After 3454 training step(s), loss on training batch is 0.00103065.
After 3455 training step(s), loss on training batch is 0.00111052.
After 3456 training step(s), loss on training batch is 0.000868252.
After 3457 training step(s), loss on training batch is 0.000825345.
After 3458 training step(s), loss on training batch is 0.00136606.
After 3459 training step(s), loss on training batch is 0.00169676.
After 3460 training step(s), loss on training batch is 0.00115167.
After 3461 training step(s), loss on training batch is 0.000913274.
After 3462 training step(s), loss on training batch is 0.000858652.
After 3463 training step(s), loss on training batch is 0.000901652.
After 3464 training step(s), loss on training batch is 0.000767089.
After 3465 training step(s), loss on training batch is 0.00120011.
After 3466 training step(s), loss on training batch is 0.00168728.
After 3467 training step(s), loss on training batch is 0.00205842.
After 3468 training step(s), loss on training batch is 0.00164049.
After 3469 training step(s), loss on training batch is 0.00159987.
After 3470 training step(s), loss on training batch is 0.00156727.
After 3471 training step(s), loss on training batch is 0.00147286.
After 3472 training step(s), loss on training batch is 0.00162332.
After 3473 training step(s), loss on training batch is 0.0035961.
After 3474 training step(s), loss on training batch is 0.00181117.
After 3475 training step(s), loss on training batch is 0.00215629.
After 3476 training step(s), loss on training batch is 0.00174236.
After 3477 training step(s), loss on training batch is 0.00153541.
After 3478 training step(s), loss on training batch is 0.00185523.
After 3479 training step(s), loss on training batch is 0.00136548.
After 3480 training step(s), loss on training batch is 0.00137172.
After 3481 training step(s), loss on training batch is 0.00140038.
After 3482 training step(s), loss on training batch is 0.00133794.
After 3483 training step(s), loss on training batch is 0.00155808.
After 3484 training step(s), loss on training batch is 0.00137069.
After 3485 training step(s), loss on training batch is 0.00131715.
After 3486 training step(s), loss on training batch is 0.00185784.
After 3487 training step(s), loss on training batch is 0.00138185.
After 3488 training step(s), loss on training batch is 0.00180872.
After 3489 training step(s), loss on training batch is 0.00148235.
After 3490 training step(s), loss on training batch is 0.0015307.
After 3491 training step(s), loss on training batch is 0.00134894.
After 3492 training step(s), loss on training batch is 0.00166445.
After 3493 training step(s), loss on training batch is 0.00135966.
After 3494 training step(s), loss on training batch is 0.0015245.
After 3495 training step(s), loss on training batch is 0.0018904.
After 3496 training step(s), loss on training batch is 0.00167306.
After 3497 training step(s), loss on training batch is 0.00156421.
After 3498 training step(s), loss on training batch is 0.00144053.
After 3499 training step(s), loss on training batch is 0.00136688.
After 3500 training step(s), loss on training batch is 0.00136753.
After 3501 training step(s), loss on training batch is 0.00117003.
After 3502 training step(s), loss on training batch is 0.00155415.
After 3503 training step(s), loss on training batch is 0.0013187.
After 3504 training step(s), loss on training batch is 0.00210693.
After 3505 training step(s), loss on training batch is 0.00173397.
After 3506 training step(s), loss on training batch is 0.00175135.
After 3507 training step(s), loss on training batch is 0.00232907.
After 3508 training step(s), loss on training batch is 0.00163733.
After 3509 training step(s), loss on training batch is 0.00482957.
After 3510 training step(s), loss on training batch is 0.00216336.
After 3511 training step(s), loss on training batch is 0.0018495.
After 3512 training step(s), loss on training batch is 0.00183846.
After 3513 training step(s), loss on training batch is 0.00169152.
After 3514 training step(s), loss on training batch is 0.00225859.
After 3515 training step(s), loss on training batch is 0.00161395.
After 3516 training step(s), loss on training batch is 0.000695408.
After 3517 training step(s), loss on training batch is 0.000587997.
After 3518 training step(s), loss on training batch is 0.000601677.
After 3519 training step(s), loss on training batch is 0.00075028.
After 3520 training step(s), loss on training batch is 0.000969228.
After 3521 training step(s), loss on training batch is 0.000750118.
After 3522 training step(s), loss on training batch is 0.000756468.
After 3523 training step(s), loss on training batch is 0.000817333.
After 3524 training step(s), loss on training batch is 0.000892971.
After 3525 training step(s), loss on training batch is 0.000914883.
After 3526 training step(s), loss on training batch is 0.00131237.
After 3527 training step(s), loss on training batch is 0.00131374.
After 3528 training step(s), loss on training batch is 0.00167017.
After 3529 training step(s), loss on training batch is 0.00132022.
After 3530 training step(s), loss on training batch is 0.00388278.
After 3531 training step(s), loss on training batch is 0.00303467.
After 3532 training step(s), loss on training batch is 0.00191367.
After 3533 training step(s), loss on training batch is 0.00199293.
After 3534 training step(s), loss on training batch is 0.0015008.
After 3535 training step(s), loss on training batch is 0.0014102.
After 3536 training step(s), loss on training batch is 0.00151926.
After 3537 training step(s), loss on training batch is 0.0013404.
After 3538 training step(s), loss on training batch is 0.00161191.
After 3539 training step(s), loss on training batch is 0.00167824.
After 3540 training step(s), loss on training batch is 0.00148684.
After 3541 training step(s), loss on training batch is 0.00134724.
After 3542 training step(s), loss on training batch is 0.00194681.
After 3543 training step(s), loss on training batch is 0.000758157.
After 3544 training step(s), loss on training batch is 0.00096764.
After 3545 training step(s), loss on training batch is 0.000741176.
After 3546 training step(s), loss on training batch is 0.00105521.
After 3547 training step(s), loss on training batch is 0.000955509.
After 3548 training step(s), loss on training batch is 0.000802641.
After 3549 training step(s), loss on training batch is 0.00065827.
After 3550 training step(s), loss on training batch is 0.0005434.
After 3551 training step(s), loss on training batch is 0.000740539.
After 3552 training step(s), loss on training batch is 0.000782908.
After 3553 training step(s), loss on training batch is 0.000708317.
After 3554 training step(s), loss on training batch is 0.00056756.
After 3555 training step(s), loss on training batch is 0.00115092.
After 3556 training step(s), loss on training batch is 0.000612471.
After 3557 training step(s), loss on training batch is 0.000517362.
After 3558 training step(s), loss on training batch is 0.000631666.
After 3559 training step(s), loss on training batch is 0.00160812.
After 3560 training step(s), loss on training batch is 0.00209703.
After 3561 training step(s), loss on training batch is 0.00113296.
After 3562 training step(s), loss on training batch is 0.000692173.
After 3563 training step(s), loss on training batch is 0.000769197.
After 3564 training step(s), loss on training batch is 0.00109427.
After 3565 training step(s), loss on training batch is 0.000901079.
After 3566 training step(s), loss on training batch is 0.000811982.
After 3567 training step(s), loss on training batch is 0.000707987.
After 3568 training step(s), loss on training batch is 0.000872781.
After 3569 training step(s), loss on training batch is 0.000563463.
After 3570 training step(s), loss on training batch is 0.000718712.
After 3571 training step(s), loss on training batch is 0.000446623.
After 3572 training step(s), loss on training batch is 0.000676222.
After 3573 training step(s), loss on training batch is 0.000529762.
After 3574 training step(s), loss on training batch is 0.000479107.
After 3575 training step(s), loss on training batch is 0.000977259.
After 3576 training step(s), loss on training batch is 0.000871439.
After 3577 training step(s), loss on training batch is 0.000845981.
After 3578 training step(s), loss on training batch is 0.000572296.
After 3579 training step(s), loss on training batch is 0.000583452.
After 3580 training step(s), loss on training batch is 0.000494126.
After 3581 training step(s), loss on training batch is 0.000597303.
After 3582 training step(s), loss on training batch is 0.000550943.
After 3583 training step(s), loss on training batch is 0.000714637.
After 3584 training step(s), loss on training batch is 0.000634606.
After 3585 training step(s), loss on training batch is 0.000666506.
After 3586 training step(s), loss on training batch is 0.000947304.
After 3587 training step(s), loss on training batch is 0.000809104.
After 3588 training step(s), loss on training batch is 0.000523364.
After 3589 training step(s), loss on training batch is 0.000560666.
After 3590 training step(s), loss on training batch is 0.000500015.
After 3591 training step(s), loss on training batch is 0.000494073.
After 3592 training step(s), loss on training batch is 0.000533792.
After 3593 training step(s), loss on training batch is 0.000463479.
After 3594 training step(s), loss on training batch is 0.000422842.
After 3595 training step(s), loss on training batch is 0.000956985.
After 3596 training step(s), loss on training batch is 0.000694815.
After 3597 training step(s), loss on training batch is 0.000416975.
After 3598 training step(s), loss on training batch is 0.000401789.
After 3599 training step(s), loss on training batch is 0.000554042.
After 3600 training step(s), loss on training batch is 0.000683305.
After 3601 training step(s), loss on training batch is 0.000928595.
After 3602 training step(s), loss on training batch is 0.000977326.
After 3603 training step(s), loss on training batch is 0.000583993.
After 3604 training step(s), loss on training batch is 0.000895126.
After 3605 training step(s), loss on training batch is 0.00244817.
After 3606 training step(s), loss on training batch is 0.00136543.
After 3607 training step(s), loss on training batch is 0.000739264.
After 3608 training step(s), loss on training batch is 0.000896521.
After 3609 training step(s), loss on training batch is 0.00104056.
After 3610 training step(s), loss on training batch is 0.00114585.
After 3611 training step(s), loss on training batch is 0.000699754.
After 3612 training step(s), loss on training batch is 0.000527738.
After 3613 training step(s), loss on training batch is 0.000598876.
After 3614 training step(s), loss on training batch is 0.000575894.
After 3615 training step(s), loss on training batch is 0.000523192.
After 3616 training step(s), loss on training batch is 0.0005.
After 3617 training step(s), loss on training batch is 0.00059251.
After 3618 training step(s), loss on training batch is 0.000507316.
After 3619 training step(s), loss on training batch is 0.000617849.
After 3620 training step(s), loss on training batch is 0.000544368.
After 3621 training step(s), loss on training batch is 0.000471688.
After 3622 training step(s), loss on training batch is 0.000558955.
After 3623 training step(s), loss on training batch is 0.000475489.
After 3624 training step(s), loss on training batch is 0.000468841.
After 3625 training step(s), loss on training batch is 0.000451379.
After 3626 training step(s), loss on training batch is 0.000620322.
After 3627 training step(s), loss on training batch is 0.00047517.
After 3628 training step(s), loss on training batch is 0.00189982.
After 3629 training step(s), loss on training batch is 0.0010611.
After 3630 training step(s), loss on training batch is 0.000944944.
After 3631 training step(s), loss on training batch is 0.000980203.
After 3632 training step(s), loss on training batch is 0.00121446.
After 3633 training step(s), loss on training batch is 0.00125855.
After 3634 training step(s), loss on training batch is 0.00101198.
After 3635 training step(s), loss on training batch is 0.000828992.
After 3636 training step(s), loss on training batch is 0.00121509.
After 3637 training step(s), loss on training batch is 0.00122997.
After 3638 training step(s), loss on training batch is 0.000831784.
After 3639 training step(s), loss on training batch is 0.00085855.
After 3640 training step(s), loss on training batch is 0.0020477.
After 3641 training step(s), loss on training batch is 0.000955433.
After 3642 training step(s), loss on training batch is 0.00106435.
After 3643 training step(s), loss on training batch is 0.00113938.
After 3644 training step(s), loss on training batch is 0.0011873.
After 3645 training step(s), loss on training batch is 0.000843521.
After 3646 training step(s), loss on training batch is 0.00133503.
After 3647 training step(s), loss on training batch is 0.00093358.
After 3648 training step(s), loss on training batch is 0.00111621.
After 3649 training step(s), loss on training batch is 0.00105852.
After 3650 training step(s), loss on training batch is 0.0011316.
After 3651 training step(s), loss on training batch is 0.00116705.
After 3652 training step(s), loss on training batch is 0.000981923.
After 3653 training step(s), loss on training batch is 0.00131937.
After 3654 training step(s), loss on training batch is 0.000768371.
After 3655 training step(s), loss on training batch is 0.00107175.
After 3656 training step(s), loss on training batch is 0.00184358.
After 3657 training step(s), loss on training batch is 0.00148488.
After 3658 training step(s), loss on training batch is 0.0015884.
After 3659 training step(s), loss on training batch is 0.00159264.
After 3660 training step(s), loss on training batch is 0.00127566.
After 3661 training step(s), loss on training batch is 0.00124703.
After 3662 training step(s), loss on training batch is 0.0016106.
After 3663 training step(s), loss on training batch is 0.00147303.
After 3664 training step(s), loss on training batch is 0.00132567.
After 3665 training step(s), loss on training batch is 0.00184906.
After 3666 training step(s), loss on training batch is 0.00139935.
After 3667 training step(s), loss on training batch is 0.00137046.
After 3668 training step(s), loss on training batch is 0.00161978.
After 3669 training step(s), loss on training batch is 0.00263379.
After 3670 training step(s), loss on training batch is 0.00272643.
After 3671 training step(s), loss on training batch is 0.00209631.
After 3672 training step(s), loss on training batch is 0.00166397.
After 3673 training step(s), loss on training batch is 0.00183696.
After 3674 training step(s), loss on training batch is 0.00160724.
After 3675 training step(s), loss on training batch is 0.00157481.
After 3676 training step(s), loss on training batch is 0.00145784.
After 3677 training step(s), loss on training batch is 0.00271708.
After 3678 training step(s), loss on training batch is 0.00149428.
After 3679 training step(s), loss on training batch is 0.00136649.
After 3680 training step(s), loss on training batch is 0.00136823.
After 3681 training step(s), loss on training batch is 0.000958628.
After 3682 training step(s), loss on training batch is 0.00103936.
After 3683 training step(s), loss on training batch is 0.000547036.
After 3684 training step(s), loss on training batch is 0.00068368.
After 3685 training step(s), loss on training batch is 0.00079245.
After 3686 training step(s), loss on training batch is 0.00100858.
After 3687 training step(s), loss on training batch is 0.00125644.
After 3688 training step(s), loss on training batch is 0.000920849.
After 3689 training step(s), loss on training batch is 0.000648541.
After 3690 training step(s), loss on training batch is 0.000666942.
After 3691 training step(s), loss on training batch is 0.000584047.
After 3692 training step(s), loss on training batch is 0.000764269.
After 3693 training step(s), loss on training batch is 0.000541248.
After 3694 training step(s), loss on training batch is 0.000571118.
After 3695 training step(s), loss on training batch is 0.000722291.
After 3696 training step(s), loss on training batch is 0.000667871.
After 3697 training step(s), loss on training batch is 0.000506292.
After 3698 training step(s), loss on training batch is 0.000496688.
After 3699 training step(s), loss on training batch is 0.000490626.
After 3700 training step(s), loss on training batch is 0.000666294.
After 3701 training step(s), loss on training batch is 0.00191539.
After 3702 training step(s), loss on training batch is 0.00100073.
After 3703 training step(s), loss on training batch is 0.000938211.
After 3704 training step(s), loss on training batch is 0.00111637.
After 3705 training step(s), loss on training batch is 0.000912095.
After 3706 training step(s), loss on training batch is 0.000985556.
After 3707 training step(s), loss on training batch is 0.0011022.
After 3708 training step(s), loss on training batch is 0.000556424.
After 3709 training step(s), loss on training batch is 0.000587729.
After 3710 training step(s), loss on training batch is 0.000695929.
After 3711 training step(s), loss on training batch is 0.000871727.
After 3712 training step(s), loss on training batch is 0.000749635.
After 3713 training step(s), loss on training batch is 0.000752537.
After 3714 training step(s), loss on training batch is 0.00151016.
After 3715 training step(s), loss on training batch is 0.00244082.
After 3716 training step(s), loss on training batch is 0.00159376.
After 3717 training step(s), loss on training batch is 0.00137133.
After 3718 training step(s), loss on training batch is 0.00119509.
After 3719 training step(s), loss on training batch is 0.000610361.
After 3720 training step(s), loss on training batch is 0.000674999.
After 3721 training step(s), loss on training batch is 0.000670544.
After 3722 training step(s), loss on training batch is 0.000525183.
After 3723 training step(s), loss on training batch is 0.000874452.
After 3724 training step(s), loss on training batch is 0.0006534.
After 3725 training step(s), loss on training batch is 0.000631735.
After 3726 training step(s), loss on training batch is 0.000546535.
After 3727 training step(s), loss on training batch is 0.0016346.
After 3728 training step(s), loss on training batch is 0.00603857.
After 3729 training step(s), loss on training batch is 0.00162011.
After 3730 training step(s), loss on training batch is 0.000983592.
After 3731 training step(s), loss on training batch is 0.000817057.
After 3732 training step(s), loss on training batch is 0.000693774.
After 3733 training step(s), loss on training batch is 0.000660825.
After 3734 training step(s), loss on training batch is 0.000599793.
After 3735 training step(s), loss on training batch is 0.000534592.
After 3736 training step(s), loss on training batch is 0.000537566.
After 3737 training step(s), loss on training batch is 0.000579102.
After 3738 training step(s), loss on training batch is 0.000563682.
After 3739 training step(s), loss on training batch is 0.000582383.
After 3740 training step(s), loss on training batch is 0.000623474.
After 3741 training step(s), loss on training batch is 0.000472182.
After 3742 training step(s), loss on training batch is 0.000557761.
After 3743 training step(s), loss on training batch is 0.000503264.
After 3744 training step(s), loss on training batch is 0.000728862.
After 3745 training step(s), loss on training batch is 0.000463819.
After 3746 training step(s), loss on training batch is 0.000646084.
After 3747 training step(s), loss on training batch is 0.000516121.
After 3748 training step(s), loss on training batch is 0.000460057.
After 3749 training step(s), loss on training batch is 0.000524128.
After 3750 training step(s), loss on training batch is 0.000635696.
After 3751 training step(s), loss on training batch is 0.000612395.
After 3752 training step(s), loss on training batch is 0.00052516.
After 3753 training step(s), loss on training batch is 0.000525357.
After 3754 training step(s), loss on training batch is 0.000463008.
After 3755 training step(s), loss on training batch is 0.000506652.
After 3756 training step(s), loss on training batch is 0.000616334.
After 3757 training step(s), loss on training batch is 0.000739127.
After 3758 training step(s), loss on training batch is 0.000444072.
After 3759 training step(s), loss on training batch is 0.000512582.
After 3760 training step(s), loss on training batch is 0.000669525.
After 3761 training step(s), loss on training batch is 0.000476775.
After 3762 training step(s), loss on training batch is 0.000433557.
After 3763 training step(s), loss on training batch is 0.000496084.
After 3764 training step(s), loss on training batch is 0.000639548.
After 3765 training step(s), loss on training batch is 0.000533826.
After 3766 training step(s), loss on training batch is 0.000513567.
After 3767 training step(s), loss on training batch is 0.000576869.
After 3768 training step(s), loss on training batch is 0.000454361.
After 3769 training step(s), loss on training batch is 0.000549424.
After 3770 training step(s), loss on training batch is 0.000481694.
After 3771 training step(s), loss on training batch is 0.000678192.
After 3772 training step(s), loss on training batch is 0.000438738.
After 3773 training step(s), loss on training batch is 0.00047592.
After 3774 training step(s), loss on training batch is 0.000489742.
After 3775 training step(s), loss on training batch is 0.000563622.
After 3776 training step(s), loss on training batch is 0.000540328.
After 3777 training step(s), loss on training batch is 0.00090597.
After 3778 training step(s), loss on training batch is 0.000581078.
After 3779 training step(s), loss on training batch is 0.000530085.
After 3780 training step(s), loss on training batch is 0.000743266.
After 3781 training step(s), loss on training batch is 0.000478362.
After 3782 training step(s), loss on training batch is 0.000572532.
After 3783 training step(s), loss on training batch is 0.00118874.
After 3784 training step(s), loss on training batch is 0.0016202.
After 3785 training step(s), loss on training batch is 0.00206273.
After 3786 training step(s), loss on training batch is 0.00117001.
After 3787 training step(s), loss on training batch is 0.00106263.
After 3788 training step(s), loss on training batch is 0.000897895.
After 3789 training step(s), loss on training batch is 0.00105921.
After 3790 training step(s), loss on training batch is 0.000955348.
After 3791 training step(s), loss on training batch is 0.00100567.
After 3792 training step(s), loss on training batch is 0.00127444.
After 3793 training step(s), loss on training batch is 0.000815316.
After 3794 training step(s), loss on training batch is 0.000879702.
After 3795 training step(s), loss on training batch is 0.00155761.
After 3796 training step(s), loss on training batch is 0.00127988.
After 3797 training step(s), loss on training batch is 0.00100266.
After 3798 training step(s), loss on training batch is 0.00110381.
After 3799 training step(s), loss on training batch is 0.0017164.
After 3800 training step(s), loss on training batch is 0.000963166.
After 3801 training step(s), loss on training batch is 0.0010525.
After 3802 training step(s), loss on training batch is 0.00105177.
After 3803 training step(s), loss on training batch is 0.00093963.
After 3804 training step(s), loss on training batch is 0.000904798.
After 3805 training step(s), loss on training batch is 0.000817741.
After 3806 training step(s), loss on training batch is 0.00103415.
After 3807 training step(s), loss on training batch is 0.00147354.
After 3808 training step(s), loss on training batch is 0.00135297.
After 3809 training step(s), loss on training batch is 0.000828399.
After 3810 training step(s), loss on training batch is 0.00175363.
After 3811 training step(s), loss on training batch is 0.00105544.
After 3812 training step(s), loss on training batch is 0.00111228.
After 3813 training step(s), loss on training batch is 0.000903428.
After 3814 training step(s), loss on training batch is 0.000784233.
After 3815 training step(s), loss on training batch is 0.000791984.
After 3816 training step(s), loss on training batch is 0.000778395.
After 3817 training step(s), loss on training batch is 0.000821916.
After 3818 training step(s), loss on training batch is 0.0012263.
After 3819 training step(s), loss on training batch is 0.00174227.
After 3820 training step(s), loss on training batch is 0.00172712.
After 3821 training step(s), loss on training batch is 0.00274704.
After 3822 training step(s), loss on training batch is 0.00118795.
After 3823 training step(s), loss on training batch is 0.0011685.
After 3824 training step(s), loss on training batch is 0.00105203.
After 3825 training step(s), loss on training batch is 0.00132717.
After 3826 training step(s), loss on training batch is 0.00108537.
After 3827 training step(s), loss on training batch is 0.000922801.
After 3828 training step(s), loss on training batch is 0.000963846.
After 3829 training step(s), loss on training batch is 0.000961069.
After 3830 training step(s), loss on training batch is 0.00122709.
After 3831 training step(s), loss on training batch is 0.00147969.
After 3832 training step(s), loss on training batch is 0.000837515.
After 3833 training step(s), loss on training batch is 0.000821333.
After 3834 training step(s), loss on training batch is 0.00080463.
After 3835 training step(s), loss on training batch is 0.000651001.
After 3836 training step(s), loss on training batch is 0.00119085.
After 3837 training step(s), loss on training batch is 0.00234867.
After 3838 training step(s), loss on training batch is 0.00079114.
After 3839 training step(s), loss on training batch is 0.000957929.
After 3840 training step(s), loss on training batch is 0.00109356.
After 3841 training step(s), loss on training batch is 0.000912614.
After 3842 training step(s), loss on training batch is 0.000813167.
After 3843 training step(s), loss on training batch is 0.000963698.
After 3844 training step(s), loss on training batch is 0.000862153.
After 3845 training step(s), loss on training batch is 0.000991883.
After 3846 training step(s), loss on training batch is 0.00112438.
After 3847 training step(s), loss on training batch is 0.00089621.
After 3848 training step(s), loss on training batch is 0.00112692.
After 3849 training step(s), loss on training batch is 0.00104434.
After 3850 training step(s), loss on training batch is 0.00116183.
After 3851 training step(s), loss on training batch is 0.00101626.
After 3852 training step(s), loss on training batch is 0.000766984.
After 3853 training step(s), loss on training batch is 0.00151242.
After 3854 training step(s), loss on training batch is 0.000995231.
After 3855 training step(s), loss on training batch is 0.0010237.
After 3856 training step(s), loss on training batch is 0.000816655.
After 3857 training step(s), loss on training batch is 0.000766097.
After 3858 training step(s), loss on training batch is 0.00127516.
After 3859 training step(s), loss on training batch is 0.00163455.
After 3860 training step(s), loss on training batch is 0.00107173.
After 3861 training step(s), loss on training batch is 0.000832469.
After 3862 training step(s), loss on training batch is 0.000799363.
After 3863 training step(s), loss on training batch is 0.000855569.
After 3864 training step(s), loss on training batch is 0.0007208.
After 3865 training step(s), loss on training batch is 0.00119428.
After 3866 training step(s), loss on training batch is 0.00160197.
After 3867 training step(s), loss on training batch is 0.00190046.
After 3868 training step(s), loss on training batch is 0.00156008.
After 3869 training step(s), loss on training batch is 0.00150093.
After 3870 training step(s), loss on training batch is 0.0014846.
After 3871 training step(s), loss on training batch is 0.00139338.
After 3872 training step(s), loss on training batch is 0.0014904.
After 3873 training step(s), loss on training batch is 0.0035815.
After 3874 training step(s), loss on training batch is 0.00168837.
After 3875 training step(s), loss on training batch is 0.0020255.
After 3876 training step(s), loss on training batch is 0.0016356.
After 3877 training step(s), loss on training batch is 0.00145988.
After 3878 training step(s), loss on training batch is 0.00173016.
After 3879 training step(s), loss on training batch is 0.00123281.
After 3880 training step(s), loss on training batch is 0.00127501.
After 3881 training step(s), loss on training batch is 0.0013058.
After 3882 training step(s), loss on training batch is 0.00125596.
After 3883 training step(s), loss on training batch is 0.00152143.
After 3884 training step(s), loss on training batch is 0.00130145.
After 3885 training step(s), loss on training batch is 0.00124369.
After 3886 training step(s), loss on training batch is 0.0017671.
After 3887 training step(s), loss on training batch is 0.00128384.
After 3888 training step(s), loss on training batch is 0.0017109.
After 3889 training step(s), loss on training batch is 0.00136986.
After 3890 training step(s), loss on training batch is 0.00139965.
After 3891 training step(s), loss on training batch is 0.00126574.
After 3892 training step(s), loss on training batch is 0.00151215.
After 3893 training step(s), loss on training batch is 0.00127808.
After 3894 training step(s), loss on training batch is 0.00143613.
After 3895 training step(s), loss on training batch is 0.0016797.
After 3896 training step(s), loss on training batch is 0.00158961.
After 3897 training step(s), loss on training batch is 0.00146401.
After 3898 training step(s), loss on training batch is 0.0013662.
After 3899 training step(s), loss on training batch is 0.00131172.
After 3900 training step(s), loss on training batch is 0.00133549.
After 3901 training step(s), loss on training batch is 0.00113979.
After 3902 training step(s), loss on training batch is 0.00189643.
After 3903 training step(s), loss on training batch is 0.00124074.
After 3904 training step(s), loss on training batch is 0.00187494.
After 3905 training step(s), loss on training batch is 0.0015975.
After 3906 training step(s), loss on training batch is 0.00163779.
After 3907 training step(s), loss on training batch is 0.00218877.
After 3908 training step(s), loss on training batch is 0.00151586.
After 3909 training step(s), loss on training batch is 0.00518285.
After 3910 training step(s), loss on training batch is 0.002204.
After 3911 training step(s), loss on training batch is 0.00189405.
After 3912 training step(s), loss on training batch is 0.00175561.
After 3913 training step(s), loss on training batch is 0.00160568.
After 3914 training step(s), loss on training batch is 0.00214124.
After 3915 training step(s), loss on training batch is 0.00150559.
After 3916 training step(s), loss on training batch is 0.000650937.
After 3917 training step(s), loss on training batch is 0.000553711.
After 3918 training step(s), loss on training batch is 0.000562077.
After 3919 training step(s), loss on training batch is 0.000705671.
After 3920 training step(s), loss on training batch is 0.000900444.
After 3921 training step(s), loss on training batch is 0.000699935.
After 3922 training step(s), loss on training batch is 0.000695723.
After 3923 training step(s), loss on training batch is 0.000742167.
After 3924 training step(s), loss on training batch is 0.000824913.
After 3925 training step(s), loss on training batch is 0.000846161.
After 3926 training step(s), loss on training batch is 0.00122264.
After 3927 training step(s), loss on training batch is 0.00118844.
After 3928 training step(s), loss on training batch is 0.00154941.
After 3929 training step(s), loss on training batch is 0.00122844.
After 3930 training step(s), loss on training batch is 0.00342085.
After 3931 training step(s), loss on training batch is 0.00281608.
After 3932 training step(s), loss on training batch is 0.00178292.
After 3933 training step(s), loss on training batch is 0.00186049.
After 3934 training step(s), loss on training batch is 0.00142821.
After 3935 training step(s), loss on training batch is 0.00134565.
After 3936 training step(s), loss on training batch is 0.00147089.
After 3937 training step(s), loss on training batch is 0.00128993.
After 3938 training step(s), loss on training batch is 0.00150795.
After 3939 training step(s), loss on training batch is 0.00151248.
After 3940 training step(s), loss on training batch is 0.00140713.
After 3941 training step(s), loss on training batch is 0.00129577.
After 3942 training step(s), loss on training batch is 0.00200843.
After 3943 training step(s), loss on training batch is 0.000659118.
After 3944 training step(s), loss on training batch is 0.000882983.
After 3945 training step(s), loss on training batch is 0.000674739.
After 3946 training step(s), loss on training batch is 0.000931878.
After 3947 training step(s), loss on training batch is 0.000866505.
After 3948 training step(s), loss on training batch is 0.000804705.
After 3949 training step(s), loss on training batch is 0.000572086.
After 3950 training step(s), loss on training batch is 0.000500547.
After 3951 training step(s), loss on training batch is 0.000716939.
After 3952 training step(s), loss on training batch is 0.000709648.
After 3953 training step(s), loss on training batch is 0.000656282.
After 3954 training step(s), loss on training batch is 0.000528008.
After 3955 training step(s), loss on training batch is 0.00102081.
After 3956 training step(s), loss on training batch is 0.000585833.
After 3957 training step(s), loss on training batch is 0.000510939.
After 3958 training step(s), loss on training batch is 0.000531988.
After 3959 training step(s), loss on training batch is 0.00140207.
After 3960 training step(s), loss on training batch is 0.00185335.
After 3961 training step(s), loss on training batch is 0.0010519.
After 3962 training step(s), loss on training batch is 0.000674786.
After 3963 training step(s), loss on training batch is 0.000729331.
After 3964 training step(s), loss on training batch is 0.00095046.
After 3965 training step(s), loss on training batch is 0.000835085.
After 3966 training step(s), loss on training batch is 0.000739385.
After 3967 training step(s), loss on training batch is 0.000637975.
After 3968 training step(s), loss on training batch is 0.000817004.
After 3969 training step(s), loss on training batch is 0.000535867.
After 3970 training step(s), loss on training batch is 0.000676097.
After 3971 training step(s), loss on training batch is 0.000413014.
After 3972 training step(s), loss on training batch is 0.000591573.
After 3973 training step(s), loss on training batch is 0.000501511.
After 3974 training step(s), loss on training batch is 0.000453431.
After 3975 training step(s), loss on training batch is 0.000857919.
After 3976 training step(s), loss on training batch is 0.000800077.
After 3977 training step(s), loss on training batch is 0.000765939.
After 3978 training step(s), loss on training batch is 0.000517677.
After 3979 training step(s), loss on training batch is 0.000535213.
After 3980 training step(s), loss on training batch is 0.00045572.
After 3981 training step(s), loss on training batch is 0.000528985.
After 3982 training step(s), loss on training batch is 0.000486372.
After 3983 training step(s), loss on training batch is 0.000624571.
After 3984 training step(s), loss on training batch is 0.000632557.
After 3985 training step(s), loss on training batch is 0.000620383.
After 3986 training step(s), loss on training batch is 0.000913372.
After 3987 training step(s), loss on training batch is 0.000746384.
After 3988 training step(s), loss on training batch is 0.000489335.
After 3989 training step(s), loss on training batch is 0.000527242.
After 3990 training step(s), loss on training batch is 0.00047891.
After 3991 training step(s), loss on training batch is 0.000464104.
After 3992 training step(s), loss on training batch is 0.000520595.
After 3993 training step(s), loss on training batch is 0.000456664.
After 3994 training step(s), loss on training batch is 0.00040966.
After 3995 training step(s), loss on training batch is 0.00082125.
After 3996 training step(s), loss on training batch is 0.00065607.
After 3997 training step(s), loss on training batch is 0.000396729.
After 3998 training step(s), loss on training batch is 0.000377606.
After 3999 training step(s), loss on training batch is 0.000502774.
After 4000 training step(s), loss on training batch is 0.00062516.
After 4001 training step(s), loss on training batch is 0.000865524.
After 4002 training step(s), loss on training batch is 0.000911987.
After 4003 training step(s), loss on training batch is 0.000536086.
After 4004 training step(s), loss on training batch is 0.00082766.
After 4005 training step(s), loss on training batch is 0.00250815.
After 4006 training step(s), loss on training batch is 0.00125619.
After 4007 training step(s), loss on training batch is 0.000676021.
After 4008 training step(s), loss on training batch is 0.000732523.
After 4009 training step(s), loss on training batch is 0.0010074.
After 4010 training step(s), loss on training batch is 0.00122463.
After 4011 training step(s), loss on training batch is 0.00065467.
After 4012 training step(s), loss on training batch is 0.000464298.
After 4013 training step(s), loss on training batch is 0.00051272.
After 4014 training step(s), loss on training batch is 0.000506259.
After 4015 training step(s), loss on training batch is 0.000464747.
After 4016 training step(s), loss on training batch is 0.000446263.
After 4017 training step(s), loss on training batch is 0.000552928.
After 4018 training step(s), loss on training batch is 0.000461937.
After 4019 training step(s), loss on training batch is 0.000573265.
After 4020 training step(s), loss on training batch is 0.000512638.
After 4021 training step(s), loss on training batch is 0.000439459.
After 4022 training step(s), loss on training batch is 0.000520199.
After 4023 training step(s), loss on training batch is 0.000439233.
After 4024 training step(s), loss on training batch is 0.000437989.
After 4025 training step(s), loss on training batch is 0.000416627.
After 4026 training step(s), loss on training batch is 0.00057516.
After 4027 training step(s), loss on training batch is 0.000432333.
After 4028 training step(s), loss on training batch is 0.00173625.
After 4029 training step(s), loss on training batch is 0.000982939.
After 4030 training step(s), loss on training batch is 0.000859822.
After 4031 training step(s), loss on training batch is 0.000901058.
After 4032 training step(s), loss on training batch is 0.00111851.
After 4033 training step(s), loss on training batch is 0.00115773.
After 4034 training step(s), loss on training batch is 0.000936953.
After 4035 training step(s), loss on training batch is 0.00076657.
After 4036 training step(s), loss on training batch is 0.00112133.
After 4037 training step(s), loss on training batch is 0.00113641.
After 4038 training step(s), loss on training batch is 0.000771381.
After 4039 training step(s), loss on training batch is 0.000786673.
After 4040 training step(s), loss on training batch is 0.00192355.
After 4041 training step(s), loss on training batch is 0.000867649.
After 4042 training step(s), loss on training batch is 0.000975636.
After 4043 training step(s), loss on training batch is 0.00104519.
After 4044 training step(s), loss on training batch is 0.00110422.
After 4045 training step(s), loss on training batch is 0.000774823.
After 4046 training step(s), loss on training batch is 0.00122598.
After 4047 training step(s), loss on training batch is 0.000856184.
After 4048 training step(s), loss on training batch is 0.00102779.
After 4049 training step(s), loss on training batch is 0.00099024.
After 4050 training step(s), loss on training batch is 0.0010537.
After 4051 training step(s), loss on training batch is 0.00107142.
After 4052 training step(s), loss on training batch is 0.00089388.
After 4053 training step(s), loss on training batch is 0.00122771.
After 4054 training step(s), loss on training batch is 0.000719101.
After 4055 training step(s), loss on training batch is 0.000997709.
After 4056 training step(s), loss on training batch is 0.00173802.
After 4057 training step(s), loss on training batch is 0.00136793.
After 4058 training step(s), loss on training batch is 0.00145254.
After 4059 training step(s), loss on training batch is 0.00142391.
After 4060 training step(s), loss on training batch is 0.00114851.
After 4061 training step(s), loss on training batch is 0.00115756.
After 4062 training step(s), loss on training batch is 0.00155884.
After 4063 training step(s), loss on training batch is 0.00136822.
After 4064 training step(s), loss on training batch is 0.00117218.
After 4065 training step(s), loss on training batch is 0.00183923.
After 4066 training step(s), loss on training batch is 0.00125999.
After 4067 training step(s), loss on training batch is 0.00121486.
After 4068 training step(s), loss on training batch is 0.00151796.
After 4069 training step(s), loss on training batch is 0.00260025.
After 4070 training step(s), loss on training batch is 0.00286931.
After 4071 training step(s), loss on training batch is 0.00198054.
After 4072 training step(s), loss on training batch is 0.00158512.
After 4073 training step(s), loss on training batch is 0.00173841.
After 4074 training step(s), loss on training batch is 0.00152881.
After 4075 training step(s), loss on training batch is 0.00147364.
After 4076 training step(s), loss on training batch is 0.00135754.
After 4077 training step(s), loss on training batch is 0.0024688.
After 4078 training step(s), loss on training batch is 0.00140568.
After 4079 training step(s), loss on training batch is 0.00131767.
After 4080 training step(s), loss on training batch is 0.00129697.
After 4081 training step(s), loss on training batch is 0.000850044.
After 4082 training step(s), loss on training batch is 0.000931601.
After 4083 training step(s), loss on training batch is 0.000499941.
After 4084 training step(s), loss on training batch is 0.000632289.
After 4085 training step(s), loss on training batch is 0.000721115.
After 4086 training step(s), loss on training batch is 0.000941519.
After 4087 training step(s), loss on training batch is 0.00116652.
After 4088 training step(s), loss on training batch is 0.0008316.
After 4089 training step(s), loss on training batch is 0.000593749.
After 4090 training step(s), loss on training batch is 0.000594728.
After 4091 training step(s), loss on training batch is 0.000519067.
After 4092 training step(s), loss on training batch is 0.000694285.
After 4093 training step(s), loss on training batch is 0.000484854.
After 4094 training step(s), loss on training batch is 0.000510133.
After 4095 training step(s), loss on training batch is 0.000659599.
After 4096 training step(s), loss on training batch is 0.000614134.
After 4097 training step(s), loss on training batch is 0.00046316.
After 4098 training step(s), loss on training batch is 0.000452569.
After 4099 training step(s), loss on training batch is 0.000455257.
After 4100 training step(s), loss on training batch is 0.000591996.
After 4101 training step(s), loss on training batch is 0.00170472.
After 4102 training step(s), loss on training batch is 0.000932414.
After 4103 training step(s), loss on training batch is 0.000885119.
After 4104 training step(s), loss on training batch is 0.00103441.
After 4105 training step(s), loss on training batch is 0.000874606.
After 4106 training step(s), loss on training batch is 0.000923697.
After 4107 training step(s), loss on training batch is 0.00101685.
After 4108 training step(s), loss on training batch is 0.000528932.
After 4109 training step(s), loss on training batch is 0.000565725.
After 4110 training step(s), loss on training batch is 0.000661676.
After 4111 training step(s), loss on training batch is 0.000828302.
After 4112 training step(s), loss on training batch is 0.000703282.
After 4113 training step(s), loss on training batch is 0.000715126.
After 4114 training step(s), loss on training batch is 0.00116032.
After 4115 training step(s), loss on training batch is 0.00232728.
After 4116 training step(s), loss on training batch is 0.00151188.
After 4117 training step(s), loss on training batch is 0.00129204.
After 4118 training step(s), loss on training batch is 0.00107489.
After 4119 training step(s), loss on training batch is 0.000554116.
After 4120 training step(s), loss on training batch is 0.000585498.
After 4121 training step(s), loss on training batch is 0.00059351.
After 4122 training step(s), loss on training batch is 0.000484795.
After 4123 training step(s), loss on training batch is 0.000809654.
After 4124 training step(s), loss on training batch is 0.000585334.
After 4125 training step(s), loss on training batch is 0.000582547.
After 4126 training step(s), loss on training batch is 0.000505729.
After 4127 training step(s), loss on training batch is 0.00173754.
After 4128 training step(s), loss on training batch is 0.00596005.
After 4129 training step(s), loss on training batch is 0.00152062.
After 4130 training step(s), loss on training batch is 0.000930284.
After 4131 training step(s), loss on training batch is 0.000767658.
After 4132 training step(s), loss on training batch is 0.000670021.
After 4133 training step(s), loss on training batch is 0.000636114.
After 4134 training step(s), loss on training batch is 0.000562008.
After 4135 training step(s), loss on training batch is 0.000508415.
After 4136 training step(s), loss on training batch is 0.000511756.
After 4137 training step(s), loss on training batch is 0.000580559.
After 4138 training step(s), loss on training batch is 0.000569154.
After 4139 training step(s), loss on training batch is 0.000600818.
After 4140 training step(s), loss on training batch is 0.000660141.
After 4141 training step(s), loss on training batch is 0.000470844.
After 4142 training step(s), loss on training batch is 0.000564764.
After 4143 training step(s), loss on training batch is 0.000498298.
After 4144 training step(s), loss on training batch is 0.00068291.
After 4145 training step(s), loss on training batch is 0.000429752.
After 4146 training step(s), loss on training batch is 0.000613962.
After 4147 training step(s), loss on training batch is 0.000484545.
After 4148 training step(s), loss on training batch is 0.000434892.
After 4149 training step(s), loss on training batch is 0.000495217.
After 4150 training step(s), loss on training batch is 0.000621914.
After 4151 training step(s), loss on training batch is 0.000586165.
After 4152 training step(s), loss on training batch is 0.000492995.
After 4153 training step(s), loss on training batch is 0.000499406.
After 4154 training step(s), loss on training batch is 0.000445139.
After 4155 training step(s), loss on training batch is 0.00047736.
After 4156 training step(s), loss on training batch is 0.000583534.
After 4157 training step(s), loss on training batch is 0.000706377.
After 4158 training step(s), loss on training batch is 0.000424876.
After 4159 training step(s), loss on training batch is 0.000497277.
After 4160 training step(s), loss on training batch is 0.000638687.
After 4161 training step(s), loss on training batch is 0.000470424.
After 4162 training step(s), loss on training batch is 0.000418293.
After 4163 training step(s), loss on training batch is 0.000482808.
After 4164 training step(s), loss on training batch is 0.00060598.
After 4165 training step(s), loss on training batch is 0.000514688.
After 4166 training step(s), loss on training batch is 0.000494925.
After 4167 training step(s), loss on training batch is 0.000549313.
After 4168 training step(s), loss on training batch is 0.00045211.
After 4169 training step(s), loss on training batch is 0.000511333.
After 4170 training step(s), loss on training batch is 0.000457813.
After 4171 training step(s), loss on training batch is 0.000636682.
After 4172 training step(s), loss on training batch is 0.000422968.
After 4173 training step(s), loss on training batch is 0.00044895.
After 4174 training step(s), loss on training batch is 0.000454601.
After 4175 training step(s), loss on training batch is 0.000534171.
After 4176 training step(s), loss on training batch is 0.000510613.
After 4177 training step(s), loss on training batch is 0.000734055.
After 4178 training step(s), loss on training batch is 0.000550985.
After 4179 training step(s), loss on training batch is 0.000533283.
After 4180 training step(s), loss on training batch is 0.000604941.
After 4181 training step(s), loss on training batch is 0.000458812.
After 4182 training step(s), loss on training batch is 0.000530831.
After 4183 training step(s), loss on training batch is 0.00112496.
After 4184 training step(s), loss on training batch is 0.00147712.
After 4185 training step(s), loss on training batch is 0.00189262.
After 4186 training step(s), loss on training batch is 0.00107854.
After 4187 training step(s), loss on training batch is 0.000995465.
After 4188 training step(s), loss on training batch is 0.000844423.
After 4189 training step(s), loss on training batch is 0.000989688.
After 4190 training step(s), loss on training batch is 0.000892135.
After 4191 training step(s), loss on training batch is 0.000955789.
After 4192 training step(s), loss on training batch is 0.00120437.
After 4193 training step(s), loss on training batch is 0.000767751.
After 4194 training step(s), loss on training batch is 0.000816299.
After 4195 training step(s), loss on training batch is 0.00148274.
After 4196 training step(s), loss on training batch is 0.00119588.
After 4197 training step(s), loss on training batch is 0.000939245.
After 4198 training step(s), loss on training batch is 0.00101336.
After 4199 training step(s), loss on training batch is 0.00157468.
After 4200 training step(s), loss on training batch is 0.000903125.
After 4201 training step(s), loss on training batch is 0.000976151.
After 4202 training step(s), loss on training batch is 0.000986086.
After 4203 training step(s), loss on training batch is 0.000877295.
After 4204 training step(s), loss on training batch is 0.000849575.
After 4205 training step(s), loss on training batch is 0.000759377.
After 4206 training step(s), loss on training batch is 0.000966809.
After 4207 training step(s), loss on training batch is 0.00138993.
After 4208 training step(s), loss on training batch is 0.00125532.
After 4209 training step(s), loss on training batch is 0.000778958.
After 4210 training step(s), loss on training batch is 0.00161871.
After 4211 training step(s), loss on training batch is 0.000997159.
After 4212 training step(s), loss on training batch is 0.0010363.
After 4213 training step(s), loss on training batch is 0.000840984.
After 4214 training step(s), loss on training batch is 0.000736745.
After 4215 training step(s), loss on training batch is 0.000737847.
After 4216 training step(s), loss on training batch is 0.000745071.
After 4217 training step(s), loss on training batch is 0.000774536.
After 4218 training step(s), loss on training batch is 0.00113313.
After 4219 training step(s), loss on training batch is 0.0015829.
After 4220 training step(s), loss on training batch is 0.00167074.
After 4221 training step(s), loss on training batch is 0.002503.
After 4222 training step(s), loss on training batch is 0.00108702.
After 4223 training step(s), loss on training batch is 0.00108723.
After 4224 training step(s), loss on training batch is 0.000912872.
After 4225 training step(s), loss on training batch is 0.00113695.
After 4226 training step(s), loss on training batch is 0.000973122.
After 4227 training step(s), loss on training batch is 0.000907326.
After 4228 training step(s), loss on training batch is 0.00093096.
After 4229 training step(s), loss on training batch is 0.000939944.
After 4230 training step(s), loss on training batch is 0.00113598.
After 4231 training step(s), loss on training batch is 0.00135987.
After 4232 training step(s), loss on training batch is 0.000814611.
After 4233 training step(s), loss on training batch is 0.000799619.
After 4234 training step(s), loss on training batch is 0.000774626.
After 4235 training step(s), loss on training batch is 0.000605855.
After 4236 training step(s), loss on training batch is 0.00114368.
After 4237 training step(s), loss on training batch is 0.00218363.
After 4238 training step(s), loss on training batch is 0.000739937.
After 4239 training step(s), loss on training batch is 0.000892403.
After 4240 training step(s), loss on training batch is 0.00102836.
After 4241 training step(s), loss on training batch is 0.000856564.
After 4242 training step(s), loss on training batch is 0.000810001.
After 4243 training step(s), loss on training batch is 0.00095084.
After 4244 training step(s), loss on training batch is 0.000857532.
After 4245 training step(s), loss on training batch is 0.000946644.
After 4246 training step(s), loss on training batch is 0.00106774.
After 4247 training step(s), loss on training batch is 0.000870273.
After 4248 training step(s), loss on training batch is 0.0010343.
After 4249 training step(s), loss on training batch is 0.000991991.
After 4250 training step(s), loss on training batch is 0.00108523.
After 4251 training step(s), loss on training batch is 0.000938737.
After 4252 training step(s), loss on training batch is 0.000723794.
After 4253 training step(s), loss on training batch is 0.0013633.
After 4254 training step(s), loss on training batch is 0.000913569.
After 4255 training step(s), loss on training batch is 0.000953216.
After 4256 training step(s), loss on training batch is 0.000782481.
After 4257 training step(s), loss on training batch is 0.000735177.
After 4258 training step(s), loss on training batch is 0.00118246.
After 4259 training step(s), loss on training batch is 0.00156846.
After 4260 training step(s), loss on training batch is 0.000995271.
After 4261 training step(s), loss on training batch is 0.00078287.
After 4262 training step(s), loss on training batch is 0.000757241.
After 4263 training step(s), loss on training batch is 0.000808752.
After 4264 training step(s), loss on training batch is 0.000692276.
After 4265 training step(s), loss on training batch is 0.00105052.
After 4266 training step(s), loss on training batch is 0.00160422.
After 4267 training step(s), loss on training batch is 0.00176379.
After 4268 training step(s), loss on training batch is 0.00149707.
After 4269 training step(s), loss on training batch is 0.00147363.
After 4270 training step(s), loss on training batch is 0.00139225.
After 4271 training step(s), loss on training batch is 0.001308.
After 4272 training step(s), loss on training batch is 0.00140085.
After 4273 training step(s), loss on training batch is 0.00300382.
After 4274 training step(s), loss on training batch is 0.00165554.
After 4275 training step(s), loss on training batch is 0.00190274.
After 4276 training step(s), loss on training batch is 0.00162264.
After 4277 training step(s), loss on training batch is 0.00141344.
After 4278 training step(s), loss on training batch is 0.00162449.
After 4279 training step(s), loss on training batch is 0.00128557.
After 4280 training step(s), loss on training batch is 0.00123875.
After 4281 training step(s), loss on training batch is 0.0012754.
After 4282 training step(s), loss on training batch is 0.0012281.
After 4283 training step(s), loss on training batch is 0.00137966.
After 4284 training step(s), loss on training batch is 0.00124065.
After 4285 training step(s), loss on training batch is 0.00120015.
After 4286 training step(s), loss on training batch is 0.0016755.
After 4287 training step(s), loss on training batch is 0.00123493.
After 4288 training step(s), loss on training batch is 0.0016002.
After 4289 training step(s), loss on training batch is 0.00129667.
After 4290 training step(s), loss on training batch is 0.00134433.
After 4291 training step(s), loss on training batch is 0.00122306.
After 4292 training step(s), loss on training batch is 0.00142014.
After 4293 training step(s), loss on training batch is 0.001208.
After 4294 training step(s), loss on training batch is 0.00135919.
After 4295 training step(s), loss on training batch is 0.00156963.
After 4296 training step(s), loss on training batch is 0.00152726.
After 4297 training step(s), loss on training batch is 0.00138457.
After 4298 training step(s), loss on training batch is 0.00130978.
After 4299 training step(s), loss on training batch is 0.00124868.
After 4300 training step(s), loss on training batch is 0.00127823.
After 4301 training step(s), loss on training batch is 0.0010832.
After 4302 training step(s), loss on training batch is 0.00137577.
After 4303 training step(s), loss on training batch is 0.00111144.
After 4304 training step(s), loss on training batch is 0.0018881.
After 4305 training step(s), loss on training batch is 0.00152747.
After 4306 training step(s), loss on training batch is 0.00156997.
After 4307 training step(s), loss on training batch is 0.00203536.
After 4308 training step(s), loss on training batch is 0.00191762.
After 4309 training step(s), loss on training batch is 0.00326645.
After 4310 training step(s), loss on training batch is 0.00211744.
After 4311 training step(s), loss on training batch is 0.00183304.
After 4312 training step(s), loss on training batch is 0.00179156.
After 4313 training step(s), loss on training batch is 0.00162823.
After 4314 training step(s), loss on training batch is 0.00198366.
After 4315 training step(s), loss on training batch is 0.00147476.
After 4316 training step(s), loss on training batch is 0.00065037.
After 4317 training step(s), loss on training batch is 0.000552595.
After 4318 training step(s), loss on training batch is 0.000550594.
After 4319 training step(s), loss on training batch is 0.000650897.
After 4320 training step(s), loss on training batch is 0.000813352.
After 4321 training step(s), loss on training batch is 0.000655195.
After 4322 training step(s), loss on training batch is 0.000656577.
After 4323 training step(s), loss on training batch is 0.000717527.
After 4324 training step(s), loss on training batch is 0.000754756.
After 4325 training step(s), loss on training batch is 0.000794553.
After 4326 training step(s), loss on training batch is 0.00116898.
After 4327 training step(s), loss on training batch is 0.00115818.
After 4328 training step(s), loss on training batch is 0.00146755.
After 4329 training step(s), loss on training batch is 0.00110179.
After 4330 training step(s), loss on training batch is 0.00349359.
After 4331 training step(s), loss on training batch is 0.00265756.
After 4332 training step(s), loss on training batch is 0.00163809.
After 4333 training step(s), loss on training batch is 0.00181075.
After 4334 training step(s), loss on training batch is 0.00132006.
After 4335 training step(s), loss on training batch is 0.00126536.
After 4336 training step(s), loss on training batch is 0.00133375.
After 4337 training step(s), loss on training batch is 0.00119102.
After 4338 training step(s), loss on training batch is 0.00141593.
After 4339 training step(s), loss on training batch is 0.0014232.
After 4340 training step(s), loss on training batch is 0.00133036.
After 4341 training step(s), loss on training batch is 0.00123206.
After 4342 training step(s), loss on training batch is 0.00193114.
After 4343 training step(s), loss on training batch is 0.000618422.
After 4344 training step(s), loss on training batch is 0.000896237.
After 4345 training step(s), loss on training batch is 0.000625889.
After 4346 training step(s), loss on training batch is 0.000884276.
After 4347 training step(s), loss on training batch is 0.000752182.
After 4348 training step(s), loss on training batch is 0.000697365.
After 4349 training step(s), loss on training batch is 0.000550047.
After 4350 training step(s), loss on training batch is 0.000472336.
After 4351 training step(s), loss on training batch is 0.000681161.
After 4352 training step(s), loss on training batch is 0.000665469.
After 4353 training step(s), loss on training batch is 0.000615063.
After 4354 training step(s), loss on training batch is 0.000503836.
After 4355 training step(s), loss on training batch is 0.000936748.
After 4356 training step(s), loss on training batch is 0.000551569.
After 4357 training step(s), loss on training batch is 0.000466003.
After 4358 training step(s), loss on training batch is 0.000527729.
After 4359 training step(s), loss on training batch is 0.00137864.
After 4360 training step(s), loss on training batch is 0.0017742.
After 4361 training step(s), loss on training batch is 0.000974986.
After 4362 training step(s), loss on training batch is 0.000608573.
After 4363 training step(s), loss on training batch is 0.000647191.
After 4364 training step(s), loss on training batch is 0.000862615.
After 4365 training step(s), loss on training batch is 0.000784835.
After 4366 training step(s), loss on training batch is 0.000661867.
After 4367 training step(s), loss on training batch is 0.000606566.
After 4368 training step(s), loss on training batch is 0.000742772.
After 4369 training step(s), loss on training batch is 0.000484878.
After 4370 training step(s), loss on training batch is 0.000613296.
After 4371 training step(s), loss on training batch is 0.000387809.
After 4372 training step(s), loss on training batch is 0.000576505.
After 4373 training step(s), loss on training batch is 0.000452445.
After 4374 training step(s), loss on training batch is 0.000429007.
After 4375 training step(s), loss on training batch is 0.000880314.
After 4376 training step(s), loss on training batch is 0.000753693.
After 4377 training step(s), loss on training batch is 0.000720093.
After 4378 training step(s), loss on training batch is 0.00049408.
After 4379 training step(s), loss on training batch is 0.000508927.
After 4380 training step(s), loss on training batch is 0.000434445.
After 4381 training step(s), loss on training batch is 0.000519799.
After 4382 training step(s), loss on training batch is 0.000475327.
After 4383 training step(s), loss on training batch is 0.000610929.
After 4384 training step(s), loss on training batch is 0.00057857.
After 4385 training step(s), loss on training batch is 0.000582033.
After 4386 training step(s), loss on training batch is 0.000815058.
After 4387 training step(s), loss on training batch is 0.000700122.
After 4388 training step(s), loss on training batch is 0.000460155.
After 4389 training step(s), loss on training batch is 0.000506026.
After 4390 training step(s), loss on training batch is 0.000459151.
After 4391 training step(s), loss on training batch is 0.000437456.
After 4392 training step(s), loss on training batch is 0.000492539.
After 4393 training step(s), loss on training batch is 0.000438602.
After 4394 training step(s), loss on training batch is 0.00038819.
After 4395 training step(s), loss on training batch is 0.000740189.
After 4396 training step(s), loss on training batch is 0.000624718.
After 4397 training step(s), loss on training batch is 0.000375443.
After 4398 training step(s), loss on training batch is 0.000363058.
After 4399 training step(s), loss on training batch is 0.000474588.
After 4400 training step(s), loss on training batch is 0.000597736.
After 4401 training step(s), loss on training batch is 0.000817533.
After 4402 training step(s), loss on training batch is 0.00086506.
After 4403 training step(s), loss on training batch is 0.000532752.
After 4404 training step(s), loss on training batch is 0.00076967.
After 4405 training step(s), loss on training batch is 0.00198245.
After 4406 training step(s), loss on training batch is 0.00116436.
After 4407 training step(s), loss on training batch is 0.000658589.
After 4408 training step(s), loss on training batch is 0.000792129.
After 4409 training step(s), loss on training batch is 0.00091028.
After 4410 training step(s), loss on training batch is 0.00095948.
After 4411 training step(s), loss on training batch is 0.000588913.
After 4412 training step(s), loss on training batch is 0.000469357.
After 4413 training step(s), loss on training batch is 0.000548874.
After 4414 training step(s), loss on training batch is 0.000530523.
After 4415 training step(s), loss on training batch is 0.000474314.
After 4416 training step(s), loss on training batch is 0.000412771.
After 4417 training step(s), loss on training batch is 0.000506069.
After 4418 training step(s), loss on training batch is 0.000436339.
After 4419 training step(s), loss on training batch is 0.000532689.
After 4420 training step(s), loss on training batch is 0.000482389.
After 4421 training step(s), loss on training batch is 0.000421116.
After 4422 training step(s), loss on training batch is 0.000482529.
After 4423 training step(s), loss on training batch is 0.000407014.
After 4424 training step(s), loss on training batch is 0.000417263.
After 4425 training step(s), loss on training batch is 0.000394689.
After 4426 training step(s), loss on training batch is 0.000550107.
After 4427 training step(s), loss on training batch is 0.000418379.
After 4428 training step(s), loss on training batch is 0.00182592.
After 4429 training step(s), loss on training batch is 0.00093652.
After 4430 training step(s), loss on training batch is 0.000803834.
After 4431 training step(s), loss on training batch is 0.000843442.
After 4432 training step(s), loss on training batch is 0.00105237.
After 4433 training step(s), loss on training batch is 0.00108857.
After 4434 training step(s), loss on training batch is 0.000887907.
After 4435 training step(s), loss on training batch is 0.000704844.
After 4436 training step(s), loss on training batch is 0.00106423.
After 4437 training step(s), loss on training batch is 0.00107451.
After 4438 training step(s), loss on training batch is 0.000727904.
After 4439 training step(s), loss on training batch is 0.000735813.
After 4440 training step(s), loss on training batch is 0.00180137.
After 4441 training step(s), loss on training batch is 0.000806879.
After 4442 training step(s), loss on training batch is 0.000901014.
After 4443 training step(s), loss on training batch is 0.00095896.
After 4444 training step(s), loss on training batch is 0.00105668.
After 4445 training step(s), loss on training batch is 0.000690664.
After 4446 training step(s), loss on training batch is 0.00122667.
After 4447 training step(s), loss on training batch is 0.000783392.
After 4448 training step(s), loss on training batch is 0.000940519.
After 4449 training step(s), loss on training batch is 0.000933308.
After 4450 training step(s), loss on training batch is 0.00101065.
After 4451 training step(s), loss on training batch is 0.00104602.
After 4452 training step(s), loss on training batch is 0.000857608.
After 4453 training step(s), loss on training batch is 0.00117591.
After 4454 training step(s), loss on training batch is 0.000659416.
After 4455 training step(s), loss on training batch is 0.000924865.
After 4456 training step(s), loss on training batch is 0.0016167.
After 4457 training step(s), loss on training batch is 0.0012906.
After 4458 training step(s), loss on training batch is 0.00137284.
After 4459 training step(s), loss on training batch is 0.00137513.
After 4460 training step(s), loss on training batch is 0.00115619.
After 4461 training step(s), loss on training batch is 0.00114287.
After 4462 training step(s), loss on training batch is 0.0014228.
After 4463 training step(s), loss on training batch is 0.00130847.
After 4464 training step(s), loss on training batch is 0.00116228.
After 4465 training step(s), loss on training batch is 0.00165953.
After 4466 training step(s), loss on training batch is 0.00115749.
After 4467 training step(s), loss on training batch is 0.00106475.
After 4468 training step(s), loss on training batch is 0.0015146.
After 4469 training step(s), loss on training batch is 0.00257688.
After 4470 training step(s), loss on training batch is 0.00273693.
After 4471 training step(s), loss on training batch is 0.00190626.
After 4472 training step(s), loss on training batch is 0.00152951.
After 4473 training step(s), loss on training batch is 0.00165309.
After 4474 training step(s), loss on training batch is 0.00144289.
After 4475 training step(s), loss on training batch is 0.0013864.
After 4476 training step(s), loss on training batch is 0.00124315.
After 4477 training step(s), loss on training batch is 0.00231508.
After 4478 training step(s), loss on training batch is 0.0013106.
After 4479 training step(s), loss on training batch is 0.00122327.
After 4480 training step(s), loss on training batch is 0.00121515.
After 4481 training step(s), loss on training batch is 0.000799928.
After 4482 training step(s), loss on training batch is 0.000858693.
After 4483 training step(s), loss on training batch is 0.000466701.
After 4484 training step(s), loss on training batch is 0.000585735.
After 4485 training step(s), loss on training batch is 0.000661787.
After 4486 training step(s), loss on training batch is 0.000853997.
After 4487 training step(s), loss on training batch is 0.00106624.
After 4488 training step(s), loss on training batch is 0.000761734.
After 4489 training step(s), loss on training batch is 0.00054954.
After 4490 training step(s), loss on training batch is 0.000553957.
After 4491 training step(s), loss on training batch is 0.000476706.
After 4492 training step(s), loss on training batch is 0.000632021.
After 4493 training step(s), loss on training batch is 0.000459236.
After 4494 training step(s), loss on training batch is 0.000497744.
After 4495 training step(s), loss on training batch is 0.000606566.
After 4496 training step(s), loss on training batch is 0.000571888.
After 4497 training step(s), loss on training batch is 0.000437711.
After 4498 training step(s), loss on training batch is 0.000424996.
After 4499 training step(s), loss on training batch is 0.00042723.
After 4500 training step(s), loss on training batch is 0.000534956.
After 4501 training step(s), loss on training batch is 0.00159324.
After 4502 training step(s), loss on training batch is 0.000865724.
After 4503 training step(s), loss on training batch is 0.000820114.
After 4504 training step(s), loss on training batch is 0.000992503.
After 4505 training step(s), loss on training batch is 0.00083167.
After 4506 training step(s), loss on training batch is 0.000878611.
After 4507 training step(s), loss on training batch is 0.000996061.
After 4508 training step(s), loss on training batch is 0.000494593.
After 4509 training step(s), loss on training batch is 0.000533075.
After 4510 training step(s), loss on training batch is 0.000616172.
After 4511 training step(s), loss on training batch is 0.000758358.
After 4512 training step(s), loss on training batch is 0.000657396.
After 4513 training step(s), loss on training batch is 0.000677855.
After 4514 training step(s), loss on training batch is 0.00115835.
After 4515 training step(s), loss on training batch is 0.00213432.
After 4516 training step(s), loss on training batch is 0.00141201.
After 4517 training step(s), loss on training batch is 0.00127079.
After 4518 training step(s), loss on training batch is 0.0010335.
After 4519 training step(s), loss on training batch is 0.000537659.
After 4520 training step(s), loss on training batch is 0.000562169.
After 4521 training step(s), loss on training batch is 0.000571809.
After 4522 training step(s), loss on training batch is 0.000471317.
After 4523 training step(s), loss on training batch is 0.000753608.
After 4524 training step(s), loss on training batch is 0.000580132.
After 4525 training step(s), loss on training batch is 0.000553352.
After 4526 training step(s), loss on training batch is 0.000483556.
After 4527 training step(s), loss on training batch is 0.00154228.
After 4528 training step(s), loss on training batch is 0.0057193.
After 4529 training step(s), loss on training batch is 0.00115008.
After 4530 training step(s), loss on training batch is 0.000790203.
After 4531 training step(s), loss on training batch is 0.000784145.
After 4532 training step(s), loss on training batch is 0.000616808.
After 4533 training step(s), loss on training batch is 0.000572744.
After 4534 training step(s), loss on training batch is 0.000524556.
After 4535 training step(s), loss on training batch is 0.000447698.
After 4536 training step(s), loss on training batch is 0.00045264.
After 4537 training step(s), loss on training batch is 0.000516916.
After 4538 training step(s), loss on training batch is 0.00050294.
After 4539 training step(s), loss on training batch is 0.000542739.
After 4540 training step(s), loss on training batch is 0.000599475.
After 4541 training step(s), loss on training batch is 0.000448013.
After 4542 training step(s), loss on training batch is 0.000539843.
After 4543 training step(s), loss on training batch is 0.000479263.
After 4544 training step(s), loss on training batch is 0.00065865.
After 4545 training step(s), loss on training batch is 0.00040987.
After 4546 training step(s), loss on training batch is 0.000585484.
After 4547 training step(s), loss on training batch is 0.00046714.
After 4548 training step(s), loss on training batch is 0.000417502.
After 4549 training step(s), loss on training batch is 0.00048194.
After 4550 training step(s), loss on training batch is 0.000589467.
After 4551 training step(s), loss on training batch is 0.000551206.
After 4552 training step(s), loss on training batch is 0.000476688.
After 4553 training step(s), loss on training batch is 0.000479079.
After 4554 training step(s), loss on training batch is 0.000437788.
After 4555 training step(s), loss on training batch is 0.00046167.
After 4556 training step(s), loss on training batch is 0.000542768.
After 4557 training step(s), loss on training batch is 0.000667273.
After 4558 training step(s), loss on training batch is 0.000411193.
After 4559 training step(s), loss on training batch is 0.000481248.
After 4560 training step(s), loss on training batch is 0.000612278.
After 4561 training step(s), loss on training batch is 0.000446766.
After 4562 training step(s), loss on training batch is 0.000401452.
After 4563 training step(s), loss on training batch is 0.000464337.
After 4564 training step(s), loss on training batch is 0.000587549.
After 4565 training step(s), loss on training batch is 0.000505705.
After 4566 training step(s), loss on training batch is 0.000483245.
After 4567 training step(s), loss on training batch is 0.00053571.
After 4568 training step(s), loss on training batch is 0.000443749.
After 4569 training step(s), loss on training batch is 0.000506177.
After 4570 training step(s), loss on training batch is 0.000444439.
After 4571 training step(s), loss on training batch is 0.000560668.
After 4572 training step(s), loss on training batch is 0.000409413.
After 4573 training step(s), loss on training batch is 0.000421028.
After 4574 training step(s), loss on training batch is 0.000431992.
After 4575 training step(s), loss on training batch is 0.000501573.
After 4576 training step(s), loss on training batch is 0.000484094.
After 4577 training step(s), loss on training batch is 0.000704048.
After 4578 training step(s), loss on training batch is 0.000528289.
After 4579 training step(s), loss on training batch is 0.00051138.
After 4580 training step(s), loss on training batch is 0.000557134.
After 4581 training step(s), loss on training batch is 0.000439113.
After 4582 training step(s), loss on training batch is 0.000500145.
After 4583 training step(s), loss on training batch is 0.00109338.
After 4584 training step(s), loss on training batch is 0.00136858.
After 4585 training step(s), loss on training batch is 0.00171592.
After 4586 training step(s), loss on training batch is 0.00100733.
After 4587 training step(s), loss on training batch is 0.000937652.
After 4588 training step(s), loss on training batch is 0.000805575.
After 4589 training step(s), loss on training batch is 0.000938264.
After 4590 training step(s), loss on training batch is 0.000839505.
After 4591 training step(s), loss on training batch is 0.00091665.
After 4592 training step(s), loss on training batch is 0.00122814.
After 4593 training step(s), loss on training batch is 0.000713037.
After 4594 training step(s), loss on training batch is 0.000750076.
After 4595 training step(s), loss on training batch is 0.00140545.
After 4596 training step(s), loss on training batch is 0.00111397.
After 4597 training step(s), loss on training batch is 0.000867388.
After 4598 training step(s), loss on training batch is 0.000960588.
After 4599 training step(s), loss on training batch is 0.00148017.
After 4600 training step(s), loss on training batch is 0.000895201.
After 4601 training step(s), loss on training batch is 0.000912183.
After 4602 training step(s), loss on training batch is 0.00096161.
After 4603 training step(s), loss on training batch is 0.000834783.
After 4604 training step(s), loss on training batch is 0.000798085.
After 4605 training step(s), loss on training batch is 0.000714622.
After 4606 training step(s), loss on training batch is 0.000918105.
After 4607 training step(s), loss on training batch is 0.00131297.
After 4608 training step(s), loss on training batch is 0.00118799.
After 4609 training step(s), loss on training batch is 0.000739082.
After 4610 training step(s), loss on training batch is 0.00153219.
After 4611 training step(s), loss on training batch is 0.000897335.
After 4612 training step(s), loss on training batch is 0.00097977.
After 4613 training step(s), loss on training batch is 0.000776791.
After 4614 training step(s), loss on training batch is 0.000680118.
After 4615 training step(s), loss on training batch is 0.000676829.
After 4616 training step(s), loss on training batch is 0.000668579.
After 4617 training step(s), loss on training batch is 0.000725143.
After 4618 training step(s), loss on training batch is 0.00114295.
After 4619 training step(s), loss on training batch is 0.00152102.
After 4620 training step(s), loss on training batch is 0.00164537.
After 4621 training step(s), loss on training batch is 0.0024777.
After 4622 training step(s), loss on training batch is 0.00100147.
After 4623 training step(s), loss on training batch is 0.000994472.
After 4624 training step(s), loss on training batch is 0.000929972.
After 4625 training step(s), loss on training batch is 0.00113742.
After 4626 training step(s), loss on training batch is 0.000978993.
After 4627 training step(s), loss on training batch is 0.000897256.
After 4628 training step(s), loss on training batch is 0.000901166.
After 4629 training step(s), loss on training batch is 0.000930933.
After 4630 training step(s), loss on training batch is 0.00106816.
After 4631 training step(s), loss on training batch is 0.00127051.
After 4632 training step(s), loss on training batch is 0.000791997.
After 4633 training step(s), loss on training batch is 0.000770125.
After 4634 training step(s), loss on training batch is 0.000750089.
After 4635 training step(s), loss on training batch is 0.000612729.
After 4636 training step(s), loss on training batch is 0.00110782.
After 4637 training step(s), loss on training batch is 0.00197547.
After 4638 training step(s), loss on training batch is 0.000701839.
After 4639 training step(s), loss on training batch is 0.000842017.
After 4640 training step(s), loss on training batch is 0.000968741.
After 4641 training step(s), loss on training batch is 0.000822241.
After 4642 training step(s), loss on training batch is 0.000763771.
After 4643 training step(s), loss on training batch is 0.000900106.
After 4644 training step(s), loss on training batch is 0.00078671.
After 4645 training step(s), loss on training batch is 0.000889289.
After 4646 training step(s), loss on training batch is 0.00106999.
After 4647 training step(s), loss on training batch is 0.000803823.
After 4648 training step(s), loss on training batch is 0.000962631.
After 4649 training step(s), loss on training batch is 0.000930654.
After 4650 training step(s), loss on training batch is 0.00102221.
After 4651 training step(s), loss on training batch is 0.000900498.
After 4652 training step(s), loss on training batch is 0.000697275.
After 4653 training step(s), loss on training batch is 0.00126173.
After 4654 training step(s), loss on training batch is 0.000862284.
After 4655 training step(s), loss on training batch is 0.000903637.
After 4656 training step(s), loss on training batch is 0.000760713.
After 4657 training step(s), loss on training batch is 0.000710572.
After 4658 training step(s), loss on training batch is 0.00110782.
After 4659 training step(s), loss on training batch is 0.00150971.
After 4660 training step(s), loss on training batch is 0.000917734.
After 4661 training step(s), loss on training batch is 0.000744092.
After 4662 training step(s), loss on training batch is 0.000715763.
After 4663 training step(s), loss on training batch is 0.00073357.
After 4664 training step(s), loss on training batch is 0.000631787.
After 4665 training step(s), loss on training batch is 0.00103198.
After 4666 training step(s), loss on training batch is 0.00148106.
After 4667 training step(s), loss on training batch is 0.00165972.
After 4668 training step(s), loss on training batch is 0.00139473.
After 4669 training step(s), loss on training batch is 0.00139121.
After 4670 training step(s), loss on training batch is 0.00135132.
After 4671 training step(s), loss on training batch is 0.00124358.
After 4672 training step(s), loss on training batch is 0.00133739.
After 4673 training step(s), loss on training batch is 0.00283145.
After 4674 training step(s), loss on training batch is 0.00154581.
After 4675 training step(s), loss on training batch is 0.00183432.
After 4676 training step(s), loss on training batch is 0.00134447.
After 4677 training step(s), loss on training batch is 0.00125264.
After 4678 training step(s), loss on training batch is 0.00154634.
After 4679 training step(s), loss on training batch is 0.00109293.
After 4680 training step(s), loss on training batch is 0.00117724.
After 4681 training step(s), loss on training batch is 0.00117334.
After 4682 training step(s), loss on training batch is 0.00115408.
After 4683 training step(s), loss on training batch is 0.00138257.
After 4684 training step(s), loss on training batch is 0.00119023.
After 4685 training step(s), loss on training batch is 0.00111766.
After 4686 training step(s), loss on training batch is 0.00167133.
After 4687 training step(s), loss on training batch is 0.00117455.
After 4688 training step(s), loss on training batch is 0.00153383.
After 4689 training step(s), loss on training batch is 0.00123609.
After 4690 training step(s), loss on training batch is 0.00125554.
After 4691 training step(s), loss on training batch is 0.00116103.
After 4692 training step(s), loss on training batch is 0.00133491.
After 4693 training step(s), loss on training batch is 0.00115274.
After 4694 training step(s), loss on training batch is 0.00129461.
After 4695 training step(s), loss on training batch is 0.0016196.
After 4696 training step(s), loss on training batch is 0.00147609.
After 4697 training step(s), loss on training batch is 0.00128286.
After 4698 training step(s), loss on training batch is 0.00123234.
After 4699 training step(s), loss on training batch is 0.00117023.
After 4700 training step(s), loss on training batch is 0.00120981.
After 4701 training step(s), loss on training batch is 0.0010407.
After 4702 training step(s), loss on training batch is 0.00130545.
After 4703 training step(s), loss on training batch is 0.00112452.
After 4704 training step(s), loss on training batch is 0.00162763.
After 4705 training step(s), loss on training batch is 0.0014632.
After 4706 training step(s), loss on training batch is 0.00151788.
After 4707 training step(s), loss on training batch is 0.00193566.
After 4708 training step(s), loss on training batch is 0.00139234.
After 4709 training step(s), loss on training batch is 0.00562389.
After 4710 training step(s), loss on training batch is 0.00218149.
After 4711 training step(s), loss on training batch is 0.00190545.
After 4712 training step(s), loss on training batch is 0.0018056.
After 4713 training step(s), loss on training batch is 0.00162036.
After 4714 training step(s), loss on training batch is 0.0018664.
After 4715 training step(s), loss on training batch is 0.00144867.
After 4716 training step(s), loss on training batch is 0.00062193.
After 4717 training step(s), loss on training batch is 0.000535553.
After 4718 training step(s), loss on training batch is 0.000528257.
After 4719 training step(s), loss on training batch is 0.000625053.
After 4720 training step(s), loss on training batch is 0.000770961.
After 4721 training step(s), loss on training batch is 0.000640528.
After 4722 training step(s), loss on training batch is 0.000620239.
After 4723 training step(s), loss on training batch is 0.000658032.
After 4724 training step(s), loss on training batch is 0.000705039.
After 4725 training step(s), loss on training batch is 0.000713278.
After 4726 training step(s), loss on training batch is 0.00110213.
After 4727 training step(s), loss on training batch is 0.00108107.
After 4728 training step(s), loss on training batch is 0.0013815.
After 4729 training step(s), loss on training batch is 0.00110778.
After 4730 training step(s), loss on training batch is 0.003128.
After 4731 training step(s), loss on training batch is 0.00249086.
After 4732 training step(s), loss on training batch is 0.00158022.
After 4733 training step(s), loss on training batch is 0.00170157.
After 4734 training step(s), loss on training batch is 0.00122954.
After 4735 training step(s), loss on training batch is 0.00120007.
After 4736 training step(s), loss on training batch is 0.00127625.
After 4737 training step(s), loss on training batch is 0.00114114.
After 4738 training step(s), loss on training batch is 0.00136698.
After 4739 training step(s), loss on training batch is 0.00134832.
After 4740 training step(s), loss on training batch is 0.0012554.
After 4741 training step(s), loss on training batch is 0.00117178.
After 4742 training step(s), loss on training batch is 0.00195405.
After 4743 training step(s), loss on training batch is 0.000567573.
After 4744 training step(s), loss on training batch is 0.000757079.
After 4745 training step(s), loss on training batch is 0.000582463.
After 4746 training step(s), loss on training batch is 0.000790502.
After 4747 training step(s), loss on training batch is 0.000696166.
After 4748 training step(s), loss on training batch is 0.000645435.
After 4749 training step(s), loss on training batch is 0.000510845.
After 4750 training step(s), loss on training batch is 0.000442155.
After 4751 training step(s), loss on training batch is 0.000709999.
After 4752 training step(s), loss on training batch is 0.000603611.
After 4753 training step(s), loss on training batch is 0.000610381.
After 4754 training step(s), loss on training batch is 0.000477412.
After 4755 training step(s), loss on training batch is 0.000862193.
After 4756 training step(s), loss on training batch is 0.000531867.
After 4757 training step(s), loss on training batch is 0.000461155.
After 4758 training step(s), loss on training batch is 0.000478313.
After 4759 training step(s), loss on training batch is 0.00126624.
After 4760 training step(s), loss on training batch is 0.00166075.
After 4761 training step(s), loss on training batch is 0.000905626.
After 4762 training step(s), loss on training batch is 0.000566428.
After 4763 training step(s), loss on training batch is 0.000587519.
After 4764 training step(s), loss on training batch is 0.000809894.
After 4765 training step(s), loss on training batch is 0.000728093.
After 4766 training step(s), loss on training batch is 0.000633988.
After 4767 training step(s), loss on training batch is 0.000556125.
After 4768 training step(s), loss on training batch is 0.000699078.
After 4769 training step(s), loss on training batch is 0.000466946.
After 4770 training step(s), loss on training batch is 0.000515866.
After 4771 training step(s), loss on training batch is 0.000332397.
After 4772 training step(s), loss on training batch is 0.000517228.
After 4773 training step(s), loss on training batch is 0.000407275.
After 4774 training step(s), loss on training batch is 0.000402708.
After 4775 training step(s), loss on training batch is 0.00089525.
After 4776 training step(s), loss on training batch is 0.000725964.
After 4777 training step(s), loss on training batch is 0.000663576.
After 4778 training step(s), loss on training batch is 0.000459757.
After 4779 training step(s), loss on training batch is 0.000476834.
After 4780 training step(s), loss on training batch is 0.000409345.
After 4781 training step(s), loss on training batch is 0.000489994.
After 4782 training step(s), loss on training batch is 0.00045082.
After 4783 training step(s), loss on training batch is 0.000575755.
After 4784 training step(s), loss on training batch is 0.000563508.
After 4785 training step(s), loss on training batch is 0.000559633.
After 4786 training step(s), loss on training batch is 0.00075235.
After 4787 training step(s), loss on training batch is 0.000661239.
After 4788 training step(s), loss on training batch is 0.000433272.
After 4789 training step(s), loss on training batch is 0.000480578.
After 4790 training step(s), loss on training batch is 0.000438451.
After 4791 training step(s), loss on training batch is 0.000415355.
After 4792 training step(s), loss on training batch is 0.000462953.
After 4793 training step(s), loss on training batch is 0.000413955.
After 4794 training step(s), loss on training batch is 0.000365616.
After 4795 training step(s), loss on training batch is 0.000714201.
After 4796 training step(s), loss on training batch is 0.000596479.
After 4797 training step(s), loss on training batch is 0.000353148.
After 4798 training step(s), loss on training batch is 0.000345146.
After 4799 training step(s), loss on training batch is 0.000449103.
After 4800 training step(s), loss on training batch is 0.000553856.
After 4801 training step(s), loss on training batch is 0.000773095.
After 4802 training step(s), loss on training batch is 0.00082162.
After 4803 training step(s), loss on training batch is 0.000502458.
After 4804 training step(s), loss on training batch is 0.000734275.
After 4805 training step(s), loss on training batch is 0.00210315.
After 4806 training step(s), loss on training batch is 0.00108354.
After 4807 training step(s), loss on training batch is 0.000603575.
After 4808 training step(s), loss on training batch is 0.000698391.
After 4809 training step(s), loss on training batch is 0.000871427.
After 4810 training step(s), loss on training batch is 0.000927785.
After 4811 training step(s), loss on training batch is 0.000562765.
After 4812 training step(s), loss on training batch is 0.000436864.
After 4813 training step(s), loss on training batch is 0.000478513.
After 4814 training step(s), loss on training batch is 0.00046843.
After 4815 training step(s), loss on training batch is 0.000431285.
After 4816 training step(s), loss on training batch is 0.000389014.
After 4817 training step(s), loss on training batch is 0.000508271.
After 4818 training step(s), loss on training batch is 0.000414437.
After 4819 training step(s), loss on training batch is 0.000508333.
After 4820 training step(s), loss on training batch is 0.000445751.
After 4821 training step(s), loss on training batch is 0.000411014.
After 4822 training step(s), loss on training batch is 0.000451416.
After 4823 training step(s), loss on training batch is 0.0004021.
After 4824 training step(s), loss on training batch is 0.000400621.
After 4825 training step(s), loss on training batch is 0.000378634.
After 4826 training step(s), loss on training batch is 0.00051466.
After 4827 training step(s), loss on training batch is 0.000380148.
After 4828 training step(s), loss on training batch is 0.00157832.
After 4829 training step(s), loss on training batch is 0.000862548.
After 4830 training step(s), loss on training batch is 0.000762491.
After 4831 training step(s), loss on training batch is 0.000796068.
After 4832 training step(s), loss on training batch is 0.00101626.
After 4833 training step(s), loss on training batch is 0.00103364.
After 4834 training step(s), loss on training batch is 0.000842668.
After 4835 training step(s), loss on training batch is 0.000673618.
After 4836 training step(s), loss on training batch is 0.000999245.
After 4837 training step(s), loss on training batch is 0.00101354.
After 4838 training step(s), loss on training batch is 0.000686468.
After 4839 training step(s), loss on training batch is 0.000688381.
After 4840 training step(s), loss on training batch is 0.00173391.
After 4841 training step(s), loss on training batch is 0.000765158.
After 4842 training step(s), loss on training batch is 0.000824827.
After 4843 training step(s), loss on training batch is 0.00087681.
After 4844 training step(s), loss on training batch is 0.000989446.
After 4845 training step(s), loss on training batch is 0.000690424.
After 4846 training step(s), loss on training batch is 0.00111119.
After 4847 training step(s), loss on training batch is 0.000768259.
After 4848 training step(s), loss on training batch is 0.000907689.
After 4849 training step(s), loss on training batch is 0.000887848.
After 4850 training step(s), loss on training batch is 0.000932896.
After 4851 training step(s), loss on training batch is 0.000955908.
After 4852 training step(s), loss on training batch is 0.000782776.
After 4853 training step(s), loss on training batch is 0.00110261.
After 4854 training step(s), loss on training batch is 0.000633687.
After 4855 training step(s), loss on training batch is 0.000884971.
After 4856 training step(s), loss on training batch is 0.00154957.
After 4857 training step(s), loss on training batch is 0.00122794.
After 4858 training step(s), loss on training batch is 0.00128826.
After 4859 training step(s), loss on training batch is 0.00127165.
After 4860 training step(s), loss on training batch is 0.00107104.
After 4861 training step(s), loss on training batch is 0.00108167.
After 4862 training step(s), loss on training batch is 0.00137937.
After 4863 training step(s), loss on training batch is 0.00124269.
After 4864 training step(s), loss on training batch is 0.00111164.
After 4865 training step(s), loss on training batch is 0.00157881.
After 4866 training step(s), loss on training batch is 0.00113189.
After 4867 training step(s), loss on training batch is 0.00109632.
After 4868 training step(s), loss on training batch is 0.00135394.
After 4869 training step(s), loss on training batch is 0.0022511.
After 4870 training step(s), loss on training batch is 0.00288464.
After 4871 training step(s), loss on training batch is 0.00185628.
After 4872 training step(s), loss on training batch is 0.00146153.
After 4873 training step(s), loss on training batch is 0.00160145.
After 4874 training step(s), loss on training batch is 0.0013298.
After 4875 training step(s), loss on training batch is 0.00131896.
After 4876 training step(s), loss on training batch is 0.00119167.
After 4877 training step(s), loss on training batch is 0.00215466.
After 4878 training step(s), loss on training batch is 0.00124365.
After 4879 training step(s), loss on training batch is 0.00116311.
After 4880 training step(s), loss on training batch is 0.00116429.
After 4881 training step(s), loss on training batch is 0.000752244.
After 4882 training step(s), loss on training batch is 0.000781695.
After 4883 training step(s), loss on training batch is 0.000416414.
After 4884 training step(s), loss on training batch is 0.000559694.
After 4885 training step(s), loss on training batch is 0.000632545.
After 4886 training step(s), loss on training batch is 0.000829363.
After 4887 training step(s), loss on training batch is 0.00102169.
After 4888 training step(s), loss on training batch is 0.0007172.
After 4889 training step(s), loss on training batch is 0.000529423.
After 4890 training step(s), loss on training batch is 0.00052898.
After 4891 training step(s), loss on training batch is 0.000438217.
After 4892 training step(s), loss on training batch is 0.000597196.
After 4893 training step(s), loss on training batch is 0.000416841.
After 4894 training step(s), loss on training batch is 0.000451281.
After 4895 training step(s), loss on training batch is 0.000557109.
After 4896 training step(s), loss on training batch is 0.000533902.
After 4897 training step(s), loss on training batch is 0.000401778.
After 4898 training step(s), loss on training batch is 0.000395444.
After 4899 training step(s), loss on training batch is 0.000403249.
After 4900 training step(s), loss on training batch is 0.000504578.
After 4901 training step(s), loss on training batch is 0.00158275.
After 4902 training step(s), loss on training batch is 0.000814403.
After 4903 training step(s), loss on training batch is 0.000759828.
After 4904 training step(s), loss on training batch is 0.000915192.
After 4905 training step(s), loss on training batch is 0.000784789.
After 4906 training step(s), loss on training batch is 0.000832412.
After 4907 training step(s), loss on training batch is 0.000905353.
After 4908 training step(s), loss on training batch is 0.000445815.
After 4909 training step(s), loss on training batch is 0.000480483.
After 4910 training step(s), loss on training batch is 0.00057413.
After 4911 training step(s), loss on training batch is 0.000667792.
After 4912 training step(s), loss on training batch is 0.000629252.
After 4913 training step(s), loss on training batch is 0.000671131.
After 4914 training step(s), loss on training batch is 0.00108736.
After 4915 training step(s), loss on training batch is 0.00202574.
After 4916 training step(s), loss on training batch is 0.00136256.
After 4917 training step(s), loss on training batch is 0.00111542.
After 4918 training step(s), loss on training batch is 0.000922665.
After 4919 training step(s), loss on training batch is 0.00051266.
After 4920 training step(s), loss on training batch is 0.000533514.
After 4921 training step(s), loss on training batch is 0.000535193.
After 4922 training step(s), loss on training batch is 0.000436172.
After 4923 training step(s), loss on training batch is 0.000726315.
After 4924 training step(s), loss on training batch is 0.000535657.
After 4925 training step(s), loss on training batch is 0.000523972.
After 4926 training step(s), loss on training batch is 0.000452856.
After 4927 training step(s), loss on training batch is 0.00154862.
After 4928 training step(s), loss on training batch is 0.00489293.
After 4929 training step(s), loss on training batch is 0.00143981.
After 4930 training step(s), loss on training batch is 0.00084934.
After 4931 training step(s), loss on training batch is 0.000703151.
After 4932 training step(s), loss on training batch is 0.000632482.
After 4933 training step(s), loss on training batch is 0.000589122.
After 4934 training step(s), loss on training batch is 0.000502886.
After 4935 training step(s), loss on training batch is 0.000450996.
After 4936 training step(s), loss on training batch is 0.000418563.
After 4937 training step(s), loss on training batch is 0.000471803.
After 4938 training step(s), loss on training batch is 0.000464175.
After 4939 training step(s), loss on training batch is 0.000508544.
After 4940 training step(s), loss on training batch is 0.000555518.
After 4941 training step(s), loss on training batch is 0.000420815.
After 4942 training step(s), loss on training batch is 0.000511117.
After 4943 training step(s), loss on training batch is 0.00046004.
After 4944 training step(s), loss on training batch is 0.000631238.
After 4945 training step(s), loss on training batch is 0.000395231.
After 4946 training step(s), loss on training batch is 0.000567908.
After 4947 training step(s), loss on training batch is 0.000447606.
After 4948 training step(s), loss on training batch is 0.000399627.
After 4949 training step(s), loss on training batch is 0.000459268.
After 4950 training step(s), loss on training batch is 0.00056066.
After 4951 training step(s), loss on training batch is 0.000528186.
After 4952 training step(s), loss on training batch is 0.000462581.
After 4953 training step(s), loss on training batch is 0.000463828.
After 4954 training step(s), loss on training batch is 0.000421549.
After 4955 training step(s), loss on training batch is 0.000434237.
After 4956 training step(s), loss on training batch is 0.000545221.
After 4957 training step(s), loss on training batch is 0.000659729.
After 4958 training step(s), loss on training batch is 0.00040026.
After 4959 training step(s), loss on training batch is 0.000461616.
After 4960 training step(s), loss on training batch is 0.000597024.
After 4961 training step(s), loss on training batch is 0.000417038.
After 4962 training step(s), loss on training batch is 0.000377386.
After 4963 training step(s), loss on training batch is 0.000430527.
After 4964 training step(s), loss on training batch is 0.000558326.
After 4965 training step(s), loss on training batch is 0.000485178.
After 4966 training step(s), loss on training batch is 0.000470351.
After 4967 training step(s), loss on training batch is 0.00051645.
After 4968 training step(s), loss on training batch is 0.00043698.
After 4969 training step(s), loss on training batch is 0.000489877.
After 4970 training step(s), loss on training batch is 0.000427419.
After 4971 training step(s), loss on training batch is 0.000535513.
After 4972 training step(s), loss on training batch is 0.000400542.
After 4973 training step(s), loss on training batch is 0.000401932.
After 4974 training step(s), loss on training batch is 0.000408892.
After 4975 training step(s), loss on training batch is 0.000485512.
After 4976 training step(s), loss on training batch is 0.000464518.
After 4977 training step(s), loss on training batch is 0.000668426.
After 4978 training step(s), loss on training batch is 0.000499641.
After 4979 training step(s), loss on training batch is 0.00047464.
After 4980 training step(s), loss on training batch is 0.000538779.
After 4981 training step(s), loss on training batch is 0.000423899.
After 4982 training step(s), loss on training batch is 0.000477759.
After 4983 training step(s), loss on training batch is 0.00101381.
After 4984 training step(s), loss on training batch is 0.00127738.
After 4985 training step(s), loss on training batch is 0.00163353.
After 4986 training step(s), loss on training batch is 0.000922376.
After 4987 training step(s), loss on training batch is 0.000913776.
After 4988 training step(s), loss on training batch is 0.000752699.
After 4989 training step(s), loss on training batch is 0.000861142.
After 4990 training step(s), loss on training batch is 0.000804823.
After 4991 training step(s), loss on training batch is 0.000893755.
After 4992 training step(s), loss on training batch is 0.00107552.
After 4993 training step(s), loss on training batch is 0.000701362.
After 4994 training step(s), loss on training batch is 0.000740724.
After 4995 training step(s), loss on training batch is 0.00130437.
After 4996 training step(s), loss on training batch is 0.00105159.
After 4997 training step(s), loss on training batch is 0.000840838.
After 4998 training step(s), loss on training batch is 0.000902819.
After 4999 training step(s), loss on training batch is 0.00139175.
After 5000 training step(s), loss on training batch is 0.000842358.
After 5001 training step(s), loss on training batch is 0.000870553.
After 5002 training step(s), loss on training batch is 0.000947582.
After 5003 training step(s), loss on training batch is 0.000793777.
After 5004 training step(s), loss on training batch is 0.000759062.
After 5005 training step(s), loss on training batch is 0.000689367.
After 5006 training step(s), loss on training batch is 0.000873784.
After 5007 training step(s), loss on training batch is 0.00125834.
After 5008 training step(s), loss on training batch is 0.00112334.
After 5009 training step(s), loss on training batch is 0.000705903.
After 5010 training step(s), loss on training batch is 0.0014325.
After 5011 training step(s), loss on training batch is 0.000885985.
After 5012 training step(s), loss on training batch is 0.000921487.
After 5013 training step(s), loss on training batch is 0.000744722.
After 5014 training step(s), loss on training batch is 0.000673938.
After 5015 training step(s), loss on training batch is 0.000669382.
After 5016 training step(s), loss on training batch is 0.000683071.
After 5017 training step(s), loss on training batch is 0.00069678.
After 5018 training step(s), loss on training batch is 0.00103417.
After 5019 training step(s), loss on training batch is 0.00142616.
After 5020 training step(s), loss on training batch is 0.00155003.
After 5021 training step(s), loss on training batch is 0.00222974.
After 5022 training step(s), loss on training batch is 0.00097351.
After 5023 training step(s), loss on training batch is 0.000956553.
After 5024 training step(s), loss on training batch is 0.000897454.
After 5025 training step(s), loss on training batch is 0.00108603.
After 5026 training step(s), loss on training batch is 0.000970777.
After 5027 training step(s), loss on training batch is 0.000876821.
After 5028 training step(s), loss on training batch is 0.000858913.
After 5029 training step(s), loss on training batch is 0.000885298.
After 5030 training step(s), loss on training batch is 0.00100865.
After 5031 training step(s), loss on training batch is 0.00122299.
After 5032 training step(s), loss on training batch is 0.000738852.
After 5033 training step(s), loss on training batch is 0.000726379.
After 5034 training step(s), loss on training batch is 0.000700091.
After 5035 training step(s), loss on training batch is 0.000579052.
After 5036 training step(s), loss on training batch is 0.0010347.
After 5037 training step(s), loss on training batch is 0.00205466.
After 5038 training step(s), loss on training batch is 0.000671239.
After 5039 training step(s), loss on training batch is 0.000798282.
After 5040 training step(s), loss on training batch is 0.000909277.
After 5041 training step(s), loss on training batch is 0.000771117.
After 5042 training step(s), loss on training batch is 0.00072854.
After 5043 training step(s), loss on training batch is 0.000859524.
After 5044 training step(s), loss on training batch is 0.000774459.
After 5045 training step(s), loss on training batch is 0.000855843.
After 5046 training step(s), loss on training batch is 0.00100566.
After 5047 training step(s), loss on training batch is 0.000790972.
After 5048 training step(s), loss on training batch is 0.000894057.
After 5049 training step(s), loss on training batch is 0.000884938.
After 5050 training step(s), loss on training batch is 0.000968889.
After 5051 training step(s), loss on training batch is 0.000832596.
After 5052 training step(s), loss on training batch is 0.000658991.
After 5053 training step(s), loss on training batch is 0.00120701.
After 5054 training step(s), loss on training batch is 0.000816385.
After 5055 training step(s), loss on training batch is 0.000863463.
After 5056 training step(s), loss on training batch is 0.000737374.
After 5057 training step(s), loss on training batch is 0.000684303.
After 5058 training step(s), loss on training batch is 0.00103812.
After 5059 training step(s), loss on training batch is 0.00145929.
After 5060 training step(s), loss on training batch is 0.000876855.
After 5061 training step(s), loss on training batch is 0.000687584.
After 5062 training step(s), loss on training batch is 0.000671419.
After 5063 training step(s), loss on training batch is 0.000708042.
After 5064 training step(s), loss on training batch is 0.00061958.
After 5065 training step(s), loss on training batch is 0.000960102.
After 5066 training step(s), loss on training batch is 0.00144453.
After 5067 training step(s), loss on training batch is 0.0015679.
After 5068 training step(s), loss on training batch is 0.00136188.
After 5069 training step(s), loss on training batch is 0.00135118.
After 5070 training step(s), loss on training batch is 0.0012847.
After 5071 training step(s), loss on training batch is 0.00119869.
After 5072 training step(s), loss on training batch is 0.00128452.
After 5073 training step(s), loss on training batch is 0.00272584.
After 5074 training step(s), loss on training batch is 0.00149532.
After 5075 training step(s), loss on training batch is 0.00174918.
After 5076 training step(s), loss on training batch is 0.00140541.
After 5077 training step(s), loss on training batch is 0.00126698.
After 5078 training step(s), loss on training batch is 0.0014352.
After 5079 training step(s), loss on training batch is 0.00116217.
After 5080 training step(s), loss on training batch is 0.00113458.
After 5081 training step(s), loss on training batch is 0.0011492.
After 5082 training step(s), loss on training batch is 0.00111499.
After 5083 training step(s), loss on training batch is 0.00128873.
After 5084 training step(s), loss on training batch is 0.00113803.
After 5085 training step(s), loss on training batch is 0.00111611.
After 5086 training step(s), loss on training batch is 0.00152732.
After 5087 training step(s), loss on training batch is 0.00115972.
After 5088 training step(s), loss on training batch is 0.00144794.
After 5089 training step(s), loss on training batch is 0.00118051.
After 5090 training step(s), loss on training batch is 0.00117628.
After 5091 training step(s), loss on training batch is 0.00110263.
After 5092 training step(s), loss on training batch is 0.00129411.
After 5093 training step(s), loss on training batch is 0.0011072.
After 5094 training step(s), loss on training batch is 0.00124343.
After 5095 training step(s), loss on training batch is 0.00140198.
After 5096 training step(s), loss on training batch is 0.0014195.
After 5097 training step(s), loss on training batch is 0.00124787.
After 5098 training step(s), loss on training batch is 0.00118116.
After 5099 training step(s), loss on training batch is 0.00112382.
After 5100 training step(s), loss on training batch is 0.00115281.
After 5101 training step(s), loss on training batch is 0.000994679.
After 5102 training step(s), loss on training batch is 0.00126934.
After 5103 training step(s), loss on training batch is 0.00110074.
After 5104 training step(s), loss on training batch is 0.00151221.
After 5105 training step(s), loss on training batch is 0.00141644.
After 5106 training step(s), loss on training batch is 0.00146947.
After 5107 training step(s), loss on training batch is 0.00171958.
After 5108 training step(s), loss on training batch is 0.00136204.
After 5109 training step(s), loss on training batch is 0.00591922.
After 5110 training step(s), loss on training batch is 0.00225373.
After 5111 training step(s), loss on training batch is 0.00190083.
After 5112 training step(s), loss on training batch is 0.00179732.
After 5113 training step(s), loss on training batch is 0.00150986.
After 5114 training step(s), loss on training batch is 0.00198957.
After 5115 training step(s), loss on training batch is 0.00134882.
After 5116 training step(s), loss on training batch is 0.000582206.
After 5117 training step(s), loss on training batch is 0.000511644.
After 5118 training step(s), loss on training batch is 0.000507098.
After 5119 training step(s), loss on training batch is 0.000592036.
After 5120 training step(s), loss on training batch is 0.000724183.
After 5121 training step(s), loss on training batch is 0.000603625.
After 5122 training step(s), loss on training batch is 0.000596589.
After 5123 training step(s), loss on training batch is 0.000631365.
After 5124 training step(s), loss on training batch is 0.000669228.
After 5125 training step(s), loss on training batch is 0.000666094.
After 5126 training step(s), loss on training batch is 0.00107013.
After 5127 training step(s), loss on training batch is 0.00107077.
After 5128 training step(s), loss on training batch is 0.00133146.
After 5129 training step(s), loss on training batch is 0.00109356.
After 5130 training step(s), loss on training batch is 0.00285519.
After 5131 training step(s), loss on training batch is 0.00230395.
After 5132 training step(s), loss on training batch is 0.00154094.
After 5133 training step(s), loss on training batch is 0.00156965.
After 5134 training step(s), loss on training batch is 0.00122009.
After 5135 training step(s), loss on training batch is 0.00115376.
After 5136 training step(s), loss on training batch is 0.00123716.
After 5137 training step(s), loss on training batch is 0.00114293.
After 5138 training step(s), loss on training batch is 0.00132807.
After 5139 training step(s), loss on training batch is 0.00127611.
After 5140 training step(s), loss on training batch is 0.00119927.
After 5141 training step(s), loss on training batch is 0.00112083.
After 5142 training step(s), loss on training batch is 0.00195106.
After 5143 training step(s), loss on training batch is 0.000550437.
After 5144 training step(s), loss on training batch is 0.000684246.
After 5145 training step(s), loss on training batch is 0.000549581.
After 5146 training step(s), loss on training batch is 0.000733382.
After 5147 training step(s), loss on training batch is 0.000678215.
After 5148 training step(s), loss on training batch is 0.000618807.
After 5149 training step(s), loss on training batch is 0.000507322.
After 5150 training step(s), loss on training batch is 0.000425814.
After 5151 training step(s), loss on training batch is 0.000640423.
After 5152 training step(s), loss on training batch is 0.000599664.
After 5153 training step(s), loss on training batch is 0.00056334.
After 5154 training step(s), loss on training batch is 0.000465317.
After 5155 training step(s), loss on training batch is 0.000804086.
After 5156 training step(s), loss on training batch is 0.000517475.
After 5157 training step(s), loss on training batch is 0.000450268.
After 5158 training step(s), loss on training batch is 0.000447377.
After 5159 training step(s), loss on training batch is 0.00112802.
After 5160 training step(s), loss on training batch is 0.00151111.
After 5161 training step(s), loss on training batch is 0.000845887.
After 5162 training step(s), loss on training batch is 0.000569424.
After 5163 training step(s), loss on training batch is 0.000576369.
After 5164 training step(s), loss on training batch is 0.000765072.
After 5165 training step(s), loss on training batch is 0.000702057.
After 5166 training step(s), loss on training batch is 0.000592506.
After 5167 training step(s), loss on training batch is 0.000524173.
After 5168 training step(s), loss on training batch is 0.000669746.
After 5169 training step(s), loss on training batch is 0.000442273.
After 5170 training step(s), loss on training batch is 0.0005502.
After 5171 training step(s), loss on training batch is 0.000356793.
After 5172 training step(s), loss on training batch is 0.000460661.
After 5173 training step(s), loss on training batch is 0.000425395.
After 5174 training step(s), loss on training batch is 0.000390617.
After 5175 training step(s), loss on training batch is 0.000716876.
After 5176 training step(s), loss on training batch is 0.000635232.
After 5177 training step(s), loss on training batch is 0.000636901.
After 5178 training step(s), loss on training batch is 0.000449542.
After 5179 training step(s), loss on training batch is 0.000464026.
After 5180 training step(s), loss on training batch is 0.000395346.
After 5181 training step(s), loss on training batch is 0.000470704.
After 5182 training step(s), loss on training batch is 0.000438349.
After 5183 training step(s), loss on training batch is 0.000536832.
After 5184 training step(s), loss on training batch is 0.000547166.
After 5185 training step(s), loss on training batch is 0.000528663.
After 5186 training step(s), loss on training batch is 0.000753575.
After 5187 training step(s), loss on training batch is 0.000617155.
After 5188 training step(s), loss on training batch is 0.000418342.
After 5189 training step(s), loss on training batch is 0.000473275.
After 5190 training step(s), loss on training batch is 0.000429635.
After 5191 training step(s), loss on training batch is 0.000396044.
After 5192 training step(s), loss on training batch is 0.000445943.
After 5193 training step(s), loss on training batch is 0.00040335.
After 5194 training step(s), loss on training batch is 0.000353154.
After 5195 training step(s), loss on training batch is 0.000666622.
After 5196 training step(s), loss on training batch is 0.000569747.
After 5197 training step(s), loss on training batch is 0.000337621.
After 5198 training step(s), loss on training batch is 0.000334316.
After 5199 training step(s), loss on training batch is 0.00043985.
After 5200 training step(s), loss on training batch is 0.000529588.
After 5201 training step(s), loss on training batch is 0.000734686.
After 5202 training step(s), loss on training batch is 0.000787289.
After 5203 training step(s), loss on training batch is 0.000485963.
After 5204 training step(s), loss on training batch is 0.000690619.
After 5205 training step(s), loss on training batch is 0.00170922.
After 5206 training step(s), loss on training batch is 0.00102638.
After 5207 training step(s), loss on training batch is 0.000587103.
After 5208 training step(s), loss on training batch is 0.000707715.
After 5209 training step(s), loss on training batch is 0.000827905.
After 5210 training step(s), loss on training batch is 0.000861817.
After 5211 training step(s), loss on training batch is 0.000536608.
After 5212 training step(s), loss on training batch is 0.000420404.
After 5213 training step(s), loss on training batch is 0.00048407.
After 5214 training step(s), loss on training batch is 0.000473753.
After 5215 training step(s), loss on training batch is 0.000410983.
After 5216 training step(s), loss on training batch is 0.000375031.
After 5217 training step(s), loss on training batch is 0.000478305.
After 5218 training step(s), loss on training batch is 0.00040315.
After 5219 training step(s), loss on training batch is 0.000508014.
After 5220 training step(s), loss on training batch is 0.000452895.
After 5221 training step(s), loss on training batch is 0.00039071.
After 5222 training step(s), loss on training batch is 0.00043646.
After 5223 training step(s), loss on training batch is 0.000381573.
After 5224 training step(s), loss on training batch is 0.000383479.
After 5225 training step(s), loss on training batch is 0.000364922.
After 5226 training step(s), loss on training batch is 0.000491876.
After 5227 training step(s), loss on training batch is 0.000369338.
After 5228 training step(s), loss on training batch is 0.00149388.
After 5229 training step(s), loss on training batch is 0.000823967.
After 5230 training step(s), loss on training batch is 0.000744457.
After 5231 training step(s), loss on training batch is 0.000769685.
After 5232 training step(s), loss on training batch is 0.000920493.
After 5233 training step(s), loss on training batch is 0.000972474.
After 5234 training step(s), loss on training batch is 0.000796452.
After 5235 training step(s), loss on training batch is 0.000655939.
After 5236 training step(s), loss on training batch is 0.000942359.
After 5237 training step(s), loss on training batch is 0.00096479.
After 5238 training step(s), loss on training batch is 0.000660351.
After 5239 training step(s), loss on training batch is 0.000669798.
After 5240 training step(s), loss on training batch is 0.00158953.
After 5241 training step(s), loss on training batch is 0.000741153.
After 5242 training step(s), loss on training batch is 0.000799784.
After 5243 training step(s), loss on training batch is 0.000849516.
After 5244 training step(s), loss on training batch is 0.000947623.
After 5245 training step(s), loss on training batch is 0.000669838.
After 5246 training step(s), loss on training batch is 0.00101928.
After 5247 training step(s), loss on training batch is 0.000740998.
After 5248 training step(s), loss on training batch is 0.000872018.
After 5249 training step(s), loss on training batch is 0.000849428.
After 5250 training step(s), loss on training batch is 0.000883077.
After 5251 training step(s), loss on training batch is 0.000895965.
After 5252 training step(s), loss on training batch is 0.000754047.
After 5253 training step(s), loss on training batch is 0.00105436.
After 5254 training step(s), loss on training batch is 0.000615681.
After 5255 training step(s), loss on training batch is 0.000853437.
After 5256 training step(s), loss on training batch is 0.00148138.
After 5257 training step(s), loss on training batch is 0.00120853.
After 5258 training step(s), loss on training batch is 0.00128155.
After 5259 training step(s), loss on training batch is 0.00127569.
After 5260 training step(s), loss on training batch is 0.00111476.
After 5261 training step(s), loss on training batch is 0.00110278.
After 5262 training step(s), loss on training batch is 0.00131465.
After 5263 training step(s), loss on training batch is 0.00122672.
After 5264 training step(s), loss on training batch is 0.00103026.
After 5265 training step(s), loss on training batch is 0.00160853.
After 5266 training step(s), loss on training batch is 0.00109036.
After 5267 training step(s), loss on training batch is 0.0010663.
After 5268 training step(s), loss on training batch is 0.00130898.
After 5269 training step(s), loss on training batch is 0.00224481.
After 5270 training step(s), loss on training batch is 0.00293099.
After 5271 training step(s), loss on training batch is 0.00179526.
After 5272 training step(s), loss on training batch is 0.00139818.
After 5273 training step(s), loss on training batch is 0.00152118.
After 5274 training step(s), loss on training batch is 0.00130669.
After 5275 training step(s), loss on training batch is 0.00126993.
After 5276 training step(s), loss on training batch is 0.00110643.
After 5277 training step(s), loss on training batch is 0.00216506.
After 5278 training step(s), loss on training batch is 0.00117825.
After 5279 training step(s), loss on training batch is 0.00107719.
After 5280 training step(s), loss on training batch is 0.00109113.
After 5281 training step(s), loss on training batch is 0.000790674.
After 5282 training step(s), loss on training batch is 0.000747388.
After 5283 training step(s), loss on training batch is 0.000407202.
After 5284 training step(s), loss on training batch is 0.00052587.
After 5285 training step(s), loss on training batch is 0.000593533.
After 5286 training step(s), loss on training batch is 0.000848443.
After 5287 training step(s), loss on training batch is 0.00100068.
After 5288 training step(s), loss on training batch is 0.0006709.
After 5289 training step(s), loss on training batch is 0.000493584.
After 5290 training step(s), loss on training batch is 0.000490127.
After 5291 training step(s), loss on training batch is 0.000413651.
After 5292 training step(s), loss on training batch is 0.000547557.
After 5293 training step(s), loss on training batch is 0.000411025.
After 5294 training step(s), loss on training batch is 0.00042964.
After 5295 training step(s), loss on training batch is 0.000530126.
After 5296 training step(s), loss on training batch is 0.000512285.
After 5297 training step(s), loss on training batch is 0.000388701.
After 5298 training step(s), loss on training batch is 0.000381896.
After 5299 training step(s), loss on training batch is 0.0003852.
After 5300 training step(s), loss on training batch is 0.000486638.
After 5301 training step(s), loss on training batch is 0.00144399.
After 5302 training step(s), loss on training batch is 0.000761531.
After 5303 training step(s), loss on training batch is 0.000736173.
After 5304 training step(s), loss on training batch is 0.000891874.
After 5305 training step(s), loss on training batch is 0.000773876.
After 5306 training step(s), loss on training batch is 0.000788215.
After 5307 training step(s), loss on training batch is 0.000834585.
After 5308 training step(s), loss on training batch is 0.000455882.
After 5309 training step(s), loss on training batch is 0.000491402.
After 5310 training step(s), loss on training batch is 0.000548553.
After 5311 training step(s), loss on training batch is 0.00066213.
After 5312 training step(s), loss on training batch is 0.000592076.
After 5313 training step(s), loss on training batch is 0.0006251.
After 5314 training step(s), loss on training batch is 0.00109454.
After 5315 training step(s), loss on training batch is 0.0019637.
After 5316 training step(s), loss on training batch is 0.00127897.
After 5317 training step(s), loss on training batch is 0.00115366.
After 5318 training step(s), loss on training batch is 0.000916497.
After 5319 training step(s), loss on training batch is 0.000500281.
After 5320 training step(s), loss on training batch is 0.000537649.
After 5321 training step(s), loss on training batch is 0.000538119.
After 5322 training step(s), loss on training batch is 0.000441596.
After 5323 training step(s), loss on training batch is 0.000723145.
After 5324 training step(s), loss on training batch is 0.00055382.
After 5325 training step(s), loss on training batch is 0.000534271.
After 5326 training step(s), loss on training batch is 0.000475422.
After 5327 training step(s), loss on training batch is 0.00125199.
After 5328 training step(s), loss on training batch is 0.00415079.
After 5329 training step(s), loss on training batch is 0.00111278.
After 5330 training step(s), loss on training batch is 0.000752064.
After 5331 training step(s), loss on training batch is 0.000687299.
After 5332 training step(s), loss on training batch is 0.000602771.
After 5333 training step(s), loss on training batch is 0.000553539.
After 5334 training step(s), loss on training batch is 0.000471808.
After 5335 training step(s), loss on training batch is 0.000426357.
After 5336 training step(s), loss on training batch is 0.000428784.
After 5337 training step(s), loss on training batch is 0.000484539.
After 5338 training step(s), loss on training batch is 0.000470678.
After 5339 training step(s), loss on training batch is 0.000501139.
After 5340 training step(s), loss on training batch is 0.00055799.
After 5341 training step(s), loss on training batch is 0.000420998.
After 5342 training step(s), loss on training batch is 0.000503627.
After 5343 training step(s), loss on training batch is 0.000458984.
After 5344 training step(s), loss on training batch is 0.000590569.
After 5345 training step(s), loss on training batch is 0.000381543.
After 5346 training step(s), loss on training batch is 0.000550548.
After 5347 training step(s), loss on training batch is 0.000428022.
After 5348 training step(s), loss on training batch is 0.000397121.
After 5349 training step(s), loss on training batch is 0.000446313.
After 5350 training step(s), loss on training batch is 0.000554058.
After 5351 training step(s), loss on training batch is 0.000511781.
After 5352 training step(s), loss on training batch is 0.000445491.
After 5353 training step(s), loss on training batch is 0.000444235.
After 5354 training step(s), loss on training batch is 0.000413586.
After 5355 training step(s), loss on training batch is 0.000422647.
After 5356 training step(s), loss on training batch is 0.000528445.
After 5357 training step(s), loss on training batch is 0.000642799.
After 5358 training step(s), loss on training batch is 0.000390572.
After 5359 training step(s), loss on training batch is 0.000446291.
After 5360 training step(s), loss on training batch is 0.000585983.
After 5361 training step(s), loss on training batch is 0.000398762.
After 5362 training step(s), loss on training batch is 0.000365626.
After 5363 training step(s), loss on training batch is 0.000410591.
After 5364 training step(s), loss on training batch is 0.000541695.
After 5365 training step(s), loss on training batch is 0.000463549.
After 5366 training step(s), loss on training batch is 0.000452063.
After 5367 training step(s), loss on training batch is 0.000492629.
After 5368 training step(s), loss on training batch is 0.000413162.
After 5369 training step(s), loss on training batch is 0.000480896.
After 5370 training step(s), loss on training batch is 0.000414535.
After 5371 training step(s), loss on training batch is 0.000489917.
After 5372 training step(s), loss on training batch is 0.000388089.
After 5373 training step(s), loss on training batch is 0.000386406.
After 5374 training step(s), loss on training batch is 0.000389769.
After 5375 training step(s), loss on training batch is 0.000462341.
After 5376 training step(s), loss on training batch is 0.000446408.
After 5377 training step(s), loss on training batch is 0.000654798.
After 5378 training step(s), loss on training batch is 0.000482181.
After 5379 training step(s), loss on training batch is 0.000452717.
After 5380 training step(s), loss on training batch is 0.000521514.
After 5381 training step(s), loss on training batch is 0.000410267.
After 5382 training step(s), loss on training batch is 0.000454612.
After 5383 training step(s), loss on training batch is 0.00101012.
After 5384 training step(s), loss on training batch is 0.00119886.
After 5385 training step(s), loss on training batch is 0.00152525.
After 5386 training step(s), loss on training batch is 0.000902884.
After 5387 training step(s), loss on training batch is 0.000870963.
After 5388 training step(s), loss on training batch is 0.000733118.
After 5389 training step(s), loss on training batch is 0.000812687.
After 5390 training step(s), loss on training batch is 0.000746933.
After 5391 training step(s), loss on training batch is 0.000853694.
After 5392 training step(s), loss on training batch is 0.0010309.
After 5393 training step(s), loss on training batch is 0.000687797.
After 5394 training step(s), loss on training batch is 0.000723111.
After 5395 training step(s), loss on training batch is 0.00119498.
After 5396 training step(s), loss on training batch is 0.000979911.
After 5397 training step(s), loss on training batch is 0.000811856.
After 5398 training step(s), loss on training batch is 0.000841969.
After 5399 training step(s), loss on training batch is 0.0013093.
After 5400 training step(s), loss on training batch is 0.000826017.
After 5401 training step(s), loss on training batch is 0.000819035.
After 5402 training step(s), loss on training batch is 0.000874214.
After 5403 training step(s), loss on training batch is 0.000782239.
After 5404 training step(s), loss on training batch is 0.000755065.
After 5405 training step(s), loss on training batch is 0.000675585.
After 5406 training step(s), loss on training batch is 0.00083327.
After 5407 training step(s), loss on training batch is 0.00116833.
After 5408 training step(s), loss on training batch is 0.00101458.
After 5409 training step(s), loss on training batch is 0.000682738.
After 5410 training step(s), loss on training batch is 0.0013264.
After 5411 training step(s), loss on training batch is 0.000866846.
After 5412 training step(s), loss on training batch is 0.000859262.
After 5413 training step(s), loss on training batch is 0.000715291.
After 5414 training step(s), loss on training batch is 0.000648618.
After 5415 training step(s), loss on training batch is 0.000644999.
After 5416 training step(s), loss on training batch is 0.000648865.
After 5417 training step(s), loss on training batch is 0.000668144.
After 5418 training step(s), loss on training batch is 0.000984093.
After 5419 training step(s), loss on training batch is 0.0013314.
After 5420 training step(s), loss on training batch is 0.0015244.
After 5421 training step(s), loss on training batch is 0.0019731.
After 5422 training step(s), loss on training batch is 0.000943264.
After 5423 training step(s), loss on training batch is 0.00090541.
After 5424 training step(s), loss on training batch is 0.000870364.
After 5425 training step(s), loss on training batch is 0.0010377.
After 5426 training step(s), loss on training batch is 0.000929088.
After 5427 training step(s), loss on training batch is 0.000836615.
After 5428 training step(s), loss on training batch is 0.000843477.
After 5429 training step(s), loss on training batch is 0.000874117.
After 5430 training step(s), loss on training batch is 0.000972517.
After 5431 training step(s), loss on training batch is 0.00115462.
After 5432 training step(s), loss on training batch is 0.000725378.
After 5433 training step(s), loss on training batch is 0.000715435.
After 5434 training step(s), loss on training batch is 0.0006898.
After 5435 training step(s), loss on training batch is 0.000582858.
After 5436 training step(s), loss on training batch is 0.000974168.
After 5437 training step(s), loss on training batch is 0.00170846.
After 5438 training step(s), loss on training batch is 0.000636698.
After 5439 training step(s), loss on training batch is 0.000763485.
After 5440 training step(s), loss on training batch is 0.000875153.
After 5441 training step(s), loss on training batch is 0.000737439.
After 5442 training step(s), loss on training batch is 0.000699959.
After 5443 training step(s), loss on training batch is 0.000813974.
After 5444 training step(s), loss on training batch is 0.000739409.
After 5445 training step(s), loss on training batch is 0.000827408.
After 5446 training step(s), loss on training batch is 0.00100628.
After 5447 training step(s), loss on training batch is 0.000716169.
After 5448 training step(s), loss on training batch is 0.000867697.
After 5449 training step(s), loss on training batch is 0.000855836.
After 5450 training step(s), loss on training batch is 0.000924848.
After 5451 training step(s), loss on training batch is 0.00096627.
After 5452 training step(s), loss on training batch is 0.000634444.
After 5453 training step(s), loss on training batch is 0.00111788.
After 5454 training step(s), loss on training batch is 0.000775229.
After 5455 training step(s), loss on training batch is 0.000823364.
After 5456 training step(s), loss on training batch is 0.000708642.
After 5457 training step(s), loss on training batch is 0.000672972.
After 5458 training step(s), loss on training batch is 0.000979641.
After 5459 training step(s), loss on training batch is 0.00143392.
After 5460 training step(s), loss on training batch is 0.00083345.
After 5461 training step(s), loss on training batch is 0.000669962.
After 5462 training step(s), loss on training batch is 0.000658094.
After 5463 training step(s), loss on training batch is 0.000695634.
After 5464 training step(s), loss on training batch is 0.000605318.
After 5465 training step(s), loss on training batch is 0.00091752.
After 5466 training step(s), loss on training batch is 0.00139305.
After 5467 training step(s), loss on training batch is 0.00149603.
After 5468 training step(s), loss on training batch is 0.00131191.
After 5469 training step(s), loss on training batch is 0.0013368.
After 5470 training step(s), loss on training batch is 0.00124844.
After 5471 training step(s), loss on training batch is 0.00116921.
After 5472 training step(s), loss on training batch is 0.00124272.
After 5473 training step(s), loss on training batch is 0.00278596.
After 5474 training step(s), loss on training batch is 0.00145584.
After 5475 training step(s), loss on training batch is 0.0017135.
After 5476 training step(s), loss on training batch is 0.00136179.
After 5477 training step(s), loss on training batch is 0.0012062.
After 5478 training step(s), loss on training batch is 0.00136955.
After 5479 training step(s), loss on training batch is 0.0010645.
After 5480 training step(s), loss on training batch is 0.00107332.
After 5481 training step(s), loss on training batch is 0.00109166.
After 5482 training step(s), loss on training batch is 0.00108505.
After 5483 training step(s), loss on training batch is 0.00126608.
After 5484 training step(s), loss on training batch is 0.00109961.
After 5485 training step(s), loss on training batch is 0.0010699.
After 5486 training step(s), loss on training batch is 0.00149007.
After 5487 training step(s), loss on training batch is 0.00111957.
After 5488 training step(s), loss on training batch is 0.00139182.
After 5489 training step(s), loss on training batch is 0.00115192.
After 5490 training step(s), loss on training batch is 0.0011397.
After 5491 training step(s), loss on training batch is 0.00106372.
After 5492 training step(s), loss on training batch is 0.00125855.
After 5493 training step(s), loss on training batch is 0.0010985.
After 5494 training step(s), loss on training batch is 0.00121407.
After 5495 training step(s), loss on training batch is 0.00153856.
After 5496 training step(s), loss on training batch is 0.00143483.
After 5497 training step(s), loss on training batch is 0.00118783.
After 5498 training step(s), loss on training batch is 0.00115143.
After 5499 training step(s), loss on training batch is 0.00107446.
After 5500 training step(s), loss on training batch is 0.00110911.
After 5501 training step(s), loss on training batch is 0.000961254.
After 5502 training step(s), loss on training batch is 0.00126424.
After 5503 training step(s), loss on training batch is 0.00106933.
After 5504 training step(s), loss on training batch is 0.00143305.
After 5505 training step(s), loss on training batch is 0.0013801.
After 5506 training step(s), loss on training batch is 0.00139359.
After 5507 training step(s), loss on training batch is 0.00163266.
After 5508 training step(s), loss on training batch is 0.00132643.
After 5509 training step(s), loss on training batch is 0.00667235.
After 5510 training step(s), loss on training batch is 0.0026193.
After 5511 training step(s), loss on training batch is 0.00211106.
After 5512 training step(s), loss on training batch is 0.00186734.
After 5513 training step(s), loss on training batch is 0.00165953.
After 5514 training step(s), loss on training batch is 0.00174895.
After 5515 training step(s), loss on training batch is 0.00140161.
After 5516 training step(s), loss on training batch is 0.000601148.
After 5517 training step(s), loss on training batch is 0.000529821.
After 5518 training step(s), loss on training batch is 0.000514199.
After 5519 training step(s), loss on training batch is 0.000583764.
After 5520 training step(s), loss on training batch is 0.000680296.
After 5521 training step(s), loss on training batch is 0.000605324.
After 5522 training step(s), loss on training batch is 0.000603052.
After 5523 training step(s), loss on training batch is 0.00061806.
After 5524 training step(s), loss on training batch is 0.000644851.
After 5525 training step(s), loss on training batch is 0.000619996.
After 5526 training step(s), loss on training batch is 0.00105055.
After 5527 training step(s), loss on training batch is 0.00105744.
After 5528 training step(s), loss on training batch is 0.00127569.
After 5529 training step(s), loss on training batch is 0.00105244.
After 5530 training step(s), loss on training batch is 0.00269636.
After 5531 training step(s), loss on training batch is 0.0021773.
After 5532 training step(s), loss on training batch is 0.00149875.
After 5533 training step(s), loss on training batch is 0.00157261.
After 5534 training step(s), loss on training batch is 0.00113815.
After 5535 training step(s), loss on training batch is 0.00110779.
After 5536 training step(s), loss on training batch is 0.0011753.
After 5537 training step(s), loss on training batch is 0.00109669.
After 5538 training step(s), loss on training batch is 0.00129156.
After 5539 training step(s), loss on training batch is 0.00122511.
After 5540 training step(s), loss on training batch is 0.001156.
After 5541 training step(s), loss on training batch is 0.00110066.
After 5542 training step(s), loss on training batch is 0.00183617.
After 5543 training step(s), loss on training batch is 0.000489111.
After 5544 training step(s), loss on training batch is 0.000625567.
After 5545 training step(s), loss on training batch is 0.000524394.
After 5546 training step(s), loss on training batch is 0.00067637.
After 5547 training step(s), loss on training batch is 0.000610143.
After 5548 training step(s), loss on training batch is 0.000594794.
After 5549 training step(s), loss on training batch is 0.000475406.
After 5550 training step(s), loss on training batch is 0.000413543.
After 5551 training step(s), loss on training batch is 0.000640389.
After 5552 training step(s), loss on training batch is 0.000574877.
After 5553 training step(s), loss on training batch is 0.000566907.
After 5554 training step(s), loss on training batch is 0.000448581.
After 5555 training step(s), loss on training batch is 0.000767816.
After 5556 training step(s), loss on training batch is 0.000493302.
After 5557 training step(s), loss on training batch is 0.000427956.
After 5558 training step(s), loss on training batch is 0.000441184.
After 5559 training step(s), loss on training batch is 0.00109702.
After 5560 training step(s), loss on training batch is 0.00143529.
After 5561 training step(s), loss on training batch is 0.000812281.
After 5562 training step(s), loss on training batch is 0.000564682.
After 5563 training step(s), loss on training batch is 0.000520114.
After 5564 training step(s), loss on training batch is 0.000713601.
After 5565 training step(s), loss on training batch is 0.000672306.
After 5566 training step(s), loss on training batch is 0.000562344.
After 5567 training step(s), loss on training batch is 0.000501503.
After 5568 training step(s), loss on training batch is 0.000622612.
After 5569 training step(s), loss on training batch is 0.0004173.
After 5570 training step(s), loss on training batch is 0.000516141.
After 5571 training step(s), loss on training batch is 0.000331617.
After 5572 training step(s), loss on training batch is 0.000448102.
After 5573 training step(s), loss on training batch is 0.000397894.
After 5574 training step(s), loss on training batch is 0.000375429.
After 5575 training step(s), loss on training batch is 0.000720185.
After 5576 training step(s), loss on training batch is 0.000614041.
After 5577 training step(s), loss on training batch is 0.000602709.
After 5578 training step(s), loss on training batch is 0.000404982.
After 5579 training step(s), loss on training batch is 0.000445301.
After 5580 training step(s), loss on training batch is 0.000383365.
After 5581 training step(s), loss on training batch is 0.000449716.
After 5582 training step(s), loss on training batch is 0.000417314.
After 5583 training step(s), loss on training batch is 0.000518755.
After 5584 training step(s), loss on training batch is 0.0005239.
After 5585 training step(s), loss on training batch is 0.000514534.
After 5586 training step(s), loss on training batch is 0.000744215.
After 5587 training step(s), loss on training batch is 0.000589872.
After 5588 training step(s), loss on training batch is 0.000398734.
After 5589 training step(s), loss on training batch is 0.000457922.
After 5590 training step(s), loss on training batch is 0.000412044.
After 5591 training step(s), loss on training batch is 0.000385684.
After 5592 training step(s), loss on training batch is 0.000422875.
After 5593 training step(s), loss on training batch is 0.000373934.
After 5594 training step(s), loss on training batch is 0.000329569.
After 5595 training step(s), loss on training batch is 0.000702629.
After 5596 training step(s), loss on training batch is 0.000558825.
After 5597 training step(s), loss on training batch is 0.000314414.
After 5598 training step(s), loss on training batch is 0.000321289.
After 5599 training step(s), loss on training batch is 0.000441834.
After 5600 training step(s), loss on training batch is 0.0005057.
After 5601 training step(s), loss on training batch is 0.000701718.
After 5602 training step(s), loss on training batch is 0.000755848.
After 5603 training step(s), loss on training batch is 0.00046583.
After 5604 training step(s), loss on training batch is 0.000676586.
After 5605 training step(s), loss on training batch is 0.00149682.
After 5606 training step(s), loss on training batch is 0.000982284.
After 5607 training step(s), loss on training batch is 0.000552662.
After 5608 training step(s), loss on training batch is 0.000665107.
After 5609 training step(s), loss on training batch is 0.000794313.
After 5610 training step(s), loss on training batch is 0.000829321.
After 5611 training step(s), loss on training batch is 0.00050858.
After 5612 training step(s), loss on training batch is 0.000407259.
After 5613 training step(s), loss on training batch is 0.000467476.
After 5614 training step(s), loss on training batch is 0.000476808.
After 5615 training step(s), loss on training batch is 0.000427452.
After 5616 training step(s), loss on training batch is 0.000369616.
After 5617 training step(s), loss on training batch is 0.000490669.
After 5618 training step(s), loss on training batch is 0.000391294.
After 5619 training step(s), loss on training batch is 0.000505167.
After 5620 training step(s), loss on training batch is 0.000451126.
After 5621 training step(s), loss on training batch is 0.000377143.
After 5622 training step(s), loss on training batch is 0.000442922.
After 5623 training step(s), loss on training batch is 0.000378713.
After 5624 training step(s), loss on training batch is 0.000370933.
After 5625 training step(s), loss on training batch is 0.00035214.
After 5626 training step(s), loss on training batch is 0.00047064.
After 5627 training step(s), loss on training batch is 0.000352466.
After 5628 training step(s), loss on training batch is 0.00142037.
After 5629 training step(s), loss on training batch is 0.000785953.
After 5630 training step(s), loss on training batch is 0.000711518.
After 5631 training step(s), loss on training batch is 0.000733567.
After 5632 training step(s), loss on training batch is 0.000876368.
After 5633 training step(s), loss on training batch is 0.000939864.
After 5634 training step(s), loss on training batch is 0.000776622.
After 5635 training step(s), loss on training batch is 0.000618205.
After 5636 training step(s), loss on training batch is 0.00090308.
After 5637 training step(s), loss on training batch is 0.000920643.
After 5638 training step(s), loss on training batch is 0.000626936.
After 5639 training step(s), loss on training batch is 0.00063144.
After 5640 training step(s), loss on training batch is 0.00152269.
After 5641 training step(s), loss on training batch is 0.000708539.
After 5642 training step(s), loss on training batch is 0.000783005.
After 5643 training step(s), loss on training batch is 0.000825082.
After 5644 training step(s), loss on training batch is 0.000900005.
After 5645 training step(s), loss on training batch is 0.000658232.
After 5646 training step(s), loss on training batch is 0.000935073.
After 5647 training step(s), loss on training batch is 0.000715569.
After 5648 training step(s), loss on training batch is 0.000828063.
After 5649 training step(s), loss on training batch is 0.000819737.
After 5650 training step(s), loss on training batch is 0.00085518.
After 5651 training step(s), loss on training batch is 0.000902527.
After 5652 training step(s), loss on training batch is 0.000736377.
After 5653 training step(s), loss on training batch is 0.00101934.
After 5654 training step(s), loss on training batch is 0.000574932.
After 5655 training step(s), loss on training batch is 0.000808524.
After 5656 training step(s), loss on training batch is 0.00139473.
After 5657 training step(s), loss on training batch is 0.00114613.
After 5658 training step(s), loss on training batch is 0.00120733.
After 5659 training step(s), loss on training batch is 0.0011764.
After 5660 training step(s), loss on training batch is 0.000991453.
After 5661 training step(s), loss on training batch is 0.000986466.
After 5662 training step(s), loss on training batch is 0.00130903.
After 5663 training step(s), loss on training batch is 0.00114519.
After 5664 training step(s), loss on training batch is 0.00101448.
After 5665 training step(s), loss on training batch is 0.00148297.
After 5666 training step(s), loss on training batch is 0.00104428.
After 5667 training step(s), loss on training batch is 0.00100882.
After 5668 training step(s), loss on training batch is 0.00125255.
After 5669 training step(s), loss on training batch is 0.00217874.
After 5670 training step(s), loss on training batch is 0.00280561.
After 5671 training step(s), loss on training batch is 0.00175672.
After 5672 training step(s), loss on training batch is 0.00138355.
After 5673 training step(s), loss on training batch is 0.00151511.
After 5674 training step(s), loss on training batch is 0.00129466.
After 5675 training step(s), loss on training batch is 0.00124659.
After 5676 training step(s), loss on training batch is 0.00117474.
After 5677 training step(s), loss on training batch is 0.00188856.
After 5678 training step(s), loss on training batch is 0.00114933.
After 5679 training step(s), loss on training batch is 0.00109524.
After 5680 training step(s), loss on training batch is 0.00109533.
After 5681 training step(s), loss on training batch is 0.000694712.
After 5682 training step(s), loss on training batch is 0.000690327.
After 5683 training step(s), loss on training batch is 0.000389991.
After 5684 training step(s), loss on training batch is 0.000497606.
After 5685 training step(s), loss on training batch is 0.000610164.
After 5686 training step(s), loss on training batch is 0.000776045.
After 5687 training step(s), loss on training batch is 0.000929415.
After 5688 training step(s), loss on training batch is 0.000625893.
After 5689 training step(s), loss on training batch is 0.000471037.
After 5690 training step(s), loss on training batch is 0.000465056.
After 5691 training step(s), loss on training batch is 0.000393157.
After 5692 training step(s), loss on training batch is 0.000517063.
After 5693 training step(s), loss on training batch is 0.000392264.
After 5694 training step(s), loss on training batch is 0.000414054.
After 5695 training step(s), loss on training batch is 0.000504437.
After 5696 training step(s), loss on training batch is 0.000489737.
After 5697 training step(s), loss on training batch is 0.000371434.
After 5698 training step(s), loss on training batch is 0.000364415.
After 5699 training step(s), loss on training batch is 0.00037129.
After 5700 training step(s), loss on training batch is 0.000445809.
After 5701 training step(s), loss on training batch is 0.00135546.
After 5702 training step(s), loss on training batch is 0.000725462.
After 5703 training step(s), loss on training batch is 0.00071226.
After 5704 training step(s), loss on training batch is 0.000859018.
After 5705 training step(s), loss on training batch is 0.000750503.
After 5706 training step(s), loss on training batch is 0.000768397.
After 5707 training step(s), loss on training batch is 0.000799437.
After 5708 training step(s), loss on training batch is 0.000440708.
After 5709 training step(s), loss on training batch is 0.000480151.
After 5710 training step(s), loss on training batch is 0.000534308.
After 5711 training step(s), loss on training batch is 0.000656308.
After 5712 training step(s), loss on training batch is 0.000570541.
After 5713 training step(s), loss on training batch is 0.000613177.
After 5714 training step(s), loss on training batch is 0.000937701.
After 5715 training step(s), loss on training batch is 0.0018147.
After 5716 training step(s), loss on training batch is 0.00124093.
After 5717 training step(s), loss on training batch is 0.00112356.
After 5718 training step(s), loss on training batch is 0.000816458.
After 5719 training step(s), loss on training batch is 0.000479511.
After 5720 training step(s), loss on training batch is 0.000480651.
After 5721 training step(s), loss on training batch is 0.00048974.
After 5722 training step(s), loss on training batch is 0.000413319.
After 5723 training step(s), loss on training batch is 0.000677605.
After 5724 training step(s), loss on training batch is 0.000512617.
After 5725 training step(s), loss on training batch is 0.000499056.
After 5726 training step(s), loss on training batch is 0.000454531.
After 5727 training step(s), loss on training batch is 0.00123216.
After 5728 training step(s), loss on training batch is 0.00404711.
After 5729 training step(s), loss on training batch is 0.00104773.
After 5730 training step(s), loss on training batch is 0.000716558.
After 5731 training step(s), loss on training batch is 0.000686662.
After 5732 training step(s), loss on training batch is 0.000587143.
After 5733 training step(s), loss on training batch is 0.000528024.
After 5734 training step(s), loss on training batch is 0.000453797.
After 5735 training step(s), loss on training batch is 0.000405702.
After 5736 training step(s), loss on training batch is 0.000385909.
After 5737 training step(s), loss on training batch is 0.000435037.
After 5738 training step(s), loss on training batch is 0.000427973.
After 5739 training step(s), loss on training batch is 0.000468564.
After 5740 training step(s), loss on training batch is 0.000508639.
After 5741 training step(s), loss on training batch is 0.000395771.
After 5742 training step(s), loss on training batch is 0.000475548.
After 5743 training step(s), loss on training batch is 0.000433089.
After 5744 training step(s), loss on training batch is 0.000597155.
After 5745 training step(s), loss on training batch is 0.000375156.
After 5746 training step(s), loss on training batch is 0.000549004.
After 5747 training step(s), loss on training batch is 0.000420496.
After 5748 training step(s), loss on training batch is 0.000380518.
After 5749 training step(s), loss on training batch is 0.000436999.
After 5750 training step(s), loss on training batch is 0.000541006.
After 5751 training step(s), loss on training batch is 0.000498324.
After 5752 training step(s), loss on training batch is 0.000440753.
After 5753 training step(s), loss on training batch is 0.000439756.
After 5754 training step(s), loss on training batch is 0.000405935.
After 5755 training step(s), loss on training batch is 0.000438445.
After 5756 training step(s), loss on training batch is 0.000509988.
After 5757 training step(s), loss on training batch is 0.000622426.
After 5758 training step(s), loss on training batch is 0.000386664.
After 5759 training step(s), loss on training batch is 0.000441157.
After 5760 training step(s), loss on training batch is 0.000570062.
After 5761 training step(s), loss on training batch is 0.000395573.
After 5762 training step(s), loss on training batch is 0.000359244.
After 5763 training step(s), loss on training batch is 0.000404091.
After 5764 training step(s), loss on training batch is 0.000531099.
After 5765 training step(s), loss on training batch is 0.000457667.
After 5766 training step(s), loss on training batch is 0.000447957.
After 5767 training step(s), loss on training batch is 0.000482629.
After 5768 training step(s), loss on training batch is 0.000403936.
After 5769 training step(s), loss on training batch is 0.000468508.
After 5770 training step(s), loss on training batch is 0.000404568.
After 5771 training step(s), loss on training batch is 0.000478779.
After 5772 training step(s), loss on training batch is 0.000383807.
After 5773 training step(s), loss on training batch is 0.000381549.
After 5774 training step(s), loss on training batch is 0.000384007.
After 5775 training step(s), loss on training batch is 0.000448279.
After 5776 training step(s), loss on training batch is 0.000432953.
After 5777 training step(s), loss on training batch is 0.00063557.
After 5778 training step(s), loss on training batch is 0.000467327.
After 5779 training step(s), loss on training batch is 0.000449578.
After 5780 training step(s), loss on training batch is 0.000484664.
After 5781 training step(s), loss on training batch is 0.000404011.
After 5782 training step(s), loss on training batch is 0.000447067.
After 5783 training step(s), loss on training batch is 0.000913556.
After 5784 training step(s), loss on training batch is 0.00115649.
After 5785 training step(s), loss on training batch is 0.00145654.
After 5786 training step(s), loss on training batch is 0.000865026.
After 5787 training step(s), loss on training batch is 0.000826246.
After 5788 training step(s), loss on training batch is 0.00073408.
After 5789 training step(s), loss on training batch is 0.00084655.
After 5790 training step(s), loss on training batch is 0.00074851.
After 5791 training step(s), loss on training batch is 0.000841717.
After 5792 training step(s), loss on training batch is 0.000963649.
After 5793 training step(s), loss on training batch is 0.000677028.
After 5794 training step(s), loss on training batch is 0.000692663.
After 5795 training step(s), loss on training batch is 0.0011382.
After 5796 training step(s), loss on training batch is 0.000947952.
After 5797 training step(s), loss on training batch is 0.000781556.
After 5798 training step(s), loss on training batch is 0.000813797.
After 5799 training step(s), loss on training batch is 0.00124223.
After 5800 training step(s), loss on training batch is 0.000791272.
After 5801 training step(s), loss on training batch is 0.000783082.
After 5802 training step(s), loss on training batch is 0.000831857.
After 5803 training step(s), loss on training batch is 0.000762591.
After 5804 training step(s), loss on training batch is 0.000738734.
After 5805 training step(s), loss on training batch is 0.00065549.
After 5806 training step(s), loss on training batch is 0.000811014.
After 5807 training step(s), loss on training batch is 0.00112011.
After 5808 training step(s), loss on training batch is 0.000956035.
After 5809 training step(s), loss on training batch is 0.000665597.
After 5810 training step(s), loss on training batch is 0.00127305.
After 5811 training step(s), loss on training batch is 0.000788121.
After 5812 training step(s), loss on training batch is 0.000845825.
After 5813 training step(s), loss on training batch is 0.000675589.
After 5814 training step(s), loss on training batch is 0.000624132.
After 5815 training step(s), loss on training batch is 0.000612391.
After 5816 training step(s), loss on training batch is 0.000621726.
After 5817 training step(s), loss on training batch is 0.000642.
After 5818 training step(s), loss on training batch is 0.00096081.
After 5819 training step(s), loss on training batch is 0.00128965.
After 5820 training step(s), loss on training batch is 0.00151108.
After 5821 training step(s), loss on training batch is 0.00198368.
After 5822 training step(s), loss on training batch is 0.0008711.
After 5823 training step(s), loss on training batch is 0.00086119.
After 5824 training step(s), loss on training batch is 0.000834211.
After 5825 training step(s), loss on training batch is 0.000968847.
After 5826 training step(s), loss on training batch is 0.000887071.
After 5827 training step(s), loss on training batch is 0.000783938.
After 5828 training step(s), loss on training batch is 0.000806071.
After 5829 training step(s), loss on training batch is 0.000833107.
After 5830 training step(s), loss on training batch is 0.000934773.
After 5831 training step(s), loss on training batch is 0.00111274.
After 5832 training step(s), loss on training batch is 0.00070376.
After 5833 training step(s), loss on training batch is 0.000714839.
After 5834 training step(s), loss on training batch is 0.000688126.
After 5835 training step(s), loss on training batch is 0.000577612.
After 5836 training step(s), loss on training batch is 0.000951249.
After 5837 training step(s), loss on training batch is 0.00160282.
After 5838 training step(s), loss on training batch is 0.000615434.
After 5839 training step(s), loss on training batch is 0.000753887.
After 5840 training step(s), loss on training batch is 0.000844943.
After 5841 training step(s), loss on training batch is 0.00069042.
After 5842 training step(s), loss on training batch is 0.000674773.
After 5843 training step(s), loss on training batch is 0.000756766.
After 5844 training step(s), loss on training batch is 0.000684771.
After 5845 training step(s), loss on training batch is 0.000793294.
After 5846 training step(s), loss on training batch is 0.000967385.
After 5847 training step(s), loss on training batch is 0.00071802.
After 5848 training step(s), loss on training batch is 0.00081476.
After 5849 training step(s), loss on training batch is 0.000832347.
After 5850 training step(s), loss on training batch is 0.00089195.
After 5851 training step(s), loss on training batch is 0.000860312.
After 5852 training step(s), loss on training batch is 0.000621508.
After 5853 training step(s), loss on training batch is 0.00106764.
After 5854 training step(s), loss on training batch is 0.000761834.
After 5855 training step(s), loss on training batch is 0.000798477.
After 5856 training step(s), loss on training batch is 0.000694505.
After 5857 training step(s), loss on training batch is 0.000634699.
After 5858 training step(s), loss on training batch is 0.000956058.
After 5859 training step(s), loss on training batch is 0.00136785.
After 5860 training step(s), loss on training batch is 0.000782327.
After 5861 training step(s), loss on training batch is 0.000651463.
After 5862 training step(s), loss on training batch is 0.000638241.
After 5863 training step(s), loss on training batch is 0.000652356.
After 5864 training step(s), loss on training batch is 0.000567915.
After 5865 training step(s), loss on training batch is 0.000908398.
After 5866 training step(s), loss on training batch is 0.00132393.
After 5867 training step(s), loss on training batch is 0.00143213.
After 5868 training step(s), loss on training batch is 0.00125149.
After 5869 training step(s), loss on training batch is 0.00126705.
After 5870 training step(s), loss on training batch is 0.00120797.
After 5871 training step(s), loss on training batch is 0.0011311.
After 5872 training step(s), loss on training batch is 0.0012107.
After 5873 training step(s), loss on training batch is 0.00262329.
After 5874 training step(s), loss on training batch is 0.0014002.
After 5875 training step(s), loss on training batch is 0.00163007.
After 5876 training step(s), loss on training batch is 0.00132195.
After 5877 training step(s), loss on training batch is 0.00119476.
After 5878 training step(s), loss on training batch is 0.0013223.
After 5879 training step(s), loss on training batch is 0.00109823.
After 5880 training step(s), loss on training batch is 0.00106201.
After 5881 training step(s), loss on training batch is 0.00107984.
After 5882 training step(s), loss on training batch is 0.00106075.
After 5883 training step(s), loss on training batch is 0.00120467.
After 5884 training step(s), loss on training batch is 0.00106946.
After 5885 training step(s), loss on training batch is 0.00104712.
After 5886 training step(s), loss on training batch is 0.00141434.
After 5887 training step(s), loss on training batch is 0.00106919.
After 5888 training step(s), loss on training batch is 0.00134866.
After 5889 training step(s), loss on training batch is 0.00110825.
After 5890 training step(s), loss on training batch is 0.00107337.
After 5891 training step(s), loss on training batch is 0.00102702.
After 5892 training step(s), loss on training batch is 0.00117515.
After 5893 training step(s), loss on training batch is 0.00103502.
After 5894 training step(s), loss on training batch is 0.00117142.
After 5895 training step(s), loss on training batch is 0.0013123.
After 5896 training step(s), loss on training batch is 0.00134734.
After 5897 training step(s), loss on training batch is 0.00116299.
After 5898 training step(s), loss on training batch is 0.00109601.
After 5899 training step(s), loss on training batch is 0.00101424.
After 5900 training step(s), loss on training batch is 0.00104607.
After 5901 training step(s), loss on training batch is 0.000926395.
After 5902 training step(s), loss on training batch is 0.00124133.
After 5903 training step(s), loss on training batch is 0.00103513.
After 5904 training step(s), loss on training batch is 0.00137892.
After 5905 training step(s), loss on training batch is 0.00135663.
After 5906 training step(s), loss on training batch is 0.00134003.
After 5907 training step(s), loss on training batch is 0.00157405.
After 5908 training step(s), loss on training batch is 0.00129828.
After 5909 training step(s), loss on training batch is 0.00691598.
After 5910 training step(s), loss on training batch is 0.0408305.
After 5911 training step(s), loss on training batch is 0.00272091.
After 5912 training step(s), loss on training batch is 0.00278077.
After 5913 training step(s), loss on training batch is 0.0024135.
After 5914 training step(s), loss on training batch is 0.00282626.
After 5915 training step(s), loss on training batch is 0.00204089.
After 5916 training step(s), loss on training batch is 0.00100227.
After 5917 training step(s), loss on training batch is 0.000845241.
After 5918 training step(s), loss on training batch is 0.000744596.
After 5919 training step(s), loss on training batch is 0.00091903.
After 5920 training step(s), loss on training batch is 0.00105577.
After 5921 training step(s), loss on training batch is 0.000912726.
After 5922 training step(s), loss on training batch is 0.000961073.
After 5923 training step(s), loss on training batch is 0.00103226.
After 5924 training step(s), loss on training batch is 0.000930995.
After 5925 training step(s), loss on training batch is 0.000971483.
After 5926 training step(s), loss on training batch is 0.00200927.
After 5927 training step(s), loss on training batch is 0.00163402.
After 5928 training step(s), loss on training batch is 0.00159771.
After 5929 training step(s), loss on training batch is 0.00151338.
After 5930 training step(s), loss on training batch is 0.00294457.
After 5931 training step(s), loss on training batch is 0.00260862.
After 5932 training step(s), loss on training batch is 0.00177993.
After 5933 training step(s), loss on training batch is 0.00186041.
After 5934 training step(s), loss on training batch is 0.00146149.
After 5935 training step(s), loss on training batch is 0.0013933.
After 5936 training step(s), loss on training batch is 0.0014051.
After 5937 training step(s), loss on training batch is 0.00136985.
After 5938 training step(s), loss on training batch is 0.00144037.
After 5939 training step(s), loss on training batch is 0.00149783.
After 5940 training step(s), loss on training batch is 0.00140303.
After 5941 training step(s), loss on training batch is 0.00135294.
After 5942 training step(s), loss on training batch is 0.00154189.
After 5943 training step(s), loss on training batch is 0.000565601.
After 5944 training step(s), loss on training batch is 0.000960127.
After 5945 training step(s), loss on training batch is 0.000731146.
After 5946 training step(s), loss on training batch is 0.00092114.
After 5947 training step(s), loss on training batch is 0.000795586.
After 5948 training step(s), loss on training batch is 0.000775316.
After 5949 training step(s), loss on training batch is 0.00060975.
After 5950 training step(s), loss on training batch is 0.000525429.
After 5951 training step(s), loss on training batch is 0.000795921.
After 5952 training step(s), loss on training batch is 0.00073376.
After 5953 training step(s), loss on training batch is 0.000689695.
After 5954 training step(s), loss on training batch is 0.000497092.
After 5955 training step(s), loss on training batch is 0.000964802.
After 5956 training step(s), loss on training batch is 0.000531655.
After 5957 training step(s), loss on training batch is 0.000507206.
After 5958 training step(s), loss on training batch is 0.000539765.
After 5959 training step(s), loss on training batch is 0.001363.
After 5960 training step(s), loss on training batch is 0.00157959.
After 5961 training step(s), loss on training batch is 0.000941616.
After 5962 training step(s), loss on training batch is 0.000573252.
After 5963 training step(s), loss on training batch is 0.000593852.
After 5964 training step(s), loss on training batch is 0.000690223.
After 5965 training step(s), loss on training batch is 0.000724129.
After 5966 training step(s), loss on training batch is 0.000591887.
After 5967 training step(s), loss on training batch is 0.000623876.
After 5968 training step(s), loss on training batch is 0.000695378.
After 5969 training step(s), loss on training batch is 0.000453365.
After 5970 training step(s), loss on training batch is 0.000575632.
After 5971 training step(s), loss on training batch is 0.000366934.
After 5972 training step(s), loss on training batch is 0.000500763.
After 5973 training step(s), loss on training batch is 0.000459809.
After 5974 training step(s), loss on training batch is 0.000431833.
After 5975 training step(s), loss on training batch is 0.000773591.
After 5976 training step(s), loss on training batch is 0.000682365.
After 5977 training step(s), loss on training batch is 0.000683281.
After 5978 training step(s), loss on training batch is 0.000495837.
After 5979 training step(s), loss on training batch is 0.000480102.
After 5980 training step(s), loss on training batch is 0.000434193.
After 5981 training step(s), loss on training batch is 0.000526843.
After 5982 training step(s), loss on training batch is 0.000463976.
After 5983 training step(s), loss on training batch is 0.000586003.
After 5984 training step(s), loss on training batch is 0.000539166.
After 5985 training step(s), loss on training batch is 0.000581748.
After 5986 training step(s), loss on training batch is 0.000780667.
After 5987 training step(s), loss on training batch is 0.000684405.
After 5988 training step(s), loss on training batch is 0.000445886.
After 5989 training step(s), loss on training batch is 0.000496071.
After 5990 training step(s), loss on training batch is 0.000456728.
After 5991 training step(s), loss on training batch is 0.000432993.
After 5992 training step(s), loss on training batch is 0.000449434.
After 5993 training step(s), loss on training batch is 0.000428077.
After 5994 training step(s), loss on training batch is 0.000368311.
After 5995 training step(s), loss on training batch is 0.000617064.
After 5996 training step(s), loss on training batch is 0.000615455.
After 5997 training step(s), loss on training batch is 0.000336289.
After 5998 training step(s), loss on training batch is 0.000349216.
After 5999 training step(s), loss on training batch is 0.00046318.
After 6000 training step(s), loss on training batch is 0.000567678.
After 6001 training step(s), loss on training batch is 0.000721135.
After 6002 training step(s), loss on training batch is 0.000778285.
After 6003 training step(s), loss on training batch is 0.000440978.
After 6004 training step(s), loss on training batch is 0.000748281.
After 6005 training step(s), loss on training batch is 0.00145523.
After 6006 training step(s), loss on training batch is 0.00108031.
After 6007 training step(s), loss on training batch is 0.000559429.
After 6008 training step(s), loss on training batch is 0.000676295.
After 6009 training step(s), loss on training batch is 0.000824257.
After 6010 training step(s), loss on training batch is 0.000828215.
After 6011 training step(s), loss on training batch is 0.000499763.
After 6012 training step(s), loss on training batch is 0.000389394.
After 6013 training step(s), loss on training batch is 0.00046432.
After 6014 training step(s), loss on training batch is 0.000472568.
After 6015 training step(s), loss on training batch is 0.000425288.
After 6016 training step(s), loss on training batch is 0.000369898.
After 6017 training step(s), loss on training batch is 0.000483638.
After 6018 training step(s), loss on training batch is 0.000400173.
After 6019 training step(s), loss on training batch is 0.000500355.
After 6020 training step(s), loss on training batch is 0.000440098.
After 6021 training step(s), loss on training batch is 0.00037329.
After 6022 training step(s), loss on training batch is 0.000458388.
After 6023 training step(s), loss on training batch is 0.000394209.
After 6024 training step(s), loss on training batch is 0.000387085.
After 6025 training step(s), loss on training batch is 0.000361506.
After 6026 training step(s), loss on training batch is 0.000487553.
After 6027 training step(s), loss on training batch is 0.000349079.
After 6028 training step(s), loss on training batch is 0.00137982.
After 6029 training step(s), loss on training batch is 0.000811148.
After 6030 training step(s), loss on training batch is 0.00069942.
After 6031 training step(s), loss on training batch is 0.000709663.
After 6032 training step(s), loss on training batch is 0.000913683.
After 6033 training step(s), loss on training batch is 0.000977379.
After 6034 training step(s), loss on training batch is 0.00079107.
After 6035 training step(s), loss on training batch is 0.0005956.
After 6036 training step(s), loss on training batch is 0.000930539.
After 6037 training step(s), loss on training batch is 0.000920428.
After 6038 training step(s), loss on training batch is 0.000641713.
After 6039 training step(s), loss on training batch is 0.000624604.
After 6040 training step(s), loss on training batch is 0.00159491.
After 6041 training step(s), loss on training batch is 0.000682789.
After 6042 training step(s), loss on training batch is 0.000764624.
After 6043 training step(s), loss on training batch is 0.000809153.
After 6044 training step(s), loss on training batch is 0.000891156.
After 6045 training step(s), loss on training batch is 0.000629362.
After 6046 training step(s), loss on training batch is 0.00103476.
After 6047 training step(s), loss on training batch is 0.000687463.
After 6048 training step(s), loss on training batch is 0.000833746.
After 6049 training step(s), loss on training batch is 0.000854757.
After 6050 training step(s), loss on training batch is 0.000854441.
After 6051 training step(s), loss on training batch is 0.000872633.
After 6052 training step(s), loss on training batch is 0.000724137.
After 6053 training step(s), loss on training batch is 0.000997974.
After 6054 training step(s), loss on training batch is 0.000557567.
After 6055 training step(s), loss on training batch is 0.000808268.
After 6056 training step(s), loss on training batch is 0.00139217.
After 6057 training step(s), loss on training batch is 0.00112368.
After 6058 training step(s), loss on training batch is 0.00118992.
After 6059 training step(s), loss on training batch is 0.00119691.
After 6060 training step(s), loss on training batch is 0.00103466.
After 6061 training step(s), loss on training batch is 0.00103004.
After 6062 training step(s), loss on training batch is 0.00125774.
After 6063 training step(s), loss on training batch is 0.00116028.
After 6064 training step(s), loss on training batch is 0.00105051.
After 6065 training step(s), loss on training batch is 0.00138346.
After 6066 training step(s), loss on training batch is 0.00102539.
After 6067 training step(s), loss on training batch is 0.00101055.
After 6068 training step(s), loss on training batch is 0.00119833.
After 6069 training step(s), loss on training batch is 0.00188438.
After 6070 training step(s), loss on training batch is 0.00290503.
After 6071 training step(s), loss on training batch is 0.00167013.
After 6072 training step(s), loss on training batch is 0.00126599.
After 6073 training step(s), loss on training batch is 0.00141655.
After 6074 training step(s), loss on training batch is 0.00123291.
After 6075 training step(s), loss on training batch is 0.00117749.
After 6076 training step(s), loss on training batch is 0.00106105.
After 6077 training step(s), loss on training batch is 0.00191071.
After 6078 training step(s), loss on training batch is 0.00111724.
After 6079 training step(s), loss on training batch is 0.00105989.
After 6080 training step(s), loss on training batch is 0.00108307.
After 6081 training step(s), loss on training batch is 0.000668049.
After 6082 training step(s), loss on training batch is 0.000731195.
After 6083 training step(s), loss on training batch is 0.00039631.
After 6084 training step(s), loss on training batch is 0.000491812.
After 6085 training step(s), loss on training batch is 0.000569311.
After 6086 training step(s), loss on training batch is 0.000733563.
After 6087 training step(s), loss on training batch is 0.000911618.
After 6088 training step(s), loss on training batch is 0.000601296.
After 6089 training step(s), loss on training batch is 0.000482062.
After 6090 training step(s), loss on training batch is 0.000473289.
After 6091 training step(s), loss on training batch is 0.000402877.
After 6092 training step(s), loss on training batch is 0.000536036.
After 6093 training step(s), loss on training batch is 0.000384504.
After 6094 training step(s), loss on training batch is 0.000411243.
After 6095 training step(s), loss on training batch is 0.000515568.
After 6096 training step(s), loss on training batch is 0.000485371.
After 6097 training step(s), loss on training batch is 0.000370906.
After 6098 training step(s), loss on training batch is 0.000366959.
After 6099 training step(s), loss on training batch is 0.000379611.
After 6100 training step(s), loss on training batch is 0.000438561.
After 6101 training step(s), loss on training batch is 0.00131268.
After 6102 training step(s), loss on training batch is 0.000752363.
After 6103 training step(s), loss on training batch is 0.000690063.
After 6104 training step(s), loss on training batch is 0.000838886.
After 6105 training step(s), loss on training batch is 0.000746255.
After 6106 training step(s), loss on training batch is 0.000817851.
After 6107 training step(s), loss on training batch is 0.000897591.
After 6108 training step(s), loss on training batch is 0.000439293.
After 6109 training step(s), loss on training batch is 0.000477575.
After 6110 training step(s), loss on training batch is 0.000547936.
After 6111 training step(s), loss on training batch is 0.000679929.
After 6112 training step(s), loss on training batch is 0.000604486.
After 6113 training step(s), loss on training batch is 0.000629679.
After 6114 training step(s), loss on training batch is 0.000988442.
After 6115 training step(s), loss on training batch is 0.0019168.
After 6116 training step(s), loss on training batch is 0.00131781.
After 6117 training step(s), loss on training batch is 0.00115723.
After 6118 training step(s), loss on training batch is 0.000934279.
After 6119 training step(s), loss on training batch is 0.000495201.
After 6120 training step(s), loss on training batch is 0.000537017.
After 6121 training step(s), loss on training batch is 0.000550094.
After 6122 training step(s), loss on training batch is 0.000452263.
After 6123 training step(s), loss on training batch is 0.000713413.
After 6124 training step(s), loss on training batch is 0.000552635.
After 6125 training step(s), loss on training batch is 0.000514204.
After 6126 training step(s), loss on training batch is 0.000459622.
After 6127 training step(s), loss on training batch is 0.00130603.
After 6128 training step(s), loss on training batch is 0.00345991.
After 6129 training step(s), loss on training batch is 0.000896782.
After 6130 training step(s), loss on training batch is 0.000652873.
After 6131 training step(s), loss on training batch is 0.000751913.
After 6132 training step(s), loss on training batch is 0.000599057.
After 6133 training step(s), loss on training batch is 0.000527287.
After 6134 training step(s), loss on training batch is 0.000465174.
After 6135 training step(s), loss on training batch is 0.000402537.
After 6136 training step(s), loss on training batch is 0.00040944.
After 6137 training step(s), loss on training batch is 0.000469643.
After 6138 training step(s), loss on training batch is 0.000469841.
After 6139 training step(s), loss on training batch is 0.00051185.
After 6140 training step(s), loss on training batch is 0.000576657.
After 6141 training step(s), loss on training batch is 0.000433564.
After 6142 training step(s), loss on training batch is 0.000532907.
After 6143 training step(s), loss on training batch is 0.000479802.
After 6144 training step(s), loss on training batch is 0.000618968.
After 6145 training step(s), loss on training batch is 0.000384345.
After 6146 training step(s), loss on training batch is 0.000541282.
After 6147 training step(s), loss on training batch is 0.000428946.
After 6148 training step(s), loss on training batch is 0.000389096.
After 6149 training step(s), loss on training batch is 0.00045304.
After 6150 training step(s), loss on training batch is 0.000560207.
After 6151 training step(s), loss on training batch is 0.000517439.
After 6152 training step(s), loss on training batch is 0.000445378.
After 6153 training step(s), loss on training batch is 0.000452411.
After 6154 training step(s), loss on training batch is 0.000440304.
After 6155 training step(s), loss on training batch is 0.000453171.
After 6156 training step(s), loss on training batch is 0.000526653.
After 6157 training step(s), loss on training batch is 0.000642591.
After 6158 training step(s), loss on training batch is 0.000394885.
After 6159 training step(s), loss on training batch is 0.000456772.
After 6160 training step(s), loss on training batch is 0.0005775.
After 6161 training step(s), loss on training batch is 0.000406193.
After 6162 training step(s), loss on training batch is 0.000364964.
After 6163 training step(s), loss on training batch is 0.000413205.
After 6164 training step(s), loss on training batch is 0.000550961.
After 6165 training step(s), loss on training batch is 0.000478775.
After 6166 training step(s), loss on training batch is 0.000463335.
After 6167 training step(s), loss on training batch is 0.000490547.
After 6168 training step(s), loss on training batch is 0.000414168.
After 6169 training step(s), loss on training batch is 0.000491207.
After 6170 training step(s), loss on training batch is 0.000417135.
After 6171 training step(s), loss on training batch is 0.000474473.
After 6172 training step(s), loss on training batch is 0.000390062.
After 6173 training step(s), loss on training batch is 0.000382078.
After 6174 training step(s), loss on training batch is 0.000385123.
After 6175 training step(s), loss on training batch is 0.000461581.
After 6176 training step(s), loss on training batch is 0.000448008.
After 6177 training step(s), loss on training batch is 0.0006242.
After 6178 training step(s), loss on training batch is 0.000494833.
After 6179 training step(s), loss on training batch is 0.000455125.
After 6180 training step(s), loss on training batch is 0.000508956.
After 6181 training step(s), loss on training batch is 0.000415693.
After 6182 training step(s), loss on training batch is 0.000455013.
After 6183 training step(s), loss on training batch is 0.000915686.
After 6184 training step(s), loss on training batch is 0.00118651.
After 6185 training step(s), loss on training batch is 0.001438.
After 6186 training step(s), loss on training batch is 0.0008561.
After 6187 training step(s), loss on training batch is 0.000830755.
After 6188 training step(s), loss on training batch is 0.000728341.
After 6189 training step(s), loss on training batch is 0.000832994.
After 6190 training step(s), loss on training batch is 0.000761444.
After 6191 training step(s), loss on training batch is 0.000839657.
After 6192 training step(s), loss on training batch is 0.000979934.
After 6193 training step(s), loss on training batch is 0.000687942.
After 6194 training step(s), loss on training batch is 0.000722819.
After 6195 training step(s), loss on training batch is 0.00108148.
After 6196 training step(s), loss on training batch is 0.000954375.
After 6197 training step(s), loss on training batch is 0.000781467.
After 6198 training step(s), loss on training batch is 0.000833006.
After 6199 training step(s), loss on training batch is 0.00126301.
After 6200 training step(s), loss on training batch is 0.00085705.
After 6201 training step(s), loss on training batch is 0.000775702.
After 6202 training step(s), loss on training batch is 0.000863602.
After 6203 training step(s), loss on training batch is 0.000729641.
After 6204 training step(s), loss on training batch is 0.000710544.
After 6205 training step(s), loss on training batch is 0.000640269.
After 6206 training step(s), loss on training batch is 0.000822631.
After 6207 training step(s), loss on training batch is 0.00112969.
After 6208 training step(s), loss on training batch is 0.00095392.
After 6209 training step(s), loss on training batch is 0.000672798.
After 6210 training step(s), loss on training batch is 0.00122915.
After 6211 training step(s), loss on training batch is 0.000808565.
After 6212 training step(s), loss on training batch is 0.000823586.
After 6213 training step(s), loss on training batch is 0.000686808.
After 6214 training step(s), loss on training batch is 0.000633611.
After 6215 training step(s), loss on training batch is 0.000606456.
After 6216 training step(s), loss on training batch is 0.000613873.
After 6217 training step(s), loss on training batch is 0.000639119.
After 6218 training step(s), loss on training batch is 0.000933526.
After 6219 training step(s), loss on training batch is 0.00138138.
After 6220 training step(s), loss on training batch is 0.00162013.
After 6221 training step(s), loss on training batch is 0.00178276.
After 6222 training step(s), loss on training batch is 0.000882719.
After 6223 training step(s), loss on training batch is 0.000861163.
After 6224 training step(s), loss on training batch is 0.00083686.
After 6225 training step(s), loss on training batch is 0.000970617.
After 6226 training step(s), loss on training batch is 0.000883048.
After 6227 training step(s), loss on training batch is 0.000799891.
After 6228 training step(s), loss on training batch is 0.000819913.
After 6229 training step(s), loss on training batch is 0.000854771.
After 6230 training step(s), loss on training batch is 0.000931593.
After 6231 training step(s), loss on training batch is 0.00109565.
After 6232 training step(s), loss on training batch is 0.000700401.
After 6233 training step(s), loss on training batch is 0.00069032.
After 6234 training step(s), loss on training batch is 0.000673208.
After 6235 training step(s), loss on training batch is 0.000532413.
After 6236 training step(s), loss on training batch is 0.00104781.
After 6237 training step(s), loss on training batch is 0.00186847.
After 6238 training step(s), loss on training batch is 0.000639402.
After 6239 training step(s), loss on training batch is 0.000748915.
After 6240 training step(s), loss on training batch is 0.000835576.
After 6241 training step(s), loss on training batch is 0.000712419.
After 6242 training step(s), loss on training batch is 0.000693236.
After 6243 training step(s), loss on training batch is 0.000810003.
After 6244 training step(s), loss on training batch is 0.000732596.
After 6245 training step(s), loss on training batch is 0.000820051.
After 6246 training step(s), loss on training batch is 0.00102804.
After 6247 training step(s), loss on training batch is 0.00071122.
After 6248 training step(s), loss on training batch is 0.000810451.
After 6249 training step(s), loss on training batch is 0.000866498.
After 6250 training step(s), loss on training batch is 0.000880849.
After 6251 training step(s), loss on training batch is 0.000807931.
After 6252 training step(s), loss on training batch is 0.000623507.
After 6253 training step(s), loss on training batch is 0.00106864.
After 6254 training step(s), loss on training batch is 0.000771487.
After 6255 training step(s), loss on training batch is 0.000803653.
After 6256 training step(s), loss on training batch is 0.000703462.
After 6257 training step(s), loss on training batch is 0.000637582.
After 6258 training step(s), loss on training batch is 0.000952807.
After 6259 training step(s), loss on training batch is 0.0013906.
After 6260 training step(s), loss on training batch is 0.000778048.
After 6261 training step(s), loss on training batch is 0.000655796.
After 6262 training step(s), loss on training batch is 0.000645299.
After 6263 training step(s), loss on training batch is 0.000685906.
After 6264 training step(s), loss on training batch is 0.000606001.
After 6265 training step(s), loss on training batch is 0.000860361.
After 6266 training step(s), loss on training batch is 0.00134463.
After 6267 training step(s), loss on training batch is 0.00144114.
After 6268 training step(s), loss on training batch is 0.00126434.
After 6269 training step(s), loss on training batch is 0.00127491.
After 6270 training step(s), loss on training batch is 0.00120486.
After 6271 training step(s), loss on training batch is 0.00112755.
After 6272 training step(s), loss on training batch is 0.00119038.
After 6273 training step(s), loss on training batch is 0.0026569.
After 6274 training step(s), loss on training batch is 0.0014311.
After 6275 training step(s), loss on training batch is 0.00178745.
After 6276 training step(s), loss on training batch is 0.0013009.
After 6277 training step(s), loss on training batch is 0.00120511.
After 6278 training step(s), loss on training batch is 0.00131961.
After 6279 training step(s), loss on training batch is 0.00106366.
After 6280 training step(s), loss on training batch is 0.00106339.
After 6281 training step(s), loss on training batch is 0.00106295.
After 6282 training step(s), loss on training batch is 0.00106241.
After 6283 training step(s), loss on training batch is 0.00119609.
After 6284 training step(s), loss on training batch is 0.00107113.
After 6285 training step(s), loss on training batch is 0.00102997.
After 6286 training step(s), loss on training batch is 0.00148249.
After 6287 training step(s), loss on training batch is 0.0010481.
After 6288 training step(s), loss on training batch is 0.00135183.
After 6289 training step(s), loss on training batch is 0.0010948.
After 6290 training step(s), loss on training batch is 0.00113194.
After 6291 training step(s), loss on training batch is 0.00106372.
After 6292 training step(s), loss on training batch is 0.00118101.
After 6293 training step(s), loss on training batch is 0.00104394.
After 6294 training step(s), loss on training batch is 0.00116926.
After 6295 training step(s), loss on training batch is 0.00129506.
After 6296 training step(s), loss on training batch is 0.0013472.
After 6297 training step(s), loss on training batch is 0.00115461.
After 6298 training step(s), loss on training batch is 0.0011231.
After 6299 training step(s), loss on training batch is 0.00104825.
After 6300 training step(s), loss on training batch is 0.00109043.
After 6301 training step(s), loss on training batch is 0.000941508.
After 6302 training step(s), loss on training batch is 0.00123938.
After 6303 training step(s), loss on training batch is 0.00100421.
After 6304 training step(s), loss on training batch is 0.00142517.
After 6305 training step(s), loss on training batch is 0.00138757.
After 6306 training step(s), loss on training batch is 0.0013586.
After 6307 training step(s), loss on training batch is 0.00152109.
After 6308 training step(s), loss on training batch is 0.0012759.
After 6309 training step(s), loss on training batch is 0.0067238.
After 6310 training step(s), loss on training batch is 0.00210725.
After 6311 training step(s), loss on training batch is 0.00186917.
After 6312 training step(s), loss on training batch is 0.0018352.
After 6313 training step(s), loss on training batch is 0.00161187.
After 6314 training step(s), loss on training batch is 0.00167953.
After 6315 training step(s), loss on training batch is 0.00137081.
After 6316 training step(s), loss on training batch is 0.000586159.
After 6317 training step(s), loss on training batch is 0.000520175.
After 6318 training step(s), loss on training batch is 0.000509414.
After 6319 training step(s), loss on training batch is 0.000562156.
After 6320 training step(s), loss on training batch is 0.000644968.
After 6321 training step(s), loss on training batch is 0.000599287.
After 6322 training step(s), loss on training batch is 0.000601282.
After 6323 training step(s), loss on training batch is 0.000602014.
After 6324 training step(s), loss on training batch is 0.000629234.
After 6325 training step(s), loss on training batch is 0.000586551.
After 6326 training step(s), loss on training batch is 0.00104272.
After 6327 training step(s), loss on training batch is 0.0010525.
After 6328 training step(s), loss on training batch is 0.00122023.
After 6329 training step(s), loss on training batch is 0.0010395.
After 6330 training step(s), loss on training batch is 0.00262504.
After 6331 training step(s), loss on training batch is 0.00210222.
After 6332 training step(s), loss on training batch is 0.00137744.
After 6333 training step(s), loss on training batch is 0.00158045.
After 6334 training step(s), loss on training batch is 0.00108769.
After 6335 training step(s), loss on training batch is 0.00108924.
After 6336 training step(s), loss on training batch is 0.00112109.
After 6337 training step(s), loss on training batch is 0.00108625.
After 6338 training step(s), loss on training batch is 0.00125415.
After 6339 training step(s), loss on training batch is 0.0011808.
After 6340 training step(s), loss on training batch is 0.00111876.
After 6341 training step(s), loss on training batch is 0.00106412.
After 6342 training step(s), loss on training batch is 0.00177676.
After 6343 training step(s), loss on training batch is 0.000468159.
After 6344 training step(s), loss on training batch is 0.000643073.
After 6345 training step(s), loss on training batch is 0.000503644.
After 6346 training step(s), loss on training batch is 0.000643186.
After 6347 training step(s), loss on training batch is 0.000598765.
After 6348 training step(s), loss on training batch is 0.000532309.
After 6349 training step(s), loss on training batch is 0.000459312.
After 6350 training step(s), loss on training batch is 0.000398772.
After 6351 training step(s), loss on training batch is 0.000638687.
After 6352 training step(s), loss on training batch is 0.000541619.
After 6353 training step(s), loss on training batch is 0.000525138.
After 6354 training step(s), loss on training batch is 0.000441312.
After 6355 training step(s), loss on training batch is 0.000731801.
After 6356 training step(s), loss on training batch is 0.000480696.
After 6357 training step(s), loss on training batch is 0.000421932.
After 6358 training step(s), loss on training batch is 0.000431065.
After 6359 training step(s), loss on training batch is 0.00107475.
After 6360 training step(s), loss on training batch is 0.00140213.
After 6361 training step(s), loss on training batch is 0.000751857.
After 6362 training step(s), loss on training batch is 0.000510623.
After 6363 training step(s), loss on training batch is 0.000512105.
After 6364 training step(s), loss on training batch is 0.000644225.
After 6365 training step(s), loss on training batch is 0.000642001.
After 6366 training step(s), loss on training batch is 0.000551097.
After 6367 training step(s), loss on training batch is 0.000502098.
After 6368 training step(s), loss on training batch is 0.000636327.
After 6369 training step(s), loss on training batch is 0.000427089.
After 6370 training step(s), loss on training batch is 0.000517114.
After 6371 training step(s), loss on training batch is 0.00032996.
After 6372 training step(s), loss on training batch is 0.000438398.
After 6373 training step(s), loss on training batch is 0.000397457.
After 6374 training step(s), loss on training batch is 0.000371731.
After 6375 training step(s), loss on training batch is 0.000784017.
After 6376 training step(s), loss on training batch is 0.000632623.
After 6377 training step(s), loss on training batch is 0.000577.
After 6378 training step(s), loss on training batch is 0.000409374.
After 6379 training step(s), loss on training batch is 0.000429121.
After 6380 training step(s), loss on training batch is 0.000377889.
After 6381 training step(s), loss on training batch is 0.000442111.
After 6382 training step(s), loss on training batch is 0.00041378.
After 6383 training step(s), loss on training batch is 0.000514524.
After 6384 training step(s), loss on training batch is 0.000494387.
After 6385 training step(s), loss on training batch is 0.000501835.
After 6386 training step(s), loss on training batch is 0.000687294.
After 6387 training step(s), loss on training batch is 0.00056838.
After 6388 training step(s), loss on training batch is 0.000392221.
After 6389 training step(s), loss on training batch is 0.000453655.
After 6390 training step(s), loss on training batch is 0.000411401.
After 6391 training step(s), loss on training batch is 0.000374289.
After 6392 training step(s), loss on training batch is 0.000413262.
After 6393 training step(s), loss on training batch is 0.000382387.
After 6394 training step(s), loss on training batch is 0.00033506.
After 6395 training step(s), loss on training batch is 0.000578118.
After 6396 training step(s), loss on training batch is 0.000542533.
After 6397 training step(s), loss on training batch is 0.000314477.
After 6398 training step(s), loss on training batch is 0.000313585.
After 6399 training step(s), loss on training batch is 0.000419815.
After 6400 training step(s), loss on training batch is 0.000487764.
After 6401 training step(s), loss on training batch is 0.000674506.
After 6402 training step(s), loss on training batch is 0.000736224.
After 6403 training step(s), loss on training batch is 0.000447098.
After 6404 training step(s), loss on training batch is 0.000661899.
After 6405 training step(s), loss on training batch is 0.00143082.
After 6406 training step(s), loss on training batch is 0.000937953.
After 6407 training step(s), loss on training batch is 0.000525184.
After 6408 training step(s), loss on training batch is 0.000621847.
After 6409 training step(s), loss on training batch is 0.000768361.
After 6410 training step(s), loss on training batch is 0.000789343.
After 6411 training step(s), loss on training batch is 0.000485673.
After 6412 training step(s), loss on training batch is 0.000381995.
After 6413 training step(s), loss on training batch is 0.000436232.
After 6414 training step(s), loss on training batch is 0.000448745.
After 6415 training step(s), loss on training batch is 0.000408269.
After 6416 training step(s), loss on training batch is 0.000358907.
After 6417 training step(s), loss on training batch is 0.000467721.
After 6418 training step(s), loss on training batch is 0.000377569.
After 6419 training step(s), loss on training batch is 0.000491615.
After 6420 training step(s), loss on training batch is 0.000413555.
After 6421 training step(s), loss on training batch is 0.000372988.
After 6422 training step(s), loss on training batch is 0.000402848.
After 6423 training step(s), loss on training batch is 0.000387126.
After 6424 training step(s), loss on training batch is 0.000363295.
After 6425 training step(s), loss on training batch is 0.000340447.
After 6426 training step(s), loss on training batch is 0.00046195.
After 6427 training step(s), loss on training batch is 0.000334889.
After 6428 training step(s), loss on training batch is 0.00136391.
After 6429 training step(s), loss on training batch is 0.000767596.
After 6430 training step(s), loss on training batch is 0.00067051.
After 6431 training step(s), loss on training batch is 0.000695786.
After 6432 training step(s), loss on training batch is 0.000839263.
After 6433 training step(s), loss on training batch is 0.000906022.
After 6434 training step(s), loss on training batch is 0.000733521.
After 6435 training step(s), loss on training batch is 0.00059542.
After 6436 training step(s), loss on training batch is 0.000865626.
After 6437 training step(s), loss on training batch is 0.000871303.
After 6438 training step(s), loss on training batch is 0.000613942.
After 6439 training step(s), loss on training batch is 0.000612244.
After 6440 training step(s), loss on training batch is 0.00141606.
After 6441 training step(s), loss on training batch is 0.000674766.
After 6442 training step(s), loss on training batch is 0.000733593.
After 6443 training step(s), loss on training batch is 0.000766542.
After 6444 training step(s), loss on training batch is 0.000874803.
After 6445 training step(s), loss on training batch is 0.000611687.
After 6446 training step(s), loss on training batch is 0.00094482.
After 6447 training step(s), loss on training batch is 0.000677297.
After 6448 training step(s), loss on training batch is 0.000795034.
After 6449 training step(s), loss on training batch is 0.000808029.
After 6450 training step(s), loss on training batch is 0.000817046.
After 6451 training step(s), loss on training batch is 0.000836738.
After 6452 training step(s), loss on training batch is 0.000691809.
After 6453 training step(s), loss on training batch is 0.000977987.
After 6454 training step(s), loss on training batch is 0.00055137.
After 6455 training step(s), loss on training batch is 0.000782249.
After 6456 training step(s), loss on training batch is 0.00134065.
After 6457 training step(s), loss on training batch is 0.00111169.
After 6458 training step(s), loss on training batch is 0.0011748.
After 6459 training step(s), loss on training batch is 0.00116699.
After 6460 training step(s), loss on training batch is 0.00103401.
After 6461 training step(s), loss on training batch is 0.00102372.
After 6462 training step(s), loss on training batch is 0.00123533.
After 6463 training step(s), loss on training batch is 0.0011156.
After 6464 training step(s), loss on training batch is 0.0009785.
After 6465 training step(s), loss on training batch is 0.00143765.
After 6466 training step(s), loss on training batch is 0.000995159.
After 6467 training step(s), loss on training batch is 0.000998108.
After 6468 training step(s), loss on training batch is 0.00118576.
After 6469 training step(s), loss on training batch is 0.00201905.
After 6470 training step(s), loss on training batch is 0.00293707.
After 6471 training step(s), loss on training batch is 0.00167127.
After 6472 training step(s), loss on training batch is 0.00126359.
After 6473 training step(s), loss on training batch is 0.00141039.
After 6474 training step(s), loss on training batch is 0.00124272.
After 6475 training step(s), loss on training batch is 0.00119491.
After 6476 training step(s), loss on training batch is 0.00111905.
After 6477 training step(s), loss on training batch is 0.00180198.
After 6478 training step(s), loss on training batch is 0.00111934.
After 6479 training step(s), loss on training batch is 0.00108041.
After 6480 training step(s), loss on training batch is 0.00107808.
After 6481 training step(s), loss on training batch is 0.00063535.
After 6482 training step(s), loss on training batch is 0.00065519.
After 6483 training step(s), loss on training batch is 0.000369937.
After 6484 training step(s), loss on training batch is 0.000470105.
After 6485 training step(s), loss on training batch is 0.00053365.
After 6486 training step(s), loss on training batch is 0.000704376.
After 6487 training step(s), loss on training batch is 0.000867556.
After 6488 training step(s), loss on training batch is 0.000588366.
After 6489 training step(s), loss on training batch is 0.000462038.
After 6490 training step(s), loss on training batch is 0.000450643.
After 6491 training step(s), loss on training batch is 0.000380216.
After 6492 training step(s), loss on training batch is 0.000487047.
After 6493 training step(s), loss on training batch is 0.000382887.
After 6494 training step(s), loss on training batch is 0.000407393.
After 6495 training step(s), loss on training batch is 0.000492734.
After 6496 training step(s), loss on training batch is 0.00047195.
After 6497 training step(s), loss on training batch is 0.000363804.
After 6498 training step(s), loss on training batch is 0.000360692.
After 6499 training step(s), loss on training batch is 0.000366558.
After 6500 training step(s), loss on training batch is 0.000402196.
After 6501 training step(s), loss on training batch is 0.00126193.
After 6502 training step(s), loss on training batch is 0.000685034.
After 6503 training step(s), loss on training batch is 0.000667382.
After 6504 training step(s), loss on training batch is 0.000799967.
After 6505 training step(s), loss on training batch is 0.000709916.
After 6506 training step(s), loss on training batch is 0.000757717.
After 6507 training step(s), loss on training batch is 0.000804479.
After 6508 training step(s), loss on training batch is 0.000419219.
After 6509 training step(s), loss on training batch is 0.000453975.
After 6510 training step(s), loss on training batch is 0.000503071.
After 6511 training step(s), loss on training batch is 0.000614264.
After 6512 training step(s), loss on training batch is 0.000549765.
After 6513 training step(s), loss on training batch is 0.000577211.
After 6514 training step(s), loss on training batch is 0.000915508.
After 6515 training step(s), loss on training batch is 0.00187732.
After 6516 training step(s), loss on training batch is 0.00121862.
After 6517 training step(s), loss on training batch is 0.00106706.
After 6518 training step(s), loss on training batch is 0.000838978.
After 6519 training step(s), loss on training batch is 0.000468481.
After 6520 training step(s), loss on training batch is 0.000484674.
After 6521 training step(s), loss on training batch is 0.00049367.
After 6522 training step(s), loss on training batch is 0.000404316.
After 6523 training step(s), loss on training batch is 0.000648268.
After 6524 training step(s), loss on training batch is 0.000508812.
After 6525 training step(s), loss on training batch is 0.000491603.
After 6526 training step(s), loss on training batch is 0.000453377.
After 6527 training step(s), loss on training batch is 0.00116027.
After 6528 training step(s), loss on training batch is 0.00358583.
After 6529 training step(s), loss on training batch is 0.0204359.
After 6530 training step(s), loss on training batch is 0.00171134.
After 6531 training step(s), loss on training batch is 0.00101545.
After 6532 training step(s), loss on training batch is 0.000919997.
After 6533 training step(s), loss on training batch is 0.000766301.
After 6534 training step(s), loss on training batch is 0.000635235.
After 6535 training step(s), loss on training batch is 0.000555105.
After 6536 training step(s), loss on training batch is 0.000547322.
After 6537 training step(s), loss on training batch is 0.000728349.
After 6538 training step(s), loss on training batch is 0.000730031.
After 6539 training step(s), loss on training batch is 0.000840042.
After 6540 training step(s), loss on training batch is 0.000884857.
After 6541 training step(s), loss on training batch is 0.000621462.
After 6542 training step(s), loss on training batch is 0.000847954.
After 6543 training step(s), loss on training batch is 0.000768823.
After 6544 training step(s), loss on training batch is 0.000982761.
After 6545 training step(s), loss on training batch is 0.000539354.
After 6546 training step(s), loss on training batch is 0.000631323.
After 6547 training step(s), loss on training batch is 0.000543411.
After 6548 training step(s), loss on training batch is 0.000441545.
After 6549 training step(s), loss on training batch is 0.000581691.
After 6550 training step(s), loss on training batch is 0.000814866.
After 6551 training step(s), loss on training batch is 0.000725307.
After 6552 training step(s), loss on training batch is 0.000645317.
After 6553 training step(s), loss on training batch is 0.000531713.
After 6554 training step(s), loss on training batch is 0.00052899.
After 6555 training step(s), loss on training batch is 0.000493519.
After 6556 training step(s), loss on training batch is 0.000620231.
After 6557 training step(s), loss on training batch is 0.000825756.
After 6558 training step(s), loss on training batch is 0.000493696.
After 6559 training step(s), loss on training batch is 0.000536075.
After 6560 training step(s), loss on training batch is 0.000617206.
After 6561 training step(s), loss on training batch is 0.000465608.
After 6562 training step(s), loss on training batch is 0.000420246.
After 6563 training step(s), loss on training batch is 0.00044333.
After 6564 training step(s), loss on training batch is 0.000681544.
After 6565 training step(s), loss on training batch is 0.000614506.
After 6566 training step(s), loss on training batch is 0.000536986.
After 6567 training step(s), loss on training batch is 0.000562918.
After 6568 training step(s), loss on training batch is 0.000434048.
After 6569 training step(s), loss on training batch is 0.000609695.
After 6570 training step(s), loss on training batch is 0.000462711.
After 6571 training step(s), loss on training batch is 0.000556631.
After 6572 training step(s), loss on training batch is 0.000436668.
After 6573 training step(s), loss on training batch is 0.000419195.
After 6574 training step(s), loss on training batch is 0.000438834.
After 6575 training step(s), loss on training batch is 0.000550872.
After 6576 training step(s), loss on training batch is 0.000541232.
After 6577 training step(s), loss on training batch is 0.000653507.
After 6578 training step(s), loss on training batch is 0.000535763.
After 6579 training step(s), loss on training batch is 0.000430472.
After 6580 training step(s), loss on training batch is 0.000527511.
After 6581 training step(s), loss on training batch is 0.000455439.
After 6582 training step(s), loss on training batch is 0.000477939.
After 6583 training step(s), loss on training batch is 0.00105835.
After 6584 training step(s), loss on training batch is 0.00127265.
After 6585 training step(s), loss on training batch is 0.00169138.
After 6586 training step(s), loss on training batch is 0.000894512.
After 6587 training step(s), loss on training batch is 0.000919897.
After 6588 training step(s), loss on training batch is 0.000767742.
After 6589 training step(s), loss on training batch is 0.000846451.
After 6590 training step(s), loss on training batch is 0.000810865.
After 6591 training step(s), loss on training batch is 0.000875361.
After 6592 training step(s), loss on training batch is 0.00107441.
After 6593 training step(s), loss on training batch is 0.000707738.
After 6594 training step(s), loss on training batch is 0.000724628.
After 6595 training step(s), loss on training batch is 0.00119964.
After 6596 training step(s), loss on training batch is 0.00100595.
After 6597 training step(s), loss on training batch is 0.000798032.
After 6598 training step(s), loss on training batch is 0.000882093.
After 6599 training step(s), loss on training batch is 0.00133993.
After 6600 training step(s), loss on training batch is 0.000824128.
After 6601 training step(s), loss on training batch is 0.000842349.
After 6602 training step(s), loss on training batch is 0.00089266.
After 6603 training step(s), loss on training batch is 0.000760953.
After 6604 training step(s), loss on training batch is 0.00074305.
After 6605 training step(s), loss on training batch is 0.000656484.
After 6606 training step(s), loss on training batch is 0.000848869.
After 6607 training step(s), loss on training batch is 0.00119128.
After 6608 training step(s), loss on training batch is 0.00104004.
After 6609 training step(s), loss on training batch is 0.000672653.
After 6610 training step(s), loss on training batch is 0.00136283.
After 6611 training step(s), loss on training batch is 0.000846247.
After 6612 training step(s), loss on training batch is 0.000855852.
After 6613 training step(s), loss on training batch is 0.000706856.
After 6614 training step(s), loss on training batch is 0.000643701.
After 6615 training step(s), loss on training batch is 0.000633506.
After 6616 training step(s), loss on training batch is 0.000649362.
After 6617 training step(s), loss on training batch is 0.000666126.
After 6618 training step(s), loss on training batch is 0.000898104.
After 6619 training step(s), loss on training batch is 0.00139292.
After 6620 training step(s), loss on training batch is 0.00110203.
After 6621 training step(s), loss on training batch is 0.00217211.
After 6622 training step(s), loss on training batch is 0.000856884.
After 6623 training step(s), loss on training batch is 0.000849238.
After 6624 training step(s), loss on training batch is 0.000829115.
After 6625 training step(s), loss on training batch is 0.000995284.
After 6626 training step(s), loss on training batch is 0.000914881.
After 6627 training step(s), loss on training batch is 0.000832513.
After 6628 training step(s), loss on training batch is 0.000846342.
After 6629 training step(s), loss on training batch is 0.000877468.
After 6630 training step(s), loss on training batch is 0.0009246.
After 6631 training step(s), loss on training batch is 0.00104896.
After 6632 training step(s), loss on training batch is 0.00071726.
After 6633 training step(s), loss on training batch is 0.000705293.
After 6634 training step(s), loss on training batch is 0.000671336.
After 6635 training step(s), loss on training batch is 0.000565639.
After 6636 training step(s), loss on training batch is 0.00096648.
After 6637 training step(s), loss on training batch is 0.00187647.
After 6638 training step(s), loss on training batch is 0.000673072.
After 6639 training step(s), loss on training batch is 0.000750559.
After 6640 training step(s), loss on training batch is 0.000800936.
After 6641 training step(s), loss on training batch is 0.000711098.
After 6642 training step(s), loss on training batch is 0.000678169.
After 6643 training step(s), loss on training batch is 0.00077349.
After 6644 training step(s), loss on training batch is 0.000714229.
After 6645 training step(s), loss on training batch is 0.000784375.
After 6646 training step(s), loss on training batch is 0.000939559.
After 6647 training step(s), loss on training batch is 0.000729089.
After 6648 training step(s), loss on training batch is 0.00084994.
After 6649 training step(s), loss on training batch is 0.000845843.
After 6650 training step(s), loss on training batch is 0.000904628.
After 6651 training step(s), loss on training batch is 0.00074104.
After 6652 training step(s), loss on training batch is 0.000610362.
After 6653 training step(s), loss on training batch is 0.00112245.
After 6654 training step(s), loss on training batch is 0.000759771.
After 6655 training step(s), loss on training batch is 0.000815545.
After 6656 training step(s), loss on training batch is 0.000715295.
After 6657 training step(s), loss on training batch is 0.000632694.
After 6658 training step(s), loss on training batch is 0.000947249.
After 6659 training step(s), loss on training batch is 0.00117655.
After 6660 training step(s), loss on training batch is 0.000790108.
After 6661 training step(s), loss on training batch is 0.000642206.
After 6662 training step(s), loss on training batch is 0.000631383.
After 6663 training step(s), loss on training batch is 0.000646929.
After 6664 training step(s), loss on training batch is 0.000579605.
After 6665 training step(s), loss on training batch is 0.000871801.
After 6666 training step(s), loss on training batch is 0.00131411.
After 6667 training step(s), loss on training batch is 0.00146524.
After 6668 training step(s), loss on training batch is 0.00125283.
After 6669 training step(s), loss on training batch is 0.00123401.
After 6670 training step(s), loss on training batch is 0.00121045.
After 6671 training step(s), loss on training batch is 0.00113953.
After 6672 training step(s), loss on training batch is 0.00120786.
After 6673 training step(s), loss on training batch is 0.00194264.
After 6674 training step(s), loss on training batch is 0.00136655.
After 6675 training step(s), loss on training batch is 0.00150945.
After 6676 training step(s), loss on training batch is 0.00118143.
After 6677 training step(s), loss on training batch is 0.00111832.
After 6678 training step(s), loss on training batch is 0.0013244.
After 6679 training step(s), loss on training batch is 0.00101119.
After 6680 training step(s), loss on training batch is 0.00103915.
After 6681 training step(s), loss on training batch is 0.00103223.
After 6682 training step(s), loss on training batch is 0.00103053.
After 6683 training step(s), loss on training batch is 0.00121272.
After 6684 training step(s), loss on training batch is 0.00107256.
After 6685 training step(s), loss on training batch is 0.00101622.
After 6686 training step(s), loss on training batch is 0.00143942.
After 6687 training step(s), loss on training batch is 0.00106088.
After 6688 training step(s), loss on training batch is 0.0013613.
After 6689 training step(s), loss on training batch is 0.00111385.
After 6690 training step(s), loss on training batch is 0.00113115.
After 6691 training step(s), loss on training batch is 0.001062.
After 6692 training step(s), loss on training batch is 0.00119616.
After 6693 training step(s), loss on training batch is 0.0010396.
After 6694 training step(s), loss on training batch is 0.0011711.
After 6695 training step(s), loss on training batch is 0.0013843.
After 6696 training step(s), loss on training batch is 0.00124342.
After 6697 training step(s), loss on training batch is 0.0010978.
After 6698 training step(s), loss on training batch is 0.00107158.
After 6699 training step(s), loss on training batch is 0.00101941.
After 6700 training step(s), loss on training batch is 0.00105808.
After 6701 training step(s), loss on training batch is 0.000909609.
After 6702 training step(s), loss on training batch is 0.00118694.
After 6703 training step(s), loss on training batch is 0.000963407.
After 6704 training step(s), loss on training batch is 0.00142115.
After 6705 training step(s), loss on training batch is 0.00134505.
After 6706 training step(s), loss on training batch is 0.00129189.
After 6707 training step(s), loss on training batch is 0.00141368.
After 6708 training step(s), loss on training batch is 0.00125163.
After 6709 training step(s), loss on training batch is 0.00704039.
After 6710 training step(s), loss on training batch is 0.00194512.
After 6711 training step(s), loss on training batch is 0.00171694.
After 6712 training step(s), loss on training batch is 0.00166663.
After 6713 training step(s), loss on training batch is 0.00159314.
After 6714 training step(s), loss on training batch is 0.0015878.
After 6715 training step(s), loss on training batch is 0.00134383.
After 6716 training step(s), loss on training batch is 0.000571071.
After 6717 training step(s), loss on training batch is 0.00050791.
After 6718 training step(s), loss on training batch is 0.00049255.
After 6719 training step(s), loss on training batch is 0.000546691.
After 6720 training step(s), loss on training batch is 0.000639828.
After 6721 training step(s), loss on training batch is 0.000571818.
After 6722 training step(s), loss on training batch is 0.000569505.
After 6723 training step(s), loss on training batch is 0.000598297.
After 6724 training step(s), loss on training batch is 0.00062514.
After 6725 training step(s), loss on training batch is 0.00061829.
After 6726 training step(s), loss on training batch is 0.00100263.
After 6727 training step(s), loss on training batch is 0.00100947.
After 6728 training step(s), loss on training batch is 0.00114785.
After 6729 training step(s), loss on training batch is 0.00100173.
After 6730 training step(s), loss on training batch is 0.00253036.
After 6731 training step(s), loss on training batch is 0.00215701.
After 6732 training step(s), loss on training batch is 0.00141373.
After 6733 training step(s), loss on training batch is 0.00148212.
After 6734 training step(s), loss on training batch is 0.00113596.
After 6735 training step(s), loss on training batch is 0.0010966.
After 6736 training step(s), loss on training batch is 0.00113243.
After 6737 training step(s), loss on training batch is 0.00107857.
After 6738 training step(s), loss on training batch is 0.00121933.
After 6739 training step(s), loss on training batch is 0.00116705.
After 6740 training step(s), loss on training batch is 0.00110822.
After 6741 training step(s), loss on training batch is 0.00104977.
After 6742 training step(s), loss on training batch is 0.00169601.
After 6743 training step(s), loss on training batch is 0.000448841.
After 6744 training step(s), loss on training batch is 0.000662488.
After 6745 training step(s), loss on training batch is 0.000505134.
After 6746 training step(s), loss on training batch is 0.000658412.
After 6747 training step(s), loss on training batch is 0.000566574.
After 6748 training step(s), loss on training batch is 0.000519674.
After 6749 training step(s), loss on training batch is 0.00044427.
After 6750 training step(s), loss on training batch is 0.000387761.
After 6751 training step(s), loss on training batch is 0.000589815.
After 6752 training step(s), loss on training batch is 0.000528034.
After 6753 training step(s), loss on training batch is 0.000494847.
After 6754 training step(s), loss on training batch is 0.000424148.
After 6755 training step(s), loss on training batch is 0.000709187.
After 6756 training step(s), loss on training batch is 0.000452467.
After 6757 training step(s), loss on training batch is 0.000398747.
After 6758 training step(s), loss on training batch is 0.000429171.
After 6759 training step(s), loss on training batch is 0.00110351.
After 6760 training step(s), loss on training batch is 0.00142249.
After 6761 training step(s), loss on training batch is 0.000742086.
After 6762 training step(s), loss on training batch is 0.000494711.
After 6763 training step(s), loss on training batch is 0.000503084.
After 6764 training step(s), loss on training batch is 0.00061136.
After 6765 training step(s), loss on training batch is 0.000595435.
After 6766 training step(s), loss on training batch is 0.000521807.
After 6767 training step(s), loss on training batch is 0.000487873.
After 6768 training step(s), loss on training batch is 0.000617435.
After 6769 training step(s), loss on training batch is 0.000404671.
After 6770 training step(s), loss on training batch is 0.00050761.
After 6771 training step(s), loss on training batch is 0.000334849.
After 6772 training step(s), loss on training batch is 0.000414913.
After 6773 training step(s), loss on training batch is 0.000383719.
After 6774 training step(s), loss on training batch is 0.000357586.
After 6775 training step(s), loss on training batch is 0.000644967.
After 6776 training step(s), loss on training batch is 0.000583614.
After 6777 training step(s), loss on training batch is 0.000573621.
After 6778 training step(s), loss on training batch is 0.000407267.
After 6779 training step(s), loss on training batch is 0.000418164.
After 6780 training step(s), loss on training batch is 0.000362497.
After 6781 training step(s), loss on training batch is 0.000422993.
After 6782 training step(s), loss on training batch is 0.000388353.
After 6783 training step(s), loss on training batch is 0.000494763.
After 6784 training step(s), loss on training batch is 0.000476978.
After 6785 training step(s), loss on training batch is 0.000481748.
After 6786 training step(s), loss on training batch is 0.000666974.
After 6787 training step(s), loss on training batch is 0.000556966.
After 6788 training step(s), loss on training batch is 0.000379274.
After 6789 training step(s), loss on training batch is 0.000427888.
After 6790 training step(s), loss on training batch is 0.000392561.
After 6791 training step(s), loss on training batch is 0.000372555.
After 6792 training step(s), loss on training batch is 0.000404315.
After 6793 training step(s), loss on training batch is 0.000377744.
After 6794 training step(s), loss on training batch is 0.000333069.
After 6795 training step(s), loss on training batch is 0.000541586.
After 6796 training step(s), loss on training batch is 0.000534991.
After 6797 training step(s), loss on training batch is 0.000306992.
After 6798 training step(s), loss on training batch is 0.000305053.
After 6799 training step(s), loss on training batch is 0.000409678.
After 6800 training step(s), loss on training batch is 0.000492461.
After 6801 training step(s), loss on training batch is 0.000670003.
After 6802 training step(s), loss on training batch is 0.000738832.
After 6803 training step(s), loss on training batch is 0.00044842.
After 6804 training step(s), loss on training batch is 0.000664452.
After 6805 training step(s), loss on training batch is 0.00131574.
After 6806 training step(s), loss on training batch is 0.000912361.
After 6807 training step(s), loss on training batch is 0.000494017.
After 6808 training step(s), loss on training batch is 0.000605846.
After 6809 training step(s), loss on training batch is 0.000772039.
After 6810 training step(s), loss on training batch is 0.000806744.
After 6811 training step(s), loss on training batch is 0.000487302.
After 6812 training step(s), loss on training batch is 0.000370306.
After 6813 training step(s), loss on training batch is 0.000421728.
After 6814 training step(s), loss on training batch is 0.000434903.
After 6815 training step(s), loss on training batch is 0.000383543.
After 6816 training step(s), loss on training batch is 0.000348355.
After 6817 training step(s), loss on training batch is 0.000438899.
After 6818 training step(s), loss on training batch is 0.000373971.
After 6819 training step(s), loss on training batch is 0.000465633.
After 6820 training step(s), loss on training batch is 0.000430949.
After 6821 training step(s), loss on training batch is 0.000352071.
After 6822 training step(s), loss on training batch is 0.000421676.
After 6823 training step(s), loss on training batch is 0.000373254.
After 6824 training step(s), loss on training batch is 0.000364457.
After 6825 training step(s), loss on training batch is 0.000346027.
After 6826 training step(s), loss on training batch is 0.000451811.
After 6827 training step(s), loss on training batch is 0.000334884.
After 6828 training step(s), loss on training batch is 0.00133215.
After 6829 training step(s), loss on training batch is 0.000749674.
After 6830 training step(s), loss on training batch is 0.00065756.
After 6831 training step(s), loss on training batch is 0.000684241.
After 6832 training step(s), loss on training batch is 0.000850908.
After 6833 training step(s), loss on training batch is 0.000903069.
After 6834 training step(s), loss on training batch is 0.000741537.
After 6835 training step(s), loss on training batch is 0.000589093.
After 6836 training step(s), loss on training batch is 0.00088989.
After 6837 training step(s), loss on training batch is 0.00087776.
After 6838 training step(s), loss on training batch is 0.00061291.
After 6839 training step(s), loss on training batch is 0.000604714.
After 6840 training step(s), loss on training batch is 0.00145757.
After 6841 training step(s), loss on training batch is 0.000664723.
After 6842 training step(s), loss on training batch is 0.000742394.
After 6843 training step(s), loss on training batch is 0.000770476.
After 6844 training step(s), loss on training batch is 0.000859156.
After 6845 training step(s), loss on training batch is 0.000618839.
After 6846 training step(s), loss on training batch is 0.000918187.
After 6847 training step(s), loss on training batch is 0.000670422.
After 6848 training step(s), loss on training batch is 0.000763901.
After 6849 training step(s), loss on training batch is 0.00079416.
After 6850 training step(s), loss on training batch is 0.000792734.
After 6851 training step(s), loss on training batch is 0.000826174.
After 6852 training step(s), loss on training batch is 0.000685758.
After 6853 training step(s), loss on training batch is 0.000983269.
After 6854 training step(s), loss on training batch is 0.000544526.
After 6855 training step(s), loss on training batch is 0.000761359.
After 6856 training step(s), loss on training batch is 0.00131817.
After 6857 training step(s), loss on training batch is 0.00108849.
After 6858 training step(s), loss on training batch is 0.00113529.
After 6859 training step(s), loss on training batch is 0.00115604.
After 6860 training step(s), loss on training batch is 0.000995801.
After 6861 training step(s), loss on training batch is 0.00100433.
After 6862 training step(s), loss on training batch is 0.00119336.
After 6863 training step(s), loss on training batch is 0.00112109.
After 6864 training step(s), loss on training batch is 0.00100277.
After 6865 training step(s), loss on training batch is 0.0013504.
After 6866 training step(s), loss on training batch is 0.000987837.
After 6867 training step(s), loss on training batch is 0.000993361.
After 6868 training step(s), loss on training batch is 0.00116311.
After 6869 training step(s), loss on training batch is 0.00187732.
After 6870 training step(s), loss on training batch is 0.00287335.
After 6871 training step(s), loss on training batch is 0.00161782.
After 6872 training step(s), loss on training batch is 0.00116433.
After 6873 training step(s), loss on training batch is 0.0013237.
After 6874 training step(s), loss on training batch is 0.00119594.
After 6875 training step(s), loss on training batch is 0.00115303.
After 6876 training step(s), loss on training batch is 0.00105988.
After 6877 training step(s), loss on training batch is 0.00182798.
After 6878 training step(s), loss on training batch is 0.00108925.
After 6879 training step(s), loss on training batch is 0.0010286.
After 6880 training step(s), loss on training batch is 0.00104565.
After 6881 training step(s), loss on training batch is 0.000643456.
After 6882 training step(s), loss on training batch is 0.000651269.
After 6883 training step(s), loss on training batch is 0.000357433.
After 6884 training step(s), loss on training batch is 0.000447658.
After 6885 training step(s), loss on training batch is 0.000518688.
After 6886 training step(s), loss on training batch is 0.000695758.
After 6887 training step(s), loss on training batch is 0.000883706.
After 6888 training step(s), loss on training batch is 0.000558706.
After 6889 training step(s), loss on training batch is 0.000442593.
After 6890 training step(s), loss on training batch is 0.000431634.
After 6891 training step(s), loss on training batch is 0.000363373.
After 6892 training step(s), loss on training batch is 0.000479131.
After 6893 training step(s), loss on training batch is 0.000369229.
After 6894 training step(s), loss on training batch is 0.000387176.
After 6895 training step(s), loss on training batch is 0.000462507.
After 6896 training step(s), loss on training batch is 0.00046013.
After 6897 training step(s), loss on training batch is 0.000352105.
After 6898 training step(s), loss on training batch is 0.000351494.
After 6899 training step(s), loss on training batch is 0.000358432.
After 6900 training step(s), loss on training batch is 0.000392103.
After 6901 training step(s), loss on training batch is 0.00121309.
After 6902 training step(s), loss on training batch is 0.000673204.
After 6903 training step(s), loss on training batch is 0.000658634.
After 6904 training step(s), loss on training batch is 0.000805321.
After 6905 training step(s), loss on training batch is 0.000719384.
After 6906 training step(s), loss on training batch is 0.000741603.
After 6907 training step(s), loss on training batch is 0.000780628.
After 6908 training step(s), loss on training batch is 0.000412303.
After 6909 training step(s), loss on training batch is 0.00045095.
After 6910 training step(s), loss on training batch is 0.000489507.
After 6911 training step(s), loss on training batch is 0.000593074.
After 6912 training step(s), loss on training batch is 0.000532769.
After 6913 training step(s), loss on training batch is 0.000571579.
After 6914 training step(s), loss on training batch is 0.000901029.
After 6915 training step(s), loss on training batch is 0.00181146.
After 6916 training step(s), loss on training batch is 0.00120905.
After 6917 training step(s), loss on training batch is 0.00101683.
After 6918 training step(s), loss on training batch is 0.000820531.
After 6919 training step(s), loss on training batch is 0.00044615.
After 6920 training step(s), loss on training batch is 0.000460425.
After 6921 training step(s), loss on training batch is 0.000472531.
After 6922 training step(s), loss on training batch is 0.000400434.
After 6923 training step(s), loss on training batch is 0.000637108.
After 6924 training step(s), loss on training batch is 0.000502342.
After 6925 training step(s), loss on training batch is 0.000487241.
After 6926 training step(s), loss on training batch is 0.000457442.
After 6927 training step(s), loss on training batch is 0.0010735.
After 6928 training step(s), loss on training batch is 0.00321916.
After 6929 training step(s), loss on training batch is 0.000820404.
After 6930 training step(s), loss on training batch is 0.00057866.
After 6931 training step(s), loss on training batch is 0.000736837.
After 6932 training step(s), loss on training batch is 0.000538868.
After 6933 training step(s), loss on training batch is 0.000481571.
After 6934 training step(s), loss on training batch is 0.000439269.
After 6935 training step(s), loss on training batch is 0.000356867.
After 6936 training step(s), loss on training batch is 0.000367569.
After 6937 training step(s), loss on training batch is 0.000412327.
After 6938 training step(s), loss on training batch is 0.000406595.
After 6939 training step(s), loss on training batch is 0.000446145.
After 6940 training step(s), loss on training batch is 0.000495315.
After 6941 training step(s), loss on training batch is 0.000383648.
After 6942 training step(s), loss on training batch is 0.000471206.
After 6943 training step(s), loss on training batch is 0.00042991.
After 6944 training step(s), loss on training batch is 0.000544813.
After 6945 training step(s), loss on training batch is 0.000360385.
After 6946 training step(s), loss on training batch is 0.00050774.
After 6947 training step(s), loss on training batch is 0.000405798.
After 6948 training step(s), loss on training batch is 0.000363205.
After 6949 training step(s), loss on training batch is 0.000415475.
After 6950 training step(s), loss on training batch is 0.000518543.
After 6951 training step(s), loss on training batch is 0.000476103.
After 6952 training step(s), loss on training batch is 0.000419599.
After 6953 training step(s), loss on training batch is 0.000424195.
After 6954 training step(s), loss on training batch is 0.000396702.
After 6955 training step(s), loss on training batch is 0.000409278.
After 6956 training step(s), loss on training batch is 0.000503415.
After 6957 training step(s), loss on training batch is 0.000608456.
After 6958 training step(s), loss on training batch is 0.000372548.
After 6959 training step(s), loss on training batch is 0.000438305.
After 6960 training step(s), loss on training batch is 0.000534333.
After 6961 training step(s), loss on training batch is 0.00038898.
After 6962 training step(s), loss on training batch is 0.000352433.
After 6963 training step(s), loss on training batch is 0.000387891.
After 6964 training step(s), loss on training batch is 0.000499466.
After 6965 training step(s), loss on training batch is 0.000433124.
After 6966 training step(s), loss on training batch is 0.000422543.
After 6967 training step(s), loss on training batch is 0.000454887.
After 6968 training step(s), loss on training batch is 0.00040062.
After 6969 training step(s), loss on training batch is 0.000433723.
After 6970 training step(s), loss on training batch is 0.000387522.
After 6971 training step(s), loss on training batch is 0.000489137.
After 6972 training step(s), loss on training batch is 0.000382086.
After 6973 training step(s), loss on training batch is 0.000369948.
After 6974 training step(s), loss on training batch is 0.000366685.
After 6975 training step(s), loss on training batch is 0.000429746.
After 6976 training step(s), loss on training batch is 0.000418746.
After 6977 training step(s), loss on training batch is 0.00060501.
After 6978 training step(s), loss on training batch is 0.000453335.
After 6979 training step(s), loss on training batch is 0.00042521.
After 6980 training step(s), loss on training batch is 0.000460306.
After 6981 training step(s), loss on training batch is 0.000383403.
After 6982 training step(s), loss on training batch is 0.000421478.
After 6983 training step(s), loss on training batch is 0.000940453.
After 6984 training step(s), loss on training batch is 0.00112172.
After 6985 training step(s), loss on training batch is 0.00138984.
After 6986 training step(s), loss on training batch is 0.0008022.
After 6987 training step(s), loss on training batch is 0.000783835.
After 6988 training step(s), loss on training batch is 0.000682322.
After 6989 training step(s), loss on training batch is 0.00076858.
After 6990 training step(s), loss on training batch is 0.000715114.
After 6991 training step(s), loss on training batch is 0.000797036.
After 6992 training step(s), loss on training batch is 0.000930381.
After 6993 training step(s), loss on training batch is 0.000637506.
After 6994 training step(s), loss on training batch is 0.000641678.
After 6995 training step(s), loss on training batch is 0.00111561.
After 6996 training step(s), loss on training batch is 0.000905109.
After 6997 training step(s), loss on training batch is 0.000721149.
After 6998 training step(s), loss on training batch is 0.000858585.
After 6999 training step(s), loss on training batch is 0.00119089.
After 7000 training step(s), loss on training batch is 0.000788799.
After 7001 training step(s), loss on training batch is 0.000736921.
After 7002 training step(s), loss on training batch is 0.000837683.
After 7003 training step(s), loss on training batch is 0.000701303.
After 7004 training step(s), loss on training batch is 0.000647552.
After 7005 training step(s), loss on training batch is 0.00060267.
After 7006 training step(s), loss on training batch is 0.000769661.
After 7007 training step(s), loss on training batch is 0.00108329.
After 7008 training step(s), loss on training batch is 0.000928528.
After 7009 training step(s), loss on training batch is 0.000632969.
After 7010 training step(s), loss on training batch is 0.00120265.
After 7011 training step(s), loss on training batch is 0.000739259.
After 7012 training step(s), loss on training batch is 0.000777514.
After 7013 training step(s), loss on training batch is 0.000637134.
After 7014 training step(s), loss on training batch is 0.000593073.
After 7015 training step(s), loss on training batch is 0.000585503.
After 7016 training step(s), loss on training batch is 0.000585021.
After 7017 training step(s), loss on training batch is 0.000607575.
After 7018 training step(s), loss on training batch is 0.000875203.
After 7019 training step(s), loss on training batch is 0.00125422.
After 7020 training step(s), loss on training batch is 0.00121168.
After 7021 training step(s), loss on training batch is 0.00168117.
After 7022 training step(s), loss on training batch is 0.000827007.
After 7023 training step(s), loss on training batch is 0.000816174.
After 7024 training step(s), loss on training batch is 0.000811264.
After 7025 training step(s), loss on training batch is 0.000943455.
After 7026 training step(s), loss on training batch is 0.000867865.
After 7027 training step(s), loss on training batch is 0.000757366.
After 7028 training step(s), loss on training batch is 0.000778906.
After 7029 training step(s), loss on training batch is 0.000808957.
After 7030 training step(s), loss on training batch is 0.000882976.
After 7031 training step(s), loss on training batch is 0.00102574.
After 7032 training step(s), loss on training batch is 0.000666948.
After 7033 training step(s), loss on training batch is 0.000677931.
After 7034 training step(s), loss on training batch is 0.000628958.
After 7035 training step(s), loss on training batch is 0.000539277.
After 7036 training step(s), loss on training batch is 0.000920528.
After 7037 training step(s), loss on training batch is 0.00162526.
After 7038 training step(s), loss on training batch is 0.000594899.
After 7039 training step(s), loss on training batch is 0.000697927.
After 7040 training step(s), loss on training batch is 0.000788709.
After 7041 training step(s), loss on training batch is 0.000669432.
After 7042 training step(s), loss on training batch is 0.000652621.
After 7043 training step(s), loss on training batch is 0.000755567.
After 7044 training step(s), loss on training batch is 0.000686999.
After 7045 training step(s), loss on training batch is 0.000770269.
After 7046 training step(s), loss on training batch is 0.000918651.
After 7047 training step(s), loss on training batch is 0.000702375.
After 7048 training step(s), loss on training batch is 0.000741711.
After 7049 training step(s), loss on training batch is 0.00079844.
After 7050 training step(s), loss on training batch is 0.000834194.
After 7051 training step(s), loss on training batch is 0.00070174.
After 7052 training step(s), loss on training batch is 0.000591825.
After 7053 training step(s), loss on training batch is 0.00100025.
After 7054 training step(s), loss on training batch is 0.000717044.
After 7055 training step(s), loss on training batch is 0.000763337.
After 7056 training step(s), loss on training batch is 0.000673661.
After 7057 training step(s), loss on training batch is 0.000613458.
After 7058 training step(s), loss on training batch is 0.000926752.
After 7059 training step(s), loss on training batch is 0.00133362.
After 7060 training step(s), loss on training batch is 0.000731772.
After 7061 training step(s), loss on training batch is 0.000611293.
After 7062 training step(s), loss on training batch is 0.000599116.
After 7063 training step(s), loss on training batch is 0.000630863.
After 7064 training step(s), loss on training batch is 0.000560826.
After 7065 training step(s), loss on training batch is 0.000833011.
After 7066 training step(s), loss on training batch is 0.00123228.
After 7067 training step(s), loss on training batch is 0.00135839.
After 7068 training step(s), loss on training batch is 0.00118538.
After 7069 training step(s), loss on training batch is 0.0011985.
After 7070 training step(s), loss on training batch is 0.00114341.
After 7071 training step(s), loss on training batch is 0.00108134.
After 7072 training step(s), loss on training batch is 0.00114146.
After 7073 training step(s), loss on training batch is 0.00235417.
After 7074 training step(s), loss on training batch is 0.0013408.
After 7075 training step(s), loss on training batch is 0.00153882.
After 7076 training step(s), loss on training batch is 0.00118364.
After 7077 training step(s), loss on training batch is 0.00109543.
After 7078 training step(s), loss on training batch is 0.00125289.
After 7079 training step(s), loss on training batch is 0.00103918.
After 7080 training step(s), loss on training batch is 0.00100039.
After 7081 training step(s), loss on training batch is 0.0010109.
After 7082 training step(s), loss on training batch is 0.00102404.
After 7083 training step(s), loss on training batch is 0.00113591.
After 7084 training step(s), loss on training batch is 0.00102929.
After 7085 training step(s), loss on training batch is 0.00100186.
After 7086 training step(s), loss on training batch is 0.00136278.
After 7087 training step(s), loss on training batch is 0.00103152.
After 7088 training step(s), loss on training batch is 0.00129678.
After 7089 training step(s), loss on training batch is 0.00105668.
After 7090 training step(s), loss on training batch is 0.00106646.
After 7091 training step(s), loss on training batch is 0.000971807.
After 7092 training step(s), loss on training batch is 0.00112078.
After 7093 training step(s), loss on training batch is 0.000991566.
After 7094 training step(s), loss on training batch is 0.00112779.
After 7095 training step(s), loss on training batch is 0.00125232.
After 7096 training step(s), loss on training batch is 0.0012562.
After 7097 training step(s), loss on training batch is 0.00108429.
After 7098 training step(s), loss on training batch is 0.00106078.
After 7099 training step(s), loss on training batch is 0.000998189.
After 7100 training step(s), loss on training batch is 0.000974705.
After 7101 training step(s), loss on training batch is 0.000896995.
After 7102 training step(s), loss on training batch is 0.00123994.
After 7103 training step(s), loss on training batch is 0.000933378.
After 7104 training step(s), loss on training batch is 0.00140192.
After 7105 training step(s), loss on training batch is 0.00131963.
After 7106 training step(s), loss on training batch is 0.00129251.
After 7107 training step(s), loss on training batch is 0.00144269.
After 7108 training step(s), loss on training batch is 0.00120839.
After 7109 training step(s), loss on training batch is 0.00673327.
After 7110 training step(s), loss on training batch is 0.00195293.
After 7111 training step(s), loss on training batch is 0.00166304.
After 7112 training step(s), loss on training batch is 0.00162402.
After 7113 training step(s), loss on training batch is 0.00145873.
After 7114 training step(s), loss on training batch is 0.00159844.
After 7115 training step(s), loss on training batch is 0.00125426.
After 7116 training step(s), loss on training batch is 0.000551393.
After 7117 training step(s), loss on training batch is 0.000487774.
After 7118 training step(s), loss on training batch is 0.000469602.
After 7119 training step(s), loss on training batch is 0.000523506.
After 7120 training step(s), loss on training batch is 0.000597044.
After 7121 training step(s), loss on training batch is 0.000555317.
After 7122 training step(s), loss on training batch is 0.00055126.
After 7123 training step(s), loss on training batch is 0.000561786.
After 7124 training step(s), loss on training batch is 0.000590867.
After 7125 training step(s), loss on training batch is 0.000561855.
After 7126 training step(s), loss on training batch is 0.000981579.
After 7127 training step(s), loss on training batch is 0.000987466.
After 7128 training step(s), loss on training batch is 0.00115848.
After 7129 training step(s), loss on training batch is 0.000975568.
After 7130 training step(s), loss on training batch is 0.00241532.
After 7131 training step(s), loss on training batch is 0.00197756.
After 7132 training step(s), loss on training batch is 0.00136223.
After 7133 training step(s), loss on training batch is 0.00134849.
After 7134 training step(s), loss on training batch is 0.00110306.
After 7135 training step(s), loss on training batch is 0.00105921.
After 7136 training step(s), loss on training batch is 0.00111774.
After 7137 training step(s), loss on training batch is 0.00106713.
After 7138 training step(s), loss on training batch is 0.00119974.
After 7139 training step(s), loss on training batch is 0.00112936.
After 7140 training step(s), loss on training batch is 0.00105535.
After 7141 training step(s), loss on training batch is 0.00100113.
After 7142 training step(s), loss on training batch is 0.00174774.
After 7143 training step(s), loss on training batch is 0.000446001.
After 7144 training step(s), loss on training batch is 0.000557222.
After 7145 training step(s), loss on training batch is 0.000465608.
After 7146 training step(s), loss on training batch is 0.000583657.
After 7147 training step(s), loss on training batch is 0.000528428.
After 7148 training step(s), loss on training batch is 0.000517173.
After 7149 training step(s), loss on training batch is 0.000423706.
After 7150 training step(s), loss on training batch is 0.000374093.
After 7151 training step(s), loss on training batch is 0.000584927.
After 7152 training step(s), loss on training batch is 0.000503748.
After 7153 training step(s), loss on training batch is 0.000473997.
After 7154 training step(s), loss on training batch is 0.000407793.
After 7155 training step(s), loss on training batch is 0.000682261.
After 7156 training step(s), loss on training batch is 0.000420243.
After 7157 training step(s), loss on training batch is 0.000383457.
After 7158 training step(s), loss on training batch is 0.000418517.
After 7159 training step(s), loss on training batch is 0.000995359.
After 7160 training step(s), loss on training batch is 0.00129553.
After 7161 training step(s), loss on training batch is 0.000680005.
After 7162 training step(s), loss on training batch is 0.000481147.
After 7163 training step(s), loss on training batch is 0.000479209.
After 7164 training step(s), loss on training batch is 0.000602443.
After 7165 training step(s), loss on training batch is 0.00058583.
After 7166 training step(s), loss on training batch is 0.000506715.
After 7167 training step(s), loss on training batch is 0.000443825.
After 7168 training step(s), loss on training batch is 0.000599322.
After 7169 training step(s), loss on training batch is 0.000395045.
After 7170 training step(s), loss on training batch is 0.000497574.
After 7171 training step(s), loss on training batch is 0.000327629.
After 7172 training step(s), loss on training batch is 0.000376428.
After 7173 training step(s), loss on training batch is 0.000387851.
After 7174 training step(s), loss on training batch is 0.000357592.
After 7175 training step(s), loss on training batch is 0.000550942.
After 7176 training step(s), loss on training batch is 0.00051489.
After 7177 training step(s), loss on training batch is 0.00053544.
After 7178 training step(s), loss on training batch is 0.000392938.
After 7179 training step(s), loss on training batch is 0.000405903.
After 7180 training step(s), loss on training batch is 0.000353773.
After 7181 training step(s), loss on training batch is 0.00040652.
After 7182 training step(s), loss on training batch is 0.000375427.
After 7183 training step(s), loss on training batch is 0.000462173.
After 7184 training step(s), loss on training batch is 0.000477109.
After 7185 training step(s), loss on training batch is 0.000465092.
After 7186 training step(s), loss on training batch is 0.000622.
After 7187 training step(s), loss on training batch is 0.000516628.
After 7188 training step(s), loss on training batch is 0.000367414.
After 7189 training step(s), loss on training batch is 0.000417805.
After 7190 training step(s), loss on training batch is 0.000382945.
After 7191 training step(s), loss on training batch is 0.00035654.
After 7192 training step(s), loss on training batch is 0.000390189.
After 7193 training step(s), loss on training batch is 0.000360772.
After 7194 training step(s), loss on training batch is 0.000316337.
After 7195 training step(s), loss on training batch is 0.000547788.
After 7196 training step(s), loss on training batch is 0.00051021.
After 7197 training step(s), loss on training batch is 0.000297229.
After 7198 training step(s), loss on training batch is 0.000297065.
After 7199 training step(s), loss on training batch is 0.00039121.
After 7200 training step(s), loss on training batch is 0.000446326.
After 7201 training step(s), loss on training batch is 0.000626748.
After 7202 training step(s), loss on training batch is 0.000699716.
After 7203 training step(s), loss on training batch is 0.000403826.
After 7204 training step(s), loss on training batch is 0.000615905.
After 7205 training step(s), loss on training batch is 0.00138178.
After 7206 training step(s), loss on training batch is 0.000866934.
After 7207 training step(s), loss on training batch is 0.000474584.
After 7208 training step(s), loss on training batch is 0.000566159.
After 7209 training step(s), loss on training batch is 0.000727359.
After 7210 training step(s), loss on training batch is 0.000752009.
After 7211 training step(s), loss on training batch is 0.000470465.
After 7212 training step(s), loss on training batch is 0.000368548.
After 7213 training step(s), loss on training batch is 0.000411909.
After 7214 training step(s), loss on training batch is 0.000429183.
After 7215 training step(s), loss on training batch is 0.00039671.
After 7216 training step(s), loss on training batch is 0.000345803.
After 7217 training step(s), loss on training batch is 0.000438313.
After 7218 training step(s), loss on training batch is 0.000366923.
After 7219 training step(s), loss on training batch is 0.000461422.
After 7220 training step(s), loss on training batch is 0.000424264.
After 7221 training step(s), loss on training batch is 0.000346356.
After 7222 training step(s), loss on training batch is 0.00041052.
After 7223 training step(s), loss on training batch is 0.000366815.
After 7224 training step(s), loss on training batch is 0.000349603.
After 7225 training step(s), loss on training batch is 0.000331923.
After 7226 training step(s), loss on training batch is 0.000435398.
After 7227 training step(s), loss on training batch is 0.000329907.
After 7228 training step(s), loss on training batch is 0.00125247.
After 7229 training step(s), loss on training batch is 0.000718913.
After 7230 training step(s), loss on training batch is 0.00063993.
After 7231 training step(s), loss on training batch is 0.000666031.
After 7232 training step(s), loss on training batch is 0.000788262.
After 7233 training step(s), loss on training batch is 0.000864446.
After 7234 training step(s), loss on training batch is 0.000712339.
After 7235 training step(s), loss on training batch is 0.000572692.
After 7236 training step(s), loss on training batch is 0.000831914.
After 7237 training step(s), loss on training batch is 0.000820419.
After 7238 training step(s), loss on training batch is 0.000583265.
After 7239 training step(s), loss on training batch is 0.000582095.
After 7240 training step(s), loss on training batch is 0.00133327.
After 7241 training step(s), loss on training batch is 0.00064033.
After 7242 training step(s), loss on training batch is 0.000712333.
After 7243 training step(s), loss on training batch is 0.000740834.
After 7244 training step(s), loss on training batch is 0.000831681.
After 7245 training step(s), loss on training batch is 0.000601492.
After 7246 training step(s), loss on training batch is 0.000843347.
After 7247 training step(s), loss on training batch is 0.000650309.
After 7248 training step(s), loss on training batch is 0.000741659.
After 7249 training step(s), loss on training batch is 0.000760936.
After 7250 training step(s), loss on training batch is 0.00076339.
After 7251 training step(s), loss on training batch is 0.00080806.
After 7252 training step(s), loss on training batch is 0.000667781.
After 7253 training step(s), loss on training batch is 0.000949569.
After 7254 training step(s), loss on training batch is 0.000521223.
After 7255 training step(s), loss on training batch is 0.000735029.
After 7256 training step(s), loss on training batch is 0.00125542.
After 7257 training step(s), loss on training batch is 0.00104728.
After 7258 training step(s), loss on training batch is 0.00109863.
After 7259 training step(s), loss on training batch is 0.00110482.
After 7260 training step(s), loss on training batch is 0.000959294.
After 7261 training step(s), loss on training batch is 0.000975052.
After 7262 training step(s), loss on training batch is 0.00119041.
After 7263 training step(s), loss on training batch is 0.0010782.
After 7264 training step(s), loss on training batch is 0.000963377.
After 7265 training step(s), loss on training batch is 0.0013221.
After 7266 training step(s), loss on training batch is 0.000968538.
After 7267 training step(s), loss on training batch is 0.00093821.
After 7268 training step(s), loss on training batch is 0.00113734.
After 7269 training step(s), loss on training batch is 0.00203706.
After 7270 training step(s), loss on training batch is 0.0029293.
After 7271 training step(s), loss on training batch is 0.00161403.
After 7272 training step(s), loss on training batch is 0.00123593.
After 7273 training step(s), loss on training batch is 0.0013584.
After 7274 training step(s), loss on training batch is 0.00120822.
After 7275 training step(s), loss on training batch is 0.00114205.
After 7276 training step(s), loss on training batch is 0.00107157.
After 7277 training step(s), loss on training batch is 0.00172471.
After 7278 training step(s), loss on training batch is 0.00105267.
After 7279 training step(s), loss on training batch is 0.00100404.
After 7280 training step(s), loss on training batch is 0.00100056.
After 7281 training step(s), loss on training batch is 0.000669832.
After 7282 training step(s), loss on training batch is 0.000632927.
After 7283 training step(s), loss on training batch is 0.000337017.
After 7284 training step(s), loss on training batch is 0.000442749.
After 7285 training step(s), loss on training batch is 0.000500706.
After 7286 training step(s), loss on training batch is 0.000690431.
After 7287 training step(s), loss on training batch is 0.000834112.
After 7288 training step(s), loss on training batch is 0.000539372.
After 7289 training step(s), loss on training batch is 0.000428331.
After 7290 training step(s), loss on training batch is 0.000417814.
After 7291 training step(s), loss on training batch is 0.000347525.
After 7292 training step(s), loss on training batch is 0.000452517.
After 7293 training step(s), loss on training batch is 0.000363601.
After 7294 training step(s), loss on training batch is 0.000381673.
After 7295 training step(s), loss on training batch is 0.000454906.
After 7296 training step(s), loss on training batch is 0.000442333.
After 7297 training step(s), loss on training batch is 0.00034301.
After 7298 training step(s), loss on training batch is 0.000337428.
After 7299 training step(s), loss on training batch is 0.000345341.
After 7300 training step(s), loss on training batch is 0.000392072.
After 7301 training step(s), loss on training batch is 0.0012578.
After 7302 training step(s), loss on training batch is 0.00064982.
After 7303 training step(s), loss on training batch is 0.000627668.
After 7304 training step(s), loss on training batch is 0.00076908.
After 7305 training step(s), loss on training batch is 0.000689384.
After 7306 training step(s), loss on training batch is 0.000717534.
After 7307 training step(s), loss on training batch is 0.000754109.
After 7308 training step(s), loss on training batch is 0.000398814.
After 7309 training step(s), loss on training batch is 0.000441682.
After 7310 training step(s), loss on training batch is 0.000474894.
After 7311 training step(s), loss on training batch is 0.000578677.
After 7312 training step(s), loss on training batch is 0.000526022.
After 7313 training step(s), loss on training batch is 0.000551736.
After 7314 training step(s), loss on training batch is 0.000803295.
After 7315 training step(s), loss on training batch is 0.00176346.
After 7316 training step(s), loss on training batch is 0.00114896.
After 7317 training step(s), loss on training batch is 0.000985269.
After 7318 training step(s), loss on training batch is 0.000768996.
After 7319 training step(s), loss on training batch is 0.000440506.
After 7320 training step(s), loss on training batch is 0.000455286.
After 7321 training step(s), loss on training batch is 0.00046593.
After 7322 training step(s), loss on training batch is 0.000395868.
After 7323 training step(s), loss on training batch is 0.000632967.
After 7324 training step(s), loss on training batch is 0.000492606.
After 7325 training step(s), loss on training batch is 0.000471348.
After 7326 training step(s), loss on training batch is 0.000446695.
After 7327 training step(s), loss on training batch is 0.00106512.
After 7328 training step(s), loss on training batch is 0.00322878.
After 7329 training step(s), loss on training batch is 0.000821893.
After 7330 training step(s), loss on training batch is 0.000609373.
After 7331 training step(s), loss on training batch is 0.000636743.
After 7332 training step(s), loss on training batch is 0.000546566.
After 7333 training step(s), loss on training batch is 0.000483417.
After 7334 training step(s), loss on training batch is 0.000415836.
After 7335 training step(s), loss on training batch is 0.000370641.
After 7336 training step(s), loss on training batch is 0.000373915.
After 7337 training step(s), loss on training batch is 0.000419662.
After 7338 training step(s), loss on training batch is 0.000414489.
After 7339 training step(s), loss on training batch is 0.000445133.
After 7340 training step(s), loss on training batch is 0.000501516.
After 7341 training step(s), loss on training batch is 0.000377071.
After 7342 training step(s), loss on training batch is 0.000455088.
After 7343 training step(s), loss on training batch is 0.000416247.
After 7344 training step(s), loss on training batch is 0.00053176.
After 7345 training step(s), loss on training batch is 0.000352788.
After 7346 training step(s), loss on training batch is 0.000507719.
After 7347 training step(s), loss on training batch is 0.00039179.
After 7348 training step(s), loss on training batch is 0.000356471.
After 7349 training step(s), loss on training batch is 0.000401494.
After 7350 training step(s), loss on training batch is 0.000510975.
After 7351 training step(s), loss on training batch is 0.000471556.
After 7352 training step(s), loss on training batch is 0.000412249.
After 7353 training step(s), loss on training batch is 0.000418361.
After 7354 training step(s), loss on training batch is 0.000390167.
After 7355 training step(s), loss on training batch is 0.000403795.
After 7356 training step(s), loss on training batch is 0.000483221.
After 7357 training step(s), loss on training batch is 0.000586322.
After 7358 training step(s), loss on training batch is 0.000364988.
After 7359 training step(s), loss on training batch is 0.000423192.
After 7360 training step(s), loss on training batch is 0.000541259.
After 7361 training step(s), loss on training batch is 0.000375498.
After 7362 training step(s), loss on training batch is 0.000342754.
After 7363 training step(s), loss on training batch is 0.00037649.
After 7364 training step(s), loss on training batch is 0.000484182.
After 7365 training step(s), loss on training batch is 0.00042767.
After 7366 training step(s), loss on training batch is 0.000422238.
After 7367 training step(s), loss on training batch is 0.00044753.
After 7368 training step(s), loss on training batch is 0.000396419.
After 7369 training step(s), loss on training batch is 0.000439464.
After 7370 training step(s), loss on training batch is 0.000380168.
After 7371 training step(s), loss on training batch is 0.000457417.
After 7372 training step(s), loss on training batch is 0.000373402.
After 7373 training step(s), loss on training batch is 0.000362382.
After 7374 training step(s), loss on training batch is 0.000351896.
After 7375 training step(s), loss on training batch is 0.000418453.
After 7376 training step(s), loss on training batch is 0.000407788.
After 7377 training step(s), loss on training batch is 0.000559011.
After 7378 training step(s), loss on training batch is 0.000443808.
After 7379 training step(s), loss on training batch is 0.000427616.
After 7380 training step(s), loss on training batch is 0.000429327.
After 7381 training step(s), loss on training batch is 0.000383314.
After 7382 training step(s), loss on training batch is 0.000414487.
After 7383 training step(s), loss on training batch is 0.000797482.
After 7384 training step(s), loss on training batch is 0.00102632.
After 7385 training step(s), loss on training batch is 0.00126828.
After 7386 training step(s), loss on training batch is 0.000774749.
After 7387 training step(s), loss on training batch is 0.00075228.
After 7388 training step(s), loss on training batch is 0.000669014.
After 7389 training step(s), loss on training batch is 0.000760515.
After 7390 training step(s), loss on training batch is 0.000673764.
After 7391 training step(s), loss on training batch is 0.00077268.
After 7392 training step(s), loss on training batch is 0.000880925.
After 7393 training step(s), loss on training batch is 0.000636911.
After 7394 training step(s), loss on training batch is 0.000658814.
After 7395 training step(s), loss on training batch is 0.00099168.
After 7396 training step(s), loss on training batch is 0.000842397.
After 7397 training step(s), loss on training batch is 0.00072336.
After 7398 training step(s), loss on training batch is 0.000739518.
After 7399 training step(s), loss on training batch is 0.0011184.
After 7400 training step(s), loss on training batch is 0.00077332.
After 7401 training step(s), loss on training batch is 0.000711038.
After 7402 training step(s), loss on training batch is 0.000823537.
After 7403 training step(s), loss on training batch is 0.000690392.
After 7404 training step(s), loss on training batch is 0.00066894.
After 7405 training step(s), loss on training batch is 0.00059426.
After 7406 training step(s), loss on training batch is 0.000754066.
After 7407 training step(s), loss on training batch is 0.00101625.
After 7408 training step(s), loss on training batch is 0.000860926.
After 7409 training step(s), loss on training batch is 0.000620403.
After 7410 training step(s), loss on training batch is 0.00109715.
After 7411 training step(s), loss on training batch is 0.000748483.
After 7412 training step(s), loss on training batch is 0.000744612.
After 7413 training step(s), loss on training batch is 0.000624463.
After 7414 training step(s), loss on training batch is 0.000584894.
After 7415 training step(s), loss on training batch is 0.000572348.
After 7416 training step(s), loss on training batch is 0.000574613.
After 7417 training step(s), loss on training batch is 0.000592374.
After 7418 training step(s), loss on training batch is 0.000861368.
After 7419 training step(s), loss on training batch is 0.00117103.
After 7420 training step(s), loss on training batch is 0.00120942.
After 7421 training step(s), loss on training batch is 0.00147622.
After 7422 training step(s), loss on training batch is 0.000824946.
After 7423 training step(s), loss on training batch is 0.000820242.
After 7424 training step(s), loss on training batch is 0.000808884.
After 7425 training step(s), loss on training batch is 0.000925407.
After 7426 training step(s), loss on training batch is 0.000882207.
After 7427 training step(s), loss on training batch is 0.000778688.
After 7428 training step(s), loss on training batch is 0.000791258.
After 7429 training step(s), loss on training batch is 0.000828393.
After 7430 training step(s), loss on training batch is 0.000861179.
After 7431 training step(s), loss on training batch is 0.000987368.
After 7432 training step(s), loss on training batch is 0.000639392.
After 7433 training step(s), loss on training batch is 0.000659693.
After 7434 training step(s), loss on training batch is 0.000616744.
After 7435 training step(s), loss on training batch is 0.000536085.
After 7436 training step(s), loss on training batch is 0.00088301.
After 7437 training step(s), loss on training batch is 0.00151295.
After 7438 training step(s), loss on training batch is 0.000575297.
After 7439 training step(s), loss on training batch is 0.000674602.
After 7440 training step(s), loss on training batch is 0.000766842.
After 7441 training step(s), loss on training batch is 0.000664184.
After 7442 training step(s), loss on training batch is 0.000644937.
After 7443 training step(s), loss on training batch is 0.0007372.
After 7444 training step(s), loss on training batch is 0.000669608.
After 7445 training step(s), loss on training batch is 0.000742524.
After 7446 training step(s), loss on training batch is 0.000892643.
After 7447 training step(s), loss on training batch is 0.000688054.
After 7448 training step(s), loss on training batch is 0.000712224.
After 7449 training step(s), loss on training batch is 0.00076516.
After 7450 training step(s), loss on training batch is 0.000806136.
After 7451 training step(s), loss on training batch is 0.000725444.
After 7452 training step(s), loss on training batch is 0.000579768.
After 7453 training step(s), loss on training batch is 0.000949729.
After 7454 training step(s), loss on training batch is 0.000698376.
After 7455 training step(s), loss on training batch is 0.00074218.
After 7456 training step(s), loss on training batch is 0.000650275.
After 7457 training step(s), loss on training batch is 0.000600069.
After 7458 training step(s), loss on training batch is 0.000849034.
After 7459 training step(s), loss on training batch is 0.00118535.
After 7460 training step(s), loss on training batch is 0.0006942.
After 7461 training step(s), loss on training batch is 0.000624791.
After 7462 training step(s), loss on training batch is 0.00062046.
After 7463 training step(s), loss on training batch is 0.000641914.
After 7464 training step(s), loss on training batch is 0.000571296.
After 7465 training step(s), loss on training batch is 0.000794737.
After 7466 training step(s), loss on training batch is 0.00121973.
After 7467 training step(s), loss on training batch is 0.00132517.
After 7468 training step(s), loss on training batch is 0.00116636.
After 7469 training step(s), loss on training batch is 0.00120239.
After 7470 training step(s), loss on training batch is 0.00112946.
After 7471 training step(s), loss on training batch is 0.00107858.
After 7472 training step(s), loss on training batch is 0.00111869.
After 7473 training step(s), loss on training batch is 0.00201541.
After 7474 training step(s), loss on training batch is 0.00130136.
After 7475 training step(s), loss on training batch is 0.00149445.
After 7476 training step(s), loss on training batch is 0.00121311.
After 7477 training step(s), loss on training batch is 0.00111948.
After 7478 training step(s), loss on training batch is 0.00122024.
After 7479 training step(s), loss on training batch is 0.00105656.
After 7480 training step(s), loss on training batch is 0.00100429.
After 7481 training step(s), loss on training batch is 0.00101221.
After 7482 training step(s), loss on training batch is 0.00102352.
After 7483 training step(s), loss on training batch is 0.00110368.
After 7484 training step(s), loss on training batch is 0.00100685.
After 7485 training step(s), loss on training batch is 0.000972605.
After 7486 training step(s), loss on training batch is 0.00134383.
After 7487 training step(s), loss on training batch is 0.00101309.
After 7488 training step(s), loss on training batch is 0.00125514.
After 7489 training step(s), loss on training batch is 0.00104508.
After 7490 training step(s), loss on training batch is 0.00105264.
After 7491 training step(s), loss on training batch is 0.0010042.
After 7492 training step(s), loss on training batch is 0.00105529.
After 7493 training step(s), loss on training batch is 0.000977851.
After 7494 training step(s), loss on training batch is 0.00118407.
After 7495 training step(s), loss on training batch is 0.00133428.
After 7496 training step(s), loss on training batch is 0.00127131.
After 7497 training step(s), loss on training batch is 0.00103544.
After 7498 training step(s), loss on training batch is 0.00102801.
After 7499 training step(s), loss on training batch is 0.000972561.
After 7500 training step(s), loss on training batch is 0.00098302.
After 7501 training step(s), loss on training batch is 0.000879247.
After 7502 training step(s), loss on training batch is 0.00111711.
After 7503 training step(s), loss on training batch is 0.000931759.
After 7504 training step(s), loss on training batch is 0.00129437.
After 7505 training step(s), loss on training batch is 0.00128561.
After 7506 training step(s), loss on training batch is 0.00129308.
After 7507 training step(s), loss on training batch is 0.0014221.
After 7508 training step(s), loss on training batch is 0.00119123.
After 7509 training step(s), loss on training batch is 0.00736233.
After 7510 training step(s), loss on training batch is 0.00219588.
After 7511 training step(s), loss on training batch is 0.0019645.
After 7512 training step(s), loss on training batch is 0.00186775.
After 7513 training step(s), loss on training batch is 0.0017069.
After 7514 training step(s), loss on training batch is 0.00158866.
After 7515 training step(s), loss on training batch is 0.00134281.
After 7516 training step(s), loss on training batch is 0.000617334.
After 7517 training step(s), loss on training batch is 0.000540313.
After 7518 training step(s), loss on training batch is 0.000486204.
After 7519 training step(s), loss on training batch is 0.000531321.
After 7520 training step(s), loss on training batch is 0.000584862.
After 7521 training step(s), loss on training batch is 0.000560918.
After 7522 training step(s), loss on training batch is 0.000570968.
After 7523 training step(s), loss on training batch is 0.000562062.
After 7524 training step(s), loss on training batch is 0.00056981.
After 7525 training step(s), loss on training batch is 0.000548451.
After 7526 training step(s), loss on training batch is 0.000952813.
After 7527 training step(s), loss on training batch is 0.000939306.
After 7528 training step(s), loss on training batch is 0.0011321.
After 7529 training step(s), loss on training batch is 0.000949257.
After 7530 training step(s), loss on training batch is 0.00230843.
After 7531 training step(s), loss on training batch is 0.00192396.
After 7532 training step(s), loss on training batch is 0.00132234.
After 7533 training step(s), loss on training batch is 0.00133849.
After 7534 training step(s), loss on training batch is 0.00106004.
After 7535 training step(s), loss on training batch is 0.00102349.
After 7536 training step(s), loss on training batch is 0.00105582.
After 7537 training step(s), loss on training batch is 0.00102595.
After 7538 training step(s), loss on training batch is 0.00116075.
After 7539 training step(s), loss on training batch is 0.00108486.
After 7540 training step(s), loss on training batch is 0.00104758.
After 7541 training step(s), loss on training batch is 0.00101616.
After 7542 training step(s), loss on training batch is 0.00181874.
After 7543 training step(s), loss on training batch is 0.000436968.
After 7544 training step(s), loss on training batch is 0.000570926.
After 7545 training step(s), loss on training batch is 0.000453809.
After 7546 training step(s), loss on training batch is 0.000567348.
After 7547 training step(s), loss on training batch is 0.000521923.
After 7548 training step(s), loss on training batch is 0.000486468.
After 7549 training step(s), loss on training batch is 0.000405311.
After 7550 training step(s), loss on training batch is 0.000370032.
After 7551 training step(s), loss on training batch is 0.00058702.
After 7552 training step(s), loss on training batch is 0.000494655.
After 7553 training step(s), loss on training batch is 0.000477479.
After 7554 training step(s), loss on training batch is 0.000402662.
After 7555 training step(s), loss on training batch is 0.000675395.
After 7556 training step(s), loss on training batch is 0.000426808.
After 7557 training step(s), loss on training batch is 0.000380367.
After 7558 training step(s), loss on training batch is 0.00040372.
After 7559 training step(s), loss on training batch is 0.000980178.
After 7560 training step(s), loss on training batch is 0.00131694.
After 7561 training step(s), loss on training batch is 0.000662384.
After 7562 training step(s), loss on training batch is 0.000460339.
After 7563 training step(s), loss on training batch is 0.000448561.
After 7564 training step(s), loss on training batch is 0.000568155.
After 7565 training step(s), loss on training batch is 0.000575135.
After 7566 training step(s), loss on training batch is 0.00047818.
After 7567 training step(s), loss on training batch is 0.000437183.
After 7568 training step(s), loss on training batch is 0.000554544.
After 7569 training step(s), loss on training batch is 0.000368137.
After 7570 training step(s), loss on training batch is 0.000455691.
After 7571 training step(s), loss on training batch is 0.000297892.
After 7572 training step(s), loss on training batch is 0.000367337.
After 7573 training step(s), loss on training batch is 0.000359553.
After 7574 training step(s), loss on training batch is 0.000345902.
After 7575 training step(s), loss on training batch is 0.000570006.
After 7576 training step(s), loss on training batch is 0.000533183.
After 7577 training step(s), loss on training batch is 0.000514521.
After 7578 training step(s), loss on training batch is 0.000376939.
After 7579 training step(s), loss on training batch is 0.000395021.
After 7580 training step(s), loss on training batch is 0.000345747.
After 7581 training step(s), loss on training batch is 0.00040058.
After 7582 training step(s), loss on training batch is 0.000369374.
After 7583 training step(s), loss on training batch is 0.000460661.
After 7584 training step(s), loss on training batch is 0.000468005.
After 7585 training step(s), loss on training batch is 0.000459979.
After 7586 training step(s), loss on training batch is 0.000628598.
After 7587 training step(s), loss on training batch is 0.000507054.
After 7588 training step(s), loss on training batch is 0.000361824.
After 7589 training step(s), loss on training batch is 0.000416458.
After 7590 training step(s), loss on training batch is 0.000374735.
After 7591 training step(s), loss on training batch is 0.000352901.
After 7592 training step(s), loss on training batch is 0.000369763.
After 7593 training step(s), loss on training batch is 0.000351838.
After 7594 training step(s), loss on training batch is 0.000305648.
After 7595 training step(s), loss on training batch is 0.000554662.
After 7596 training step(s), loss on training batch is 0.000507958.
After 7597 training step(s), loss on training batch is 0.000284101.
After 7598 training step(s), loss on training batch is 0.000290764.
After 7599 training step(s), loss on training batch is 0.000386295.
After 7600 training step(s), loss on training batch is 0.000432069.
After 7601 training step(s), loss on training batch is 0.000621867.
After 7602 training step(s), loss on training batch is 0.00069295.
After 7603 training step(s), loss on training batch is 0.000421684.
After 7604 training step(s), loss on training batch is 0.000612346.
After 7605 training step(s), loss on training batch is 0.00126953.
After 7606 training step(s), loss on training batch is 0.00084473.
After 7607 training step(s), loss on training batch is 0.000454681.
After 7608 training step(s), loss on training batch is 0.000557812.
After 7609 training step(s), loss on training batch is 0.000714819.
After 7610 training step(s), loss on training batch is 0.000730709.
After 7611 training step(s), loss on training batch is 0.000452395.
After 7612 training step(s), loss on training batch is 0.000359472.
After 7613 training step(s), loss on training batch is 0.000413981.
After 7614 training step(s), loss on training batch is 0.000436909.
After 7615 training step(s), loss on training batch is 0.000399589.
After 7616 training step(s), loss on training batch is 0.000342497.
After 7617 training step(s), loss on training batch is 0.000448389.
After 7618 training step(s), loss on training batch is 0.000365899.
After 7619 training step(s), loss on training batch is 0.000465829.
After 7620 training step(s), loss on training batch is 0.000411159.
After 7621 training step(s), loss on training batch is 0.000343516.
After 7622 training step(s), loss on training batch is 0.000400844.
After 7623 training step(s), loss on training batch is 0.000368692.
After 7624 training step(s), loss on training batch is 0.000344854.
After 7625 training step(s), loss on training batch is 0.000330575.
After 7626 training step(s), loss on training batch is 0.000422116.
After 7627 training step(s), loss on training batch is 0.000326665.
After 7628 training step(s), loss on training batch is 0.00117412.
After 7629 training step(s), loss on training batch is 0.000696715.
After 7630 training step(s), loss on training batch is 0.000620126.
After 7631 training step(s), loss on training batch is 0.00064471.
After 7632 training step(s), loss on training batch is 0.000766526.
After 7633 training step(s), loss on training batch is 0.000851894.
After 7634 training step(s), loss on training batch is 0.000699347.
After 7635 training step(s), loss on training batch is 0.00055553.
After 7636 training step(s), loss on training batch is 0.000815093.
After 7637 training step(s), loss on training batch is 0.000806582.
After 7638 training step(s), loss on training batch is 0.00056929.
After 7639 training step(s), loss on training batch is 0.00056658.
After 7640 training step(s), loss on training batch is 0.00130139.
After 7641 training step(s), loss on training batch is 0.000624638.
After 7642 training step(s), loss on training batch is 0.000695615.
After 7643 training step(s), loss on training batch is 0.000717789.
After 7644 training step(s), loss on training batch is 0.000809699.
After 7645 training step(s), loss on training batch is 0.000592141.
After 7646 training step(s), loss on training batch is 0.000815478.
After 7647 training step(s), loss on training batch is 0.000637517.
After 7648 training step(s), loss on training batch is 0.000727188.
After 7649 training step(s), loss on training batch is 0.000740901.
After 7650 training step(s), loss on training batch is 0.000738995.
After 7651 training step(s), loss on training batch is 0.00074451.
After 7652 training step(s), loss on training batch is 0.000636485.
After 7653 training step(s), loss on training batch is 0.000896989.
After 7654 training step(s), loss on training batch is 0.000514396.
After 7655 training step(s), loss on training batch is 0.000715912.
After 7656 training step(s), loss on training batch is 0.00123141.
After 7657 training step(s), loss on training batch is 0.00101924.
After 7658 training step(s), loss on training batch is 0.00107375.
After 7659 training step(s), loss on training batch is 0.0010765.
After 7660 training step(s), loss on training batch is 0.000933216.
After 7661 training step(s), loss on training batch is 0.000951088.
After 7662 training step(s), loss on training batch is 0.00116577.
After 7663 training step(s), loss on training batch is 0.00104675.
After 7664 training step(s), loss on training batch is 0.000935838.
After 7665 training step(s), loss on training batch is 0.00131372.
After 7666 training step(s), loss on training batch is 0.000929687.
After 7667 training step(s), loss on training batch is 0.000918344.
After 7668 training step(s), loss on training batch is 0.00110395.
After 7669 training step(s), loss on training batch is 0.00189073.
After 7670 training step(s), loss on training batch is 0.00274102.
After 7671 training step(s), loss on training batch is 0.00158137.
After 7672 training step(s), loss on training batch is 0.00120478.
After 7673 training step(s), loss on training batch is 0.00134641.
After 7674 training step(s), loss on training batch is 0.00117165.
After 7675 training step(s), loss on training batch is 0.00111645.
After 7676 training step(s), loss on training batch is 0.00103722.
After 7677 training step(s), loss on training batch is 0.00162908.
After 7678 training step(s), loss on training batch is 0.00103171.
After 7679 training step(s), loss on training batch is 0.000985577.
After 7680 training step(s), loss on training batch is 0.000976059.
After 7681 training step(s), loss on training batch is 0.000597415.
After 7682 training step(s), loss on training batch is 0.000575384.
After 7683 training step(s), loss on training batch is 0.000345323.
After 7684 training step(s), loss on training batch is 0.000430162.
After 7685 training step(s), loss on training batch is 0.000470754.
After 7686 training step(s), loss on training batch is 0.000651165.
After 7687 training step(s), loss on training batch is 0.000796936.
After 7688 training step(s), loss on training batch is 0.000519104.
After 7689 training step(s), loss on training batch is 0.000421507.
After 7690 training step(s), loss on training batch is 0.000406721.
After 7691 training step(s), loss on training batch is 0.000335658.
After 7692 training step(s), loss on training batch is 0.000436616.
After 7693 training step(s), loss on training batch is 0.000356171.
After 7694 training step(s), loss on training batch is 0.00037393.
After 7695 training step(s), loss on training batch is 0.000445436.
After 7696 training step(s), loss on training batch is 0.000432606.
After 7697 training step(s), loss on training batch is 0.000337272.
After 7698 training step(s), loss on training batch is 0.000325047.
After 7699 training step(s), loss on training batch is 0.000332885.
After 7700 training step(s), loss on training batch is 0.000363599.
After 7701 training step(s), loss on training batch is 0.00116955.
After 7702 training step(s), loss on training batch is 0.000622082.
After 7703 training step(s), loss on training batch is 0.00062104.
After 7704 training step(s), loss on training batch is 0.000745867.
After 7705 training step(s), loss on training batch is 0.000673243.
After 7706 training step(s), loss on training batch is 0.000697165.
After 7707 training step(s), loss on training batch is 0.000740888.
After 7708 training step(s), loss on training batch is 0.00038266.
After 7709 training step(s), loss on training batch is 0.000424867.
After 7710 training step(s), loss on training batch is 0.000456905.
After 7711 training step(s), loss on training batch is 0.000547811.
After 7712 training step(s), loss on training batch is 0.000505729.
After 7713 training step(s), loss on training batch is 0.000538156.
After 7714 training step(s), loss on training batch is 0.000812981.
After 7715 training step(s), loss on training batch is 0.00172521.
After 7716 training step(s), loss on training batch is 0.0011212.
After 7717 training step(s), loss on training batch is 0.00095746.
After 7718 training step(s), loss on training batch is 0.000734942.
After 7719 training step(s), loss on training batch is 0.000428922.
After 7720 training step(s), loss on training batch is 0.000442069.
After 7721 training step(s), loss on training batch is 0.000450919.
After 7722 training step(s), loss on training batch is 0.000388262.
After 7723 training step(s), loss on training batch is 0.000617048.
After 7724 training step(s), loss on training batch is 0.000478828.
After 7725 training step(s), loss on training batch is 0.000462573.
After 7726 training step(s), loss on training batch is 0.000439272.
After 7727 training step(s), loss on training batch is 0.00101239.
After 7728 training step(s), loss on training batch is 0.00313148.
After 7729 training step(s), loss on training batch is 0.00080602.
After 7730 training step(s), loss on training batch is 0.000594882.
After 7731 training step(s), loss on training batch is 0.000623635.
After 7732 training step(s), loss on training batch is 0.000540705.
After 7733 training step(s), loss on training batch is 0.000472584.
After 7734 training step(s), loss on training batch is 0.000404584.
After 7735 training step(s), loss on training batch is 0.000357139.
After 7736 training step(s), loss on training batch is 0.000362697.
After 7737 training step(s), loss on training batch is 0.000405868.
After 7738 training step(s), loss on training batch is 0.00040054.
After 7739 training step(s), loss on training batch is 0.000436694.
After 7740 training step(s), loss on training batch is 0.000489055.
After 7741 training step(s), loss on training batch is 0.000382034.
After 7742 training step(s), loss on training batch is 0.000461875.
After 7743 training step(s), loss on training batch is 0.00043431.
After 7744 training step(s), loss on training batch is 0.000524134.
After 7745 training step(s), loss on training batch is 0.000349319.
After 7746 training step(s), loss on training batch is 0.000496364.
After 7747 training step(s), loss on training batch is 0.000387681.
After 7748 training step(s), loss on training batch is 0.000357092.
After 7749 training step(s), loss on training batch is 0.000398296.
After 7750 training step(s), loss on training batch is 0.000513841.
After 7751 training step(s), loss on training batch is 0.000467343.
After 7752 training step(s), loss on training batch is 0.000407492.
After 7753 training step(s), loss on training batch is 0.000414381.
After 7754 training step(s), loss on training batch is 0.000386245.
After 7755 training step(s), loss on training batch is 0.000389617.
After 7756 training step(s), loss on training batch is 0.000486584.
After 7757 training step(s), loss on training batch is 0.000569122.
After 7758 training step(s), loss on training batch is 0.000394081.
After 7759 training step(s), loss on training batch is 0.000441323.
After 7760 training step(s), loss on training batch is 0.00051064.
After 7761 training step(s), loss on training batch is 0.000382624.
After 7762 training step(s), loss on training batch is 0.000349147.
After 7763 training step(s), loss on training batch is 0.000389466.
After 7764 training step(s), loss on training batch is 0.000489409.
After 7765 training step(s), loss on training batch is 0.000418209.
After 7766 training step(s), loss on training batch is 0.000416596.
After 7767 training step(s), loss on training batch is 0.000438535.
After 7768 training step(s), loss on training batch is 0.00038968.
After 7769 training step(s), loss on training batch is 0.000436116.
After 7770 training step(s), loss on training batch is 0.00037409.
After 7771 training step(s), loss on training batch is 0.000424445.
After 7772 training step(s), loss on training batch is 0.000365946.
After 7773 training step(s), loss on training batch is 0.000351401.
After 7774 training step(s), loss on training batch is 0.000345054.
After 7775 training step(s), loss on training batch is 0.000413593.
After 7776 training step(s), loss on training batch is 0.000401322.
After 7777 training step(s), loss on training batch is 0.000656224.
After 7778 training step(s), loss on training batch is 0.00042174.
After 7779 training step(s), loss on training batch is 0.000397224.
After 7780 training step(s), loss on training batch is 0.000442777.
After 7781 training step(s), loss on training batch is 0.000371268.
After 7782 training step(s), loss on training batch is 0.00041762.
After 7783 training step(s), loss on training batch is 0.000832524.
After 7784 training step(s), loss on training batch is 0.00100856.
After 7785 training step(s), loss on training batch is 0.001232.
After 7786 training step(s), loss on training batch is 0.000753351.
After 7787 training step(s), loss on training batch is 0.00073536.
After 7788 training step(s), loss on training batch is 0.000650825.
After 7789 training step(s), loss on training batch is 0.000737759.
After 7790 training step(s), loss on training batch is 0.000677198.
After 7791 training step(s), loss on training batch is 0.000772431.
After 7792 training step(s), loss on training batch is 0.000842015.
After 7793 training step(s), loss on training batch is 0.000635991.
After 7794 training step(s), loss on training batch is 0.00065748.
After 7795 training step(s), loss on training batch is 0.000944239.
After 7796 training step(s), loss on training batch is 0.000819696.
After 7797 training step(s), loss on training batch is 0.000702199.
After 7798 training step(s), loss on training batch is 0.000726269.
After 7799 training step(s), loss on training batch is 0.00107872.
After 7800 training step(s), loss on training batch is 0.00072505.
After 7801 training step(s), loss on training batch is 0.000688653.
After 7802 training step(s), loss on training batch is 0.000782421.
After 7803 training step(s), loss on training batch is 0.000686489.
After 7804 training step(s), loss on training batch is 0.000664684.
After 7805 training step(s), loss on training batch is 0.000583968.
After 7806 training step(s), loss on training batch is 0.000734497.
After 7807 training step(s), loss on training batch is 0.000985046.
After 7808 training step(s), loss on training batch is 0.000839546.
After 7809 training step(s), loss on training batch is 0.000602257.
After 7810 training step(s), loss on training batch is 0.00107252.
After 7811 training step(s), loss on training batch is 0.00073112.
After 7812 training step(s), loss on training batch is 0.000729364.
After 7813 training step(s), loss on training batch is 0.000601221.
After 7814 training step(s), loss on training batch is 0.000567211.
After 7815 training step(s), loss on training batch is 0.000559332.
After 7816 training step(s), loss on training batch is 0.00056539.
After 7817 training step(s), loss on training batch is 0.000581501.
After 7818 training step(s), loss on training batch is 0.00083038.
After 7819 training step(s), loss on training batch is 0.00114282.
After 7820 training step(s), loss on training batch is 0.00114713.
After 7821 training step(s), loss on training batch is 0.0015397.
After 7822 training step(s), loss on training batch is 0.000771046.
After 7823 training step(s), loss on training batch is 0.00076137.
After 7824 training step(s), loss on training batch is 0.000767079.
After 7825 training step(s), loss on training batch is 0.000804376.
After 7826 training step(s), loss on training batch is 0.000767733.
After 7827 training step(s), loss on training batch is 0.000669498.
After 7828 training step(s), loss on training batch is 0.000715373.
After 7829 training step(s), loss on training batch is 0.000732092.
After 7830 training step(s), loss on training batch is 0.000842052.
After 7831 training step(s), loss on training batch is 0.000991793.
After 7832 training step(s), loss on training batch is 0.000628296.
After 7833 training step(s), loss on training batch is 0.000647354.
After 7834 training step(s), loss on training batch is 0.0006099.
After 7835 training step(s), loss on training batch is 0.000532287.
After 7836 training step(s), loss on training batch is 0.000873142.
After 7837 training step(s), loss on training batch is 0.00146099.
After 7838 training step(s), loss on training batch is 0.000561404.
After 7839 training step(s), loss on training batch is 0.000659665.
After 7840 training step(s), loss on training batch is 0.000743152.
After 7841 training step(s), loss on training batch is 0.000639449.
After 7842 training step(s), loss on training batch is 0.00062254.
After 7843 training step(s), loss on training batch is 0.000699292.
After 7844 training step(s), loss on training batch is 0.000633394.
After 7845 training step(s), loss on training batch is 0.000713627.
After 7846 training step(s), loss on training batch is 0.000878957.
After 7847 training step(s), loss on training batch is 0.000662911.
After 7848 training step(s), loss on training batch is 0.000692969.
After 7849 training step(s), loss on training batch is 0.000742492.
After 7850 training step(s), loss on training batch is 0.000781632.
After 7851 training step(s), loss on training batch is 0.000695741.
After 7852 training step(s), loss on training batch is 0.000564762.
After 7853 training step(s), loss on training batch is 0.000898696.
After 7854 training step(s), loss on training batch is 0.000675054.
After 7855 training step(s), loss on training batch is 0.000726836.
After 7856 training step(s), loss on training batch is 0.000643799.
After 7857 training step(s), loss on training batch is 0.000589007.
After 7858 training step(s), loss on training batch is 0.000829952.
After 7859 training step(s), loss on training batch is 0.00114323.
After 7860 training step(s), loss on training batch is 0.000668196.
After 7861 training step(s), loss on training batch is 0.000593034.
After 7862 training step(s), loss on training batch is 0.000582949.
After 7863 training step(s), loss on training batch is 0.000609462.
After 7864 training step(s), loss on training batch is 0.000546293.
After 7865 training step(s), loss on training batch is 0.000777856.
After 7866 training step(s), loss on training batch is 0.00119624.
After 7867 training step(s), loss on training batch is 0.00128024.
After 7868 training step(s), loss on training batch is 0.00114046.
After 7869 training step(s), loss on training batch is 0.00117703.
After 7870 training step(s), loss on training batch is 0.00110333.
After 7871 training step(s), loss on training batch is 0.00104924.
After 7872 training step(s), loss on training batch is 0.0010872.
After 7873 training step(s), loss on training batch is 0.00209606.
After 7874 training step(s), loss on training batch is 0.00129028.
After 7875 training step(s), loss on training batch is 0.00145601.
After 7876 training step(s), loss on training batch is 0.00115017.
After 7877 training step(s), loss on training batch is 0.00106051.
After 7878 training step(s), loss on training batch is 0.00117466.
After 7879 training step(s), loss on training batch is 0.00100194.
After 7880 training step(s), loss on training batch is 0.000974125.
After 7881 training step(s), loss on training batch is 0.000952489.
After 7882 training step(s), loss on training batch is 0.000964359.
After 7883 training step(s), loss on training batch is 0.00111969.
After 7884 training step(s), loss on training batch is 0.000987416.
After 7885 training step(s), loss on training batch is 0.00095489.
After 7886 training step(s), loss on training batch is 0.00131869.
After 7887 training step(s), loss on training batch is 0.000982643.
After 7888 training step(s), loss on training batch is 0.00122341.
After 7889 training step(s), loss on training batch is 0.000992777.
After 7890 training step(s), loss on training batch is 0.000999445.
After 7891 training step(s), loss on training batch is 0.000971517.
After 7892 training step(s), loss on training batch is 0.0010397.
After 7893 training step(s), loss on training batch is 0.000945386.
After 7894 training step(s), loss on training batch is 0.00115076.
After 7895 training step(s), loss on training batch is 0.00131932.
After 7896 training step(s), loss on training batch is 0.00124904.
After 7897 training step(s), loss on training batch is 0.00102808.
After 7898 training step(s), loss on training batch is 0.00100602.
After 7899 training step(s), loss on training batch is 0.000965578.
After 7900 training step(s), loss on training batch is 0.000975315.
After 7901 training step(s), loss on training batch is 0.00086163.
After 7902 training step(s), loss on training batch is 0.00110996.
After 7903 training step(s), loss on training batch is 0.000914432.
After 7904 training step(s), loss on training batch is 0.00127482.
After 7905 training step(s), loss on training batch is 0.00128371.
After 7906 training step(s), loss on training batch is 0.00122726.
After 7907 training step(s), loss on training batch is 0.00140669.
After 7908 training step(s), loss on training batch is 0.00118303.
After 7909 training step(s), loss on training batch is 0.00698631.
After 7910 training step(s), loss on training batch is 0.00197496.
After 7911 training step(s), loss on training batch is 0.00177276.
After 7912 training step(s), loss on training batch is 0.00168824.
After 7913 training step(s), loss on training batch is 0.00153451.
After 7914 training step(s), loss on training batch is 0.00152332.
After 7915 training step(s), loss on training batch is 0.00126614.
After 7916 training step(s), loss on training batch is 0.000560101.
After 7917 training step(s), loss on training batch is 0.000498955.
After 7918 training step(s), loss on training batch is 0.000473139.
After 7919 training step(s), loss on training batch is 0.000520377.
After 7920 training step(s), loss on training batch is 0.000582062.
After 7921 training step(s), loss on training batch is 0.000554476.
After 7922 training step(s), loss on training batch is 0.000543785.
After 7923 training step(s), loss on training batch is 0.000552513.
After 7924 training step(s), loss on training batch is 0.000560633.
After 7925 training step(s), loss on training batch is 0.000524106.
After 7926 training step(s), loss on training batch is 0.000941365.
After 7927 training step(s), loss on training batch is 0.000959691.
After 7928 training step(s), loss on training batch is 0.00111214.
After 7929 training step(s), loss on training batch is 0.000943283.
After 7930 training step(s), loss on training batch is 0.00222895.
After 7931 training step(s), loss on training batch is 0.00188375.
After 7932 training step(s), loss on training batch is 0.00126277.
After 7933 training step(s), loss on training batch is 0.00130762.
After 7934 training step(s), loss on training batch is 0.00103465.
After 7935 training step(s), loss on training batch is 0.0009904.
After 7936 training step(s), loss on training batch is 0.00100915.
After 7937 training step(s), loss on training batch is 0.000976965.
After 7938 training step(s), loss on training batch is 0.00113118.
After 7939 training step(s), loss on training batch is 0.00105647.
After 7940 training step(s), loss on training batch is 0.00101295.
After 7941 training step(s), loss on training batch is 0.000975426.
After 7942 training step(s), loss on training batch is 0.00173271.
After 7943 training step(s), loss on training batch is 0.000424669.
After 7944 training step(s), loss on training batch is 0.000514338.
After 7945 training step(s), loss on training batch is 0.000443652.
After 7946 training step(s), loss on training batch is 0.000553181.
After 7947 training step(s), loss on training batch is 0.000518234.
After 7948 training step(s), loss on training batch is 0.000469346.
After 7949 training step(s), loss on training batch is 0.000410562.
After 7950 training step(s), loss on training batch is 0.000366247.
After 7951 training step(s), loss on training batch is 0.000550984.
After 7952 training step(s), loss on training batch is 0.00046759.
After 7953 training step(s), loss on training batch is 0.000481658.
After 7954 training step(s), loss on training batch is 0.000394812.
After 7955 training step(s), loss on training batch is 0.000654975.
After 7956 training step(s), loss on training batch is 0.000426204.
After 7957 training step(s), loss on training batch is 0.00037453.
After 7958 training step(s), loss on training batch is 0.000383409.
After 7959 training step(s), loss on training batch is 0.000961023.
After 7960 training step(s), loss on training batch is 0.00122321.
After 7961 training step(s), loss on training batch is 0.00065183.
After 7962 training step(s), loss on training batch is 0.000473769.
After 7963 training step(s), loss on training batch is 0.000459806.
After 7964 training step(s), loss on training batch is 0.00056055.
After 7965 training step(s), loss on training batch is 0.000553894.
After 7966 training step(s), loss on training batch is 0.000491028.
After 7967 training step(s), loss on training batch is 0.000425726.
After 7968 training step(s), loss on training batch is 0.000522313.
After 7969 training step(s), loss on training batch is 0.000347993.
After 7970 training step(s), loss on training batch is 0.000432901.
After 7971 training step(s), loss on training batch is 0.000288548.
After 7972 training step(s), loss on training batch is 0.000360395.
After 7973 training step(s), loss on training batch is 0.000349302.
After 7974 training step(s), loss on training batch is 0.000341582.
After 7975 training step(s), loss on training batch is 0.000560208.
After 7976 training step(s), loss on training batch is 0.000513363.
After 7977 training step(s), loss on training batch is 0.000494057.
After 7978 training step(s), loss on training batch is 0.000359707.
After 7979 training step(s), loss on training batch is 0.000390491.
After 7980 training step(s), loss on training batch is 0.000341913.
After 7981 training step(s), loss on training batch is 0.00039366.
After 7982 training step(s), loss on training batch is 0.000362575.
After 7983 training step(s), loss on training batch is 0.000442995.
After 7984 training step(s), loss on training batch is 0.0004569.
After 7985 training step(s), loss on training batch is 0.00044951.
After 7986 training step(s), loss on training batch is 0.000613438.
After 7987 training step(s), loss on training batch is 0.000496513.
After 7988 training step(s), loss on training batch is 0.000357721.
After 7989 training step(s), loss on training batch is 0.000412794.
After 7990 training step(s), loss on training batch is 0.00037295.
After 7991 training step(s), loss on training batch is 0.00034612.
After 7992 training step(s), loss on training batch is 0.000368142.
After 7993 training step(s), loss on training batch is 0.000349273.
After 7994 training step(s), loss on training batch is 0.000295267.
After 7995 training step(s), loss on training batch is 0.000558805.
After 7996 training step(s), loss on training batch is 0.000501596.
After 7997 training step(s), loss on training batch is 0.000277577.
After 7998 training step(s), loss on training batch is 0.00028615.
After 7999 training step(s), loss on training batch is 0.00039514.
After 8000 training step(s), loss on training batch is 0.000432593.
After 8001 training step(s), loss on training batch is 0.000597101.
After 8002 training step(s), loss on training batch is 0.000670825.
After 8003 training step(s), loss on training batch is 0.000409298.
After 8004 training step(s), loss on training batch is 0.000599045.
After 8005 training step(s), loss on training batch is 0.00123109.
After 8006 training step(s), loss on training batch is 0.000817983.
After 8007 training step(s), loss on training batch is 0.000442906.
After 8008 training step(s), loss on training batch is 0.000532203.
After 8009 training step(s), loss on training batch is 0.000700682.
After 8010 training step(s), loss on training batch is 0.00071813.
After 8011 training step(s), loss on training batch is 0.000440491.
After 8012 training step(s), loss on training batch is 0.000349683.
After 8013 training step(s), loss on training batch is 0.000401291.
After 8014 training step(s), loss on training batch is 0.000425436.
After 8015 training step(s), loss on training batch is 0.000383675.
After 8016 training step(s), loss on training batch is 0.000333306.
After 8017 training step(s), loss on training batch is 0.000419938.
After 8018 training step(s), loss on training batch is 0.000363913.
After 8019 training step(s), loss on training batch is 0.00044937.
After 8020 training step(s), loss on training batch is 0.000411582.
After 8021 training step(s), loss on training batch is 0.000336466.
After 8022 training step(s), loss on training batch is 0.000403837.
After 8023 training step(s), loss on training batch is 0.00036595.
After 8024 training step(s), loss on training batch is 0.000337789.
After 8025 training step(s), loss on training batch is 0.000327803.
After 8026 training step(s), loss on training batch is 0.00041033.
After 8027 training step(s), loss on training batch is 0.000326226.
After 8028 training step(s), loss on training batch is 0.00112616.
After 8029 training step(s), loss on training batch is 0.00067435.
After 8030 training step(s), loss on training batch is 0.000615953.
After 8031 training step(s), loss on training batch is 0.000636782.
After 8032 training step(s), loss on training batch is 0.000739796.
After 8033 training step(s), loss on training batch is 0.000827662.
After 8034 training step(s), loss on training batch is 0.00071279.
After 8035 training step(s), loss on training batch is 0.000527546.
After 8036 training step(s), loss on training batch is 0.000810668.
After 8037 training step(s), loss on training batch is 0.000788384.
After 8038 training step(s), loss on training batch is 0.000556321.
After 8039 training step(s), loss on training batch is 0.000554245.
After 8040 training step(s), loss on training batch is 0.00128569.
After 8041 training step(s), loss on training batch is 0.000601847.
After 8042 training step(s), loss on training batch is 0.000669451.
After 8043 training step(s), loss on training batch is 0.00067654.
After 8044 training step(s), loss on training batch is 0.000800635.
After 8045 training step(s), loss on training batch is 0.000560778.
After 8046 training step(s), loss on training batch is 0.00090707.
After 8047 training step(s), loss on training batch is 0.000607015.
After 8048 training step(s), loss on training batch is 0.000681912.
After 8049 training step(s), loss on training batch is 0.000738606.
After 8050 training step(s), loss on training batch is 0.000730942.
After 8051 training step(s), loss on training batch is 0.000771315.
After 8052 training step(s), loss on training batch is 0.000648077.
After 8053 training step(s), loss on training batch is 0.000890764.
After 8054 training step(s), loss on training batch is 0.000497783.
After 8055 training step(s), loss on training batch is 0.000697251.
After 8056 training step(s), loss on training batch is 0.00121451.
After 8057 training step(s), loss on training batch is 0.000990595.
After 8058 training step(s), loss on training batch is 0.00104875.
After 8059 training step(s), loss on training batch is 0.00105011.
After 8060 training step(s), loss on training batch is 0.000919663.
After 8061 training step(s), loss on training batch is 0.000931561.
After 8062 training step(s), loss on training batch is 0.00115373.
After 8063 training step(s), loss on training batch is 0.00102116.
After 8064 training step(s), loss on training batch is 0.000921536.
After 8065 training step(s), loss on training batch is 0.00126865.
After 8066 training step(s), loss on training batch is 0.000921627.
After 8067 training step(s), loss on training batch is 0.000926298.
After 8068 training step(s), loss on training batch is 0.00108776.
After 8069 training step(s), loss on training batch is 0.00177202.
After 8070 training step(s), loss on training batch is 0.00285263.
After 8071 training step(s), loss on training batch is 0.00155163.
After 8072 training step(s), loss on training batch is 0.00112766.
After 8073 training step(s), loss on training batch is 0.00129268.
After 8074 training step(s), loss on training batch is 0.00112732.
After 8075 training step(s), loss on training batch is 0.00108298.
After 8076 training step(s), loss on training batch is 0.000999903.
After 8077 training step(s), loss on training batch is 0.00159042.
After 8078 training step(s), loss on training batch is 0.000996354.
After 8079 training step(s), loss on training batch is 0.000951596.
After 8080 training step(s), loss on training batch is 0.000950635.
After 8081 training step(s), loss on training batch is 0.000585592.
After 8082 training step(s), loss on training batch is 0.0005612.
After 8083 training step(s), loss on training batch is 0.000330755.
After 8084 training step(s), loss on training batch is 0.000420422.
After 8085 training step(s), loss on training batch is 0.000464072.
After 8086 training step(s), loss on training batch is 0.000636584.
After 8087 training step(s), loss on training batch is 0.000769344.
After 8088 training step(s), loss on training batch is 0.000500017.
After 8089 training step(s), loss on training batch is 0.000409922.
After 8090 training step(s), loss on training batch is 0.000389741.
After 8091 training step(s), loss on training batch is 0.00032501.
After 8092 training step(s), loss on training batch is 0.000430457.
After 8093 training step(s), loss on training batch is 0.000347666.
After 8094 training step(s), loss on training batch is 0.000360817.
After 8095 training step(s), loss on training batch is 0.000428594.
After 8096 training step(s), loss on training batch is 0.000423828.
After 8097 training step(s), loss on training batch is 0.000329391.
After 8098 training step(s), loss on training batch is 0.000319898.
After 8099 training step(s), loss on training batch is 0.0003289.
After 8100 training step(s), loss on training batch is 0.000351876.
After 8101 training step(s), loss on training batch is 0.0011322.
After 8102 training step(s), loss on training batch is 0.000604366.
After 8103 training step(s), loss on training batch is 0.000615381.
After 8104 training step(s), loss on training batch is 0.00074139.
After 8105 training step(s), loss on training batch is 0.000670166.
After 8106 training step(s), loss on training batch is 0.000686722.
After 8107 training step(s), loss on training batch is 0.000694314.
After 8108 training step(s), loss on training batch is 0.000360772.
After 8109 training step(s), loss on training batch is 0.000403981.
After 8110 training step(s), loss on training batch is 0.000441255.
After 8111 training step(s), loss on training batch is 0.000529243.
After 8112 training step(s), loss on training batch is 0.000494237.
After 8113 training step(s), loss on training batch is 0.000535084.
After 8114 training step(s), loss on training batch is 0.000840115.
After 8115 training step(s), loss on training batch is 0.00161217.
After 8116 training step(s), loss on training batch is 0.00110071.
After 8117 training step(s), loss on training batch is 0.000922424.
After 8118 training step(s), loss on training batch is 0.000719795.
After 8119 training step(s), loss on training batch is 0.000425671.
After 8120 training step(s), loss on training batch is 0.000435378.
After 8121 training step(s), loss on training batch is 0.000444503.
After 8122 training step(s), loss on training batch is 0.000378506.
After 8123 training step(s), loss on training batch is 0.00060327.
After 8124 training step(s), loss on training batch is 0.000471815.
After 8125 training step(s), loss on training batch is 0.000453405.
After 8126 training step(s), loss on training batch is 0.000436698.
After 8127 training step(s), loss on training batch is 0.00095976.
After 8128 training step(s), loss on training batch is 0.00315598.
After 8129 training step(s), loss on training batch is 0.000779206.
After 8130 training step(s), loss on training batch is 0.000582285.
After 8131 training step(s), loss on training batch is 0.000612494.
After 8132 training step(s), loss on training batch is 0.00053953.
After 8133 training step(s), loss on training batch is 0.000463613.
After 8134 training step(s), loss on training batch is 0.000396784.
After 8135 training step(s), loss on training batch is 0.000346451.
After 8136 training step(s), loss on training batch is 0.000350743.
After 8137 training step(s), loss on training batch is 0.000396726.
After 8138 training step(s), loss on training batch is 0.000390587.
After 8139 training step(s), loss on training batch is 0.000429102.
After 8140 training step(s), loss on training batch is 0.000479671.
After 8141 training step(s), loss on training batch is 0.000374979.
After 8142 training step(s), loss on training batch is 0.00045457.
After 8143 training step(s), loss on training batch is 0.000431301.
After 8144 training step(s), loss on training batch is 0.000521111.
After 8145 training step(s), loss on training batch is 0.000349382.
After 8146 training step(s), loss on training batch is 0.000494337.
After 8147 training step(s), loss on training batch is 0.000381441.
After 8148 training step(s), loss on training batch is 0.000346776.
After 8149 training step(s), loss on training batch is 0.000390919.
After 8150 training step(s), loss on training batch is 0.000502903.
After 8151 training step(s), loss on training batch is 0.00046291.
After 8152 training step(s), loss on training batch is 0.000407126.
After 8153 training step(s), loss on training batch is 0.000412572.
After 8154 training step(s), loss on training batch is 0.000420422.
After 8155 training step(s), loss on training batch is 0.000395896.
After 8156 training step(s), loss on training batch is 0.000470921.
After 8157 training step(s), loss on training batch is 0.00057145.
After 8158 training step(s), loss on training batch is 0.000361181.
After 8159 training step(s), loss on training batch is 0.000412181.
After 8160 training step(s), loss on training batch is 0.00052793.
After 8161 training step(s), loss on training batch is 0.000364852.
After 8162 training step(s), loss on training batch is 0.000340372.
After 8163 training step(s), loss on training batch is 0.000365615.
After 8164 training step(s), loss on training batch is 0.000481169.
After 8165 training step(s), loss on training batch is 0.000412549.
After 8166 training step(s), loss on training batch is 0.000406082.
After 8167 training step(s), loss on training batch is 0.000433023.
After 8168 training step(s), loss on training batch is 0.000437213.
After 8169 training step(s), loss on training batch is 0.000422874.
After 8170 training step(s), loss on training batch is 0.000407347.
After 8171 training step(s), loss on training batch is 0.00048225.
After 8172 training step(s), loss on training batch is 0.000381079.
After 8173 training step(s), loss on training batch is 0.000353839.
After 8174 training step(s), loss on training batch is 0.00034855.
After 8175 training step(s), loss on training batch is 0.000405907.
After 8176 training step(s), loss on training batch is 0.000390503.
After 8177 training step(s), loss on training batch is 0.000554807.
After 8178 training step(s), loss on training batch is 0.000424717.
After 8179 training step(s), loss on training batch is 0.000404301.
After 8180 training step(s), loss on training batch is 0.000410097.
After 8181 training step(s), loss on training batch is 0.000374723.
After 8182 training step(s), loss on training batch is 0.000403742.
After 8183 training step(s), loss on training batch is 0.000759121.
After 8184 training step(s), loss on training batch is 0.00095128.
After 8185 training step(s), loss on training batch is 0.00115998.
After 8186 training step(s), loss on training batch is 0.000735226.
After 8187 training step(s), loss on training batch is 0.000724602.
After 8188 training step(s), loss on training batch is 0.000636026.
After 8189 training step(s), loss on training batch is 0.000683512.
After 8190 training step(s), loss on training batch is 0.00064719.
After 8191 training step(s), loss on training batch is 0.000748909.
After 8192 training step(s), loss on training batch is 0.00084385.
After 8193 training step(s), loss on training batch is 0.000613848.
After 8194 training step(s), loss on training batch is 0.000622567.
After 8195 training step(s), loss on training batch is 0.00094846.
After 8196 training step(s), loss on training batch is 0.000804027.
After 8197 training step(s), loss on training batch is 0.000685483.
After 8198 training step(s), loss on training batch is 0.000712221.
After 8199 training step(s), loss on training batch is 0.00104564.
After 8200 training step(s), loss on training batch is 0.00071914.
After 8201 training step(s), loss on training batch is 0.000674555.
After 8202 training step(s), loss on training batch is 0.000762932.
After 8203 training step(s), loss on training batch is 0.000679824.
After 8204 training step(s), loss on training batch is 0.000653931.
After 8205 training step(s), loss on training batch is 0.000570601.
After 8206 training step(s), loss on training batch is 0.000721798.
After 8207 training step(s), loss on training batch is 0.000983445.
After 8208 training step(s), loss on training batch is 0.000831428.
After 8209 training step(s), loss on training batch is 0.000589107.
After 8210 training step(s), loss on training batch is 0.00103477.
After 8211 training step(s), loss on training batch is 0.00071513.
After 8212 training step(s), loss on training batch is 0.000711004.
After 8213 training step(s), loss on training batch is 0.000579589.
After 8214 training step(s), loss on training batch is 0.000554998.
After 8215 training step(s), loss on training batch is 0.000542949.
After 8216 training step(s), loss on training batch is 0.000550045.
After 8217 training step(s), loss on training batch is 0.000569959.
After 8218 training step(s), loss on training batch is 0.000823458.
After 8219 training step(s), loss on training batch is 0.00112174.
After 8220 training step(s), loss on training batch is 0.00108381.
After 8221 training step(s), loss on training batch is 0.001489.
After 8222 training step(s), loss on training batch is 0.000753267.
After 8223 training step(s), loss on training batch is 0.000734843.
After 8224 training step(s), loss on training batch is 0.000737972.
After 8225 training step(s), loss on training batch is 0.000831124.
After 8226 training step(s), loss on training batch is 0.000803355.
After 8227 training step(s), loss on training batch is 0.000699227.
After 8228 training step(s), loss on training batch is 0.000726583.
After 8229 training step(s), loss on training batch is 0.00074629.
After 8230 training step(s), loss on training batch is 0.00081912.
After 8231 training step(s), loss on training batch is 0.000945283.
After 8232 training step(s), loss on training batch is 0.000624961.
After 8233 training step(s), loss on training batch is 0.0006351.
After 8234 training step(s), loss on training batch is 0.000596909.
After 8235 training step(s), loss on training batch is 0.000526826.
After 8236 training step(s), loss on training batch is 0.000838432.
After 8237 training step(s), loss on training batch is 0.00143752.
After 8238 training step(s), loss on training batch is 0.00055167.
After 8239 training step(s), loss on training batch is 0.000643038.
After 8240 training step(s), loss on training batch is 0.000724417.
After 8241 training step(s), loss on training batch is 0.000622542.
After 8242 training step(s), loss on training batch is 0.000605645.
After 8243 training step(s), loss on training batch is 0.000686403.
After 8244 training step(s), loss on training batch is 0.000590632.
After 8245 training step(s), loss on training batch is 0.000690341.
After 8246 training step(s), loss on training batch is 0.00090686.
After 8247 training step(s), loss on training batch is 0.000626972.
After 8248 training step(s), loss on training batch is 0.000699785.
After 8249 training step(s), loss on training batch is 0.000728904.
After 8250 training step(s), loss on training batch is 0.000764029.
After 8251 training step(s), loss on training batch is 0.000699352.
After 8252 training step(s), loss on training batch is 0.000553877.
After 8253 training step(s), loss on training batch is 0.000866993.
After 8254 training step(s), loss on training batch is 0.00066119.
After 8255 training step(s), loss on training batch is 0.000716414.
After 8256 training step(s), loss on training batch is 0.000635456.
After 8257 training step(s), loss on training batch is 0.000580161.
After 8258 training step(s), loss on training batch is 0.000795215.
After 8259 training step(s), loss on training batch is 0.000976569.
After 8260 training step(s), loss on training batch is 0.000646851.
After 8261 training step(s), loss on training batch is 0.000585489.
After 8262 training step(s), loss on training batch is 0.000583081.
After 8263 training step(s), loss on training batch is 0.000602197.
After 8264 training step(s), loss on training batch is 0.000540694.
After 8265 training step(s), loss on training batch is 0.000753476.
After 8266 training step(s), loss on training batch is 0.00118388.
After 8267 training step(s), loss on training batch is 0.00124928.
After 8268 training step(s), loss on training batch is 0.00112321.
After 8269 training step(s), loss on training batch is 0.00116545.
After 8270 training step(s), loss on training batch is 0.00107989.
After 8271 training step(s), loss on training batch is 0.00102444.
After 8272 training step(s), loss on training batch is 0.00107004.
After 8273 training step(s), loss on training batch is 0.0019037.
After 8274 training step(s), loss on training batch is 0.00127626.
After 8275 training step(s), loss on training batch is 0.00141562.
After 8276 training step(s), loss on training batch is 0.00113932.
After 8277 training step(s), loss on training batch is 0.00101879.
After 8278 training step(s), loss on training batch is 0.00114379.
After 8279 training step(s), loss on training batch is 0.000961203.
After 8280 training step(s), loss on training batch is 0.000954274.
After 8281 training step(s), loss on training batch is 0.000952721.
After 8282 training step(s), loss on training batch is 0.000970353.
After 8283 training step(s), loss on training batch is 0.00107537.
After 8284 training step(s), loss on training batch is 0.000980996.
After 8285 training step(s), loss on training batch is 0.000926628.
After 8286 training step(s), loss on training batch is 0.00137582.
After 8287 training step(s), loss on training batch is 0.000951842.
After 8288 training step(s), loss on training batch is 0.00120491.
After 8289 training step(s), loss on training batch is 0.000964362.
After 8290 training step(s), loss on training batch is 0.000941359.
After 8291 training step(s), loss on training batch is 0.000926302.
After 8292 training step(s), loss on training batch is 0.00104333.
After 8293 training step(s), loss on training batch is 0.000927655.
After 8294 training step(s), loss on training batch is 0.0010544.
After 8295 training step(s), loss on training batch is 0.0011544.
After 8296 training step(s), loss on training batch is 0.00119817.
After 8297 training step(s), loss on training batch is 0.00101196.
After 8298 training step(s), loss on training batch is 0.000990117.
After 8299 training step(s), loss on training batch is 0.000914526.
After 8300 training step(s), loss on training batch is 0.000929587.
After 8301 training step(s), loss on training batch is 0.000839184.
After 8302 training step(s), loss on training batch is 0.00115695.
After 8303 training step(s), loss on training batch is 0.000920948.
After 8304 training step(s), loss on training batch is 0.00120763.
After 8305 training step(s), loss on training batch is 0.00123666.
After 8306 training step(s), loss on training batch is 0.00122909.
After 8307 training step(s), loss on training batch is 0.00152457.
After 8308 training step(s), loss on training batch is 0.00116765.
After 8309 training step(s), loss on training batch is 0.00832719.
After 8310 training step(s), loss on training batch is 0.00313587.
After 8311 training step(s), loss on training batch is 0.0023508.
After 8312 training step(s), loss on training batch is 0.00211726.
After 8313 training step(s), loss on training batch is 0.00183133.
After 8314 training step(s), loss on training batch is 0.00156103.
After 8315 training step(s), loss on training batch is 0.00129677.
After 8316 training step(s), loss on training batch is 0.000580114.
After 8317 training step(s), loss on training batch is 0.000508753.
After 8318 training step(s), loss on training batch is 0.000478954.
After 8319 training step(s), loss on training batch is 0.000528829.
After 8320 training step(s), loss on training batch is 0.00058228.
After 8321 training step(s), loss on training batch is 0.000560463.
After 8322 training step(s), loss on training batch is 0.000568251.
After 8323 training step(s), loss on training batch is 0.000561713.
After 8324 training step(s), loss on training batch is 0.000559843.
After 8325 training step(s), loss on training batch is 0.000520206.
After 8326 training step(s), loss on training batch is 0.000954854.
After 8327 training step(s), loss on training batch is 0.000962881.
After 8328 training step(s), loss on training batch is 0.00111478.
After 8329 training step(s), loss on training batch is 0.000905303.
After 8330 training step(s), loss on training batch is 0.002288.
After 8331 training step(s), loss on training batch is 0.00179017.
After 8332 training step(s), loss on training batch is 0.00122086.
After 8333 training step(s), loss on training batch is 0.00129594.
After 8334 training step(s), loss on training batch is 0.00099467.
After 8335 training step(s), loss on training batch is 0.000988631.
After 8336 training step(s), loss on training batch is 0.000987811.
After 8337 training step(s), loss on training batch is 0.000986085.
After 8338 training step(s), loss on training batch is 0.00112087.
After 8339 training step(s), loss on training batch is 0.0010421.
After 8340 training step(s), loss on training batch is 0.000999909.
After 8341 training step(s), loss on training batch is 0.000974318.
After 8342 training step(s), loss on training batch is 0.00164136.
After 8343 training step(s), loss on training batch is 0.000437978.
After 8344 training step(s), loss on training batch is 0.000543926.
After 8345 training step(s), loss on training batch is 0.000476275.
After 8346 training step(s), loss on training batch is 0.000594832.
After 8347 training step(s), loss on training batch is 0.000459778.
After 8348 training step(s), loss on training batch is 0.000575447.
After 8349 training step(s), loss on training batch is 0.000381134.
After 8350 training step(s), loss on training batch is 0.000357876.
After 8351 training step(s), loss on training batch is 0.000637452.
After 8352 training step(s), loss on training batch is 0.000466488.
After 8353 training step(s), loss on training batch is 0.000493274.
After 8354 training step(s), loss on training batch is 0.00039602.
After 8355 training step(s), loss on training batch is 0.000661973.
After 8356 training step(s), loss on training batch is 0.000422622.
After 8357 training step(s), loss on training batch is 0.000371109.
After 8358 training step(s), loss on training batch is 0.000398157.
After 8359 training step(s), loss on training batch is 0.000914567.
After 8360 training step(s), loss on training batch is 0.00115917.
After 8361 training step(s), loss on training batch is 0.000640937.
After 8362 training step(s), loss on training batch is 0.000471783.
After 8363 training step(s), loss on training batch is 0.000412273.
After 8364 training step(s), loss on training batch is 0.000544793.
After 8365 training step(s), loss on training batch is 0.000565055.
After 8366 training step(s), loss on training batch is 0.000462472.
After 8367 training step(s), loss on training batch is 0.000421482.
After 8368 training step(s), loss on training batch is 0.000555218.
After 8369 training step(s), loss on training batch is 0.000363545.
After 8370 training step(s), loss on training batch is 0.000434597.
After 8371 training step(s), loss on training batch is 0.000291382.
After 8372 training step(s), loss on training batch is 0.000353674.
After 8373 training step(s), loss on training batch is 0.000348004.
After 8374 training step(s), loss on training batch is 0.000339747.
After 8375 training step(s), loss on training batch is 0.000547628.
After 8376 training step(s), loss on training batch is 0.00050034.
After 8377 training step(s), loss on training batch is 0.000493102.
After 8378 training step(s), loss on training batch is 0.000370201.
After 8379 training step(s), loss on training batch is 0.000388831.
After 8380 training step(s), loss on training batch is 0.000341426.
After 8381 training step(s), loss on training batch is 0.000390287.
After 8382 training step(s), loss on training batch is 0.000358409.
After 8383 training step(s), loss on training batch is 0.000438935.
After 8384 training step(s), loss on training batch is 0.000451161.
After 8385 training step(s), loss on training batch is 0.000448206.
After 8386 training step(s), loss on training batch is 0.000621307.
After 8387 training step(s), loss on training batch is 0.000489581.
After 8388 training step(s), loss on training batch is 0.000359448.
After 8389 training step(s), loss on training batch is 0.000416158.
After 8390 training step(s), loss on training batch is 0.000374794.
After 8391 training step(s), loss on training batch is 0.00034401.
After 8392 training step(s), loss on training batch is 0.000367914.
After 8393 training step(s), loss on training batch is 0.000353508.
After 8394 training step(s), loss on training batch is 0.000301262.
After 8395 training step(s), loss on training batch is 0.000511658.
After 8396 training step(s), loss on training batch is 0.000491094.
After 8397 training step(s), loss on training batch is 0.000276149.
After 8398 training step(s), loss on training batch is 0.000286026.
After 8399 training step(s), loss on training batch is 0.000374641.
After 8400 training step(s), loss on training batch is 0.000409729.
After 8401 training step(s), loss on training batch is 0.000592856.
After 8402 training step(s), loss on training batch is 0.000665144.
After 8403 training step(s), loss on training batch is 0.000403534.
After 8404 training step(s), loss on training batch is 0.00059366.
After 8405 training step(s), loss on training batch is 0.00120178.
After 8406 training step(s), loss on training batch is 0.000799393.
After 8407 training step(s), loss on training batch is 0.000450005.
After 8408 training step(s), loss on training batch is 0.000537486.
After 8409 training step(s), loss on training batch is 0.00067639.
After 8410 training step(s), loss on training batch is 0.000692968.
After 8411 training step(s), loss on training batch is 0.000435145.
After 8412 training step(s), loss on training batch is 0.000352593.
After 8413 training step(s), loss on training batch is 0.000405905.
After 8414 training step(s), loss on training batch is 0.000429719.
After 8415 training step(s), loss on training batch is 0.000393466.
After 8416 training step(s), loss on training batch is 0.000335152.
After 8417 training step(s), loss on training batch is 0.000443008.
After 8418 training step(s), loss on training batch is 0.000360701.
After 8419 training step(s), loss on training batch is 0.000460159.
After 8420 training step(s), loss on training batch is 0.000415053.
After 8421 training step(s), loss on training batch is 0.000329904.
After 8422 training step(s), loss on training batch is 0.000408641.
After 8423 training step(s), loss on training batch is 0.00036289.
After 8424 training step(s), loss on training batch is 0.000334575.
After 8425 training step(s), loss on training batch is 0.000325907.
After 8426 training step(s), loss on training batch is 0.000405902.
After 8427 training step(s), loss on training batch is 0.000323575.
After 8428 training step(s), loss on training batch is 0.00111745.
After 8429 training step(s), loss on training batch is 0.000664606.
After 8430 training step(s), loss on training batch is 0.000605712.
After 8431 training step(s), loss on training batch is 0.000621159.
After 8432 training step(s), loss on training batch is 0.000720045.
After 8433 training step(s), loss on training batch is 0.000817332.
After 8434 training step(s), loss on training batch is 0.000672816.
After 8435 training step(s), loss on training batch is 0.00052819.
After 8436 training step(s), loss on training batch is 0.0007783.
After 8437 training step(s), loss on training batch is 0.000765034.
After 8438 training step(s), loss on training batch is 0.000547623.
After 8439 training step(s), loss on training batch is 0.00054556.
After 8440 training step(s), loss on training batch is 0.00122615.
After 8441 training step(s), loss on training batch is 0.000590414.
After 8442 training step(s), loss on training batch is 0.000655847.
After 8443 training step(s), loss on training batch is 0.000681771.
After 8444 training step(s), loss on training batch is 0.000766924.
After 8445 training step(s), loss on training batch is 0.000565065.
After 8446 training step(s), loss on training batch is 0.000783638.
After 8447 training step(s), loss on training batch is 0.000605602.
After 8448 training step(s), loss on training batch is 0.000683834.
After 8449 training step(s), loss on training batch is 0.000710483.
After 8450 training step(s), loss on training batch is 0.000703868.
After 8451 training step(s), loss on training batch is 0.00069413.
After 8452 training step(s), loss on training batch is 0.000608021.
After 8453 training step(s), loss on training batch is 0.000719813.
After 8454 training step(s), loss on training batch is 0.000501994.
After 8455 training step(s), loss on training batch is 0.000685714.
After 8456 training step(s), loss on training batch is 0.0012038.
After 8457 training step(s), loss on training batch is 0.00100613.
After 8458 training step(s), loss on training batch is 0.00105769.
After 8459 training step(s), loss on training batch is 0.00104703.
After 8460 training step(s), loss on training batch is 0.000930635.
After 8461 training step(s), loss on training batch is 0.000950778.
After 8462 training step(s), loss on training batch is 0.00116671.
After 8463 training step(s), loss on training batch is 0.00101244.
After 8464 training step(s), loss on training batch is 0.000899265.
After 8465 training step(s), loss on training batch is 0.00126447.
After 8466 training step(s), loss on training batch is 0.000893061.
After 8467 training step(s), loss on training batch is 0.000905468.
After 8468 training step(s), loss on training batch is 0.0010653.
After 8469 training step(s), loss on training batch is 0.00170103.
After 8470 training step(s), loss on training batch is 0.00276162.
After 8471 training step(s), loss on training batch is 0.00156402.
After 8472 training step(s), loss on training batch is 0.00120849.
After 8473 training step(s), loss on training batch is 0.0013319.
After 8474 training step(s), loss on training batch is 0.00117191.
After 8475 training step(s), loss on training batch is 0.00110561.
After 8476 training step(s), loss on training batch is 0.00104146.
After 8477 training step(s), loss on training batch is 0.00150849.
After 8478 training step(s), loss on training batch is 0.00100957.
After 8479 training step(s), loss on training batch is 0.000979832.
After 8480 training step(s), loss on training batch is 0.000955459.
After 8481 training step(s), loss on training batch is 0.0005702.
After 8482 training step(s), loss on training batch is 0.000538648.
After 8483 training step(s), loss on training batch is 0.00033441.
After 8484 training step(s), loss on training batch is 0.000406892.
After 8485 training step(s), loss on training batch is 0.000450192.
After 8486 training step(s), loss on training batch is 0.0006052.
After 8487 training step(s), loss on training batch is 0.000737151.
After 8488 training step(s), loss on training batch is 0.000493029.
After 8489 training step(s), loss on training batch is 0.000413194.
After 8490 training step(s), loss on training batch is 0.000390501.
After 8491 training step(s), loss on training batch is 0.000325936.
After 8492 training step(s), loss on training batch is 0.000415776.
After 8493 training step(s), loss on training batch is 0.000345233.
After 8494 training step(s), loss on training batch is 0.000360471.
After 8495 training step(s), loss on training batch is 0.000418488.
After 8496 training step(s), loss on training batch is 0.000414744.
After 8497 training step(s), loss on training batch is 0.000323272.
After 8498 training step(s), loss on training batch is 0.000312224.
After 8499 training step(s), loss on training batch is 0.000319773.
After 8500 training step(s), loss on training batch is 0.000345673.
After 8501 training step(s), loss on training batch is 0.00117983.
After 8502 training step(s), loss on training batch is 0.000584345.
After 8503 training step(s), loss on training batch is 0.000587002.
After 8504 training step(s), loss on training batch is 0.000706469.
After 8505 training step(s), loss on training batch is 0.000647372.
After 8506 training step(s), loss on training batch is 0.000666297.
After 8507 training step(s), loss on training batch is 0.000709322.
After 8508 training step(s), loss on training batch is 0.000360145.
After 8509 training step(s), loss on training batch is 0.00040258.
After 8510 training step(s), loss on training batch is 0.000435624.
After 8511 training step(s), loss on training batch is 0.000522977.
After 8512 training step(s), loss on training batch is 0.000486506.
After 8513 training step(s), loss on training batch is 0.000519885.
After 8514 training step(s), loss on training batch is 0.000767716.
After 8515 training step(s), loss on training batch is 0.00158002.
After 8516 training step(s), loss on training batch is 0.00108423.
After 8517 training step(s), loss on training batch is 0.000915785.
After 8518 training step(s), loss on training batch is 0.000702872.
After 8519 training step(s), loss on training batch is 0.000420921.
After 8520 training step(s), loss on training batch is 0.000430188.
After 8521 training step(s), loss on training batch is 0.000432777.
After 8522 training step(s), loss on training batch is 0.000369246.
After 8523 training step(s), loss on training batch is 0.000588841.
After 8524 training step(s), loss on training batch is 0.000458336.
After 8525 training step(s), loss on training batch is 0.000434671.
After 8526 training step(s), loss on training batch is 0.000417563.
After 8527 training step(s), loss on training batch is 0.000971324.
After 8528 training step(s), loss on training batch is 0.00308298.
After 8529 training step(s), loss on training batch is 0.000760172.
After 8530 training step(s), loss on training batch is 0.000569513.
After 8531 training step(s), loss on training batch is 0.00065167.
After 8532 training step(s), loss on training batch is 0.000532508.
After 8533 training step(s), loss on training batch is 0.000458307.
After 8534 training step(s), loss on training batch is 0.000385619.
After 8535 training step(s), loss on training batch is 0.00033584.
After 8536 training step(s), loss on training batch is 0.000339796.
After 8537 training step(s), loss on training batch is 0.000379191.
After 8538 training step(s), loss on training batch is 0.000374825.
After 8539 training step(s), loss on training batch is 0.000417204.
After 8540 training step(s), loss on training batch is 0.000465203.
After 8541 training step(s), loss on training batch is 0.000368936.
After 8542 training step(s), loss on training batch is 0.000445369.
After 8543 training step(s), loss on training batch is 0.000419777.
After 8544 training step(s), loss on training batch is 0.000524614.
After 8545 training step(s), loss on training batch is 0.000344918.
After 8546 training step(s), loss on training batch is 0.000501061.
After 8547 training step(s), loss on training batch is 0.000377136.
After 8548 training step(s), loss on training batch is 0.000346377.
After 8549 training step(s), loss on training batch is 0.000392936.
After 8550 training step(s), loss on training batch is 0.000499552.
After 8551 training step(s), loss on training batch is 0.00045996.
After 8552 training step(s), loss on training batch is 0.00040347.
After 8553 training step(s), loss on training batch is 0.000406151.
After 8554 training step(s), loss on training batch is 0.000380027.
After 8555 training step(s), loss on training batch is 0.000380603.
After 8556 training step(s), loss on training batch is 0.000478703.
After 8557 training step(s), loss on training batch is 0.000574596.
After 8558 training step(s), loss on training batch is 0.00035849.
After 8559 training step(s), loss on training batch is 0.000406466.
After 8560 training step(s), loss on training batch is 0.000527805.
After 8561 training step(s), loss on training batch is 0.000362917.
After 8562 training step(s), loss on training batch is 0.00033222.
After 8563 training step(s), loss on training batch is 0.000356284.
After 8564 training step(s), loss on training batch is 0.000482279.
After 8565 training step(s), loss on training batch is 0.000422248.
After 8566 training step(s), loss on training batch is 0.00041208.
After 8567 training step(s), loss on training batch is 0.000427793.
After 8568 training step(s), loss on training batch is 0.000379909.
After 8569 training step(s), loss on training batch is 0.000433927.
After 8570 training step(s), loss on training batch is 0.000364685.
After 8571 training step(s), loss on training batch is 0.000401795.
After 8572 training step(s), loss on training batch is 0.000356759.
After 8573 training step(s), loss on training batch is 0.000340484.
After 8574 training step(s), loss on training batch is 0.000336195.
After 8575 training step(s), loss on training batch is 0.000401031.
After 8576 training step(s), loss on training batch is 0.000390821.
After 8577 training step(s), loss on training batch is 0.000539236.
After 8578 training step(s), loss on training batch is 0.000417824.
After 8579 training step(s), loss on training batch is 0.000400843.
After 8580 training step(s), loss on training batch is 0.00041511.
After 8581 training step(s), loss on training batch is 0.000367505.
After 8582 training step(s), loss on training batch is 0.000396983.
After 8583 training step(s), loss on training batch is 0.000777816.
After 8584 training step(s), loss on training batch is 0.000938733.
After 8585 training step(s), loss on training batch is 0.0011278.
After 8586 training step(s), loss on training batch is 0.000718703.
After 8587 training step(s), loss on training batch is 0.000713657.
After 8588 training step(s), loss on training batch is 0.000644663.
After 8589 training step(s), loss on training batch is 0.000732286.
After 8590 training step(s), loss on training batch is 0.000670565.
After 8591 training step(s), loss on training batch is 0.000754935.
After 8592 training step(s), loss on training batch is 0.000809021.
After 8593 training step(s), loss on training batch is 0.000626214.
After 8594 training step(s), loss on training batch is 0.000640104.
After 8595 training step(s), loss on training batch is 0.000857425.
After 8596 training step(s), loss on training batch is 0.000779908.
After 8597 training step(s), loss on training batch is 0.00069211.
After 8598 training step(s), loss on training batch is 0.000697944.
After 8599 training step(s), loss on training batch is 0.00101999.
After 8600 training step(s), loss on training batch is 0.000718561.
After 8601 training step(s), loss on training batch is 0.000656578.
After 8602 training step(s), loss on training batch is 0.000767596.
After 8603 training step(s), loss on training batch is 0.000654254.
After 8604 training step(s), loss on training batch is 0.000635615.
After 8605 training step(s), loss on training batch is 0.00055432.
After 8606 training step(s), loss on training batch is 0.000710716.
After 8607 training step(s), loss on training batch is 0.000942589.
After 8608 training step(s), loss on training batch is 0.000814835.
After 8609 training step(s), loss on training batch is 0.000576837.
After 8610 training step(s), loss on training batch is 0.00100572.
After 8611 training step(s), loss on training batch is 0.00070035.
After 8612 training step(s), loss on training batch is 0.000689812.
After 8613 training step(s), loss on training batch is 0.000580719.
After 8614 training step(s), loss on training batch is 0.000559313.
After 8615 training step(s), loss on training batch is 0.000545224.
After 8616 training step(s), loss on training batch is 0.000554474.
After 8617 training step(s), loss on training batch is 0.000565025.
After 8618 training step(s), loss on training batch is 0.000818333.
After 8619 training step(s), loss on training batch is 0.00113655.
After 8620 training step(s), loss on training batch is 0.00108814.
After 8621 training step(s), loss on training batch is 0.00144767.
After 8622 training step(s), loss on training batch is 0.000711661.
After 8623 training step(s), loss on training batch is 0.000693809.
After 8624 training step(s), loss on training batch is 0.000692952.
After 8625 training step(s), loss on training batch is 0.000761264.
After 8626 training step(s), loss on training batch is 0.000731104.
After 8627 training step(s), loss on training batch is 0.000632477.
After 8628 training step(s), loss on training batch is 0.000683624.
After 8629 training step(s), loss on training batch is 0.000695186.
After 8630 training step(s), loss on training batch is 0.000801993.
After 8631 training step(s), loss on training batch is 0.000953059.
After 8632 training step(s), loss on training batch is 0.000589935.
After 8633 training step(s), loss on training batch is 0.000580122.
After 8634 training step(s), loss on training batch is 0.000548313.
After 8635 training step(s), loss on training batch is 0.000486358.
After 8636 training step(s), loss on training batch is 0.00087101.
After 8637 training step(s), loss on training batch is 0.00148223.
After 8638 training step(s), loss on training batch is 0.000546764.
After 8639 training step(s), loss on training batch is 0.000634691.
After 8640 training step(s), loss on training batch is 0.000704659.
After 8641 training step(s), loss on training batch is 0.000616663.
After 8642 training step(s), loss on training batch is 0.000591752.
After 8643 training step(s), loss on training batch is 0.000655762.
After 8644 training step(s), loss on training batch is 0.000595302.
After 8645 training step(s), loss on training batch is 0.000680856.
After 8646 training step(s), loss on training batch is 0.000862741.
After 8647 training step(s), loss on training batch is 0.000631082.
After 8648 training step(s), loss on training batch is 0.000663299.
After 8649 training step(s), loss on training batch is 0.000712658.
After 8650 training step(s), loss on training batch is 0.000743064.
After 8651 training step(s), loss on training batch is 0.00064128.
After 8652 training step(s), loss on training batch is 0.000543352.
After 8653 training step(s), loss on training batch is 0.000846878.
After 8654 training step(s), loss on training batch is 0.000657667.
After 8655 training step(s), loss on training batch is 0.000704665.
After 8656 training step(s), loss on training batch is 0.000632467.
After 8657 training step(s), loss on training batch is 0.000568434.
After 8658 training step(s), loss on training batch is 0.000782583.
After 8659 training step(s), loss on training batch is 0.000958715.
After 8660 training step(s), loss on training batch is 0.000631938.
After 8661 training step(s), loss on training batch is 0.000561639.
After 8662 training step(s), loss on training batch is 0.000554232.
After 8663 training step(s), loss on training batch is 0.000586407.
After 8664 training step(s), loss on training batch is 0.000528537.
After 8665 training step(s), loss on training batch is 0.000745928.
After 8666 training step(s), loss on training batch is 0.00114645.
After 8667 training step(s), loss on training batch is 0.00121657.
After 8668 training step(s), loss on training batch is 0.00109787.
After 8669 training step(s), loss on training batch is 0.00112529.
After 8670 training step(s), loss on training batch is 0.00106253.
After 8671 training step(s), loss on training batch is 0.00100325.
After 8672 training step(s), loss on training batch is 0.00105435.
After 8673 training step(s), loss on training batch is 0.00187147.
After 8674 training step(s), loss on training batch is 0.0012717.
After 8675 training step(s), loss on training batch is 0.00139714.
After 8676 training step(s), loss on training batch is 0.0011197.
After 8677 training step(s), loss on training batch is 0.00102466.
After 8678 training step(s), loss on training batch is 0.00113219.
After 8679 training step(s), loss on training batch is 0.000906382.
After 8680 training step(s), loss on training batch is 0.00093498.
After 8681 training step(s), loss on training batch is 0.000917794.
After 8682 training step(s), loss on training batch is 0.000944686.
After 8683 training step(s), loss on training batch is 0.00107021.
After 8684 training step(s), loss on training batch is 0.00095345.
After 8685 training step(s), loss on training batch is 0.000911653.
After 8686 training step(s), loss on training batch is 0.00140489.
After 8687 training step(s), loss on training batch is 0.000929579.
After 8688 training step(s), loss on training batch is 0.00117784.
After 8689 training step(s), loss on training batch is 0.000969366.
After 8690 training step(s), loss on training batch is 0.000946968.
After 8691 training step(s), loss on training batch is 0.000916846.
After 8692 training step(s), loss on training batch is 0.00102697.
After 8693 training step(s), loss on training batch is 0.00091618.
After 8694 training step(s), loss on training batch is 0.00105036.
After 8695 training step(s), loss on training batch is 0.00116401.
After 8696 training step(s), loss on training batch is 0.00120331.
After 8697 training step(s), loss on training batch is 0.000973794.
After 8698 training step(s), loss on training batch is 0.000971762.
After 8699 training step(s), loss on training batch is 0.000893296.
After 8700 training step(s), loss on training batch is 0.000935461.
After 8701 training step(s), loss on training batch is 0.000834706.
After 8702 training step(s), loss on training batch is 0.00111777.
After 8703 training step(s), loss on training batch is 0.000911543.
After 8704 training step(s), loss on training batch is 0.00121151.
After 8705 training step(s), loss on training batch is 0.00123291.
After 8706 training step(s), loss on training batch is 0.00117955.
After 8707 training step(s), loss on training batch is 0.00134286.
After 8708 training step(s), loss on training batch is 0.0011631.
After 8709 training step(s), loss on training batch is 0.00666606.
After 8710 training step(s), loss on training batch is 0.00192933.
After 8711 training step(s), loss on training batch is 0.00172753.
After 8712 training step(s), loss on training batch is 0.00162564.
After 8713 training step(s), loss on training batch is 0.00142733.
After 8714 training step(s), loss on training batch is 0.00150032.
After 8715 training step(s), loss on training batch is 0.00120923.
After 8716 training step(s), loss on training batch is 0.000537417.
After 8717 training step(s), loss on training batch is 0.000474923.
After 8718 training step(s), loss on training batch is 0.000458668.
After 8719 training step(s), loss on training batch is 0.000493837.
After 8720 training step(s), loss on training batch is 0.000545344.
After 8721 training step(s), loss on training batch is 0.000520257.
After 8722 training step(s), loss on training batch is 0.000526137.
After 8723 training step(s), loss on training batch is 0.000529826.
After 8724 training step(s), loss on training batch is 0.000530885.
After 8725 training step(s), loss on training batch is 0.000495128.
After 8726 training step(s), loss on training batch is 0.000907698.
After 8727 training step(s), loss on training batch is 0.000932558.
After 8728 training step(s), loss on training batch is 0.00109033.
After 8729 training step(s), loss on training batch is 0.000919721.
After 8730 training step(s), loss on training batch is 0.00210983.
After 8731 training step(s), loss on training batch is 0.00171943.
After 8732 training step(s), loss on training batch is 0.0012284.
After 8733 training step(s), loss on training batch is 0.00122709.
After 8734 training step(s), loss on training batch is 0.00100712.
After 8735 training step(s), loss on training batch is 0.000969485.
After 8736 training step(s), loss on training batch is 0.00100265.
After 8737 training step(s), loss on training batch is 0.00100075.
After 8738 training step(s), loss on training batch is 0.00112459.
After 8739 training step(s), loss on training batch is 0.00102204.
After 8740 training step(s), loss on training batch is 0.000989389.
After 8741 training step(s), loss on training batch is 0.000952095.
After 8742 training step(s), loss on training batch is 0.00166291.
After 8743 training step(s), loss on training batch is 0.000399328.
After 8744 training step(s), loss on training batch is 0.000470616.
After 8745 training step(s), loss on training batch is 0.000417985.
After 8746 training step(s), loss on training batch is 0.000526592.
After 8747 training step(s), loss on training batch is 0.000491955.
After 8748 training step(s), loss on training batch is 0.000447566.
After 8749 training step(s), loss on training batch is 0.000388697.
After 8750 training step(s), loss on training batch is 0.000354077.
After 8751 training step(s), loss on training batch is 0.000553597.
After 8752 training step(s), loss on training batch is 0.000472408.
After 8753 training step(s), loss on training batch is 0.000459034.
After 8754 training step(s), loss on training batch is 0.00039207.
After 8755 training step(s), loss on training batch is 0.000609584.
After 8756 training step(s), loss on training batch is 0.000422778.
After 8757 training step(s), loss on training batch is 0.000365641.
After 8758 training step(s), loss on training batch is 0.000374141.
After 8759 training step(s), loss on training batch is 0.000877064.
After 8760 training step(s), loss on training batch is 0.00112219.
After 8761 training step(s), loss on training batch is 0.000615181.
After 8762 training step(s), loss on training batch is 0.000459203.
After 8763 training step(s), loss on training batch is 0.000421674.
After 8764 training step(s), loss on training batch is 0.000533588.
After 8765 training step(s), loss on training batch is 0.000539165.
After 8766 training step(s), loss on training batch is 0.00046762.
After 8767 training step(s), loss on training batch is 0.000408753.
After 8768 training step(s), loss on training batch is 0.00054742.
After 8769 training step(s), loss on training batch is 0.000356169.
After 8770 training step(s), loss on training batch is 0.000443983.
After 8771 training step(s), loss on training batch is 0.000289594.
After 8772 training step(s), loss on training batch is 0.000339864.
After 8773 training step(s), loss on training batch is 0.000344016.
After 8774 training step(s), loss on training batch is 0.000333482.
After 8775 training step(s), loss on training batch is 0.000533572.
After 8776 training step(s), loss on training batch is 0.000478986.
After 8777 training step(s), loss on training batch is 0.000480386.
After 8778 training step(s), loss on training batch is 0.000362932.
After 8779 training step(s), loss on training batch is 0.000381724.
After 8780 training step(s), loss on training batch is 0.000336118.
After 8781 training step(s), loss on training batch is 0.000387023.
After 8782 training step(s), loss on training batch is 0.000357945.
After 8783 training step(s), loss on training batch is 0.000435608.
After 8784 training step(s), loss on training batch is 0.000433774.
After 8785 training step(s), loss on training batch is 0.000439338.
After 8786 training step(s), loss on training batch is 0.000605833.
After 8787 training step(s), loss on training batch is 0.000470598.
After 8788 training step(s), loss on training batch is 0.000349326.
After 8789 training step(s), loss on training batch is 0.000411241.
After 8790 training step(s), loss on training batch is 0.000368615.
After 8791 training step(s), loss on training batch is 0.000332589.
After 8792 training step(s), loss on training batch is 0.000358238.
After 8793 training step(s), loss on training batch is 0.000341335.
After 8794 training step(s), loss on training batch is 0.000291954.
After 8795 training step(s), loss on training batch is 0.000504321.
After 8796 training step(s), loss on training batch is 0.000474265.
After 8797 training step(s), loss on training batch is 0.000274903.
After 8798 training step(s), loss on training batch is 0.000284987.
After 8799 training step(s), loss on training batch is 0.000371882.
After 8800 training step(s), loss on training batch is 0.000402658.
After 8801 training step(s), loss on training batch is 0.000589917.
After 8802 training step(s), loss on training batch is 0.000660871.
After 8803 training step(s), loss on training batch is 0.00040188.
After 8804 training step(s), loss on training batch is 0.000571766.
After 8805 training step(s), loss on training batch is 0.0011271.
After 8806 training step(s), loss on training batch is 0.000780194.
After 8807 training step(s), loss on training batch is 0.00038296.
After 8808 training step(s), loss on training batch is 0.000461419.
After 8809 training step(s), loss on training batch is 0.000678933.
After 8810 training step(s), loss on training batch is 0.000712557.
After 8811 training step(s), loss on training batch is 0.000434395.
After 8812 training step(s), loss on training batch is 0.000329619.
After 8813 training step(s), loss on training batch is 0.000377379.
After 8814 training step(s), loss on training batch is 0.000397021.
After 8815 training step(s), loss on training batch is 0.000370005.
After 8816 training step(s), loss on training batch is 0.000325366.
After 8817 training step(s), loss on training batch is 0.000413038.
After 8818 training step(s), loss on training batch is 0.000353754.
After 8819 training step(s), loss on training batch is 0.000445064.
After 8820 training step(s), loss on training batch is 0.000407405.
After 8821 training step(s), loss on training batch is 0.000327888.
After 8822 training step(s), loss on training batch is 0.000388851.
After 8823 training step(s), loss on training batch is 0.000352631.
After 8824 training step(s), loss on training batch is 0.000337622.
After 8825 training step(s), loss on training batch is 0.000322633.
After 8826 training step(s), loss on training batch is 0.000400746.
After 8827 training step(s), loss on training batch is 0.000317649.
After 8828 training step(s), loss on training batch is 0.00112868.
After 8829 training step(s), loss on training batch is 0.000666086.
After 8830 training step(s), loss on training batch is 0.000592658.
After 8831 training step(s), loss on training batch is 0.000608758.
After 8832 training step(s), loss on training batch is 0.000715253.
After 8833 training step(s), loss on training batch is 0.000782146.
After 8834 training step(s), loss on training batch is 0.00065876.
After 8835 training step(s), loss on training batch is 0.000528101.
After 8836 training step(s), loss on training batch is 0.000776809.
After 8837 training step(s), loss on training batch is 0.000759842.
After 8838 training step(s), loss on training batch is 0.000540669.
After 8839 training step(s), loss on training batch is 0.000536629.
After 8840 training step(s), loss on training batch is 0.00121536.
After 8841 training step(s), loss on training batch is 0.000581474.
After 8842 training step(s), loss on training batch is 0.00064815.
After 8843 training step(s), loss on training batch is 0.000672399.
After 8844 training step(s), loss on training batch is 0.000753359.
After 8845 training step(s), loss on training batch is 0.000555263.
After 8846 training step(s), loss on training batch is 0.000755672.
After 8847 training step(s), loss on training batch is 0.000597329.
After 8848 training step(s), loss on training batch is 0.000669678.
After 8849 training step(s), loss on training batch is 0.000694326.
After 8850 training step(s), loss on training batch is 0.000693816.
After 8851 training step(s), loss on training batch is 0.000693183.
After 8852 training step(s), loss on training batch is 0.000598892.
After 8853 training step(s), loss on training batch is 0.000716187.
After 8854 training step(s), loss on training batch is 0.000492845.
After 8855 training step(s), loss on training batch is 0.00066953.
After 8856 training step(s), loss on training batch is 0.00117749.
After 8857 training step(s), loss on training batch is 0.000987056.
After 8858 training step(s), loss on training batch is 0.00104221.
After 8859 training step(s), loss on training batch is 0.00102625.
After 8860 training step(s), loss on training batch is 0.000910475.
After 8861 training step(s), loss on training batch is 0.00092528.
After 8862 training step(s), loss on training batch is 0.00110807.
After 8863 training step(s), loss on training batch is 0.00101373.
After 8864 training step(s), loss on training batch is 0.000878876.
After 8865 training step(s), loss on training batch is 0.00124349.
After 8866 training step(s), loss on training batch is 0.000890286.
After 8867 training step(s), loss on training batch is 0.000898401.
After 8868 training step(s), loss on training batch is 0.00106327.
After 8869 training step(s), loss on training batch is 0.0016744.
After 8870 training step(s), loss on training batch is 0.00273233.
After 8871 training step(s), loss on training batch is 0.0015434.
After 8872 training step(s), loss on training batch is 0.00120182.
After 8873 training step(s), loss on training batch is 0.00131978.
After 8874 training step(s), loss on training batch is 0.00114742.
After 8875 training step(s), loss on training batch is 0.00109103.
After 8876 training step(s), loss on training batch is 0.00102641.
After 8877 training step(s), loss on training batch is 0.00148244.
After 8878 training step(s), loss on training batch is 0.000993391.
After 8879 training step(s), loss on training batch is 0.000961008.
After 8880 training step(s), loss on training batch is 0.000939959.
After 8881 training step(s), loss on training batch is 0.000626613.
After 8882 training step(s), loss on training batch is 0.000546097.
After 8883 training step(s), loss on training batch is 0.000310084.
After 8884 training step(s), loss on training batch is 0.000398826.
After 8885 training step(s), loss on training batch is 0.000444319.
After 8886 training step(s), loss on training batch is 0.000606761.
After 8887 training step(s), loss on training batch is 0.000746304.
After 8888 training step(s), loss on training batch is 0.000483877.
After 8889 training step(s), loss on training batch is 0.000401.
After 8890 training step(s), loss on training batch is 0.000377291.
After 8891 training step(s), loss on training batch is 0.000313382.
After 8892 training step(s), loss on training batch is 0.000407656.
After 8893 training step(s), loss on training batch is 0.000338222.
After 8894 training step(s), loss on training batch is 0.000345787.
After 8895 training step(s), loss on training batch is 0.000403616.
After 8896 training step(s), loss on training batch is 0.00040995.
After 8897 training step(s), loss on training batch is 0.00031607.
After 8898 training step(s), loss on training batch is 0.000307165.
After 8899 training step(s), loss on training batch is 0.000319415.
After 8900 training step(s), loss on training batch is 0.000342898.
After 8901 training step(s), loss on training batch is 0.00109629.
After 8902 training step(s), loss on training batch is 0.00056894.
After 8903 training step(s), loss on training batch is 0.000583007.
After 8904 training step(s), loss on training batch is 0.000701252.
After 8905 training step(s), loss on training batch is 0.000634915.
After 8906 training step(s), loss on training batch is 0.000657107.
After 8907 training step(s), loss on training batch is 0.000658918.
After 8908 training step(s), loss on training batch is 0.000365205.
After 8909 training step(s), loss on training batch is 0.000407377.
After 8910 training step(s), loss on training batch is 0.000433495.
After 8911 training step(s), loss on training batch is 0.000519635.
After 8912 training step(s), loss on training batch is 0.000483755.
After 8913 training step(s), loss on training batch is 0.000515823.
After 8914 training step(s), loss on training batch is 0.000749901.
After 8915 training step(s), loss on training batch is 0.00155191.
After 8916 training step(s), loss on training batch is 0.00106025.
After 8917 training step(s), loss on training batch is 0.000878628.
After 8918 training step(s), loss on training batch is 0.000680606.
After 8919 training step(s), loss on training batch is 0.000409312.
After 8920 training step(s), loss on training batch is 0.000414778.
After 8921 training step(s), loss on training batch is 0.00042087.
After 8922 training step(s), loss on training batch is 0.000365013.
After 8923 training step(s), loss on training batch is 0.000576756.
After 8924 training step(s), loss on training batch is 0.000449303.
After 8925 training step(s), loss on training batch is 0.000432963.
After 8926 training step(s), loss on training batch is 0.000419278.
After 8927 training step(s), loss on training batch is 0.000906592.
After 8928 training step(s), loss on training batch is 0.00304794.
After 8929 training step(s), loss on training batch is 0.000741265.
After 8930 training step(s), loss on training batch is 0.000560019.
After 8931 training step(s), loss on training batch is 0.000596972.
After 8932 training step(s), loss on training batch is 0.000527394.
After 8933 training step(s), loss on training batch is 0.000454698.
After 8934 training step(s), loss on training batch is 0.000379372.
After 8935 training step(s), loss on training batch is 0.000338968.
After 8936 training step(s), loss on training batch is 0.000337271.
After 8937 training step(s), loss on training batch is 0.000376023.
After 8938 training step(s), loss on training batch is 0.000370692.
After 8939 training step(s), loss on training batch is 0.000412752.
After 8940 training step(s), loss on training batch is 0.000461429.
After 8941 training step(s), loss on training batch is 0.000354299.
After 8942 training step(s), loss on training batch is 0.000428514.
After 8943 training step(s), loss on training batch is 0.00040735.
After 8944 training step(s), loss on training batch is 0.000522931.
After 8945 training step(s), loss on training batch is 0.000342776.
After 8946 training step(s), loss on training batch is 0.000492708.
After 8947 training step(s), loss on training batch is 0.000370571.
After 8948 training step(s), loss on training batch is 0.000338801.
After 8949 training step(s), loss on training batch is 0.00038362.
After 8950 training step(s), loss on training batch is 0.000487676.
After 8951 training step(s), loss on training batch is 0.000449663.
After 8952 training step(s), loss on training batch is 0.000397981.
After 8953 training step(s), loss on training batch is 0.000400691.
After 8954 training step(s), loss on training batch is 0.000381607.
After 8955 training step(s), loss on training batch is 0.000372671.
After 8956 training step(s), loss on training batch is 0.000472271.
After 8957 training step(s), loss on training batch is 0.000563649.
After 8958 training step(s), loss on training batch is 0.000354223.
After 8959 training step(s), loss on training batch is 0.000400599.
After 8960 training step(s), loss on training batch is 0.000517498.
After 8961 training step(s), loss on training batch is 0.000355694.
After 8962 training step(s), loss on training batch is 0.000323797.
After 8963 training step(s), loss on training batch is 0.000346215.
After 8964 training step(s), loss on training batch is 0.000477848.
After 8965 training step(s), loss on training batch is 0.000415424.
After 8966 training step(s), loss on training batch is 0.000413971.
After 8967 training step(s), loss on training batch is 0.000423105.
After 8968 training step(s), loss on training batch is 0.000395243.
After 8969 training step(s), loss on training batch is 0.000425616.
After 8970 training step(s), loss on training batch is 0.000360372.
After 8971 training step(s), loss on training batch is 0.000398214.
After 8972 training step(s), loss on training batch is 0.000357327.
After 8973 training step(s), loss on training batch is 0.000338098.
After 8974 training step(s), loss on training batch is 0.000331309.
After 8975 training step(s), loss on training batch is 0.00039663.
After 8976 training step(s), loss on training batch is 0.000385474.
After 8977 training step(s), loss on training batch is 0.000535823.
After 8978 training step(s), loss on training batch is 0.000407034.
After 8979 training step(s), loss on training batch is 0.000387928.
After 8980 training step(s), loss on training batch is 0.000398043.
After 8981 training step(s), loss on training batch is 0.000363764.
After 8982 training step(s), loss on training batch is 0.000399048.
After 8983 training step(s), loss on training batch is 0.000794831.
After 8984 training step(s), loss on training batch is 0.000912858.
After 8985 training step(s), loss on training batch is 0.00110056.
After 8986 training step(s), loss on training batch is 0.000702873.
After 8987 training step(s), loss on training batch is 0.000700545.
After 8988 training step(s), loss on training batch is 0.000635063.
After 8989 training step(s), loss on training batch is 0.000716234.
After 8990 training step(s), loss on training batch is 0.000656306.
After 8991 training step(s), loss on training batch is 0.000746012.
After 8992 training step(s), loss on training batch is 0.000785457.
After 8993 training step(s), loss on training batch is 0.000611643.
After 8994 training step(s), loss on training batch is 0.000616028.
After 8995 training step(s), loss on training batch is 0.000855294.
After 8996 training step(s), loss on training batch is 0.000765118.
After 8997 training step(s), loss on training batch is 0.000654979.
After 8998 training step(s), loss on training batch is 0.000700588.
After 8999 training step(s), loss on training batch is 0.000986448.
After 9000 training step(s), loss on training batch is 0.000718905.
After 9001 training step(s), loss on training batch is 0.000647132.
After 9002 training step(s), loss on training batch is 0.000789821.
After 9003 training step(s), loss on training batch is 0.000627848.
After 9004 training step(s), loss on training batch is 0.000599107.
After 9005 training step(s), loss on training batch is 0.000535732.
After 9006 training step(s), loss on training batch is 0.000702313.
After 9007 training step(s), loss on training batch is 0.000944104.
After 9008 training step(s), loss on training batch is 0.000911236.
After 9009 training step(s), loss on training batch is 0.000562773.
After 9010 training step(s), loss on training batch is 0.00104464.
After 9011 training step(s), loss on training batch is 0.000664275.
After 9012 training step(s), loss on training batch is 0.00070128.
After 9013 training step(s), loss on training batch is 0.000557614.
After 9014 training step(s), loss on training batch is 0.000538259.
After 9015 training step(s), loss on training batch is 0.000518855.
After 9016 training step(s), loss on training batch is 0.000532903.
After 9017 training step(s), loss on training batch is 0.000550762.
After 9018 training step(s), loss on training batch is 0.000824051.
After 9019 training step(s), loss on training batch is 0.00111021.
After 9020 training step(s), loss on training batch is 0.00105577.
After 9021 training step(s), loss on training batch is 0.00143782.
After 9022 training step(s), loss on training batch is 0.000688133.
After 9023 training step(s), loss on training batch is 0.000674905.
After 9024 training step(s), loss on training batch is 0.000682543.
After 9025 training step(s), loss on training batch is 0.000749732.
After 9026 training step(s), loss on training batch is 0.000738533.
After 9027 training step(s), loss on training batch is 0.000632683.
After 9028 training step(s), loss on training batch is 0.000677331.
After 9029 training step(s), loss on training batch is 0.000688822.
After 9030 training step(s), loss on training batch is 0.000789827.
After 9031 training step(s), loss on training batch is 0.000939744.
After 9032 training step(s), loss on training batch is 0.000584276.
After 9033 training step(s), loss on training batch is 0.000606562.
After 9034 training step(s), loss on training batch is 0.000566295.
After 9035 training step(s), loss on training batch is 0.000502788.
After 9036 training step(s), loss on training batch is 0.000817637.
After 9037 training step(s), loss on training batch is 0.0013769.
After 9038 training step(s), loss on training batch is 0.000533368.
After 9039 training step(s), loss on training batch is 0.000618832.
After 9040 training step(s), loss on training batch is 0.000692454.
After 9041 training step(s), loss on training batch is 0.000604767.
After 9042 training step(s), loss on training batch is 0.000582893.
After 9043 training step(s), loss on training batch is 0.000662172.
After 9044 training step(s), loss on training batch is 0.00060622.
After 9045 training step(s), loss on training batch is 0.000673322.
After 9046 training step(s), loss on training batch is 0.000825932.
After 9047 training step(s), loss on training batch is 0.000639525.
After 9048 training step(s), loss on training batch is 0.000647566.
After 9049 training step(s), loss on training batch is 0.000696585.
After 9050 training step(s), loss on training batch is 0.000727242.
After 9051 training step(s), loss on training batch is 0.000629344.
After 9052 training step(s), loss on training batch is 0.000531887.
After 9053 training step(s), loss on training batch is 0.000816261.
After 9054 training step(s), loss on training batch is 0.000634096.
After 9055 training step(s), loss on training batch is 0.000692646.
After 9056 training step(s), loss on training batch is 0.000625141.
After 9057 training step(s), loss on training batch is 0.000560991.
After 9058 training step(s), loss on training batch is 0.000758698.
After 9059 training step(s), loss on training batch is 0.000926003.
After 9060 training step(s), loss on training batch is 0.000616956.
After 9061 training step(s), loss on training batch is 0.000556686.
After 9062 training step(s), loss on training batch is 0.000547872.
After 9063 training step(s), loss on training batch is 0.000576147.
After 9064 training step(s), loss on training batch is 0.00051839.
After 9065 training step(s), loss on training batch is 0.000729325.
After 9066 training step(s), loss on training batch is 0.00111015.
After 9067 training step(s), loss on training batch is 0.00119281.
After 9068 training step(s), loss on training batch is 0.00106837.
After 9069 training step(s), loss on training batch is 0.00110647.
After 9070 training step(s), loss on training batch is 0.00105182.
After 9071 training step(s), loss on training batch is 0.000996508.
After 9072 training step(s), loss on training batch is 0.00104192.
After 9073 training step(s), loss on training batch is 0.00174404.
After 9074 training step(s), loss on training batch is 0.00123374.
After 9075 training step(s), loss on training batch is 0.00135311.
After 9076 training step(s), loss on training batch is 0.0010806.
After 9077 training step(s), loss on training batch is 0.00100941.
After 9078 training step(s), loss on training batch is 0.00110805.
After 9079 training step(s), loss on training batch is 0.000947161.
After 9080 training step(s), loss on training batch is 0.000927948.
After 9081 training step(s), loss on training batch is 0.000910474.
After 9082 training step(s), loss on training batch is 0.000934772.
After 9083 training step(s), loss on training batch is 0.00104976.
After 9084 training step(s), loss on training batch is 0.000940646.
After 9085 training step(s), loss on training batch is 0.000937573.
After 9086 training step(s), loss on training batch is 0.00120648.
After 9087 training step(s), loss on training batch is 0.000959509.
After 9088 training step(s), loss on training batch is 0.00114741.
After 9089 training step(s), loss on training batch is 0.000972323.
After 9090 training step(s), loss on training batch is 0.000978834.
After 9091 training step(s), loss on training batch is 0.000938373.
After 9092 training step(s), loss on training batch is 0.000981015.
After 9093 training step(s), loss on training batch is 0.000900141.
After 9094 training step(s), loss on training batch is 0.00103236.
After 9095 training step(s), loss on training batch is 0.00109978.
After 9096 training step(s), loss on training batch is 0.00117139.
After 9097 training step(s), loss on training batch is 0.000979279.
After 9098 training step(s), loss on training batch is 0.00095762.
After 9099 training step(s), loss on training batch is 0.000891.
After 9100 training step(s), loss on training batch is 0.000936226.
After 9101 training step(s), loss on training batch is 0.000824811.
After 9102 training step(s), loss on training batch is 0.00107937.
After 9103 training step(s), loss on training batch is 0.000904371.
After 9104 training step(s), loss on training batch is 0.00117828.
After 9105 training step(s), loss on training batch is 0.00120325.
After 9106 training step(s), loss on training batch is 0.00126572.
After 9107 training step(s), loss on training batch is 0.00136777.
After 9108 training step(s), loss on training batch is 0.00114963.
After 9109 training step(s), loss on training batch is 0.00790544.
After 9110 training step(s), loss on training batch is 0.00573172.
After 9111 training step(s), loss on training batch is 0.00259926.
After 9112 training step(s), loss on training batch is 0.00220302.
After 9113 training step(s), loss on training batch is 0.00184216.
After 9114 training step(s), loss on training batch is 0.00166075.
After 9115 training step(s), loss on training batch is 0.00139639.
After 9116 training step(s), loss on training batch is 0.000658097.
After 9117 training step(s), loss on training batch is 0.000521711.
After 9118 training step(s), loss on training batch is 0.000489676.
After 9119 training step(s), loss on training batch is 0.000556064.
After 9120 training step(s), loss on training batch is 0.000615641.
After 9121 training step(s), loss on training batch is 0.000584367.
After 9122 training step(s), loss on training batch is 0.000604282.
After 9123 training step(s), loss on training batch is 0.000599191.
After 9124 training step(s), loss on training batch is 0.000565393.
After 9125 training step(s), loss on training batch is 0.000518548.
After 9126 training step(s), loss on training batch is 0.000990215.
After 9127 training step(s), loss on training batch is 0.000996357.
After 9128 training step(s), loss on training batch is 0.00112958.
After 9129 training step(s), loss on training batch is 0.000966521.
After 9130 training step(s), loss on training batch is 0.00199175.
After 9131 training step(s), loss on training batch is 0.00176787.
After 9132 training step(s), loss on training batch is 0.00123143.
After 9133 training step(s), loss on training batch is 0.00124993.
After 9134 training step(s), loss on training batch is 0.0010217.
After 9135 training step(s), loss on training batch is 0.000967085.
After 9136 training step(s), loss on training batch is 0.000995148.
After 9137 training step(s), loss on training batch is 0.000989399.
After 9138 training step(s), loss on training batch is 0.00109576.
After 9139 training step(s), loss on training batch is 0.00102733.
After 9140 training step(s), loss on training batch is 0.000999014.
After 9141 training step(s), loss on training batch is 0.000965157.
After 9142 training step(s), loss on training batch is 0.00158089.
After 9143 training step(s), loss on training batch is 0.000402183.
After 9144 training step(s), loss on training batch is 0.000459031.
After 9145 training step(s), loss on training batch is 0.000430949.
After 9146 training step(s), loss on training batch is 0.000514472.
After 9147 training step(s), loss on training batch is 0.000495825.
After 9148 training step(s), loss on training batch is 0.000492076.
After 9149 training step(s), loss on training batch is 0.000388005.
After 9150 training step(s), loss on training batch is 0.000353525.
After 9151 training step(s), loss on training batch is 0.000564513.
After 9152 training step(s), loss on training batch is 0.000473809.
After 9153 training step(s), loss on training batch is 0.000449975.
After 9154 training step(s), loss on training batch is 0.000390003.
After 9155 training step(s), loss on training batch is 0.00061806.
After 9156 training step(s), loss on training batch is 0.000419455.
After 9157 training step(s), loss on training batch is 0.000371186.
After 9158 training step(s), loss on training batch is 0.000363989.
After 9159 training step(s), loss on training batch is 0.000905483.
After 9160 training step(s), loss on training batch is 0.00112605.
After 9161 training step(s), loss on training batch is 0.00061487.
After 9162 training step(s), loss on training batch is 0.000450505.
After 9163 training step(s), loss on training batch is 0.000429953.
After 9164 training step(s), loss on training batch is 0.000519059.
After 9165 training step(s), loss on training batch is 0.000531623.
After 9166 training step(s), loss on training batch is 0.000456014.
After 9167 training step(s), loss on training batch is 0.00039688.
After 9168 training step(s), loss on training batch is 0.000548452.
After 9169 training step(s), loss on training batch is 0.000348513.
After 9170 training step(s), loss on training batch is 0.000434551.
After 9171 training step(s), loss on training batch is 0.000286871.
After 9172 training step(s), loss on training batch is 0.000326223.
After 9173 training step(s), loss on training batch is 0.000346371.
After 9174 training step(s), loss on training batch is 0.00033581.
After 9175 training step(s), loss on training batch is 0.000503857.
After 9176 training step(s), loss on training batch is 0.000484349.
After 9177 training step(s), loss on training batch is 0.000476665.
After 9178 training step(s), loss on training batch is 0.000354596.
After 9179 training step(s), loss on training batch is 0.000368305.
After 9180 training step(s), loss on training batch is 0.000332967.
After 9181 training step(s), loss on training batch is 0.000383626.
After 9182 training step(s), loss on training batch is 0.000350591.
After 9183 training step(s), loss on training batch is 0.000429578.
After 9184 training step(s), loss on training batch is 0.00043992.
After 9185 training step(s), loss on training batch is 0.000438613.
After 9186 training step(s), loss on training batch is 0.000562385.
After 9187 training step(s), loss on training batch is 0.000477818.
After 9188 training step(s), loss on training batch is 0.000352641.
After 9189 training step(s), loss on training batch is 0.000398726.
After 9190 training step(s), loss on training batch is 0.000359029.
After 9191 training step(s), loss on training batch is 0.000336844.
After 9192 training step(s), loss on training batch is 0.000336601.
After 9193 training step(s), loss on training batch is 0.000327364.
After 9194 training step(s), loss on training batch is 0.000283632.
After 9195 training step(s), loss on training batch is 0.000546283.
After 9196 training step(s), loss on training batch is 0.000485156.
After 9197 training step(s), loss on training batch is 0.000266853.
After 9198 training step(s), loss on training batch is 0.000287335.
After 9199 training step(s), loss on training batch is 0.000385165.
After 9200 training step(s), loss on training batch is 0.000409028.
After 9201 training step(s), loss on training batch is 0.000568682.
After 9202 training step(s), loss on training batch is 0.000642534.
After 9203 training step(s), loss on training batch is 0.000378376.
After 9204 training step(s), loss on training batch is 0.000572604.
After 9205 training step(s), loss on training batch is 0.00114131.
After 9206 training step(s), loss on training batch is 0.000778861.
After 9207 training step(s), loss on training batch is 0.00042713.
After 9208 training step(s), loss on training batch is 0.000523021.
After 9209 training step(s), loss on training batch is 0.000650455.
After 9210 training step(s), loss on training batch is 0.000665839.
After 9211 training step(s), loss on training batch is 0.000417605.
After 9212 training step(s), loss on training batch is 0.000339561.
After 9213 training step(s), loss on training batch is 0.000394862.
After 9214 training step(s), loss on training batch is 0.000429677.
After 9215 training step(s), loss on training batch is 0.000389693.
After 9216 training step(s), loss on training batch is 0.000330436.
After 9217 training step(s), loss on training batch is 0.000434616.
After 9218 training step(s), loss on training batch is 0.000355805.
After 9219 training step(s), loss on training batch is 0.000451749.
After 9220 training step(s), loss on training batch is 0.000406433.
After 9221 training step(s), loss on training batch is 0.000327113.
After 9222 training step(s), loss on training batch is 0.00040272.
After 9223 training step(s), loss on training batch is 0.000357182.
After 9224 training step(s), loss on training batch is 0.000327669.
After 9225 training step(s), loss on training batch is 0.000322152.
After 9226 training step(s), loss on training batch is 0.000392198.
After 9227 training step(s), loss on training batch is 0.000323421.
After 9228 training step(s), loss on training batch is 0.00100816.
After 9229 training step(s), loss on training batch is 0.000651125.
After 9230 training step(s), loss on training batch is 0.000590621.
After 9231 training step(s), loss on training batch is 0.000599521.
After 9232 training step(s), loss on training batch is 0.000719351.
After 9233 training step(s), loss on training batch is 0.000816568.
After 9234 training step(s), loss on training batch is 0.000668937.
After 9235 training step(s), loss on training batch is 0.000517796.
After 9236 training step(s), loss on training batch is 0.000750553.
After 9237 training step(s), loss on training batch is 0.000741134.
After 9238 training step(s), loss on training batch is 0.000530398.
After 9239 training step(s), loss on training batch is 0.000528423.
After 9240 training step(s), loss on training batch is 0.00117862.
After 9241 training step(s), loss on training batch is 0.000572441.
After 9242 training step(s), loss on training batch is 0.000642052.
After 9243 training step(s), loss on training batch is 0.000658501.
After 9244 training step(s), loss on training batch is 0.000773563.
After 9245 training step(s), loss on training batch is 0.00051838.
After 9246 training step(s), loss on training batch is 0.0007854.
After 9247 training step(s), loss on training batch is 0.000583819.
After 9248 training step(s), loss on training batch is 0.000660977.
After 9249 training step(s), loss on training batch is 0.000694265.
After 9250 training step(s), loss on training batch is 0.000683447.
After 9251 training step(s), loss on training batch is 0.00068003.
After 9252 training step(s), loss on training batch is 0.00059157.
After 9253 training step(s), loss on training batch is 0.000688885.
After 9254 training step(s), loss on training batch is 0.000478886.
After 9255 training step(s), loss on training batch is 0.000663413.
After 9256 training step(s), loss on training batch is 0.0011575.
After 9257 training step(s), loss on training batch is 0.000972072.
After 9258 training step(s), loss on training batch is 0.00100446.
After 9259 training step(s), loss on training batch is 0.000999889.
After 9260 training step(s), loss on training batch is 0.000855075.
After 9261 training step(s), loss on training batch is 0.00088932.
After 9262 training step(s), loss on training batch is 0.00113654.
After 9263 training step(s), loss on training batch is 0.000985908.
After 9264 training step(s), loss on training batch is 0.000889732.
After 9265 training step(s), loss on training batch is 0.00119907.
After 9266 training step(s), loss on training batch is 0.000893664.
After 9267 training step(s), loss on training batch is 0.000909923.
After 9268 training step(s), loss on training batch is 0.00103451.
After 9269 training step(s), loss on training batch is 0.00170337.
After 9270 training step(s), loss on training batch is 0.00301065.
After 9271 training step(s), loss on training batch is 0.00152794.
After 9272 training step(s), loss on training batch is 0.00117499.
After 9273 training step(s), loss on training batch is 0.00130734.
After 9274 training step(s), loss on training batch is 0.0011393.
After 9275 training step(s), loss on training batch is 0.00108852.
After 9276 training step(s), loss on training batch is 0.00102103.
After 9277 training step(s), loss on training batch is 0.00143742.
After 9278 training step(s), loss on training batch is 0.000983873.
After 9279 training step(s), loss on training batch is 0.000963288.
After 9280 training step(s), loss on training batch is 0.000928735.
After 9281 training step(s), loss on training batch is 0.000549707.
After 9282 training step(s), loss on training batch is 0.000504568.
After 9283 training step(s), loss on training batch is 0.000324649.
After 9284 training step(s), loss on training batch is 0.000404226.
After 9285 training step(s), loss on training batch is 0.000428252.
After 9286 training step(s), loss on training batch is 0.000583876.
After 9287 training step(s), loss on training batch is 0.000699041.
After 9288 training step(s), loss on training batch is 0.000470527.
After 9289 training step(s), loss on training batch is 0.000404649.
After 9290 training step(s), loss on training batch is 0.000379787.
After 9291 training step(s), loss on training batch is 0.00031364.
After 9292 training step(s), loss on training batch is 0.000395653.
After 9293 training step(s), loss on training batch is 0.000341418.
After 9294 training step(s), loss on training batch is 0.000354759.
After 9295 training step(s), loss on training batch is 0.000419101.
After 9296 training step(s), loss on training batch is 0.000405659.
After 9297 training step(s), loss on training batch is 0.00032368.
After 9298 training step(s), loss on training batch is 0.000311111.
After 9299 training step(s), loss on training batch is 0.000317415.
After 9300 training step(s), loss on training batch is 0.000325415.
After 9301 training step(s), loss on training batch is 0.00107964.
After 9302 training step(s), loss on training batch is 0.000561192.
After 9303 training step(s), loss on training batch is 0.000585372.
After 9304 training step(s), loss on training batch is 0.000707863.
After 9305 training step(s), loss on training batch is 0.000647636.
After 9306 training step(s), loss on training batch is 0.000652744.
After 9307 training step(s), loss on training batch is 0.000653095.
After 9308 training step(s), loss on training batch is 0.000365369.
After 9309 training step(s), loss on training batch is 0.000409178.
After 9310 training step(s), loss on training batch is 0.000438915.
After 9311 training step(s), loss on training batch is 0.000528318.
After 9312 training step(s), loss on training batch is 0.00049029.
After 9313 training step(s), loss on training batch is 0.000514756.
After 9314 training step(s), loss on training batch is 0.000798032.
After 9315 training step(s), loss on training batch is 0.00157239.
After 9316 training step(s), loss on training batch is 0.00106295.
After 9317 training step(s), loss on training batch is 0.000973561.
After 9318 training step(s), loss on training batch is 0.000733079.
After 9319 training step(s), loss on training batch is 0.000417828.
After 9320 training step(s), loss on training batch is 0.000429132.
After 9321 training step(s), loss on training batch is 0.000429774.
After 9322 training step(s), loss on training batch is 0.000363548.
After 9323 training step(s), loss on training batch is 0.000581329.
After 9324 training step(s), loss on training batch is 0.000449642.
After 9325 training step(s), loss on training batch is 0.000428879.
After 9326 training step(s), loss on training batch is 0.000419625.
After 9327 training step(s), loss on training batch is 0.000907141.
After 9328 training step(s), loss on training batch is 0.00222133.
After 9329 training step(s), loss on training batch is 0.000927744.
After 9330 training step(s), loss on training batch is 0.000646997.
After 9331 training step(s), loss on training batch is 0.000592637.
After 9332 training step(s), loss on training batch is 0.000580438.
After 9333 training step(s), loss on training batch is 0.000486479.
After 9334 training step(s), loss on training batch is 0.000390265.
After 9335 training step(s), loss on training batch is 0.000366908.
After 9336 training step(s), loss on training batch is 0.000361383.
After 9337 training step(s), loss on training batch is 0.000420202.
After 9338 training step(s), loss on training batch is 0.000419572.
After 9339 training step(s), loss on training batch is 0.000460024.
After 9340 training step(s), loss on training batch is 0.000518537.
After 9341 training step(s), loss on training batch is 0.000393412.
After 9342 training step(s), loss on training batch is 0.0004812.
After 9343 training step(s), loss on training batch is 0.000459643.
After 9344 training step(s), loss on training batch is 0.000547372.
After 9345 training step(s), loss on training batch is 0.000351708.
After 9346 training step(s), loss on training batch is 0.000481969.
After 9347 training step(s), loss on training batch is 0.000377996.
After 9348 training step(s), loss on training batch is 0.000345405.
After 9349 training step(s), loss on training batch is 0.000389866.
After 9350 training step(s), loss on training batch is 0.0005101.
After 9351 training step(s), loss on training batch is 0.000461198.
After 9352 training step(s), loss on training batch is 0.000408888.
After 9353 training step(s), loss on training batch is 0.00040529.
After 9354 training step(s), loss on training batch is 0.000372306.
After 9355 training step(s), loss on training batch is 0.000375023.
After 9356 training step(s), loss on training batch is 0.000479282.
After 9357 training step(s), loss on training batch is 0.000566915.
After 9358 training step(s), loss on training batch is 0.000359258.
After 9359 training step(s), loss on training batch is 0.000403428.
After 9360 training step(s), loss on training batch is 0.00050747.
After 9361 training step(s), loss on training batch is 0.000362249.
After 9362 training step(s), loss on training batch is 0.000326608.
After 9363 training step(s), loss on training batch is 0.000347692.
After 9364 training step(s), loss on training batch is 0.000474497.
After 9365 training step(s), loss on training batch is 0.000415668.
After 9366 training step(s), loss on training batch is 0.00040651.
After 9367 training step(s), loss on training batch is 0.000471523.
After 9368 training step(s), loss on training batch is 0.000456055.
After 9369 training step(s), loss on training batch is 0.000426204.
After 9370 training step(s), loss on training batch is 0.000364023.
After 9371 training step(s), loss on training batch is 0.000409588.
After 9372 training step(s), loss on training batch is 0.000360527.
After 9373 training step(s), loss on training batch is 0.000337836.
After 9374 training step(s), loss on training batch is 0.000342948.
After 9375 training step(s), loss on training batch is 0.00039569.
After 9376 training step(s), loss on training batch is 0.000378575.
After 9377 training step(s), loss on training batch is 0.000555022.
After 9378 training step(s), loss on training batch is 0.000402075.
After 9379 training step(s), loss on training batch is 0.000376218.
After 9380 training step(s), loss on training batch is 0.000394375.
After 9381 training step(s), loss on training batch is 0.000360235.
After 9382 training step(s), loss on training batch is 0.00039143.
After 9383 training step(s), loss on training batch is 0.000763653.
After 9384 training step(s), loss on training batch is 0.000903055.
After 9385 training step(s), loss on training batch is 0.00108751.
After 9386 training step(s), loss on training batch is 0.000698274.
After 9387 training step(s), loss on training batch is 0.000705066.
After 9388 training step(s), loss on training batch is 0.000620264.
After 9389 training step(s), loss on training batch is 0.000677341.
After 9390 training step(s), loss on training batch is 0.00063646.
After 9391 training step(s), loss on training batch is 0.000731988.
After 9392 training step(s), loss on training batch is 0.000793009.
After 9393 training step(s), loss on training batch is 0.000606066.
After 9394 training step(s), loss on training batch is 0.000605987.
After 9395 training step(s), loss on training batch is 0.000872613.
After 9396 training step(s), loss on training batch is 0.000761277.
After 9397 training step(s), loss on training batch is 0.000658753.
After 9398 training step(s), loss on training batch is 0.000693219.
After 9399 training step(s), loss on training batch is 0.000995731.
After 9400 training step(s), loss on training batch is 0.000699698.
After 9401 training step(s), loss on training batch is 0.000642155.
After 9402 training step(s), loss on training batch is 0.000747542.
After 9403 training step(s), loss on training batch is 0.000633154.
After 9404 training step(s), loss on training batch is 0.000613821.
After 9405 training step(s), loss on training batch is 0.000540074.
After 9406 training step(s), loss on training batch is 0.000696589.
After 9407 training step(s), loss on training batch is 0.000918774.
After 9408 training step(s), loss on training batch is 0.000776737.
After 9409 training step(s), loss on training batch is 0.000560959.
After 9410 training step(s), loss on training batch is 0.000983276.
After 9411 training step(s), loss on training batch is 0.000671209.
After 9412 training step(s), loss on training batch is 0.000677374.
After 9413 training step(s), loss on training batch is 0.00055588.
After 9414 training step(s), loss on training batch is 0.000543756.
After 9415 training step(s), loss on training batch is 0.000526909.
After 9416 training step(s), loss on training batch is 0.000538411.
After 9417 training step(s), loss on training batch is 0.000553032.
After 9418 training step(s), loss on training batch is 0.000763275.
After 9419 training step(s), loss on training batch is 0.0010641.
After 9420 training step(s), loss on training batch is 0.00102759.
After 9421 training step(s), loss on training batch is 0.0013797.
After 9422 training step(s), loss on training batch is 0.000701739.
After 9423 training step(s), loss on training batch is 0.000690177.
After 9424 training step(s), loss on training batch is 0.000697455.
After 9425 training step(s), loss on training batch is 0.000773769.
After 9426 training step(s), loss on training batch is 0.000775222.
After 9427 training step(s), loss on training batch is 0.000664158.
After 9428 training step(s), loss on training batch is 0.000685348.
After 9429 training step(s), loss on training batch is 0.00069556.
After 9430 training step(s), loss on training batch is 0.000779133.
After 9431 training step(s), loss on training batch is 0.000898177.
After 9432 training step(s), loss on training batch is 0.000593531.
After 9433 training step(s), loss on training batch is 0.000610529.
After 9434 training step(s), loss on training batch is 0.000569584.
After 9435 training step(s), loss on training batch is 0.000515592.
After 9436 training step(s), loss on training batch is 0.000803087.
After 9437 training step(s), loss on training batch is 0.00129213.
After 9438 training step(s), loss on training batch is 0.00053328.
After 9439 training step(s), loss on training batch is 0.00061961.
After 9440 training step(s), loss on training batch is 0.000678004.
After 9441 training step(s), loss on training batch is 0.000610024.
After 9442 training step(s), loss on training batch is 0.000563738.
After 9443 training step(s), loss on training batch is 0.000643656.
After 9444 training step(s), loss on training batch is 0.000587313.
After 9445 training step(s), loss on training batch is 0.000663077.
After 9446 training step(s), loss on training batch is 0.000840026.
After 9447 training step(s), loss on training batch is 0.00060786.
After 9448 training step(s), loss on training batch is 0.00068419.
After 9449 training step(s), loss on training batch is 0.000711287.
After 9450 training step(s), loss on training batch is 0.000726092.
After 9451 training step(s), loss on training batch is 0.000657615.
After 9452 training step(s), loss on training batch is 0.000528131.
After 9453 training step(s), loss on training batch is 0.00083225.
After 9454 training step(s), loss on training batch is 0.000646003.
After 9455 training step(s), loss on training batch is 0.00069365.
After 9456 training step(s), loss on training batch is 0.000620595.
After 9457 training step(s), loss on training batch is 0.000541672.
After 9458 training step(s), loss on training batch is 0.00077076.
After 9459 training step(s), loss on training batch is 0.000932625.
After 9460 training step(s), loss on training batch is 0.000612619.
After 9461 training step(s), loss on training batch is 0.000544344.
After 9462 training step(s), loss on training batch is 0.000541209.
After 9463 training step(s), loss on training batch is 0.000565144.
After 9464 training step(s), loss on training batch is 0.000510624.
After 9465 training step(s), loss on training batch is 0.000717134.
After 9466 training step(s), loss on training batch is 0.00112287.
After 9467 training step(s), loss on training batch is 0.00118936.
After 9468 training step(s), loss on training batch is 0.00107903.
After 9469 training step(s), loss on training batch is 0.00111169.
After 9470 training step(s), loss on training batch is 0.00105187.
After 9471 training step(s), loss on training batch is 0.00098974.
After 9472 training step(s), loss on training batch is 0.00103729.
After 9473 training step(s), loss on training batch is 0.00169724.
After 9474 training step(s), loss on training batch is 0.00123583.
After 9475 training step(s), loss on training batch is 0.00133588.
After 9476 training step(s), loss on training batch is 0.00108676.
After 9477 training step(s), loss on training batch is 0.00101427.
After 9478 training step(s), loss on training batch is 0.00109105.
After 9479 training step(s), loss on training batch is 0.000921369.
After 9480 training step(s), loss on training batch is 0.000917123.
After 9481 training step(s), loss on training batch is 0.000898389.
After 9482 training step(s), loss on training batch is 0.000923048.
After 9483 training step(s), loss on training batch is 0.00105101.
After 9484 training step(s), loss on training batch is 0.000928033.
After 9485 training step(s), loss on training batch is 0.00091806.
After 9486 training step(s), loss on training batch is 0.00122666.
After 9487 training step(s), loss on training batch is 0.000931285.
After 9488 training step(s), loss on training batch is 0.001145.
After 9489 training step(s), loss on training batch is 0.000955078.
After 9490 training step(s), loss on training batch is 0.000950855.
After 9491 training step(s), loss on training batch is 0.000930073.
After 9492 training step(s), loss on training batch is 0.000980548.
After 9493 training step(s), loss on training batch is 0.000891199.
After 9494 training step(s), loss on training batch is 0.00103026.
After 9495 training step(s), loss on training batch is 0.00110682.
After 9496 training step(s), loss on training batch is 0.00116465.
After 9497 training step(s), loss on training batch is 0.000951843.
After 9498 training step(s), loss on training batch is 0.000949479.
After 9499 training step(s), loss on training batch is 0.000880316.
After 9500 training step(s), loss on training batch is 0.000934678.
After 9501 training step(s), loss on training batch is 0.000827006.
After 9502 training step(s), loss on training batch is 0.00103754.
After 9503 training step(s), loss on training batch is 0.000911486.
After 9504 training step(s), loss on training batch is 0.00115506.
After 9505 training step(s), loss on training batch is 0.0012254.
After 9506 training step(s), loss on training batch is 0.00116019.
After 9507 training step(s), loss on training batch is 0.00132787.
After 9508 training step(s), loss on training batch is 0.00113615.
After 9509 training step(s), loss on training batch is 0.00745615.
After 9510 training step(s), loss on training batch is 0.017043.
After 9511 training step(s), loss on training batch is 0.00186748.
After 9512 training step(s), loss on training batch is 0.00175424.
After 9513 training step(s), loss on training batch is 0.00159855.
After 9514 training step(s), loss on training batch is 0.00150114.
After 9515 training step(s), loss on training batch is 0.00128007.
After 9516 training step(s), loss on training batch is 0.000598805.
After 9517 training step(s), loss on training batch is 0.000530529.
After 9518 training step(s), loss on training batch is 0.000496401.
After 9519 training step(s), loss on training batch is 0.00054396.
After 9520 training step(s), loss on training batch is 0.000606928.
After 9521 training step(s), loss on training batch is 0.000576879.
After 9522 training step(s), loss on training batch is 0.000576846.
After 9523 training step(s), loss on training batch is 0.000561405.
After 9524 training step(s), loss on training batch is 0.000559446.
After 9525 training step(s), loss on training batch is 0.00052503.
After 9526 training step(s), loss on training batch is 0.000952909.
After 9527 training step(s), loss on training batch is 0.000950592.
After 9528 training step(s), loss on training batch is 0.00108123.
After 9529 training step(s), loss on training batch is 0.000926166.
After 9530 training step(s), loss on training batch is 0.00204346.
After 9531 training step(s), loss on training batch is 0.00173425.
After 9532 training step(s), loss on training batch is 0.001222.
After 9533 training step(s), loss on training batch is 0.00122793.
After 9534 training step(s), loss on training batch is 0.000993441.
After 9535 training step(s), loss on training batch is 0.000963152.
After 9536 training step(s), loss on training batch is 0.000978235.
After 9537 training step(s), loss on training batch is 0.000994872.
After 9538 training step(s), loss on training batch is 0.00109749.
After 9539 training step(s), loss on training batch is 0.00100019.
After 9540 training step(s), loss on training batch is 0.000975672.
After 9541 training step(s), loss on training batch is 0.000944205.
After 9542 training step(s), loss on training batch is 0.00162467.
After 9543 training step(s), loss on training batch is 0.000370451.
After 9544 training step(s), loss on training batch is 0.00052775.
After 9545 training step(s), loss on training batch is 0.000418372.
After 9546 training step(s), loss on training batch is 0.000535957.
After 9547 training step(s), loss on training batch is 0.000481144.
After 9548 training step(s), loss on training batch is 0.000428525.
After 9549 training step(s), loss on training batch is 0.000385638.
After 9550 training step(s), loss on training batch is 0.000352873.
After 9551 training step(s), loss on training batch is 0.000542781.
After 9552 training step(s), loss on training batch is 0.000466264.
After 9553 training step(s), loss on training batch is 0.000444297.
After 9554 training step(s), loss on training batch is 0.000390553.
After 9555 training step(s), loss on training batch is 0.000592928.
After 9556 training step(s), loss on training batch is 0.000416059.
After 9557 training step(s), loss on training batch is 0.000360427.
After 9558 training step(s), loss on training batch is 0.000358223.
After 9559 training step(s), loss on training batch is 0.000909459.
After 9560 training step(s), loss on training batch is 0.00116286.
After 9561 training step(s), loss on training batch is 0.000612729.
After 9562 training step(s), loss on training batch is 0.000448909.
After 9563 training step(s), loss on training batch is 0.000426883.
After 9564 training step(s), loss on training batch is 0.000495323.
After 9565 training step(s), loss on training batch is 0.000511245.
After 9566 training step(s), loss on training batch is 0.00045742.
After 9567 training step(s), loss on training batch is 0.000407903.
After 9568 training step(s), loss on training batch is 0.000554517.
After 9569 training step(s), loss on training batch is 0.000349105.
After 9570 training step(s), loss on training batch is 0.000438951.
After 9571 training step(s), loss on training batch is 0.00028956.
After 9572 training step(s), loss on training batch is 0.000334263.
After 9573 training step(s), loss on training batch is 0.000327562.
After 9574 training step(s), loss on training batch is 0.000330447.
After 9575 training step(s), loss on training batch is 0.000537753.
After 9576 training step(s), loss on training batch is 0.00050643.
After 9577 training step(s), loss on training batch is 0.00047182.
After 9578 training step(s), loss on training batch is 0.000355279.
After 9579 training step(s), loss on training batch is 0.000368207.
After 9580 training step(s), loss on training batch is 0.00033346.
After 9581 training step(s), loss on training batch is 0.000377654.
After 9582 training step(s), loss on training batch is 0.000347464.
After 9583 training step(s), loss on training batch is 0.000431534.
After 9584 training step(s), loss on training batch is 0.000428756.
After 9585 training step(s), loss on training batch is 0.000430198.
After 9586 training step(s), loss on training batch is 0.000581725.
After 9587 training step(s), loss on training batch is 0.000478595.
After 9588 training step(s), loss on training batch is 0.000344896.
After 9589 training step(s), loss on training batch is 0.000392862.
After 9590 training step(s), loss on training batch is 0.000364391.
After 9591 training step(s), loss on training batch is 0.000333556.
After 9592 training step(s), loss on training batch is 0.000348695.
After 9593 training step(s), loss on training batch is 0.000339641.
After 9594 training step(s), loss on training batch is 0.000288155.
After 9595 training step(s), loss on training batch is 0.000501333.
After 9596 training step(s), loss on training batch is 0.000474825.
After 9597 training step(s), loss on training batch is 0.000267757.
After 9598 training step(s), loss on training batch is 0.000280484.
After 9599 training step(s), loss on training batch is 0.000356737.
After 9600 training step(s), loss on training batch is 0.000406481.
After 9601 training step(s), loss on training batch is 0.000576951.
After 9602 training step(s), loss on training batch is 0.000649744.
After 9603 training step(s), loss on training batch is 0.000401367.
After 9604 training step(s), loss on training batch is 0.000583711.
After 9605 training step(s), loss on training batch is 0.00109671.
After 9606 training step(s), loss on training batch is 0.00078163.
After 9607 training step(s), loss on training batch is 0.000435707.
After 9608 training step(s), loss on training batch is 0.000514657.
After 9609 training step(s), loss on training batch is 0.000664338.
After 9610 training step(s), loss on training batch is 0.000678708.
After 9611 training step(s), loss on training batch is 0.000415254.
After 9612 training step(s), loss on training batch is 0.000328534.
After 9613 training step(s), loss on training batch is 0.000381397.
After 9614 training step(s), loss on training batch is 0.000408854.
After 9615 training step(s), loss on training batch is 0.000377282.
After 9616 training step(s), loss on training batch is 0.000324722.
After 9617 training step(s), loss on training batch is 0.000422184.
After 9618 training step(s), loss on training batch is 0.00035536.
After 9619 training step(s), loss on training batch is 0.000444774.
After 9620 training step(s), loss on training batch is 0.000400754.
After 9621 training step(s), loss on training batch is 0.000319754.
After 9622 training step(s), loss on training batch is 0.000401484.
After 9623 training step(s), loss on training batch is 0.00036486.
After 9624 training step(s), loss on training batch is 0.000329121.
After 9625 training step(s), loss on training batch is 0.000319448.
After 9626 training step(s), loss on training batch is 0.000395359.
After 9627 training step(s), loss on training batch is 0.00030571.
After 9628 training step(s), loss on training batch is 0.00111964.
After 9629 training step(s), loss on training batch is 0.000657427.
After 9630 training step(s), loss on training batch is 0.000575657.
After 9631 training step(s), loss on training batch is 0.000592853.
After 9632 training step(s), loss on training batch is 0.000710868.
After 9633 training step(s), loss on training batch is 0.000788609.
After 9634 training step(s), loss on training batch is 0.00066203.
After 9635 training step(s), loss on training batch is 0.00050502.
After 9636 training step(s), loss on training batch is 0.000763142.
After 9637 training step(s), loss on training batch is 0.00074591.
After 9638 training step(s), loss on training batch is 0.000532112.
After 9639 training step(s), loss on training batch is 0.00052579.
After 9640 training step(s), loss on training batch is 0.001198.
After 9641 training step(s), loss on training batch is 0.000570398.
After 9642 training step(s), loss on training batch is 0.000641728.
After 9643 training step(s), loss on training batch is 0.000655587.
After 9644 training step(s), loss on training batch is 0.000748813.
After 9645 training step(s), loss on training batch is 0.000528983.
After 9646 training step(s), loss on training batch is 0.000771403.
After 9647 training step(s), loss on training batch is 0.000579353.
After 9648 training step(s), loss on training batch is 0.000650707.
After 9649 training step(s), loss on training batch is 0.000691635.
After 9650 training step(s), loss on training batch is 0.000681033.
After 9651 training step(s), loss on training batch is 0.000694322.
After 9652 training step(s), loss on training batch is 0.00059439.
After 9653 training step(s), loss on training batch is 0.000690391.
After 9654 training step(s), loss on training batch is 0.000473142.
After 9655 training step(s), loss on training batch is 0.000656582.
After 9656 training step(s), loss on training batch is 0.00115624.
After 9657 training step(s), loss on training batch is 0.000963212.
After 9658 training step(s), loss on training batch is 0.00101166.
After 9659 training step(s), loss on training batch is 0.00101035.
After 9660 training step(s), loss on training batch is 0.00090246.
After 9661 training step(s), loss on training batch is 0.00091488.
After 9662 training step(s), loss on training batch is 0.00108341.
After 9663 training step(s), loss on training batch is 0.000992609.
After 9664 training step(s), loss on training batch is 0.000893596.
After 9665 training step(s), loss on training batch is 0.00118198.
After 9666 training step(s), loss on training batch is 0.000883318.
After 9667 training step(s), loss on training batch is 0.000879382.
After 9668 training step(s), loss on training batch is 0.00101825.
After 9669 training step(s), loss on training batch is 0.00160129.
After 9670 training step(s), loss on training batch is 0.00293891.
After 9671 training step(s), loss on training batch is 0.00147431.
After 9672 training step(s), loss on training batch is 0.00104651.
After 9673 training step(s), loss on training batch is 0.00123131.
After 9674 training step(s), loss on training batch is 0.00105829.
After 9675 training step(s), loss on training batch is 0.00101671.
After 9676 training step(s), loss on training batch is 0.00091974.
After 9677 training step(s), loss on training batch is 0.00149276.
After 9678 training step(s), loss on training batch is 0.000951484.
After 9679 training step(s), loss on training batch is 0.000907859.
After 9680 training step(s), loss on training batch is 0.000904089.
After 9681 training step(s), loss on training batch is 0.000582321.
After 9682 training step(s), loss on training batch is 0.000540571.
After 9683 training step(s), loss on training batch is 0.000304857.
After 9684 training step(s), loss on training batch is 0.000389855.
After 9685 training step(s), loss on training batch is 0.000427341.
After 9686 training step(s), loss on training batch is 0.000649969.
After 9687 training step(s), loss on training batch is 0.000804613.
After 9688 training step(s), loss on training batch is 0.000456993.
After 9689 training step(s), loss on training batch is 0.000376672.
After 9690 training step(s), loss on training batch is 0.00034841.
After 9691 training step(s), loss on training batch is 0.000303259.
After 9692 training step(s), loss on training batch is 0.000408033.
After 9693 training step(s), loss on training batch is 0.000331443.
After 9694 training step(s), loss on training batch is 0.000335241.
After 9695 training step(s), loss on training batch is 0.000395876.
After 9696 training step(s), loss on training batch is 0.000400349.
After 9697 training step(s), loss on training batch is 0.000308776.
After 9698 training step(s), loss on training batch is 0.000300415.
After 9699 training step(s), loss on training batch is 0.000310669.
After 9700 training step(s), loss on training batch is 0.000348896.
After 9701 training step(s), loss on training batch is 0.00109429.
After 9702 training step(s), loss on training batch is 0.000574393.
After 9703 training step(s), loss on training batch is 0.000544116.
After 9704 training step(s), loss on training batch is 0.000654901.
After 9705 training step(s), loss on training batch is 0.000609791.
After 9706 training step(s), loss on training batch is 0.000664952.
After 9707 training step(s), loss on training batch is 0.00069124.
After 9708 training step(s), loss on training batch is 0.000350852.
After 9709 training step(s), loss on training batch is 0.000385601.
After 9710 training step(s), loss on training batch is 0.000429532.
After 9711 training step(s), loss on training batch is 0.000511281.
After 9712 training step(s), loss on training batch is 0.000470784.
After 9713 training step(s), loss on training batch is 0.000533976.
After 9714 training step(s), loss on training batch is 0.000754349.
After 9715 training step(s), loss on training batch is 0.0015263.
After 9716 training step(s), loss on training batch is 0.00105151.
After 9717 training step(s), loss on training batch is 0.000918514.
After 9718 training step(s), loss on training batch is 0.000699226.
After 9719 training step(s), loss on training batch is 0.000412179.
After 9720 training step(s), loss on training batch is 0.000414692.
After 9721 training step(s), loss on training batch is 0.000426231.
After 9722 training step(s), loss on training batch is 0.000366726.
After 9723 training step(s), loss on training batch is 0.000578856.
After 9724 training step(s), loss on training batch is 0.000448919.
After 9725 training step(s), loss on training batch is 0.000430082.
After 9726 training step(s), loss on training batch is 0.000409288.
After 9727 training step(s), loss on training batch is 0.000913354.
After 9728 training step(s), loss on training batch is 0.00246107.
After 9729 training step(s), loss on training batch is 0.000796081.
After 9730 training step(s), loss on training batch is 0.000579518.
After 9731 training step(s), loss on training batch is 0.000574064.
After 9732 training step(s), loss on training batch is 0.000538151.
After 9733 training step(s), loss on training batch is 0.000458748.
After 9734 training step(s), loss on training batch is 0.000377846.
After 9735 training step(s), loss on training batch is 0.000341435.
After 9736 training step(s), loss on training batch is 0.000343767.
After 9737 training step(s), loss on training batch is 0.000395575.
After 9738 training step(s), loss on training batch is 0.000392511.
After 9739 training step(s), loss on training batch is 0.00043251.
After 9740 training step(s), loss on training batch is 0.000489473.
After 9741 training step(s), loss on training batch is 0.00037786.
After 9742 training step(s), loss on training batch is 0.00046725.
After 9743 training step(s), loss on training batch is 0.000447184.
After 9744 training step(s), loss on training batch is 0.000530619.
After 9745 training step(s), loss on training batch is 0.000346981.
After 9746 training step(s), loss on training batch is 0.000486495.
After 9747 training step(s), loss on training batch is 0.000368977.
After 9748 training step(s), loss on training batch is 0.000336165.
After 9749 training step(s), loss on training batch is 0.000380086.
After 9750 training step(s), loss on training batch is 0.000485873.
After 9751 training step(s), loss on training batch is 0.000449275.
After 9752 training step(s), loss on training batch is 0.000406583.
After 9753 training step(s), loss on training batch is 0.000404567.
After 9754 training step(s), loss on training batch is 0.000414846.
After 9755 training step(s), loss on training batch is 0.000380383.
After 9756 training step(s), loss on training batch is 0.00046734.
After 9757 training step(s), loss on training batch is 0.000562777.
After 9758 training step(s), loss on training batch is 0.00035877.
After 9759 training step(s), loss on training batch is 0.000404218.
After 9760 training step(s), loss on training batch is 0.000515051.
After 9761 training step(s), loss on training batch is 0.00035651.
After 9762 training step(s), loss on training batch is 0.000322952.
After 9763 training step(s), loss on training batch is 0.00034891.
After 9764 training step(s), loss on training batch is 0.000469529.
After 9765 training step(s), loss on training batch is 0.000418535.
After 9766 training step(s), loss on training batch is 0.000412842.
After 9767 training step(s), loss on training batch is 0.000418484.
After 9768 training step(s), loss on training batch is 0.000388082.
After 9769 training step(s), loss on training batch is 0.000430204.
After 9770 training step(s), loss on training batch is 0.000366619.
After 9771 training step(s), loss on training batch is 0.00040233.
After 9772 training step(s), loss on training batch is 0.000359563.
After 9773 training step(s), loss on training batch is 0.000354544.
After 9774 training step(s), loss on training batch is 0.000334314.
After 9775 training step(s), loss on training batch is 0.000398779.
After 9776 training step(s), loss on training batch is 0.000383029.
After 9777 training step(s), loss on training batch is 0.00052978.
After 9778 training step(s), loss on training batch is 0.000405729.
After 9779 training step(s), loss on training batch is 0.00037724.
After 9780 training step(s), loss on training batch is 0.00039434.
After 9781 training step(s), loss on training batch is 0.000361718.
After 9782 training step(s), loss on training batch is 0.000388115.
After 9783 training step(s), loss on training batch is 0.000734772.
After 9784 training step(s), loss on training batch is 0.000895199.
After 9785 training step(s), loss on training batch is 0.00108243.
After 9786 training step(s), loss on training batch is 0.000692591.
After 9787 training step(s), loss on training batch is 0.000719137.
After 9788 training step(s), loss on training batch is 0.000613655.
After 9789 training step(s), loss on training batch is 0.000663978.
After 9790 training step(s), loss on training batch is 0.000630156.
After 9791 training step(s), loss on training batch is 0.000725103.
After 9792 training step(s), loss on training batch is 0.000793033.
After 9793 training step(s), loss on training batch is 0.000605126.
After 9794 training step(s), loss on training batch is 0.00060854.
After 9795 training step(s), loss on training batch is 0.000845761.
After 9796 training step(s), loss on training batch is 0.000756483.
After 9797 training step(s), loss on training batch is 0.000665133.
After 9798 training step(s), loss on training batch is 0.00068504.
After 9799 training step(s), loss on training batch is 0.000976027.
After 9800 training step(s), loss on training batch is 0.000692503.
After 9801 training step(s), loss on training batch is 0.000640165.
After 9802 training step(s), loss on training batch is 0.000744458.
After 9803 training step(s), loss on training batch is 0.000644868.
After 9804 training step(s), loss on training batch is 0.000628829.
After 9805 training step(s), loss on training batch is 0.0005435.
After 9806 training step(s), loss on training batch is 0.000701294.
After 9807 training step(s), loss on training batch is 0.000903849.
After 9808 training step(s), loss on training batch is 0.00084867.
After 9809 training step(s), loss on training batch is 0.000564513.
After 9810 training step(s), loss on training batch is 0.000983998.
After 9811 training step(s), loss on training batch is 0.00066907.
After 9812 training step(s), loss on training batch is 0.000676426.
After 9813 training step(s), loss on training batch is 0.000551849.
After 9814 training step(s), loss on training batch is 0.00053957.
After 9815 training step(s), loss on training batch is 0.000516863.
After 9816 training step(s), loss on training batch is 0.000528323.
After 9817 training step(s), loss on training batch is 0.000547768.
After 9818 training step(s), loss on training batch is 0.000773005.
After 9819 training step(s), loss on training batch is 0.00105041.
After 9820 training step(s), loss on training batch is 0.00104779.
After 9821 training step(s), loss on training batch is 0.00136999.
After 9822 training step(s), loss on training batch is 0.000698229.
After 9823 training step(s), loss on training batch is 0.000664945.
After 9824 training step(s), loss on training batch is 0.000660577.
After 9825 training step(s), loss on training batch is 0.000706347.
After 9826 training step(s), loss on training batch is 0.000690945.
After 9827 training step(s), loss on training batch is 0.000604562.
After 9828 training step(s), loss on training batch is 0.000668964.
After 9829 training step(s), loss on training batch is 0.000665133.
After 9830 training step(s), loss on training batch is 0.000791349.
After 9831 training step(s), loss on training batch is 0.000939594.
After 9832 training step(s), loss on training batch is 0.000576432.
After 9833 training step(s), loss on training batch is 0.000601271.
After 9834 training step(s), loss on training batch is 0.000562817.
After 9835 training step(s), loss on training batch is 0.000502653.
After 9836 training step(s), loss on training batch is 0.000803057.
After 9837 training step(s), loss on training batch is 0.00129398.
After 9838 training step(s), loss on training batch is 0.000530515.
After 9839 training step(s), loss on training batch is 0.000616757.
After 9840 training step(s), loss on training batch is 0.000675097.
After 9841 training step(s), loss on training batch is 0.000608462.
After 9842 training step(s), loss on training batch is 0.00058859.
After 9843 training step(s), loss on training batch is 0.000675678.
After 9844 training step(s), loss on training batch is 0.000614323.
After 9845 training step(s), loss on training batch is 0.000660267.
After 9846 training step(s), loss on training batch is 0.000818813.
After 9847 training step(s), loss on training batch is 0.000624075.
After 9848 training step(s), loss on training batch is 0.000642058.
After 9849 training step(s), loss on training batch is 0.000689606.
After 9850 training step(s), loss on training batch is 0.000719223.
After 9851 training step(s), loss on training batch is 0.000625471.
After 9852 training step(s), loss on training batch is 0.000530537.
After 9853 training step(s), loss on training batch is 0.000797527.
After 9854 training step(s), loss on training batch is 0.00062863.
After 9855 training step(s), loss on training batch is 0.000683819.
After 9856 training step(s), loss on training batch is 0.000611739.
After 9857 training step(s), loss on training batch is 0.000559677.
After 9858 training step(s), loss on training batch is 0.000741322.
After 9859 training step(s), loss on training batch is 0.000920534.
After 9860 training step(s), loss on training batch is 0.000604997.
After 9861 training step(s), loss on training batch is 0.000550736.
After 9862 training step(s), loss on training batch is 0.000540681.
After 9863 training step(s), loss on training batch is 0.000563584.
After 9864 training step(s), loss on training batch is 0.00051038.
After 9865 training step(s), loss on training batch is 0.000714823.
After 9866 training step(s), loss on training batch is 0.00111346.
After 9867 training step(s), loss on training batch is 0.00118385.
After 9868 training step(s), loss on training batch is 0.00107076.
After 9869 training step(s), loss on training batch is 0.00106495.
After 9870 training step(s), loss on training batch is 0.00102784.
After 9871 training step(s), loss on training batch is 0.000962063.
After 9872 training step(s), loss on training batch is 0.00103392.
After 9873 training step(s), loss on training batch is 0.00179093.
After 9874 training step(s), loss on training batch is 0.00122971.
After 9875 training step(s), loss on training batch is 0.00133902.
After 9876 training step(s), loss on training batch is 0.00104414.
After 9877 training step(s), loss on training batch is 0.000988602.
After 9878 training step(s), loss on training batch is 0.00109223.
After 9879 training step(s), loss on training batch is 0.000940444.
After 9880 training step(s), loss on training batch is 0.000925747.
After 9881 training step(s), loss on training batch is 0.000918113.
After 9882 training step(s), loss on training batch is 0.000929883.
After 9883 training step(s), loss on training batch is 0.00102638.
After 9884 training step(s), loss on training batch is 0.000925977.
After 9885 training step(s), loss on training batch is 0.000920916.
After 9886 training step(s), loss on training batch is 0.00121584.
After 9887 training step(s), loss on training batch is 0.000932438.
After 9888 training step(s), loss on training batch is 0.00115316.
After 9889 training step(s), loss on training batch is 0.000944391.
After 9890 training step(s), loss on training batch is 0.000932426.
After 9891 training step(s), loss on training batch is 0.000915193.
After 9892 training step(s), loss on training batch is 0.000987881.
After 9893 training step(s), loss on training batch is 0.000886807.
After 9894 training step(s), loss on training batch is 0.00102463.
After 9895 training step(s), loss on training batch is 0.00112034.
After 9896 training step(s), loss on training batch is 0.00114732.
After 9897 training step(s), loss on training batch is 0.00097292.
After 9898 training step(s), loss on training batch is 0.000953039.
After 9899 training step(s), loss on training batch is 0.000908093.
After 9900 training step(s), loss on training batch is 0.000943732.
After 9901 training step(s), loss on training batch is 0.000819943.
After 9902 training step(s), loss on training batch is 0.00103039.
After 9903 training step(s), loss on training batch is 0.000902471.
After 9904 training step(s), loss on training batch is 0.00114465.
After 9905 training step(s), loss on training batch is 0.00117701.
After 9906 training step(s), loss on training batch is 0.00114286.
After 9907 training step(s), loss on training batch is 0.00128006.
After 9908 training step(s), loss on training batch is 0.00112771.
After 9909 training step(s), loss on training batch is 0.00677689.
After 9910 training step(s), loss on training batch is 0.00176493.
After 9911 training step(s), loss on training batch is 0.00155725.
After 9912 training step(s), loss on training batch is 0.00151675.
After 9913 training step(s), loss on training batch is 0.00138771.
After 9914 training step(s), loss on training batch is 0.00142732.
After 9915 training step(s), loss on training batch is 0.00119222.
After 9916 training step(s), loss on training batch is 0.000547296.
After 9917 training step(s), loss on training batch is 0.000486759.
After 9918 training step(s), loss on training batch is 0.000464616.
After 9919 training step(s), loss on training batch is 0.000496954.
After 9920 training step(s), loss on training batch is 0.000531389.
After 9921 training step(s), loss on training batch is 0.000521236.
After 9922 training step(s), loss on training batch is 0.000533593.
After 9923 training step(s), loss on training batch is 0.000533325.
After 9924 training step(s), loss on training batch is 0.00052186.
After 9925 training step(s), loss on training batch is 0.000476404.
After 9926 training step(s), loss on training batch is 0.000920147.
After 9927 training step(s), loss on training batch is 0.000935814.
After 9928 training step(s), loss on training batch is 0.00104388.
After 9929 training step(s), loss on training batch is 0.000908666.
After 9930 training step(s), loss on training batch is 0.00194121.
After 9931 training step(s), loss on training batch is 0.00164107.
After 9932 training step(s), loss on training batch is 0.00117785.
After 9933 training step(s), loss on training batch is 0.00127545.
After 9934 training step(s), loss on training batch is 0.000941533.
After 9935 training step(s), loss on training batch is 0.00093396.
After 9936 training step(s), loss on training batch is 0.000964509.
After 9937 training step(s), loss on training batch is 0.000965462.
After 9938 training step(s), loss on training batch is 0.00109082.
After 9939 training step(s), loss on training batch is 0.000983127.
After 9940 training step(s), loss on training batch is 0.000957691.
After 9941 training step(s), loss on training batch is 0.000934532.
After 9942 training step(s), loss on training batch is 0.00162046.
After 9943 training step(s), loss on training batch is 0.000367911.
After 9944 training step(s), loss on training batch is 0.000436764.
After 9945 training step(s), loss on training batch is 0.000395766.
After 9946 training step(s), loss on training batch is 0.000501743.
After 9947 training step(s), loss on training batch is 0.000473798.
After 9948 training step(s), loss on training batch is 0.000416562.
After 9949 training step(s), loss on training batch is 0.000357207.
After 9950 training step(s), loss on training batch is 0.000338978.
After 9951 training step(s), loss on training batch is 0.000558141.
After 9952 training step(s), loss on training batch is 0.000428881.
After 9953 training step(s), loss on training batch is 0.000470509.
After 9954 training step(s), loss on training batch is 0.000373007.
After 9955 training step(s), loss on training batch is 0.000607567.
After 9956 training step(s), loss on training batch is 0.000390995.
After 9957 training step(s), loss on training batch is 0.000348876.
After 9958 training step(s), loss on training batch is 0.000361002.
After 9959 training step(s), loss on training batch is 0.000859534.
After 9960 training step(s), loss on training batch is 0.00109488.
After 9961 training step(s), loss on training batch is 0.000580587.
After 9962 training step(s), loss on training batch is 0.000432006.
After 9963 training step(s), loss on training batch is 0.000408355.
After 9964 training step(s), loss on training batch is 0.000493364.
After 9965 training step(s), loss on training batch is 0.000507203.
After 9966 training step(s), loss on training batch is 0.000445246.
After 9967 training step(s), loss on training batch is 0.000396529.
After 9968 training step(s), loss on training batch is 0.00051172.
After 9969 training step(s), loss on training batch is 0.000329399.
After 9970 training step(s), loss on training batch is 0.000405333.
After 9971 training step(s), loss on training batch is 0.000277387.
After 9972 training step(s), loss on training batch is 0.000329412.
After 9973 training step(s), loss on training batch is 0.000332571.
After 9974 training step(s), loss on training batch is 0.000325471.
After 9975 training step(s), loss on training batch is 0.000512461.
After 9976 training step(s), loss on training batch is 0.000509287.
After 9977 training step(s), loss on training batch is 0.000447889.
After 9978 training step(s), loss on training batch is 0.000338094.
After 9979 training step(s), loss on training batch is 0.000371412.
After 9980 training step(s), loss on training batch is 0.000322662.
After 9981 training step(s), loss on training batch is 0.000368328.
After 9982 training step(s), loss on training batch is 0.000341823.
After 9983 training step(s), loss on training batch is 0.000420452.
After 9984 training step(s), loss on training batch is 0.000417074.
After 9985 training step(s), loss on training batch is 0.000421439.
After 9986 training step(s), loss on training batch is 0.000565883.
After 9987 training step(s), loss on training batch is 0.000454032.
After 9988 training step(s), loss on training batch is 0.000338803.
After 9989 training step(s), loss on training batch is 0.000391639.
After 9990 training step(s), loss on training batch is 0.000361961.
After 9991 training step(s), loss on training batch is 0.000326992.
After 9992 training step(s), loss on training batch is 0.00035301.
After 9993 training step(s), loss on training batch is 0.000339625.
After 9994 training step(s), loss on training batch is 0.000285592.
After 9995 training step(s), loss on training batch is 0.00048578.
After 9996 training step(s), loss on training batch is 0.000460398.
After 9997 training step(s), loss on training batch is 0.000263648.
After 9998 training step(s), loss on training batch is 0.000272782.
After 9999 training step(s), loss on training batch is 0.000356005.
After 10000 training step(s), loss on training batch is 0.000385982.
After 10001 training step(s), loss on training batch is 0.000568032.
After 10002 training step(s), loss on training batch is 0.000642135.
After 10003 training step(s), loss on training batch is 0.000397352.
After 10004 training step(s), loss on training batch is 0.000576112.
After 10005 training step(s), loss on training batch is 0.00108948.
After 10006 training step(s), loss on training batch is 0.000753598.
After 10007 training step(s), loss on training batch is 0.00042797.
After 10008 training step(s), loss on training batch is 0.000512823.
After 10009 training step(s), loss on training batch is 0.000640498.
After 10010 training step(s), loss on training batch is 0.000656163.
After 10011 training step(s), loss on training batch is 0.00042189.
After 10012 training step(s), loss on training batch is 0.000335908.
After 10013 training step(s), loss on training batch is 0.000392229.
After 10014 training step(s), loss on training batch is 0.000416573.
After 10015 training step(s), loss on training batch is 0.000385689.
After 10016 training step(s), loss on training batch is 0.000327477.
After 10017 training step(s), loss on training batch is 0.000428446.
After 10018 training step(s), loss on training batch is 0.000353902.
After 10019 training step(s), loss on training batch is 0.00045242.
After 10020 training step(s), loss on training batch is 0.000407315.
After 10021 training step(s), loss on training batch is 0.00031613.
After 10022 training step(s), loss on training batch is 0.000403021.
After 10023 training step(s), loss on training batch is 0.000361454.
After 10024 training step(s), loss on training batch is 0.000325393.
After 10025 training step(s), loss on training batch is 0.000318366.
After 10026 training step(s), loss on training batch is 0.000389652.
After 10027 training step(s), loss on training batch is 0.000315633.
After 10028 training step(s), loss on training batch is 0.00099862.
After 10029 training step(s), loss on training batch is 0.00064614.
After 10030 training step(s), loss on training batch is 0.000575565.
After 10031 training step(s), loss on training batch is 0.000583333.
After 10032 training step(s), loss on training batch is 0.000702429.
After 10033 training step(s), loss on training batch is 0.000772803.
After 10034 training step(s), loss on training batch is 0.000644296.
After 10035 training step(s), loss on training batch is 0.000508387.
After 10036 training step(s), loss on training batch is 0.000747615.
After 10037 training step(s), loss on training batch is 0.000722031.
After 10038 training step(s), loss on training batch is 0.000522554.
After 10039 training step(s), loss on training batch is 0.000519262.
After 10040 training step(s), loss on training batch is 0.00115452.
After 10041 training step(s), loss on training batch is 0.000559334.
After 10042 training step(s), loss on training batch is 0.000624232.
After 10043 training step(s), loss on training batch is 0.000632062.
After 10044 training step(s), loss on training batch is 0.000736646.
After 10045 training step(s), loss on training batch is 0.000526447.
After 10046 training step(s), loss on training batch is 0.0007382.
After 10047 training step(s), loss on training batch is 0.000575333.
After 10048 training step(s), loss on training batch is 0.000639246.
After 10049 training step(s), loss on training batch is 0.000671645.
After 10050 training step(s), loss on training batch is 0.00066649.
After 10051 training step(s), loss on training batch is 0.000663875.
After 10052 training step(s), loss on training batch is 0.000581701.
After 10053 training step(s), loss on training batch is 0.000679679.
After 10054 training step(s), loss on training batch is 0.000468569.
After 10055 training step(s), loss on training batch is 0.000646492.
After 10056 training step(s), loss on training batch is 0.00112772.
After 10057 training step(s), loss on training batch is 0.000947235.
After 10058 training step(s), loss on training batch is 0.00100295.
After 10059 training step(s), loss on training batch is 0.000991275.
After 10060 training step(s), loss on training batch is 0.000886935.
After 10061 training step(s), loss on training batch is 0.000877114.
After 10062 training step(s), loss on training batch is 0.00109766.
After 10063 training step(s), loss on training batch is 0.000972886.
After 10064 training step(s), loss on training batch is 0.000873185.
After 10065 training step(s), loss on training batch is 0.00118613.
After 10066 training step(s), loss on training batch is 0.000869917.
After 10067 training step(s), loss on training batch is 0.000877504.
After 10068 training step(s), loss on training batch is 0.00101589.
After 10069 training step(s), loss on training batch is 0.00157309.
After 10070 training step(s), loss on training batch is 0.00307818.
After 10071 training step(s), loss on training batch is 0.00148674.
After 10072 training step(s), loss on training batch is 0.00110848.
After 10073 training step(s), loss on training batch is 0.00124246.
After 10074 training step(s), loss on training batch is 0.00109759.
After 10075 training step(s), loss on training batch is 0.00105127.
After 10076 training step(s), loss on training batch is 0.000953696.
After 10077 training step(s), loss on training batch is 0.00142134.
After 10078 training step(s), loss on training batch is 0.00094412.
After 10079 training step(s), loss on training batch is 0.000912407.
After 10080 training step(s), loss on training batch is 0.000890034.
After 10081 training step(s), loss on training batch is 0.000573479.
After 10082 training step(s), loss on training batch is 0.000516952.
After 10083 training step(s), loss on training batch is 0.000304959.
After 10084 training step(s), loss on training batch is 0.000381557.
After 10085 training step(s), loss on training batch is 0.000426862.
After 10086 training step(s), loss on training batch is 0.000574852.
After 10087 training step(s), loss on training batch is 0.000700824.
After 10088 training step(s), loss on training batch is 0.000453225.
After 10089 training step(s), loss on training batch is 0.000386768.
After 10090 training step(s), loss on training batch is 0.000358433.
After 10091 training step(s), loss on training batch is 0.000301433.
After 10092 training step(s), loss on training batch is 0.000382312.
After 10093 training step(s), loss on training batch is 0.000328698.
After 10094 training step(s), loss on training batch is 0.000342028.
After 10095 training step(s), loss on training batch is 0.000399836.
After 10096 training step(s), loss on training batch is 0.00039467.
After 10097 training step(s), loss on training batch is 0.000311614.
After 10098 training step(s), loss on training batch is 0.000299021.
After 10099 training step(s), loss on training batch is 0.00030606.
After 10100 training step(s), loss on training batch is 0.000320336.
After 10101 training step(s), loss on training batch is 0.0010455.
After 10102 training step(s), loss on training batch is 0.000538452.
After 10103 training step(s), loss on training batch is 0.000567868.
After 10104 training step(s), loss on training batch is 0.000678382.
After 10105 training step(s), loss on training batch is 0.000596727.
After 10106 training step(s), loss on training batch is 0.000641389.
After 10107 training step(s), loss on training batch is 0.000650834.
After 10108 training step(s), loss on training batch is 0.00034501.
After 10109 training step(s), loss on training batch is 0.000384222.
After 10110 training step(s), loss on training batch is 0.000416192.
After 10111 training step(s), loss on training batch is 0.000493152.
After 10112 training step(s), loss on training batch is 0.000464836.
After 10113 training step(s), loss on training batch is 0.000512328.
After 10114 training step(s), loss on training batch is 0.000722372.
After 10115 training step(s), loss on training batch is 0.00150093.
After 10116 training step(s), loss on training batch is 0.00103516.
After 10117 training step(s), loss on training batch is 0.000877418.
After 10118 training step(s), loss on training batch is 0.000664264.
After 10119 training step(s), loss on training batch is 0.000402655.
After 10120 training step(s), loss on training batch is 0.000404874.
After 10121 training step(s), loss on training batch is 0.000413852.
After 10122 training step(s), loss on training batch is 0.000358091.
After 10123 training step(s), loss on training batch is 0.000565274.
After 10124 training step(s), loss on training batch is 0.000439398.
After 10125 training step(s), loss on training batch is 0.000423098.
After 10126 training step(s), loss on training batch is 0.000412559.
After 10127 training step(s), loss on training batch is 0.000847677.
After 10128 training step(s), loss on training batch is 0.00241044.
After 10129 training step(s), loss on training batch is 0.000740901.
After 10130 training step(s), loss on training batch is 0.000548663.
After 10131 training step(s), loss on training batch is 0.0005751.
After 10132 training step(s), loss on training batch is 0.000526618.
After 10133 training step(s), loss on training batch is 0.000448917.
After 10134 training step(s), loss on training batch is 0.000364438.
After 10135 training step(s), loss on training batch is 0.000330317.
After 10136 training step(s), loss on training batch is 0.000328796.
After 10137 training step(s), loss on training batch is 0.000372472.
After 10138 training step(s), loss on training batch is 0.00036981.
After 10139 training step(s), loss on training batch is 0.000409761.
After 10140 training step(s), loss on training batch is 0.000461822.
After 10141 training step(s), loss on training batch is 0.000360825.
After 10142 training step(s), loss on training batch is 0.000438776.
After 10143 training step(s), loss on training batch is 0.000420969.
After 10144 training step(s), loss on training batch is 0.000507619.
After 10145 training step(s), loss on training batch is 0.00034035.
After 10146 training step(s), loss on training batch is 0.000473502.
After 10147 training step(s), loss on training batch is 0.000360448.
After 10148 training step(s), loss on training batch is 0.000334814.
After 10149 training step(s), loss on training batch is 0.000374193.
After 10150 training step(s), loss on training batch is 0.000488631.
After 10151 training step(s), loss on training batch is 0.000450612.
After 10152 training step(s), loss on training batch is 0.000398284.
After 10153 training step(s), loss on training batch is 0.000399274.
After 10154 training step(s), loss on training batch is 0.000370465.
After 10155 training step(s), loss on training batch is 0.000362661.
After 10156 training step(s), loss on training batch is 0.000469479.
After 10157 training step(s), loss on training batch is 0.000555192.
After 10158 training step(s), loss on training batch is 0.00035552.
After 10159 training step(s), loss on training batch is 0.000396659.
After 10160 training step(s), loss on training batch is 0.00050797.
After 10161 training step(s), loss on training batch is 0.000351945.
After 10162 training step(s), loss on training batch is 0.000322286.
After 10163 training step(s), loss on training batch is 0.000343266.
After 10164 training step(s), loss on training batch is 0.000461484.
After 10165 training step(s), loss on training batch is 0.000408917.
After 10166 training step(s), loss on training batch is 0.000394113.
After 10167 training step(s), loss on training batch is 0.00041071.
After 10168 training step(s), loss on training batch is 0.000386689.
After 10169 training step(s), loss on training batch is 0.000426123.
After 10170 training step(s), loss on training batch is 0.000358958.
After 10171 training step(s), loss on training batch is 0.000384521.
After 10172 training step(s), loss on training batch is 0.000351415.
After 10173 training step(s), loss on training batch is 0.000331618.
After 10174 training step(s), loss on training batch is 0.000323723.
After 10175 training step(s), loss on training batch is 0.000391051.
After 10176 training step(s), loss on training batch is 0.000377922.
After 10177 training step(s), loss on training batch is 0.000509422.
After 10178 training step(s), loss on training batch is 0.000402586.
After 10179 training step(s), loss on training batch is 0.000373871.
After 10180 training step(s), loss on training batch is 0.000383087.
After 10181 training step(s), loss on training batch is 0.000360202.
After 10182 training step(s), loss on training batch is 0.00037902.
After 10183 training step(s), loss on training batch is 0.000696948.
After 10184 training step(s), loss on training batch is 0.000868959.
After 10185 training step(s), loss on training batch is 0.00103789.
After 10186 training step(s), loss on training batch is 0.000678044.
After 10187 training step(s), loss on training batch is 0.000687789.
After 10188 training step(s), loss on training batch is 0.000609881.
After 10189 training step(s), loss on training batch is 0.000670582.
After 10190 training step(s), loss on training batch is 0.000628393.
After 10191 training step(s), loss on training batch is 0.00072043.
After 10192 training step(s), loss on training batch is 0.000767601.
After 10193 training step(s), loss on training batch is 0.000596959.
After 10194 training step(s), loss on training batch is 0.00058472.
After 10195 training step(s), loss on training batch is 0.000835657.
After 10196 training step(s), loss on training batch is 0.000742054.
After 10197 training step(s), loss on training batch is 0.000640867.
After 10198 training step(s), loss on training batch is 0.000675296.
After 10199 training step(s), loss on training batch is 0.000941221.
After 10200 training step(s), loss on training batch is 0.00068591.
After 10201 training step(s), loss on training batch is 0.000627586.
After 10202 training step(s), loss on training batch is 0.000726193.
After 10203 training step(s), loss on training batch is 0.000630958.
After 10204 training step(s), loss on training batch is 0.000617087.
After 10205 training step(s), loss on training batch is 0.000533965.
After 10206 training step(s), loss on training batch is 0.000688067.
After 10207 training step(s), loss on training batch is 0.000879906.
After 10208 training step(s), loss on training batch is 0.000734667.
After 10209 training step(s), loss on training batch is 0.000559822.
After 10210 training step(s), loss on training batch is 0.000916484.
After 10211 training step(s), loss on training batch is 0.00067946.
After 10212 training step(s), loss on training batch is 0.00065439.
After 10213 training step(s), loss on training batch is 0.000553125.
After 10214 training step(s), loss on training batch is 0.000532564.
After 10215 training step(s), loss on training batch is 0.000519955.
After 10216 training step(s), loss on training batch is 0.000513953.
After 10217 training step(s), loss on training batch is 0.000538446.
After 10218 training step(s), loss on training batch is 0.000775744.
After 10219 training step(s), loss on training batch is 0.00104931.
After 10220 training step(s), loss on training batch is 0.00101121.
After 10221 training step(s), loss on training batch is 0.00139319.
After 10222 training step(s), loss on training batch is 0.000672.
After 10223 training step(s), loss on training batch is 0.000665007.
After 10224 training step(s), loss on training batch is 0.000676943.
After 10225 training step(s), loss on training batch is 0.000728741.
After 10226 training step(s), loss on training batch is 0.000741403.
After 10227 training step(s), loss on training batch is 0.000641989.
After 10228 training step(s), loss on training batch is 0.000674593.
After 10229 training step(s), loss on training batch is 0.00068857.
After 10230 training step(s), loss on training batch is 0.000763926.
After 10231 training step(s), loss on training batch is 0.00088119.
After 10232 training step(s), loss on training batch is 0.000581546.
After 10233 training step(s), loss on training batch is 0.000606518.
After 10234 training step(s), loss on training batch is 0.000538428.
After 10235 training step(s), loss on training batch is 0.000485327.
After 10236 training step(s), loss on training batch is 0.000798969.
After 10237 training step(s), loss on training batch is 0.0013124.
After 10238 training step(s), loss on training batch is 0.000520266.
After 10239 training step(s), loss on training batch is 0.000603196.
After 10240 training step(s), loss on training batch is 0.000661841.
After 10241 training step(s), loss on training batch is 0.000587963.
After 10242 training step(s), loss on training batch is 0.000573562.
After 10243 training step(s), loss on training batch is 0.000655413.
After 10244 training step(s), loss on training batch is 0.00060107.
After 10245 training step(s), loss on training batch is 0.000654561.
After 10246 training step(s), loss on training batch is 0.000787956.
After 10247 training step(s), loss on training batch is 0.000615301.
After 10248 training step(s), loss on training batch is 0.00062048.
After 10249 training step(s), loss on training batch is 0.000675098.
After 10250 training step(s), loss on training batch is 0.000701296.
After 10251 training step(s), loss on training batch is 0.000593817.
After 10252 training step(s), loss on training batch is 0.000521328.
After 10253 training step(s), loss on training batch is 0.000770983.
After 10254 training step(s), loss on training batch is 0.00061448.
After 10255 training step(s), loss on training batch is 0.000669939.
After 10256 training step(s), loss on training batch is 0.000602756.
After 10257 training step(s), loss on training batch is 0.000543938.
After 10258 training step(s), loss on training batch is 0.000733918.
After 10259 training step(s), loss on training batch is 0.00092381.
After 10260 training step(s), loss on training batch is 0.000596452.
After 10261 training step(s), loss on training batch is 0.000535701.
After 10262 training step(s), loss on training batch is 0.000535217.
After 10263 training step(s), loss on training batch is 0.000558518.
After 10264 training step(s), loss on training batch is 0.000508816.
After 10265 training step(s), loss on training batch is 0.000691475.
After 10266 training step(s), loss on training batch is 0.00110372.
After 10267 training step(s), loss on training batch is 0.00116536.
After 10268 training step(s), loss on training batch is 0.00103357.
After 10269 training step(s), loss on training batch is 0.001064.
After 10270 training step(s), loss on training batch is 0.00101685.
After 10271 training step(s), loss on training batch is 0.000965731.
After 10272 training step(s), loss on training batch is 0.00100767.
After 10273 training step(s), loss on training batch is 0.00147607.
After 10274 training step(s), loss on training batch is 0.0012039.
After 10275 training step(s), loss on training batch is 0.00131946.
After 10276 training step(s), loss on training batch is 0.000987604.
After 10277 training step(s), loss on training batch is 0.000950875.
After 10278 training step(s), loss on training batch is 0.00108708.
After 10279 training step(s), loss on training batch is 0.000887715.
After 10280 training step(s), loss on training batch is 0.000908512.
After 10281 training step(s), loss on training batch is 0.000861051.
After 10282 training step(s), loss on training batch is 0.00090846.
After 10283 training step(s), loss on training batch is 0.00101953.
After 10284 training step(s), loss on training batch is 0.000911856.
After 10285 training step(s), loss on training batch is 0.000905094.
After 10286 training step(s), loss on training batch is 0.00131619.
After 10287 training step(s), loss on training batch is 0.000893017.
After 10288 training step(s), loss on training batch is 0.00113506.
After 10289 training step(s), loss on training batch is 0.000930234.
After 10290 training step(s), loss on training batch is 0.000926943.
After 10291 training step(s), loss on training batch is 0.00091288.
After 10292 training step(s), loss on training batch is 0.000953325.
After 10293 training step(s), loss on training batch is 0.000874594.
After 10294 training step(s), loss on training batch is 0.00103485.
After 10295 training step(s), loss on training batch is 0.0011471.
After 10296 training step(s), loss on training batch is 0.00113638.
After 10297 training step(s), loss on training batch is 0.000946567.
After 10298 training step(s), loss on training batch is 0.000932983.
After 10299 training step(s), loss on training batch is 0.000887083.
After 10300 training step(s), loss on training batch is 0.000937461.
After 10301 training step(s), loss on training batch is 0.000820785.
After 10302 training step(s), loss on training batch is 0.00103595.
After 10303 training step(s), loss on training batch is 0.000877307.
After 10304 training step(s), loss on training batch is 0.00113492.
After 10305 training step(s), loss on training batch is 0.00117456.
After 10306 training step(s), loss on training batch is 0.00111811.
After 10307 training step(s), loss on training batch is 0.00130293.
After 10308 training step(s), loss on training batch is 0.00111951.
After 10309 training step(s), loss on training batch is 0.00725793.
After 10310 training step(s), loss on training batch is 0.00321227.
After 10311 training step(s), loss on training batch is 0.00246946.
After 10312 training step(s), loss on training batch is 0.00212669.
After 10313 training step(s), loss on training batch is 0.00183813.
After 10314 training step(s), loss on training batch is 0.00161147.
After 10315 training step(s), loss on training batch is 0.00136953.
After 10316 training step(s), loss on training batch is 0.000656956.
After 10317 training step(s), loss on training batch is 0.000579903.
After 10318 training step(s), loss on training batch is 0.000529497.
After 10319 training step(s), loss on training batch is 0.000560837.
After 10320 training step(s), loss on training batch is 0.00059146.
After 10321 training step(s), loss on training batch is 0.000594019.
After 10322 training step(s), loss on training batch is 0.000617702.
After 10323 training step(s), loss on training batch is 0.000610477.
After 10324 training step(s), loss on training batch is 0.000564016.
After 10325 training step(s), loss on training batch is 0.000503919.
After 10326 training step(s), loss on training batch is 0.000990673.
After 10327 training step(s), loss on training batch is 0.000998757.
After 10328 training step(s), loss on training batch is 0.00108856.
After 10329 training step(s), loss on training batch is 0.00094685.
After 10330 training step(s), loss on training batch is 0.0018512.
After 10331 training step(s), loss on training batch is 0.00161674.
After 10332 training step(s), loss on training batch is 0.00121278.
After 10333 training step(s), loss on training batch is 0.00117638.
After 10334 training step(s), loss on training batch is 0.00102798.
After 10335 training step(s), loss on training batch is 0.000950373.
After 10336 training step(s), loss on training batch is 0.000978923.
After 10337 training step(s), loss on training batch is 0.00098526.
After 10338 training step(s), loss on training batch is 0.00107945.
After 10339 training step(s), loss on training batch is 0.000995799.
After 10340 training step(s), loss on training batch is 0.000971897.
After 10341 training step(s), loss on training batch is 0.000942034.
After 10342 training step(s), loss on training batch is 0.00155817.
After 10343 training step(s), loss on training batch is 0.000373804.
After 10344 training step(s), loss on training batch is 0.000435586.
After 10345 training step(s), loss on training batch is 0.000414206.
After 10346 training step(s), loss on training batch is 0.000504593.
After 10347 training step(s), loss on training batch is 0.00047906.
After 10348 training step(s), loss on training batch is 0.000444853.
After 10349 training step(s), loss on training batch is 0.000382622.
After 10350 training step(s), loss on training batch is 0.000346532.
After 10351 training step(s), loss on training batch is 0.000531928.
After 10352 training step(s), loss on training batch is 0.000462372.
After 10353 training step(s), loss on training batch is 0.000433353.
After 10354 training step(s), loss on training batch is 0.000385443.
After 10355 training step(s), loss on training batch is 0.00059244.
After 10356 training step(s), loss on training batch is 0.000399441.
After 10357 training step(s), loss on training batch is 0.000357459.
After 10358 training step(s), loss on training batch is 0.00035192.
After 10359 training step(s), loss on training batch is 0.000863181.
After 10360 training step(s), loss on training batch is 0.00106951.
After 10361 training step(s), loss on training batch is 0.00058145.
After 10362 training step(s), loss on training batch is 0.000430312.
After 10363 training step(s), loss on training batch is 0.000409018.
After 10364 training step(s), loss on training batch is 0.000487541.
After 10365 training step(s), loss on training batch is 0.000505394.
After 10366 training step(s), loss on training batch is 0.000396027.
After 10367 training step(s), loss on training batch is 0.000404505.
After 10368 training step(s), loss on training batch is 0.000492714.
After 10369 training step(s), loss on training batch is 0.000324482.
After 10370 training step(s), loss on training batch is 0.000386027.
After 10371 training step(s), loss on training batch is 0.000264164.
After 10372 training step(s), loss on training batch is 0.00032979.
After 10373 training step(s), loss on training batch is 0.000322539.
After 10374 training step(s), loss on training batch is 0.000326871.
After 10375 training step(s), loss on training batch is 0.000505188.
After 10376 training step(s), loss on training batch is 0.000468259.
After 10377 training step(s), loss on training batch is 0.000448053.
After 10378 training step(s), loss on training batch is 0.000342307.
After 10379 training step(s), loss on training batch is 0.00036455.
After 10380 training step(s), loss on training batch is 0.000324663.
After 10381 training step(s), loss on training batch is 0.000372281.
After 10382 training step(s), loss on training batch is 0.000342454.
After 10383 training step(s), loss on training batch is 0.00041963.
After 10384 training step(s), loss on training batch is 0.000415392.
After 10385 training step(s), loss on training batch is 0.000421278.
After 10386 training step(s), loss on training batch is 0.000564606.
After 10387 training step(s), loss on training batch is 0.000455146.
After 10388 training step(s), loss on training batch is 0.000339574.
After 10389 training step(s), loss on training batch is 0.000388517.
After 10390 training step(s), loss on training batch is 0.000359189.
After 10391 training step(s), loss on training batch is 0.000326962.
After 10392 training step(s), loss on training batch is 0.000344304.
After 10393 training step(s), loss on training batch is 0.000337729.
After 10394 training step(s), loss on training batch is 0.000283764.
After 10395 training step(s), loss on training batch is 0.000481006.
After 10396 training step(s), loss on training batch is 0.000462306.
After 10397 training step(s), loss on training batch is 0.000264911.
After 10398 training step(s), loss on training batch is 0.000274359.
After 10399 training step(s), loss on training batch is 0.000348339.
After 10400 training step(s), loss on training batch is 0.000379854.
After 10401 training step(s), loss on training batch is 0.000553641.
After 10402 training step(s), loss on training batch is 0.000630487.
After 10403 training step(s), loss on training batch is 0.000387643.
After 10404 training step(s), loss on training batch is 0.000566591.
After 10405 training step(s), loss on training batch is 0.00106169.
After 10406 training step(s), loss on training batch is 0.000764385.
After 10407 training step(s), loss on training batch is 0.000395318.
After 10408 training step(s), loss on training batch is 0.000481177.
After 10409 training step(s), loss on training batch is 0.000628903.
After 10410 training step(s), loss on training batch is 0.00064726.
After 10411 training step(s), loss on training batch is 0.000407118.
After 10412 training step(s), loss on training batch is 0.00032488.
After 10413 training step(s), loss on training batch is 0.000375126.
After 10414 training step(s), loss on training batch is 0.000404625.
After 10415 training step(s), loss on training batch is 0.000371177.
After 10416 training step(s), loss on training batch is 0.000321402.
After 10417 training step(s), loss on training batch is 0.000414981.
After 10418 training step(s), loss on training batch is 0.000353389.
After 10419 training step(s), loss on training batch is 0.000438912.
After 10420 training step(s), loss on training batch is 0.000387798.
After 10421 training step(s), loss on training batch is 0.000321961.
After 10422 training step(s), loss on training batch is 0.000376072.
After 10423 training step(s), loss on training batch is 0.00036175.
After 10424 training step(s), loss on training batch is 0.000326495.
After 10425 training step(s), loss on training batch is 0.000315295.
After 10426 training step(s), loss on training batch is 0.000386544.
After 10427 training step(s), loss on training batch is 0.000309434.
After 10428 training step(s), loss on training batch is 0.000986334.
After 10429 training step(s), loss on training batch is 0.000623669.
After 10430 training step(s), loss on training batch is 0.000568394.
After 10431 training step(s), loss on training batch is 0.000575808.
After 10432 training step(s), loss on training batch is 0.000665065.
After 10433 training step(s), loss on training batch is 0.000754225.
After 10434 training step(s), loss on training batch is 0.000634469.
After 10435 training step(s), loss on training batch is 0.000493418.
After 10436 training step(s), loss on training batch is 0.00072649.
After 10437 training step(s), loss on training batch is 0.000707705.
After 10438 training step(s), loss on training batch is 0.000515824.
After 10439 training step(s), loss on training batch is 0.000512771.
After 10440 training step(s), loss on training batch is 0.00114364.
After 10441 training step(s), loss on training batch is 0.000546764.
After 10442 training step(s), loss on training batch is 0.000614894.
After 10443 training step(s), loss on training batch is 0.000625356.
After 10444 training step(s), loss on training batch is 0.000722586.
After 10445 training step(s), loss on training batch is 0.000524714.
After 10446 training step(s), loss on training batch is 0.000710505.
After 10447 training step(s), loss on training batch is 0.000566554.
After 10448 training step(s), loss on training batch is 0.000626107.
After 10449 training step(s), loss on training batch is 0.000667543.
After 10450 training step(s), loss on training batch is 0.000654624.
After 10451 training step(s), loss on training batch is 0.000671754.
After 10452 training step(s), loss on training batch is 0.000582944.
After 10453 training step(s), loss on training batch is 0.000653979.
After 10454 training step(s), loss on training batch is 0.00045934.
After 10455 training step(s), loss on training batch is 0.000631985.
After 10456 training step(s), loss on training batch is 0.00110434.
After 10457 training step(s), loss on training batch is 0.00092022.
After 10458 training step(s), loss on training batch is 0.000975477.
After 10459 training step(s), loss on training batch is 0.000977628.
After 10460 training step(s), loss on training batch is 0.000835457.
After 10461 training step(s), loss on training batch is 0.000867685.
After 10462 training step(s), loss on training batch is 0.00108144.
After 10463 training step(s), loss on training batch is 0.000951097.
After 10464 training step(s), loss on training batch is 0.000850928.
After 10465 training step(s), loss on training batch is 0.00115483.
After 10466 training step(s), loss on training batch is 0.000841186.
After 10467 training step(s), loss on training batch is 0.00085504.
After 10468 training step(s), loss on training batch is 0.00100251.
After 10469 training step(s), loss on training batch is 0.00152654.
After 10470 training step(s), loss on training batch is 0.00304862.
After 10471 training step(s), loss on training batch is 0.00146615.
After 10472 training step(s), loss on training batch is 0.00111797.
After 10473 training step(s), loss on training batch is 0.00126542.
After 10474 training step(s), loss on training batch is 0.00111053.
After 10475 training step(s), loss on training batch is 0.00104599.
After 10476 training step(s), loss on training batch is 0.000983502.
After 10477 training step(s), loss on training batch is 0.00135912.
After 10478 training step(s), loss on training batch is 0.000948759.
After 10479 training step(s), loss on training batch is 0.000931905.
After 10480 training step(s), loss on training batch is 0.000898441.
After 10481 training step(s), loss on training batch is 0.000542204.
After 10482 training step(s), loss on training batch is 0.000477421.
After 10483 training step(s), loss on training batch is 0.000306518.
After 10484 training step(s), loss on training batch is 0.000378825.
After 10485 training step(s), loss on training batch is 0.000410555.
After 10486 training step(s), loss on training batch is 0.000555459.
After 10487 training step(s), loss on training batch is 0.000667616.
After 10488 training step(s), loss on training batch is 0.000439346.
After 10489 training step(s), loss on training batch is 0.000379009.
After 10490 training step(s), loss on training batch is 0.000351612.
After 10491 training step(s), loss on training batch is 0.000295028.
After 10492 training step(s), loss on training batch is 0.000375438.
After 10493 training step(s), loss on training batch is 0.000316486.
After 10494 training step(s), loss on training batch is 0.000327142.
After 10495 training step(s), loss on training batch is 0.00038683.
After 10496 training step(s), loss on training batch is 0.000384349.
After 10497 training step(s), loss on training batch is 0.000301216.
After 10498 training step(s), loss on training batch is 0.000287553.
After 10499 training step(s), loss on training batch is 0.000297869.
After 10500 training step(s), loss on training batch is 0.000316479.
After 10501 training step(s), loss on training batch is 0.00105432.
After 10502 training step(s), loss on training batch is 0.000530833.
After 10503 training step(s), loss on training batch is 0.000552208.
After 10504 training step(s), loss on training batch is 0.000657647.
After 10505 training step(s), loss on training batch is 0.000613595.
After 10506 training step(s), loss on training batch is 0.000631039.
After 10507 training step(s), loss on training batch is 0.000621878.
After 10508 training step(s), loss on training batch is 0.000345186.
After 10509 training step(s), loss on training batch is 0.000381914.
After 10510 training step(s), loss on training batch is 0.000411248.
After 10511 training step(s), loss on training batch is 0.000489204.
After 10512 training step(s), loss on training batch is 0.000460981.
After 10513 training step(s), loss on training batch is 0.000555727.
After 10514 training step(s), loss on training batch is 0.000797044.
After 10515 training step(s), loss on training batch is 0.00151332.
After 10516 training step(s), loss on training batch is 0.00104525.
After 10517 training step(s), loss on training batch is 0.000868503.
After 10518 training step(s), loss on training batch is 0.00065473.
After 10519 training step(s), loss on training batch is 0.000405102.
After 10520 training step(s), loss on training batch is 0.000402053.
After 10521 training step(s), loss on training batch is 0.000413394.
After 10522 training step(s), loss on training batch is 0.000359034.
After 10523 training step(s), loss on training batch is 0.000570543.
After 10524 training step(s), loss on training batch is 0.000441313.
After 10525 training step(s), loss on training batch is 0.000423303.
After 10526 training step(s), loss on training batch is 0.000416439.
After 10527 training step(s), loss on training batch is 0.00083838.
After 10528 training step(s), loss on training batch is 0.0024683.
After 10529 training step(s), loss on training batch is 0.00074296.
After 10530 training step(s), loss on training batch is 0.000554846.
After 10531 training step(s), loss on training batch is 0.00056769.
After 10532 training step(s), loss on training batch is 0.000534525.
After 10533 training step(s), loss on training batch is 0.000440752.
After 10534 training step(s), loss on training batch is 0.000364802.
After 10535 training step(s), loss on training batch is 0.000325005.
After 10536 training step(s), loss on training batch is 0.000325111.
After 10537 training step(s), loss on training batch is 0.00036974.
After 10538 training step(s), loss on training batch is 0.000363361.
After 10539 training step(s), loss on training batch is 0.000407626.
After 10540 training step(s), loss on training batch is 0.000456331.
After 10541 training step(s), loss on training batch is 0.000361193.
After 10542 training step(s), loss on training batch is 0.000430876.
After 10543 training step(s), loss on training batch is 0.000411813.
After 10544 training step(s), loss on training batch is 0.000520985.
After 10545 training step(s), loss on training batch is 0.000344187.
After 10546 training step(s), loss on training batch is 0.000480892.
After 10547 training step(s), loss on training batch is 0.000360524.
After 10548 training step(s), loss on training batch is 0.000332447.
After 10549 training step(s), loss on training batch is 0.000372772.
After 10550 training step(s), loss on training batch is 0.00048581.
After 10551 training step(s), loss on training batch is 0.000448079.
After 10552 training step(s), loss on training batch is 0.000400495.
After 10553 training step(s), loss on training batch is 0.00040026.
After 10554 training step(s), loss on training batch is 0.000375118.
After 10555 training step(s), loss on training batch is 0.000365063.
After 10556 training step(s), loss on training batch is 0.000467557.
After 10557 training step(s), loss on training batch is 0.00056027.
After 10558 training step(s), loss on training batch is 0.000355968.
After 10559 training step(s), loss on training batch is 0.000395593.
After 10560 training step(s), loss on training batch is 0.000510224.
After 10561 training step(s), loss on training batch is 0.000350599.
After 10562 training step(s), loss on training batch is 0.000319128.
After 10563 training step(s), loss on training batch is 0.000339772.
After 10564 training step(s), loss on training batch is 0.000462998.
After 10565 training step(s), loss on training batch is 0.000411684.
After 10566 training step(s), loss on training batch is 0.000408382.
After 10567 training step(s), loss on training batch is 0.000409001.
After 10568 training step(s), loss on training batch is 0.000370124.
After 10569 training step(s), loss on training batch is 0.000431614.
After 10570 training step(s), loss on training batch is 0.000358834.
After 10571 training step(s), loss on training batch is 0.000380234.
After 10572 training step(s), loss on training batch is 0.000352045.
After 10573 training step(s), loss on training batch is 0.000333418.
After 10574 training step(s), loss on training batch is 0.0003212.
After 10575 training step(s), loss on training batch is 0.000388015.
After 10576 training step(s), loss on training batch is 0.000377456.
After 10577 training step(s), loss on training batch is 0.000514069.
After 10578 training step(s), loss on training batch is 0.000394843.
After 10579 training step(s), loss on training batch is 0.000372349.
After 10580 training step(s), loss on training batch is 0.00038272.
After 10581 training step(s), loss on training batch is 0.000359605.
After 10582 training step(s), loss on training batch is 0.000380531.
After 10583 training step(s), loss on training batch is 0.000689566.
After 10584 training step(s), loss on training batch is 0.000852849.
After 10585 training step(s), loss on training batch is 0.00100856.
After 10586 training step(s), loss on training batch is 0.0006676.
After 10587 training step(s), loss on training batch is 0.000675928.
After 10588 training step(s), loss on training batch is 0.000609493.
After 10589 training step(s), loss on training batch is 0.000678917.
After 10590 training step(s), loss on training batch is 0.000627053.
After 10591 training step(s), loss on training batch is 0.000718727.
After 10592 training step(s), loss on training batch is 0.000752071.
After 10593 training step(s), loss on training batch is 0.000598044.
After 10594 training step(s), loss on training batch is 0.00059884.
After 10595 training step(s), loss on training batch is 0.000797339.
After 10596 training step(s), loss on training batch is 0.000736662.
After 10597 training step(s), loss on training batch is 0.000637088.
After 10598 training step(s), loss on training batch is 0.000674048.
After 10599 training step(s), loss on training batch is 0.00093051.
After 10600 training step(s), loss on training batch is 0.000680246.
After 10601 training step(s), loss on training batch is 0.000618211.
After 10602 training step(s), loss on training batch is 0.000732985.
After 10603 training step(s), loss on training batch is 0.000620717.
After 10604 training step(s), loss on training batch is 0.000605815.
After 10605 training step(s), loss on training batch is 0.000525013.
After 10606 training step(s), loss on training batch is 0.000682041.
After 10607 training step(s), loss on training batch is 0.000868992.
After 10608 training step(s), loss on training batch is 0.000728608.
After 10609 training step(s), loss on training batch is 0.000551931.
After 10610 training step(s), loss on training batch is 0.000892537.
After 10611 training step(s), loss on training batch is 0.00066466.
After 10612 training step(s), loss on training batch is 0.0006543.
After 10613 training step(s), loss on training batch is 0.000530609.
After 10614 training step(s), loss on training batch is 0.000524254.
After 10615 training step(s), loss on training batch is 0.00050429.
After 10616 training step(s), loss on training batch is 0.000511976.
After 10617 training step(s), loss on training batch is 0.000530761.
After 10618 training step(s), loss on training batch is 0.000771808.
After 10619 training step(s), loss on training batch is 0.00105275.
After 10620 training step(s), loss on training batch is 0.00102598.
After 10621 training step(s), loss on training batch is 0.00128598.
After 10622 training step(s), loss on training batch is 0.000670448.
After 10623 training step(s), loss on training batch is 0.000659067.
After 10624 training step(s), loss on training batch is 0.0006667.
After 10625 training step(s), loss on training batch is 0.000729619.
After 10626 training step(s), loss on training batch is 0.000742013.
After 10627 training step(s), loss on training batch is 0.000646193.
After 10628 training step(s), loss on training batch is 0.000677212.
After 10629 training step(s), loss on training batch is 0.000700159.
After 10630 training step(s), loss on training batch is 0.000755585.
After 10631 training step(s), loss on training batch is 0.00084916.
After 10632 training step(s), loss on training batch is 0.000592261.
After 10633 training step(s), loss on training batch is 0.000603078.
After 10634 training step(s), loss on training batch is 0.000558581.
After 10635 training step(s), loss on training batch is 0.00049772.
After 10636 training step(s), loss on training batch is 0.000778214.
After 10637 training step(s), loss on training batch is 0.00122025.
After 10638 training step(s), loss on training batch is 0.000514192.
After 10639 training step(s), loss on training batch is 0.000600691.
After 10640 training step(s), loss on training batch is 0.000655049.
After 10641 training step(s), loss on training batch is 0.000586464.
After 10642 training step(s), loss on training batch is 0.000548204.
After 10643 training step(s), loss on training batch is 0.000614049.
After 10644 training step(s), loss on training batch is 0.000557259.
After 10645 training step(s), loss on training batch is 0.000636931.
After 10646 training step(s), loss on training batch is 0.000836787.
After 10647 training step(s), loss on training batch is 0.000586609.
After 10648 training step(s), loss on training batch is 0.000623037.
After 10649 training step(s), loss on training batch is 0.000668828.
After 10650 training step(s), loss on training batch is 0.000690961.
After 10651 training step(s), loss on training batch is 0.000606605.
After 10652 training step(s), loss on training batch is 0.000511238.
After 10653 training step(s), loss on training batch is 0.000763997.
After 10654 training step(s), loss on training batch is 0.000610186.
After 10655 training step(s), loss on training batch is 0.000666375.
After 10656 training step(s), loss on training batch is 0.000606669.
After 10657 training step(s), loss on training batch is 0.000546051.
After 10658 training step(s), loss on training batch is 0.000713522.
After 10659 training step(s), loss on training batch is 0.0008947.
After 10660 training step(s), loss on training batch is 0.000584297.
After 10661 training step(s), loss on training batch is 0.00053388.
After 10662 training step(s), loss on training batch is 0.000532193.
After 10663 training step(s), loss on training batch is 0.000560163.
After 10664 training step(s), loss on training batch is 0.00050291.
After 10665 training step(s), loss on training batch is 0.000689219.
After 10666 training step(s), loss on training batch is 0.00109355.
After 10667 training step(s), loss on training batch is 0.0011522.
After 10668 training step(s), loss on training batch is 0.00104254.
After 10669 training step(s), loss on training batch is 0.00106699.
After 10670 training step(s), loss on training batch is 0.00101486.
After 10671 training step(s), loss on training batch is 0.000957185.
After 10672 training step(s), loss on training batch is 0.00100388.
After 10673 training step(s), loss on training batch is 0.00164048.
After 10674 training step(s), loss on training batch is 0.00120244.
After 10675 training step(s), loss on training batch is 0.00129683.
After 10676 training step(s), loss on training batch is 0.00104788.
After 10677 training step(s), loss on training batch is 0.000997466.
After 10678 training step(s), loss on training batch is 0.00105707.
After 10679 training step(s), loss on training batch is 0.00091652.
After 10680 training step(s), loss on training batch is 0.000906873.
After 10681 training step(s), loss on training batch is 0.000879724.
After 10682 training step(s), loss on training batch is 0.000914919.
After 10683 training step(s), loss on training batch is 0.00100274.
After 10684 training step(s), loss on training batch is 0.000910015.
After 10685 training step(s), loss on training batch is 0.00089776.
After 10686 training step(s), loss on training batch is 0.00121329.
After 10687 training step(s), loss on training batch is 0.000888441.
After 10688 training step(s), loss on training batch is 0.00111931.
After 10689 training step(s), loss on training batch is 0.000920469.
After 10690 training step(s), loss on training batch is 0.000906839.
After 10691 training step(s), loss on training batch is 0.000902323.
After 10692 training step(s), loss on training batch is 0.000942363.
After 10693 training step(s), loss on training batch is 0.000870205.
After 10694 training step(s), loss on training batch is 0.000999993.
After 10695 training step(s), loss on training batch is 0.00105123.
After 10696 training step(s), loss on training batch is 0.00113538.
After 10697 training step(s), loss on training batch is 0.000960275.
After 10698 training step(s), loss on training batch is 0.000933683.
After 10699 training step(s), loss on training batch is 0.000892143.
After 10700 training step(s), loss on training batch is 0.000939875.
After 10701 training step(s), loss on training batch is 0.000816718.
After 10702 training step(s), loss on training batch is 0.00101711.
After 10703 training step(s), loss on training batch is 0.000888482.
After 10704 training step(s), loss on training batch is 0.00112688.
After 10705 training step(s), loss on training batch is 0.00118599.
After 10706 training step(s), loss on training batch is 0.00115989.
After 10707 training step(s), loss on training batch is 0.00128892.
After 10708 training step(s), loss on training batch is 0.00112048.
After 10709 training step(s), loss on training batch is 0.00630455.
After 10710 training step(s), loss on training batch is 0.00170121.
After 10711 training step(s), loss on training batch is 0.00152018.
After 10712 training step(s), loss on training batch is 0.00147992.
After 10713 training step(s), loss on training batch is 0.00138322.
After 10714 training step(s), loss on training batch is 0.00140694.
After 10715 training step(s), loss on training batch is 0.00116195.
After 10716 training step(s), loss on training batch is 0.000521686.
After 10717 training step(s), loss on training batch is 0.000468104.
After 10718 training step(s), loss on training batch is 0.000436283.
After 10719 training step(s), loss on training batch is 0.000470172.
After 10720 training step(s), loss on training batch is 0.000504981.
After 10721 training step(s), loss on training batch is 0.000470142.
After 10722 training step(s), loss on training batch is 0.00047567.
After 10723 training step(s), loss on training batch is 0.000480552.
After 10724 training step(s), loss on training batch is 0.000484349.
After 10725 training step(s), loss on training batch is 0.000458166.
After 10726 training step(s), loss on training batch is 0.000846161.
After 10727 training step(s), loss on training batch is 0.000871179.
After 10728 training step(s), loss on training batch is 0.00103101.
After 10729 training step(s), loss on training batch is 0.00087603.
After 10730 training step(s), loss on training batch is 0.00194766.
After 10731 training step(s), loss on training batch is 0.00157668.
After 10732 training step(s), loss on training batch is 0.00115457.
After 10733 training step(s), loss on training batch is 0.0011377.
After 10734 training step(s), loss on training batch is 0.000969389.
After 10735 training step(s), loss on training batch is 0.000922799.
After 10736 training step(s), loss on training batch is 0.000958758.
After 10737 training step(s), loss on training batch is 0.000962324.
After 10738 training step(s), loss on training batch is 0.00108161.
After 10739 training step(s), loss on training batch is 0.000960755.
After 10740 training step(s), loss on training batch is 0.000942195.
After 10741 training step(s), loss on training batch is 0.000919246.
After 10742 training step(s), loss on training batch is 0.00158792.
After 10743 training step(s), loss on training batch is 0.000364067.
After 10744 training step(s), loss on training batch is 0.000413194.
After 10745 training step(s), loss on training batch is 0.000383194.
After 10746 training step(s), loss on training batch is 0.000487393.
After 10747 training step(s), loss on training batch is 0.000458957.
After 10748 training step(s), loss on training batch is 0.000410356.
After 10749 training step(s), loss on training batch is 0.000365255.
After 10750 training step(s), loss on training batch is 0.000337413.
After 10751 training step(s), loss on training batch is 0.000508653.
After 10752 training step(s), loss on training batch is 0.000442867.
After 10753 training step(s), loss on training batch is 0.000433539.
After 10754 training step(s), loss on training batch is 0.000374793.
After 10755 training step(s), loss on training batch is 0.000555781.
After 10756 training step(s), loss on training batch is 0.000398476.
After 10757 training step(s), loss on training batch is 0.000343473.
After 10758 training step(s), loss on training batch is 0.000344099.
After 10759 training step(s), loss on training batch is 0.000897097.
After 10760 training step(s), loss on training batch is 0.00107621.
After 10761 training step(s), loss on training batch is 0.000549252.
After 10762 training step(s), loss on training batch is 0.000408407.
After 10763 training step(s), loss on training batch is 0.000384125.
After 10764 training step(s), loss on training batch is 0.000489028.
After 10765 training step(s), loss on training batch is 0.000502786.
After 10766 training step(s), loss on training batch is 0.000428494.
After 10767 training step(s), loss on training batch is 0.000388753.
After 10768 training step(s), loss on training batch is 0.000524371.
After 10769 training step(s), loss on training batch is 0.000329435.
After 10770 training step(s), loss on training batch is 0.000408013.
After 10771 training step(s), loss on training batch is 0.000274828.
After 10772 training step(s), loss on training batch is 0.000317642.
After 10773 training step(s), loss on training batch is 0.000324027.
After 10774 training step(s), loss on training batch is 0.000319275.
After 10775 training step(s), loss on training batch is 0.000501381.
After 10776 training step(s), loss on training batch is 0.000437232.
After 10777 training step(s), loss on training batch is 0.000436521.
After 10778 training step(s), loss on training batch is 0.000339239.
After 10779 training step(s), loss on training batch is 0.000366827.
After 10780 training step(s), loss on training batch is 0.000320218.
After 10781 training step(s), loss on training batch is 0.000361909.
After 10782 training step(s), loss on training batch is 0.000339199.
After 10783 training step(s), loss on training batch is 0.000414752.
After 10784 training step(s), loss on training batch is 0.00040344.
After 10785 training step(s), loss on training batch is 0.000417022.
After 10786 training step(s), loss on training batch is 0.000567419.
After 10787 training step(s), loss on training batch is 0.000436683.
After 10788 training step(s), loss on training batch is 0.00033333.
After 10789 training step(s), loss on training batch is 0.000390436.
After 10790 training step(s), loss on training batch is 0.000343779.
After 10791 training step(s), loss on training batch is 0.000317415.
After 10792 training step(s), loss on training batch is 0.000331537.
After 10793 training step(s), loss on training batch is 0.000316705.
After 10794 training step(s), loss on training batch is 0.000276246.
After 10795 training step(s), loss on training batch is 0.000485556.
After 10796 training step(s), loss on training batch is 0.00045802.
After 10797 training step(s), loss on training batch is 0.000251538.
After 10798 training step(s), loss on training batch is 0.00026674.
After 10799 training step(s), loss on training batch is 0.000358237.
After 10800 training step(s), loss on training batch is 0.00037549.
After 10801 training step(s), loss on training batch is 0.00054743.
After 10802 training step(s), loss on training batch is 0.000623967.
After 10803 training step(s), loss on training batch is 0.000391192.
After 10804 training step(s), loss on training batch is 0.000542554.
After 10805 training step(s), loss on training batch is 0.00103793.
After 10806 training step(s), loss on training batch is 0.000711301.
After 10807 training step(s), loss on training batch is 0.000409958.
After 10808 training step(s), loss on training batch is 0.00049474.
After 10809 training step(s), loss on training batch is 0.000605907.
After 10810 training step(s), loss on training batch is 0.000630211.
After 10811 training step(s), loss on training batch is 0.000411289.
After 10812 training step(s), loss on training batch is 0.000328331.
After 10813 training step(s), loss on training batch is 0.000383348.
After 10814 training step(s), loss on training batch is 0.000404619.
After 10815 training step(s), loss on training batch is 0.000375238.
After 10816 training step(s), loss on training batch is 0.000320453.
After 10817 training step(s), loss on training batch is 0.000415228.
After 10818 training step(s), loss on training batch is 0.000343533.
After 10819 training step(s), loss on training batch is 0.000438083.
After 10820 training step(s), loss on training batch is 0.0003959.
After 10821 training step(s), loss on training batch is 0.000309555.
After 10822 training step(s), loss on training batch is 0.000387933.
After 10823 training step(s), loss on training batch is 0.000348632.
After 10824 training step(s), loss on training batch is 0.000318167.
After 10825 training step(s), loss on training batch is 0.000312138.
After 10826 training step(s), loss on training batch is 0.000381827.
After 10827 training step(s), loss on training batch is 0.000308199.
After 10828 training step(s), loss on training batch is 0.000951137.
After 10829 training step(s), loss on training batch is 0.000614263.
After 10830 training step(s), loss on training batch is 0.0005545.
After 10831 training step(s), loss on training batch is 0.000564089.
After 10832 training step(s), loss on training batch is 0.000658842.
After 10833 training step(s), loss on training batch is 0.000729174.
After 10834 training step(s), loss on training batch is 0.000620235.
After 10835 training step(s), loss on training batch is 0.000489091.
After 10836 training step(s), loss on training batch is 0.000719787.
After 10837 training step(s), loss on training batch is 0.000695876.
After 10838 training step(s), loss on training batch is 0.000510124.
After 10839 training step(s), loss on training batch is 0.000508901.
After 10840 training step(s), loss on training batch is 0.00110894.
After 10841 training step(s), loss on training batch is 0.000515421.
After 10842 training step(s), loss on training batch is 0.000562055.
After 10843 training step(s), loss on training batch is 0.00058836.
After 10844 training step(s), loss on training batch is 0.000719212.
After 10845 training step(s), loss on training batch is 0.00048815.
After 10846 training step(s), loss on training batch is 0.000734423.
After 10847 training step(s), loss on training batch is 0.000551765.
After 10848 training step(s), loss on training batch is 0.000607152.
After 10849 training step(s), loss on training batch is 0.000649335.
After 10850 training step(s), loss on training batch is 0.000664471.
After 10851 training step(s), loss on training batch is 0.000676337.
After 10852 training step(s), loss on training batch is 0.000579244.
After 10853 training step(s), loss on training batch is 0.000674573.
After 10854 training step(s), loss on training batch is 0.00044669.
After 10855 training step(s), loss on training batch is 0.000617267.
After 10856 training step(s), loss on training batch is 0.00107023.
After 10857 training step(s), loss on training batch is 0.000889188.
After 10858 training step(s), loss on training batch is 0.000960144.
After 10859 training step(s), loss on training batch is 0.00095258.
After 10860 training step(s), loss on training batch is 0.000817789.
After 10861 training step(s), loss on training batch is 0.000851195.
After 10862 training step(s), loss on training batch is 0.00106322.
After 10863 training step(s), loss on training batch is 0.000938417.
After 10864 training step(s), loss on training batch is 0.00082544.
After 10865 training step(s), loss on training batch is 0.00116628.
After 10866 training step(s), loss on training batch is 0.000838379.
After 10867 training step(s), loss on training batch is 0.000853566.
After 10868 training step(s), loss on training batch is 0.000999296.
After 10869 training step(s), loss on training batch is 0.00156395.
After 10870 training step(s), loss on training batch is 0.00286366.
After 10871 training step(s), loss on training batch is 0.00146181.
After 10872 training step(s), loss on training batch is 0.00116227.
After 10873 training step(s), loss on training batch is 0.00127793.
After 10874 training step(s), loss on training batch is 0.0011271.
After 10875 training step(s), loss on training batch is 0.00104212.
After 10876 training step(s), loss on training batch is 0.000974426.
After 10877 training step(s), loss on training batch is 0.00133362.
After 10878 training step(s), loss on training batch is 0.000947507.
After 10879 training step(s), loss on training batch is 0.000905453.
After 10880 training step(s), loss on training batch is 0.000896764.
After 10881 training step(s), loss on training batch is 0.000540381.
After 10882 training step(s), loss on training batch is 0.000447996.
After 10883 training step(s), loss on training batch is 0.000304528.
After 10884 training step(s), loss on training batch is 0.000366688.
After 10885 training step(s), loss on training batch is 0.000400969.
After 10886 training step(s), loss on training batch is 0.000538044.
After 10887 training step(s), loss on training batch is 0.00066437.
After 10888 training step(s), loss on training batch is 0.000443749.
After 10889 training step(s), loss on training batch is 0.000379233.
After 10890 training step(s), loss on training batch is 0.000350305.
After 10891 training step(s), loss on training batch is 0.000292194.
After 10892 training step(s), loss on training batch is 0.000356788.
After 10893 training step(s), loss on training batch is 0.000321312.
After 10894 training step(s), loss on training batch is 0.000329212.
After 10895 training step(s), loss on training batch is 0.000375619.
After 10896 training step(s), loss on training batch is 0.000376942.
After 10897 training step(s), loss on training batch is 0.000296451.
After 10898 training step(s), loss on training batch is 0.000285133.
After 10899 training step(s), loss on training batch is 0.000295997.
After 10900 training step(s), loss on training batch is 0.000303269.
After 10901 training step(s), loss on training batch is 0.00101129.
After 10902 training step(s), loss on training batch is 0.000514794.
After 10903 training step(s), loss on training batch is 0.000547549.
After 10904 training step(s), loss on training batch is 0.000644774.
After 10905 training step(s), loss on training batch is 0.000587609.
After 10906 training step(s), loss on training batch is 0.000618059.
After 10907 training step(s), loss on training batch is 0.000622425.
After 10908 training step(s), loss on training batch is 0.000336223.
After 10909 training step(s), loss on training batch is 0.000376034.
After 10910 training step(s), loss on training batch is 0.00040146.
After 10911 training step(s), loss on training batch is 0.000476491.
After 10912 training step(s), loss on training batch is 0.000453781.
After 10913 training step(s), loss on training batch is 0.00047683.
After 10914 training step(s), loss on training batch is 0.00068556.
After 10915 training step(s), loss on training batch is 0.00144651.
After 10916 training step(s), loss on training batch is 0.000996813.
After 10917 training step(s), loss on training batch is 0.000839871.
After 10918 training step(s), loss on training batch is 0.000654533.
After 10919 training step(s), loss on training batch is 0.000393955.
After 10920 training step(s), loss on training batch is 0.000382832.
After 10921 training step(s), loss on training batch is 0.000391705.
After 10922 training step(s), loss on training batch is 0.000346799.
After 10923 training step(s), loss on training batch is 0.000522021.
After 10924 training step(s), loss on training batch is 0.00040792.
After 10925 training step(s), loss on training batch is 0.000394036.
After 10926 training step(s), loss on training batch is 0.000376822.
After 10927 training step(s), loss on training batch is 0.000907355.
After 10928 training step(s), loss on training batch is 0.00290322.
After 10929 training step(s), loss on training batch is 0.000658484.
After 10930 training step(s), loss on training batch is 0.000506455.
After 10931 training step(s), loss on training batch is 0.00059203.
After 10932 training step(s), loss on training batch is 0.000506624.
After 10933 training step(s), loss on training batch is 0.000433698.
After 10934 training step(s), loss on training batch is 0.000362522.
After 10935 training step(s), loss on training batch is 0.000317854.
After 10936 training step(s), loss on training batch is 0.000322339.
After 10937 training step(s), loss on training batch is 0.000360216.
After 10938 training step(s), loss on training batch is 0.000355274.
After 10939 training step(s), loss on training batch is 0.000398156.
After 10940 training step(s), loss on training batch is 0.000444321.
After 10941 training step(s), loss on training batch is 0.000343371.
After 10942 training step(s), loss on training batch is 0.000420844.
After 10943 training step(s), loss on training batch is 0.000400802.
After 10944 training step(s), loss on training batch is 0.00050278.
After 10945 training step(s), loss on training batch is 0.000334027.
After 10946 training step(s), loss on training batch is 0.000474761.
After 10947 training step(s), loss on training batch is 0.000358124.
After 10948 training step(s), loss on training batch is 0.000329596.
After 10949 training step(s), loss on training batch is 0.000372991.
After 10950 training step(s), loss on training batch is 0.000473384.
After 10951 training step(s), loss on training batch is 0.000437119.
After 10952 training step(s), loss on training batch is 0.000390071.
After 10953 training step(s), loss on training batch is 0.000393978.
After 10954 training step(s), loss on training batch is 0.000382835.
After 10955 training step(s), loss on training batch is 0.000362448.
After 10956 training step(s), loss on training batch is 0.000457329.
After 10957 training step(s), loss on training batch is 0.000550208.
After 10958 training step(s), loss on training batch is 0.000350643.
After 10959 training step(s), loss on training batch is 0.000392528.
After 10960 training step(s), loss on training batch is 0.000502559.
After 10961 training step(s), loss on training batch is 0.000347337.
After 10962 training step(s), loss on training batch is 0.000314334.
After 10963 training step(s), loss on training batch is 0.000332922.
After 10964 training step(s), loss on training batch is 0.000461429.
After 10965 training step(s), loss on training batch is 0.000406347.
After 10966 training step(s), loss on training batch is 0.00039401.
After 10967 training step(s), loss on training batch is 0.000405085.
After 10968 training step(s), loss on training batch is 0.000378597.
After 10969 training step(s), loss on training batch is 0.000420757.
After 10970 training step(s), loss on training batch is 0.000352119.
After 10971 training step(s), loss on training batch is 0.000371436.
After 10972 training step(s), loss on training batch is 0.000347347.
After 10973 training step(s), loss on training batch is 0.000327662.
After 10974 training step(s), loss on training batch is 0.000316102.
After 10975 training step(s), loss on training batch is 0.000381274.
After 10976 training step(s), loss on training batch is 0.00037377.
After 10977 training step(s), loss on training batch is 0.000503043.
After 10978 training step(s), loss on training batch is 0.0003888.
After 10979 training step(s), loss on training batch is 0.000364592.
After 10980 training step(s), loss on training batch is 0.000378039.
After 10981 training step(s), loss on training batch is 0.000353605.
After 10982 training step(s), loss on training batch is 0.000374446.
After 10983 training step(s), loss on training batch is 0.00068747.
After 10984 training step(s), loss on training batch is 0.000828224.
After 10985 training step(s), loss on training batch is 0.000977025.
After 10986 training step(s), loss on training batch is 0.000656769.
After 10987 training step(s), loss on training batch is 0.000671921.
After 10988 training step(s), loss on training batch is 0.00060741.
After 10989 training step(s), loss on training batch is 0.000670364.
After 10990 training step(s), loss on training batch is 0.000618119.
After 10991 training step(s), loss on training batch is 0.000705752.
After 10992 training step(s), loss on training batch is 0.000753117.
After 10993 training step(s), loss on training batch is 0.000591341.
After 10994 training step(s), loss on training batch is 0.000592573.
After 10995 training step(s), loss on training batch is 0.000761111.
After 10996 training step(s), loss on training batch is 0.000714045.
After 10997 training step(s), loss on training batch is 0.000637228.
After 10998 training step(s), loss on training batch is 0.00065931.
After 10999 training step(s), loss on training batch is 0.000901452.
After 11000 training step(s), loss on training batch is 0.000675297.
After 11001 training step(s), loss on training batch is 0.000610497.
After 11002 training step(s), loss on training batch is 0.000712552.
After 11003 training step(s), loss on training batch is 0.000613267.
After 11004 training step(s), loss on training batch is 0.000589408.
After 11005 training step(s), loss on training batch is 0.000511772.
After 11006 training step(s), loss on training batch is 0.000675026.
After 11007 training step(s), loss on training batch is 0.000863742.
After 11008 training step(s), loss on training batch is 0.000723804.
After 11009 training step(s), loss on training batch is 0.000541886.
After 11010 training step(s), loss on training batch is 0.00088035.
After 11011 training step(s), loss on training batch is 0.000655639.
After 11012 training step(s), loss on training batch is 0.000637786.
After 11013 training step(s), loss on training batch is 0.000533187.
After 11014 training step(s), loss on training batch is 0.000526779.
After 11015 training step(s), loss on training batch is 0.000510603.
After 11016 training step(s), loss on training batch is 0.000513049.
After 11017 training step(s), loss on training batch is 0.000526441.
After 11018 training step(s), loss on training batch is 0.000750825.
After 11019 training step(s), loss on training batch is 0.00105859.
After 11020 training step(s), loss on training batch is 0.00101432.
After 11021 training step(s), loss on training batch is 0.00127587.
After 11022 training step(s), loss on training batch is 0.00065421.
After 11023 training step(s), loss on training batch is 0.000638548.
After 11024 training step(s), loss on training batch is 0.000658385.
After 11025 training step(s), loss on training batch is 0.000718241.
After 11026 training step(s), loss on training batch is 0.000734361.
After 11027 training step(s), loss on training batch is 0.000629732.
After 11028 training step(s), loss on training batch is 0.000661705.
After 11029 training step(s), loss on training batch is 0.000677957.
After 11030 training step(s), loss on training batch is 0.000741669.
After 11031 training step(s), loss on training batch is 0.000842163.
After 11032 training step(s), loss on training batch is 0.000576101.
After 11033 training step(s), loss on training batch is 0.000600211.
After 11034 training step(s), loss on training batch is 0.000559736.
After 11035 training step(s), loss on training batch is 0.000505691.
After 11036 training step(s), loss on training batch is 0.000751274.
After 11037 training step(s), loss on training batch is 0.00115342.
After 11038 training step(s), loss on training batch is 0.000506171.
After 11039 training step(s), loss on training batch is 0.000591199.
After 11040 training step(s), loss on training batch is 0.000646024.
After 11041 training step(s), loss on training batch is 0.000572714.
After 11042 training step(s), loss on training batch is 0.000558833.
After 11043 training step(s), loss on training batch is 0.000632732.
After 11044 training step(s), loss on training batch is 0.000583418.
After 11045 training step(s), loss on training batch is 0.000634737.
After 11046 training step(s), loss on training batch is 0.000779783.
After 11047 training step(s), loss on training batch is 0.000603189.
After 11048 training step(s), loss on training batch is 0.000604412.
After 11049 training step(s), loss on training batch is 0.000656028.
After 11050 training step(s), loss on training batch is 0.000680246.
After 11051 training step(s), loss on training batch is 0.00058602.
After 11052 training step(s), loss on training batch is 0.000509914.
After 11053 training step(s), loss on training batch is 0.000754648.
After 11054 training step(s), loss on training batch is 0.000611189.
After 11055 training step(s), loss on training batch is 0.000664245.
After 11056 training step(s), loss on training batch is 0.000597521.
After 11057 training step(s), loss on training batch is 0.000539908.
After 11058 training step(s), loss on training batch is 0.000704193.
After 11059 training step(s), loss on training batch is 0.000865671.
After 11060 training step(s), loss on training batch is 0.00057586.
After 11061 training step(s), loss on training batch is 0.000513254.
After 11062 training step(s), loss on training batch is 0.000517742.
After 11063 training step(s), loss on training batch is 0.00054169.
After 11064 training step(s), loss on training batch is 0.000493326.
After 11065 training step(s), loss on training batch is 0.00068682.
After 11066 training step(s), loss on training batch is 0.00107151.
After 11067 training step(s), loss on training batch is 0.00112846.
After 11068 training step(s), loss on training batch is 0.0010342.
After 11069 training step(s), loss on training batch is 0.00106391.
After 11070 training step(s), loss on training batch is 0.00100651.
After 11071 training step(s), loss on training batch is 0.000952889.
After 11072 training step(s), loss on training batch is 0.000994532.
After 11073 training step(s), loss on training batch is 0.00145343.
After 11074 training step(s), loss on training batch is 0.00118371.
After 11075 training step(s), loss on training batch is 0.00124932.
After 11076 training step(s), loss on training batch is 0.00104356.
After 11077 training step(s), loss on training batch is 0.000987204.
After 11078 training step(s), loss on training batch is 0.00104802.
After 11079 training step(s), loss on training batch is 0.000927538.
After 11080 training step(s), loss on training batch is 0.000905305.
After 11081 training step(s), loss on training batch is 0.000887575.
After 11082 training step(s), loss on training batch is 0.000911103.
After 11083 training step(s), loss on training batch is 0.000992561.
After 11084 training step(s), loss on training batch is 0.00090008.
After 11085 training step(s), loss on training batch is 0.000878577.
After 11086 training step(s), loss on training batch is 0.00123532.
After 11087 training step(s), loss on training batch is 0.00088256.
After 11088 training step(s), loss on training batch is 0.00110336.
After 11089 training step(s), loss on training batch is 0.00091115.
After 11090 training step(s), loss on training batch is 0.000907589.
After 11091 training step(s), loss on training batch is 0.000902019.
After 11092 training step(s), loss on training batch is 0.000933585.
After 11093 training step(s), loss on training batch is 0.000864906.
After 11094 training step(s), loss on training batch is 0.000990766.
After 11095 training step(s), loss on training batch is 0.00103065.
After 11096 training step(s), loss on training batch is 0.00110897.
After 11097 training step(s), loss on training batch is 0.000920794.
After 11098 training step(s), loss on training batch is 0.000919219.
After 11099 training step(s), loss on training batch is 0.000850513.
After 11100 training step(s), loss on training batch is 0.000896187.
After 11101 training step(s), loss on training batch is 0.000803837.
After 11102 training step(s), loss on training batch is 0.00103879.
After 11103 training step(s), loss on training batch is 0.000859771.
After 11104 training step(s), loss on training batch is 0.00113234.
After 11105 training step(s), loss on training batch is 0.00120788.
After 11106 training step(s), loss on training batch is 0.0011407.
After 11107 training step(s), loss on training batch is 0.00128107.
After 11108 training step(s), loss on training batch is 0.0010994.
After 11109 training step(s), loss on training batch is 0.00897.
After 11110 training step(s), loss on training batch is 0.00306169.
After 11111 training step(s), loss on training batch is 0.00192484.
After 11112 training step(s), loss on training batch is 0.00175425.
After 11113 training step(s), loss on training batch is 0.00158679.
After 11114 training step(s), loss on training batch is 0.00147835.
After 11115 training step(s), loss on training batch is 0.00126416.
After 11116 training step(s), loss on training batch is 0.000596293.
After 11117 training step(s), loss on training batch is 0.000520926.
After 11118 training step(s), loss on training batch is 0.000486472.
After 11119 training step(s), loss on training batch is 0.000513317.
After 11120 training step(s), loss on training batch is 0.000538634.
After 11121 training step(s), loss on training batch is 0.000543594.
After 11122 training step(s), loss on training batch is 0.000561111.
After 11123 training step(s), loss on training batch is 0.00055777.
After 11124 training step(s), loss on training batch is 0.000517231.
After 11125 training step(s), loss on training batch is 0.000462365.
After 11126 training step(s), loss on training batch is 0.000923019.
After 11127 training step(s), loss on training batch is 0.000935115.
After 11128 training step(s), loss on training batch is 0.00103838.
After 11129 training step(s), loss on training batch is 0.000887082.
After 11130 training step(s), loss on training batch is 0.00182252.
After 11131 training step(s), loss on training batch is 0.0015594.
After 11132 training step(s), loss on training batch is 0.00116041.
After 11133 training step(s), loss on training batch is 0.00113123.
After 11134 training step(s), loss on training batch is 0.000977023.
After 11135 training step(s), loss on training batch is 0.000930106.
After 11136 training step(s), loss on training batch is 0.000957044.
After 11137 training step(s), loss on training batch is 0.000958246.
After 11138 training step(s), loss on training batch is 0.00106988.
After 11139 training step(s), loss on training batch is 0.000965384.
After 11140 training step(s), loss on training batch is 0.00094839.
After 11141 training step(s), loss on training batch is 0.000930759.
After 11142 training step(s), loss on training batch is 0.00153987.
After 11143 training step(s), loss on training batch is 0.000359432.
After 11144 training step(s), loss on training batch is 0.000422034.
After 11145 training step(s), loss on training batch is 0.000391441.
After 11146 training step(s), loss on training batch is 0.000489469.
After 11147 training step(s), loss on training batch is 0.000456952.
After 11148 training step(s), loss on training batch is 0.000426741.
After 11149 training step(s), loss on training batch is 0.000367433.
After 11150 training step(s), loss on training batch is 0.000337909.
After 11151 training step(s), loss on training batch is 0.000519647.
After 11152 training step(s), loss on training batch is 0.000444535.
After 11153 training step(s), loss on training batch is 0.00042521.
After 11154 training step(s), loss on training batch is 0.000376495.
After 11155 training step(s), loss on training batch is 0.000560188.
After 11156 training step(s), loss on training batch is 0.000401448.
After 11157 training step(s), loss on training batch is 0.000349237.
After 11158 training step(s), loss on training batch is 0.000341622.
After 11159 training step(s), loss on training batch is 0.000797184.
After 11160 training step(s), loss on training batch is 0.00100896.
After 11161 training step(s), loss on training batch is 0.000560883.
After 11162 training step(s), loss on training batch is 0.000427783.
After 11163 training step(s), loss on training batch is 0.000397889.
After 11164 training step(s), loss on training batch is 0.000479083.
After 11165 training step(s), loss on training batch is 0.000489746.
After 11166 training step(s), loss on training batch is 0.000432832.
After 11167 training step(s), loss on training batch is 0.000380499.
After 11168 training step(s), loss on training batch is 0.000533153.
After 11169 training step(s), loss on training batch is 0.000333626.
After 11170 training step(s), loss on training batch is 0.000418937.
After 11171 training step(s), loss on training batch is 0.000281596.
After 11172 training step(s), loss on training batch is 0.000306547.
After 11173 training step(s), loss on training batch is 0.000330055.
After 11174 training step(s), loss on training batch is 0.000323292.
After 11175 training step(s), loss on training batch is 0.000475799.
After 11176 training step(s), loss on training batch is 0.000434096.
After 11177 training step(s), loss on training batch is 0.000435951.
After 11178 training step(s), loss on training batch is 0.000340515.
After 11179 training step(s), loss on training batch is 0.000354536.
After 11180 training step(s), loss on training batch is 0.0003217.
After 11181 training step(s), loss on training batch is 0.000367259.
After 11182 training step(s), loss on training batch is 0.00033797.
After 11183 training step(s), loss on training batch is 0.000418469.
After 11184 training step(s), loss on training batch is 0.000404254.
After 11185 training step(s), loss on training batch is 0.000417744.
After 11186 training step(s), loss on training batch is 0.000539599.
After 11187 training step(s), loss on training batch is 0.000441911.
After 11188 training step(s), loss on training batch is 0.000334375.
After 11189 training step(s), loss on training batch is 0.000382563.
After 11190 training step(s), loss on training batch is 0.000352581.
After 11191 training step(s), loss on training batch is 0.000321115.
After 11192 training step(s), loss on training batch is 0.000336651.
After 11193 training step(s), loss on training batch is 0.000330974.
After 11194 training step(s), loss on training batch is 0.00028032.
After 11195 training step(s), loss on training batch is 0.000465312.
After 11196 training step(s), loss on training batch is 0.000452417.
After 11197 training step(s), loss on training batch is 0.000259027.
After 11198 training step(s), loss on training batch is 0.000270918.
After 11199 training step(s), loss on training batch is 0.000339155.
After 11200 training step(s), loss on training batch is 0.000369557.
After 11201 training step(s), loss on training batch is 0.000544923.
After 11202 training step(s), loss on training batch is 0.000619073.
After 11203 training step(s), loss on training batch is 0.000387338.
After 11204 training step(s), loss on training batch is 0.000549243.
After 11205 training step(s), loss on training batch is 0.000998119.
After 11206 training step(s), loss on training batch is 0.000712559.
After 11207 training step(s), loss on training batch is 0.000408414.
After 11208 training step(s), loss on training batch is 0.000492792.
After 11209 training step(s), loss on training batch is 0.000598699.
After 11210 training step(s), loss on training batch is 0.000617646.
After 11211 training step(s), loss on training batch is 0.000400338.
After 11212 training step(s), loss on training batch is 0.000323058.
After 11213 training step(s), loss on training batch is 0.000380951.
After 11214 training step(s), loss on training batch is 0.00040952.
After 11215 training step(s), loss on training batch is 0.000380892.
After 11216 training step(s), loss on training batch is 0.000313653.
After 11217 training step(s), loss on training batch is 0.000405022.
After 11218 training step(s), loss on training batch is 0.000339523.
After 11219 training step(s), loss on training batch is 0.000425845.
After 11220 training step(s), loss on training batch is 0.000384839.
After 11221 training step(s), loss on training batch is 0.000306731.
After 11222 training step(s), loss on training batch is 0.000377288.
After 11223 training step(s), loss on training batch is 0.000345008.
After 11224 training step(s), loss on training batch is 0.000313709.
After 11225 training step(s), loss on training batch is 0.000307159.
After 11226 training step(s), loss on training batch is 0.000372721.
After 11227 training step(s), loss on training batch is 0.000302306.
After 11228 training step(s), loss on training batch is 0.000930036.
After 11229 training step(s), loss on training batch is 0.000617967.
After 11230 training step(s), loss on training batch is 0.000543079.
After 11231 training step(s), loss on training batch is 0.000553429.
After 11232 training step(s), loss on training batch is 0.000684434.
After 11233 training step(s), loss on training batch is 0.000754696.
After 11234 training step(s), loss on training batch is 0.000628201.
After 11235 training step(s), loss on training batch is 0.000472876.
After 11236 training step(s), loss on training batch is 0.000712704.
After 11237 training step(s), loss on training batch is 0.000686781.
After 11238 training step(s), loss on training batch is 0.000500935.
After 11239 training step(s), loss on training batch is 0.000497162.
After 11240 training step(s), loss on training batch is 0.00115516.
After 11241 training step(s), loss on training batch is 0.000513378.
After 11242 training step(s), loss on training batch is 0.000584434.
After 11243 training step(s), loss on training batch is 0.00059574.
After 11244 training step(s), loss on training batch is 0.000696468.
After 11245 training step(s), loss on training batch is 0.000501931.
After 11246 training step(s), loss on training batch is 0.00068068.
After 11247 training step(s), loss on training batch is 0.000548817.
After 11248 training step(s), loss on training batch is 0.000604234.
After 11249 training step(s), loss on training batch is 0.000642661.
After 11250 training step(s), loss on training batch is 0.000627256.
After 11251 training step(s), loss on training batch is 0.000618104.
After 11252 training step(s), loss on training batch is 0.000549434.
After 11253 training step(s), loss on training batch is 0.000629136.
After 11254 training step(s), loss on training batch is 0.000452023.
After 11255 training step(s), loss on training batch is 0.000606437.
After 11256 training step(s), loss on training batch is 0.00108192.
After 11257 training step(s), loss on training batch is 0.000913728.
After 11258 training step(s), loss on training batch is 0.0009689.
After 11259 training step(s), loss on training batch is 0.000968042.
After 11260 training step(s), loss on training batch is 0.000853801.
After 11261 training step(s), loss on training batch is 0.000884347.
After 11262 training step(s), loss on training batch is 0.00105425.
After 11263 training step(s), loss on training batch is 0.000935544.
After 11264 training step(s), loss on training batch is 0.000840275.
After 11265 training step(s), loss on training batch is 0.00112898.
After 11266 training step(s), loss on training batch is 0.000838868.
After 11267 training step(s), loss on training batch is 0.000843631.
After 11268 training step(s), loss on training batch is 0.000975664.
After 11269 training step(s), loss on training batch is 0.00148539.
After 11270 training step(s), loss on training batch is 0.00300862.
After 11271 training step(s), loss on training batch is 0.00143822.
After 11272 training step(s), loss on training batch is 0.00104615.
After 11273 training step(s), loss on training batch is 0.00120621.
After 11274 training step(s), loss on training batch is 0.00106154.
After 11275 training step(s), loss on training batch is 0.00100205.
After 11276 training step(s), loss on training batch is 0.000931432.
After 11277 training step(s), loss on training batch is 0.0013177.
After 11278 training step(s), loss on training batch is 0.000913288.
After 11279 training step(s), loss on training batch is 0.000886043.
After 11280 training step(s), loss on training batch is 0.000868706.
After 11281 training step(s), loss on training batch is 0.000534341.
After 11282 training step(s), loss on training batch is 0.000447254.
After 11283 training step(s), loss on training batch is 0.000293743.
After 11284 training step(s), loss on training batch is 0.000366428.
After 11285 training step(s), loss on training batch is 0.000392102.
After 11286 training step(s), loss on training batch is 0.000545639.
After 11287 training step(s), loss on training batch is 0.000656631.
After 11288 training step(s), loss on training batch is 0.000427194.
After 11289 training step(s), loss on training batch is 0.000364506.
After 11290 training step(s), loss on training batch is 0.000336505.
After 11291 training step(s), loss on training batch is 0.000280397.
After 11292 training step(s), loss on training batch is 0.000354044.
After 11293 training step(s), loss on training batch is 0.000314389.
After 11294 training step(s), loss on training batch is 0.000321065.
After 11295 training step(s), loss on training batch is 0.000372412.
After 11296 training step(s), loss on training batch is 0.000370083.
After 11297 training step(s), loss on training batch is 0.000291237.
After 11298 training step(s), loss on training batch is 0.000276492.
After 11299 training step(s), loss on training batch is 0.000289699.
After 11300 training step(s), loss on training batch is 0.000303987.
After 11301 training step(s), loss on training batch is 0.00101835.
After 11302 training step(s), loss on training batch is 0.000516508.
After 11303 training step(s), loss on training batch is 0.00053862.
After 11304 training step(s), loss on training batch is 0.000644473.
After 11305 training step(s), loss on training batch is 0.000600577.
After 11306 training step(s), loss on training batch is 0.00061612.
After 11307 training step(s), loss on training batch is 0.000610123.
After 11308 training step(s), loss on training batch is 0.000341714.
After 11309 training step(s), loss on training batch is 0.00038292.
After 11310 training step(s), loss on training batch is 0.00040571.
After 11311 training step(s), loss on training batch is 0.000482107.
After 11312 training step(s), loss on training batch is 0.000462758.
After 11313 training step(s), loss on training batch is 0.000482448.
After 11314 training step(s), loss on training batch is 0.000640115.
After 11315 training step(s), loss on training batch is 0.00143033.
After 11316 training step(s), loss on training batch is 0.00101009.
After 11317 training step(s), loss on training batch is 0.000837826.
After 11318 training step(s), loss on training batch is 0.000631368.
After 11319 training step(s), loss on training batch is 0.000397493.
After 11320 training step(s), loss on training batch is 0.00039172.
After 11321 training step(s), loss on training batch is 0.000389297.
After 11322 training step(s), loss on training batch is 0.000347865.
After 11323 training step(s), loss on training batch is 0.000531384.
After 11324 training step(s), loss on training batch is 0.000413507.
After 11325 training step(s), loss on training batch is 0.000400945.
After 11326 training step(s), loss on training batch is 0.000384152.
After 11327 training step(s), loss on training batch is 0.000879681.
After 11328 training step(s), loss on training batch is 0.00245162.
After 11329 training step(s), loss on training batch is 0.000785018.
After 11330 training step(s), loss on training batch is 0.000568983.
After 11331 training step(s), loss on training batch is 0.000547662.
After 11332 training step(s), loss on training batch is 0.000538509.
After 11333 training step(s), loss on training batch is 0.000449854.
After 11334 training step(s), loss on training batch is 0.000362416.
After 11335 training step(s), loss on training batch is 0.000335179.
After 11336 training step(s), loss on training batch is 0.000331206.
After 11337 training step(s), loss on training batch is 0.000379042.
After 11338 training step(s), loss on training batch is 0.000376942.
After 11339 training step(s), loss on training batch is 0.000416857.
After 11340 training step(s), loss on training batch is 0.000460884.
After 11341 training step(s), loss on training batch is 0.000364621.
After 11342 training step(s), loss on training batch is 0.000448732.
After 11343 training step(s), loss on training batch is 0.000433386.
After 11344 training step(s), loss on training batch is 0.00051467.
After 11345 training step(s), loss on training batch is 0.00033752.
After 11346 training step(s), loss on training batch is 0.000460899.
After 11347 training step(s), loss on training batch is 0.00035902.
After 11348 training step(s), loss on training batch is 0.000330655.
After 11349 training step(s), loss on training batch is 0.000371272.
After 11350 training step(s), loss on training batch is 0.000492428.
After 11351 training step(s), loss on training batch is 0.000448342.
After 11352 training step(s), loss on training batch is 0.000398104.
After 11353 training step(s), loss on training batch is 0.00039716.
After 11354 training step(s), loss on training batch is 0.000374644.
After 11355 training step(s), loss on training batch is 0.000358904.
After 11356 training step(s), loss on training batch is 0.00046448.
After 11357 training step(s), loss on training batch is 0.00056165.
After 11358 training step(s), loss on training batch is 0.000354523.
After 11359 training step(s), loss on training batch is 0.00038953.
After 11360 training step(s), loss on training batch is 0.000508795.
After 11361 training step(s), loss on training batch is 0.000347834.
After 11362 training step(s), loss on training batch is 0.000315497.
After 11363 training step(s), loss on training batch is 0.000333806.
After 11364 training step(s), loss on training batch is 0.000458465.
After 11365 training step(s), loss on training batch is 0.00040235.
After 11366 training step(s), loss on training batch is 0.000394086.
After 11367 training step(s), loss on training batch is 0.00040385.
After 11368 training step(s), loss on training batch is 0.000381792.
After 11369 training step(s), loss on training batch is 0.000426881.
After 11370 training step(s), loss on training batch is 0.000354026.
After 11371 training step(s), loss on training batch is 0.000368609.
After 11372 training step(s), loss on training batch is 0.000345974.
After 11373 training step(s), loss on training batch is 0.000330166.
After 11374 training step(s), loss on training batch is 0.000315476.
After 11375 training step(s), loss on training batch is 0.000383245.
After 11376 training step(s), loss on training batch is 0.000375433.
After 11377 training step(s), loss on training batch is 0.000497639.
After 11378 training step(s), loss on training batch is 0.000387131.
After 11379 training step(s), loss on training batch is 0.000361244.
After 11380 training step(s), loss on training batch is 0.000372169.
After 11381 training step(s), loss on training batch is 0.000354536.
After 11382 training step(s), loss on training batch is 0.000371757.
After 11383 training step(s), loss on training batch is 0.000674879.
After 11384 training step(s), loss on training batch is 0.000824548.
After 11385 training step(s), loss on training batch is 0.000993986.
After 11386 training step(s), loss on training batch is 0.000647877.
After 11387 training step(s), loss on training batch is 0.000671194.
After 11388 training step(s), loss on training batch is 0.000600061.
After 11389 training step(s), loss on training batch is 0.000657091.
After 11390 training step(s), loss on training batch is 0.00061462.
After 11391 training step(s), loss on training batch is 0.000704472.
After 11392 training step(s), loss on training batch is 0.000737727.
After 11393 training step(s), loss on training batch is 0.000590724.
After 11394 training step(s), loss on training batch is 0.000568922.
After 11395 training step(s), loss on training batch is 0.000785751.
After 11396 training step(s), loss on training batch is 0.000738265.
After 11397 training step(s), loss on training batch is 0.000609175.
After 11398 training step(s), loss on training batch is 0.000673082.
After 11399 training step(s), loss on training batch is 0.000897618.
After 11400 training step(s), loss on training batch is 0.000680234.
After 11401 training step(s), loss on training batch is 0.000604561.
After 11402 training step(s), loss on training batch is 0.000724221.
After 11403 training step(s), loss on training batch is 0.000591523.
After 11404 training step(s), loss on training batch is 0.000568797.
After 11405 training step(s), loss on training batch is 0.000502077.
After 11406 training step(s), loss on training batch is 0.000690064.
After 11407 training step(s), loss on training batch is 0.000896809.
After 11408 training step(s), loss on training batch is 0.000754252.
After 11409 training step(s), loss on training batch is 0.00053695.
After 11410 training step(s), loss on training batch is 0.000886984.
After 11411 training step(s), loss on training batch is 0.000638568.
After 11412 training step(s), loss on training batch is 0.000644584.
After 11413 training step(s), loss on training batch is 0.000525026.
After 11414 training step(s), loss on training batch is 0.000519466.
After 11415 training step(s), loss on training batch is 0.000496988.
After 11416 training step(s), loss on training batch is 0.000515348.
After 11417 training step(s), loss on training batch is 0.000524567.
After 11418 training step(s), loss on training batch is 0.000772551.
After 11419 training step(s), loss on training batch is 0.00102873.
After 11420 training step(s), loss on training batch is 0.000999446.
After 11421 training step(s), loss on training batch is 0.00124437.
After 11422 training step(s), loss on training batch is 0.000650742.
After 11423 training step(s), loss on training batch is 0.000637262.
After 11424 training step(s), loss on training batch is 0.000656998.
After 11425 training step(s), loss on training batch is 0.000719059.
After 11426 training step(s), loss on training batch is 0.000738551.
After 11427 training step(s), loss on training batch is 0.000638004.
After 11428 training step(s), loss on training batch is 0.000664328.
After 11429 training step(s), loss on training batch is 0.000687888.
After 11430 training step(s), loss on training batch is 0.000738131.
After 11431 training step(s), loss on training batch is 0.000831742.
After 11432 training step(s), loss on training batch is 0.000541493.
After 11433 training step(s), loss on training batch is 0.000573243.
After 11434 training step(s), loss on training batch is 0.000522895.
After 11435 training step(s), loss on training batch is 0.000470557.
After 11436 training step(s), loss on training batch is 0.000770274.
After 11437 training step(s), loss on training batch is 0.00127088.
After 11438 training step(s), loss on training batch is 0.00050545.
After 11439 training step(s), loss on training batch is 0.000586739.
After 11440 training step(s), loss on training batch is 0.000640448.
After 11441 training step(s), loss on training batch is 0.00056919.
After 11442 training step(s), loss on training batch is 0.000551841.
After 11443 training step(s), loss on training batch is 0.000627929.
After 11444 training step(s), loss on training batch is 0.000579399.
After 11445 training step(s), loss on training batch is 0.000632091.
After 11446 training step(s), loss on training batch is 0.000777844.
After 11447 training step(s), loss on training batch is 0.000601799.
After 11448 training step(s), loss on training batch is 0.000597259.
After 11449 training step(s), loss on training batch is 0.000653339.
After 11450 training step(s), loss on training batch is 0.000670756.
After 11451 training step(s), loss on training batch is 0.000570544.
After 11452 training step(s), loss on training batch is 0.000507794.
After 11453 training step(s), loss on training batch is 0.000729498.
After 11454 training step(s), loss on training batch is 0.000600334.
After 11455 training step(s), loss on training batch is 0.000656671.
After 11456 training step(s), loss on training batch is 0.000598017.
After 11457 training step(s), loss on training batch is 0.000534468.
After 11458 training step(s), loss on training batch is 0.00069649.
After 11459 training step(s), loss on training batch is 0.000859739.
After 11460 training step(s), loss on training batch is 0.000572649.
After 11461 training step(s), loss on training batch is 0.000518035.
After 11462 training step(s), loss on training batch is 0.000509651.
After 11463 training step(s), loss on training batch is 0.000535607.
After 11464 training step(s), loss on training batch is 0.000489263.
After 11465 training step(s), loss on training batch is 0.000676646.
After 11466 training step(s), loss on training batch is 0.00106326.
After 11467 training step(s), loss on training batch is 0.00111789.
After 11468 training step(s), loss on training batch is 0.00102326.
After 11469 training step(s), loss on training batch is 0.00105215.
After 11470 training step(s), loss on training batch is 0.000997994.
After 11471 training step(s), loss on training batch is 0.000941397.
After 11472 training step(s), loss on training batch is 0.000986032.
After 11473 training step(s), loss on training batch is 0.00143401.
After 11474 training step(s), loss on training batch is 0.00118269.
After 11475 training step(s), loss on training batch is 0.00124288.
After 11476 training step(s), loss on training batch is 0.00101069.
After 11477 training step(s), loss on training batch is 0.000947031.
After 11478 training step(s), loss on training batch is 0.00103398.
After 11479 training step(s), loss on training batch is 0.000894917.
After 11480 training step(s), loss on training batch is 0.000891729.
After 11481 training step(s), loss on training batch is 0.00086022.
After 11482 training step(s), loss on training batch is 0.000886415.
After 11483 training step(s), loss on training batch is 0.000990592.
After 11484 training step(s), loss on training batch is 0.000892812.
After 11485 training step(s), loss on training batch is 0.0008811.
After 11486 training step(s), loss on training batch is 0.00118882.
After 11487 training step(s), loss on training batch is 0.000895463.
After 11488 training step(s), loss on training batch is 0.00108702.
After 11489 training step(s), loss on training batch is 0.000908584.
After 11490 training step(s), loss on training batch is 0.000901406.
After 11491 training step(s), loss on training batch is 0.000897341.
After 11492 training step(s), loss on training batch is 0.00092451.
After 11493 training step(s), loss on training batch is 0.000855303.
After 11494 training step(s), loss on training batch is 0.000985761.
After 11495 training step(s), loss on training batch is 0.00103503.
After 11496 training step(s), loss on training batch is 0.00111465.
After 11497 training step(s), loss on training batch is 0.000928453.
After 11498 training step(s), loss on training batch is 0.000915522.
After 11499 training step(s), loss on training batch is 0.000868763.
After 11500 training step(s), loss on training batch is 0.000916161.
After 11501 training step(s), loss on training batch is 0.000798807.
After 11502 training step(s), loss on training batch is 0.00100338.
After 11503 training step(s), loss on training batch is 0.000876265.
After 11504 training step(s), loss on training batch is 0.00110019.
After 11505 training step(s), loss on training batch is 0.00114103.
After 11506 training step(s), loss on training batch is 0.0010982.
After 11507 training step(s), loss on training batch is 0.0014316.
After 11508 training step(s), loss on training batch is 0.00109904.
After 11509 training step(s), loss on training batch is 0.00811667.
After 11510 training step(s), loss on training batch is 0.00176325.
After 11511 training step(s), loss on training batch is 0.00156172.
After 11512 training step(s), loss on training batch is 0.00148677.
After 11513 training step(s), loss on training batch is 0.00138086.
After 11514 training step(s), loss on training batch is 0.00137613.
After 11515 training step(s), loss on training batch is 0.00112834.
After 11516 training step(s), loss on training batch is 0.000525498.
After 11517 training step(s), loss on training batch is 0.000447737.
After 11518 training step(s), loss on training batch is 0.000429112.
After 11519 training step(s), loss on training batch is 0.000459004.
After 11520 training step(s), loss on training batch is 0.000484908.
After 11521 training step(s), loss on training batch is 0.000480665.
After 11522 training step(s), loss on training batch is 0.000484729.
After 11523 training step(s), loss on training batch is 0.00048241.
After 11524 training step(s), loss on training batch is 0.000477674.
After 11525 training step(s), loss on training batch is 0.000444195.
After 11526 training step(s), loss on training batch is 0.00087104.
After 11527 training step(s), loss on training batch is 0.000897298.
After 11528 training step(s), loss on training batch is 0.00100629.
After 11529 training step(s), loss on training batch is 0.00088355.
After 11530 training step(s), loss on training batch is 0.00180061.
After 11531 training step(s), loss on training batch is 0.001503.
After 11532 training step(s), loss on training batch is 0.00113804.
After 11533 training step(s), loss on training batch is 0.00110365.
After 11534 training step(s), loss on training batch is 0.000950561.
After 11535 training step(s), loss on training batch is 0.000911475.
After 11536 training step(s), loss on training batch is 0.000935948.
After 11537 training step(s), loss on training batch is 0.000955757.
After 11538 training step(s), loss on training batch is 0.0010794.
After 11539 training step(s), loss on training batch is 0.000948389.
After 11540 training step(s), loss on training batch is 0.000926632.
After 11541 training step(s), loss on training batch is 0.000911473.
After 11542 training step(s), loss on training batch is 0.00159442.
After 11543 training step(s), loss on training batch is 0.000350465.
After 11544 training step(s), loss on training batch is 0.000413657.
After 11545 training step(s), loss on training batch is 0.000375541.
After 11546 training step(s), loss on training batch is 0.000472305.
After 11547 training step(s), loss on training batch is 0.000413991.
After 11548 training step(s), loss on training batch is 0.000414257.
After 11549 training step(s), loss on training batch is 0.000346601.
After 11550 training step(s), loss on training batch is 0.000329312.
After 11551 training step(s), loss on training batch is 0.000558274.
After 11552 training step(s), loss on training batch is 0.000412826.
After 11553 training step(s), loss on training batch is 0.000438072.
After 11554 training step(s), loss on training batch is 0.000367727.
After 11555 training step(s), loss on training batch is 0.000559736.
After 11556 training step(s), loss on training batch is 0.000389767.
After 11557 training step(s), loss on training batch is 0.000338517.
After 11558 training step(s), loss on training batch is 0.00034307.
After 11559 training step(s), loss on training batch is 0.000769535.
After 11560 training step(s), loss on training batch is 0.000975998.
After 11561 training step(s), loss on training batch is 0.000541198.
After 11562 training step(s), loss on training batch is 0.000424482.
After 11563 training step(s), loss on training batch is 0.000391119.
After 11564 training step(s), loss on training batch is 0.000465417.
After 11565 training step(s), loss on training batch is 0.000481934.
After 11566 training step(s), loss on training batch is 0.00043026.
After 11567 training step(s), loss on training batch is 0.000376906.
After 11568 training step(s), loss on training batch is 0.000532077.
After 11569 training step(s), loss on training batch is 0.000333422.
After 11570 training step(s), loss on training batch is 0.000404515.
After 11571 training step(s), loss on training batch is 0.000278297.
After 11572 training step(s), loss on training batch is 0.000305679.
After 11573 training step(s), loss on training batch is 0.000325062.
After 11574 training step(s), loss on training batch is 0.000319851.
After 11575 training step(s), loss on training batch is 0.000474214.
After 11576 training step(s), loss on training batch is 0.00042693.
After 11577 training step(s), loss on training batch is 0.00042492.
After 11578 training step(s), loss on training batch is 0.000333023.
After 11579 training step(s), loss on training batch is 0.000358973.
After 11580 training step(s), loss on training batch is 0.00031777.
After 11581 training step(s), loss on training batch is 0.000359214.
After 11582 training step(s), loss on training batch is 0.000333676.
After 11583 training step(s), loss on training batch is 0.000410238.
After 11584 training step(s), loss on training batch is 0.0003956.
After 11585 training step(s), loss on training batch is 0.000411069.
After 11586 training step(s), loss on training batch is 0.000544806.
After 11587 training step(s), loss on training batch is 0.000429455.
After 11588 training step(s), loss on training batch is 0.000330423.
After 11589 training step(s), loss on training batch is 0.000381834.
After 11590 training step(s), loss on training batch is 0.000354544.
After 11591 training step(s), loss on training batch is 0.000319753.
After 11592 training step(s), loss on training batch is 0.000339879.
After 11593 training step(s), loss on training batch is 0.000333371.
After 11594 training step(s), loss on training batch is 0.00027968.
After 11595 training step(s), loss on training batch is 0.000450188.
After 11596 training step(s), loss on training batch is 0.000444843.
After 11597 training step(s), loss on training batch is 0.000256585.
After 11598 training step(s), loss on training batch is 0.000266886.
After 11599 training step(s), loss on training batch is 0.000340588.
After 11600 training step(s), loss on training batch is 0.000362416.
After 11601 training step(s), loss on training batch is 0.000527544.
After 11602 training step(s), loss on training batch is 0.000606643.
After 11603 training step(s), loss on training batch is 0.000379232.
After 11604 training step(s), loss on training batch is 0.000525653.
After 11605 training step(s), loss on training batch is 0.000989164.
After 11606 training step(s), loss on training batch is 0.000692298.
After 11607 training step(s), loss on training batch is 0.000400234.
After 11608 training step(s), loss on training batch is 0.000480767.
After 11609 training step(s), loss on training batch is 0.000584173.
After 11610 training step(s), loss on training batch is 0.00060666.
After 11611 training step(s), loss on training batch is 0.000398474.
After 11612 training step(s), loss on training batch is 0.000320911.
After 11613 training step(s), loss on training batch is 0.000376191.
After 11614 training step(s), loss on training batch is 0.000402069.
After 11615 training step(s), loss on training batch is 0.000369154.
After 11616 training step(s), loss on training batch is 0.000314662.
After 11617 training step(s), loss on training batch is 0.000403224.
After 11618 training step(s), loss on training batch is 0.000337362.
After 11619 training step(s), loss on training batch is 0.000428437.
After 11620 training step(s), loss on training batch is 0.000388721.
After 11621 training step(s), loss on training batch is 0.000303213.
After 11622 training step(s), loss on training batch is 0.000380691.
After 11623 training step(s), loss on training batch is 0.000342353.
After 11624 training step(s), loss on training batch is 0.000311432.
After 11625 training step(s), loss on training batch is 0.000307642.
After 11626 training step(s), loss on training batch is 0.000372228.
After 11627 training step(s), loss on training batch is 0.00030404.
After 11628 training step(s), loss on training batch is 0.000912108.
After 11629 training step(s), loss on training batch is 0.000596663.
After 11630 training step(s), loss on training batch is 0.000538862.
After 11631 training step(s), loss on training batch is 0.000547514.
After 11632 training step(s), loss on training batch is 0.000659116.
After 11633 training step(s), loss on training batch is 0.000723563.
After 11634 training step(s), loss on training batch is 0.000616585.
After 11635 training step(s), loss on training batch is 0.000480559.
After 11636 training step(s), loss on training batch is 0.000710874.
After 11637 training step(s), loss on training batch is 0.000713764.
After 11638 training step(s), loss on training batch is 0.000506759.
After 11639 training step(s), loss on training batch is 0.000492644.
After 11640 training step(s), loss on training batch is 0.00115425.
After 11641 training step(s), loss on training batch is 0.000512568.
After 11642 training step(s), loss on training batch is 0.000578315.
After 11643 training step(s), loss on training batch is 0.000590349.
After 11644 training step(s), loss on training batch is 0.000681868.
After 11645 training step(s), loss on training batch is 0.000496073.
After 11646 training step(s), loss on training batch is 0.000674788.
After 11647 training step(s), loss on training batch is 0.000541765.
After 11648 training step(s), loss on training batch is 0.000595627.
After 11649 training step(s), loss on training batch is 0.000632345.
After 11650 training step(s), loss on training batch is 0.000621774.
After 11651 training step(s), loss on training batch is 0.00061018.
After 11652 training step(s), loss on training batch is 0.00055545.
After 11653 training step(s), loss on training batch is 0.000638428.
After 11654 training step(s), loss on training batch is 0.000434471.
After 11655 training step(s), loss on training batch is 0.000600071.
After 11656 training step(s), loss on training batch is 0.00105362.
After 11657 training step(s), loss on training batch is 0.000885574.
After 11658 training step(s), loss on training batch is 0.000958473.
After 11659 training step(s), loss on training batch is 0.0009463.
After 11660 training step(s), loss on training batch is 0.000815908.
After 11661 training step(s), loss on training batch is 0.000852142.
After 11662 training step(s), loss on training batch is 0.00103958.
After 11663 training step(s), loss on training batch is 0.000917659.
After 11664 training step(s), loss on training batch is 0.000822454.
After 11665 training step(s), loss on training batch is 0.00114636.
After 11666 training step(s), loss on training batch is 0.000813923.
After 11667 training step(s), loss on training batch is 0.000819927.
After 11668 training step(s), loss on training batch is 0.000985432.
After 11669 training step(s), loss on training batch is 0.00155315.
After 11670 training step(s), loss on training batch is 0.00342321.
After 11671 training step(s), loss on training batch is 0.00142648.
After 11672 training step(s), loss on training batch is 0.00107686.
After 11673 training step(s), loss on training batch is 0.001215.
After 11674 training step(s), loss on training batch is 0.00107994.
After 11675 training step(s), loss on training batch is 0.00100056.
After 11676 training step(s), loss on training batch is 0.000937417.
After 11677 training step(s), loss on training batch is 0.00129432.
After 11678 training step(s), loss on training batch is 0.000924973.
After 11679 training step(s), loss on training batch is 0.00090617.
After 11680 training step(s), loss on training batch is 0.000877032.
After 11681 training step(s), loss on training batch is 0.000542087.
After 11682 training step(s), loss on training batch is 0.000441782.
After 11683 training step(s), loss on training batch is 0.000288495.
After 11684 training step(s), loss on training batch is 0.000359809.
After 11685 training step(s), loss on training batch is 0.000388546.
After 11686 training step(s), loss on training batch is 0.000532202.
After 11687 training step(s), loss on training batch is 0.000646221.
After 11688 training step(s), loss on training batch is 0.000425215.
After 11689 training step(s), loss on training batch is 0.000366519.
After 11690 training step(s), loss on training batch is 0.000336577.
After 11691 training step(s), loss on training batch is 0.000282844.
After 11692 training step(s), loss on training batch is 0.000349399.
After 11693 training step(s), loss on training batch is 0.000318296.
After 11694 training step(s), loss on training batch is 0.000318978.
After 11695 training step(s), loss on training batch is 0.000362061.
After 11696 training step(s), loss on training batch is 0.000368589.
After 11697 training step(s), loss on training batch is 0.000290224.
After 11698 training step(s), loss on training batch is 0.00027968.
After 11699 training step(s), loss on training batch is 0.000290997.
After 11700 training step(s), loss on training batch is 0.000296304.
After 11701 training step(s), loss on training batch is 0.000776373.
After 11702 training step(s), loss on training batch is 0.000560407.
After 11703 training step(s), loss on training batch is 0.000643532.
After 11704 training step(s), loss on training batch is 0.000742547.
After 11705 training step(s), loss on training batch is 0.000681074.
After 11706 training step(s), loss on training batch is 0.000639882.
After 11707 training step(s), loss on training batch is 0.000595366.
After 11708 training step(s), loss on training batch is 0.000371453.
After 11709 training step(s), loss on training batch is 0.000414943.
After 11710 training step(s), loss on training batch is 0.000427495.
After 11711 training step(s), loss on training batch is 0.000515007.
After 11712 training step(s), loss on training batch is 0.000496836.
After 11713 training step(s), loss on training batch is 0.000484758.
After 11714 training step(s), loss on training batch is 0.000580974.
After 11715 training step(s), loss on training batch is 0.0013441.
After 11716 training step(s), loss on training batch is 0.000985087.
After 11717 training step(s), loss on training batch is 0.000842218.
After 11718 training step(s), loss on training batch is 0.000648563.
After 11719 training step(s), loss on training batch is 0.00039217.
After 11720 training step(s), loss on training batch is 0.000377733.
After 11721 training step(s), loss on training batch is 0.000387034.
After 11722 training step(s), loss on training batch is 0.000337933.
After 11723 training step(s), loss on training batch is 0.000524023.
After 11724 training step(s), loss on training batch is 0.000401327.
After 11725 training step(s), loss on training batch is 0.00038787.
After 11726 training step(s), loss on training batch is 0.000378412.
After 11727 training step(s), loss on training batch is 0.000811096.
After 11728 training step(s), loss on training batch is 0.00193019.
After 11729 training step(s), loss on training batch is 0.000717996.
After 11730 training step(s), loss on training batch is 0.000538864.
After 11731 training step(s), loss on training batch is 0.000544167.
After 11732 training step(s), loss on training batch is 0.000526751.
After 11733 training step(s), loss on training batch is 0.00044581.
After 11734 training step(s), loss on training batch is 0.000354705.
After 11735 training step(s), loss on training batch is 0.000320316.
After 11736 training step(s), loss on training batch is 0.000318022.
After 11737 training step(s), loss on training batch is 0.000364043.
After 11738 training step(s), loss on training batch is 0.000358403.
After 11739 training step(s), loss on training batch is 0.000397204.
After 11740 training step(s), loss on training batch is 0.000444211.
After 11741 training step(s), loss on training batch is 0.00035427.
After 11742 training step(s), loss on training batch is 0.000434066.
After 11743 training step(s), loss on training batch is 0.000419211.
After 11744 training step(s), loss on training batch is 0.000512058.
After 11745 training step(s), loss on training batch is 0.000338841.
After 11746 training step(s), loss on training batch is 0.000505909.
After 11747 training step(s), loss on training batch is 0.00036507.
After 11748 training step(s), loss on training batch is 0.000319117.
After 11749 training step(s), loss on training batch is 0.000369811.
After 11750 training step(s), loss on training batch is 0.00046696.
After 11751 training step(s), loss on training batch is 0.000437887.
After 11752 training step(s), loss on training batch is 0.000402967.
After 11753 training step(s), loss on training batch is 0.000400238.
After 11754 training step(s), loss on training batch is 0.000383113.
After 11755 training step(s), loss on training batch is 0.000360066.
After 11756 training step(s), loss on training batch is 0.000453664.
After 11757 training step(s), loss on training batch is 0.000548038.
After 11758 training step(s), loss on training batch is 0.000353385.
After 11759 training step(s), loss on training batch is 0.000396683.
After 11760 training step(s), loss on training batch is 0.000494089.
After 11761 training step(s), loss on training batch is 0.000347057.
After 11762 training step(s), loss on training batch is 0.000316541.
After 11763 training step(s), loss on training batch is 0.000331538.
After 11764 training step(s), loss on training batch is 0.000456703.
After 11765 training step(s), loss on training batch is 0.000407723.
After 11766 training step(s), loss on training batch is 0.000404702.
After 11767 training step(s), loss on training batch is 0.000401819.
After 11768 training step(s), loss on training batch is 0.0003658.
After 11769 training step(s), loss on training batch is 0.000428827.
After 11770 training step(s), loss on training batch is 0.000351994.
After 11771 training step(s), loss on training batch is 0.000370618.
After 11772 training step(s), loss on training batch is 0.000345481.
After 11773 training step(s), loss on training batch is 0.000326637.
After 11774 training step(s), loss on training batch is 0.000312417.
After 11775 training step(s), loss on training batch is 0.000385365.
After 11776 training step(s), loss on training batch is 0.000371256.
After 11777 training step(s), loss on training batch is 0.000489822.
After 11778 training step(s), loss on training batch is 0.000381412.
After 11779 training step(s), loss on training batch is 0.000355451.
After 11780 training step(s), loss on training batch is 0.000373919.
After 11781 training step(s), loss on training batch is 0.000350355.
After 11782 training step(s), loss on training batch is 0.000374067.
After 11783 training step(s), loss on training batch is 0.00068387.
After 11784 training step(s), loss on training batch is 0.000821616.
After 11785 training step(s), loss on training batch is 0.000988722.
After 11786 training step(s), loss on training batch is 0.000644862.
After 11787 training step(s), loss on training batch is 0.000674519.
After 11788 training step(s), loss on training batch is 0.000589912.
After 11789 training step(s), loss on training batch is 0.000606906.
After 11790 training step(s), loss on training batch is 0.000592878.
After 11791 training step(s), loss on training batch is 0.000695064.
After 11792 training step(s), loss on training batch is 0.000758095.
After 11793 training step(s), loss on training batch is 0.000578966.
After 11794 training step(s), loss on training batch is 0.000575071.
After 11795 training step(s), loss on training batch is 0.000796971.
After 11796 training step(s), loss on training batch is 0.000711782.
After 11797 training step(s), loss on training batch is 0.000619007.
After 11798 training step(s), loss on training batch is 0.000657934.
After 11799 training step(s), loss on training batch is 0.00089391.
After 11800 training step(s), loss on training batch is 0.000674198.
After 11801 training step(s), loss on training batch is 0.00060464.
After 11802 training step(s), loss on training batch is 0.000709597.
After 11803 training step(s), loss on training batch is 0.000600444.
After 11804 training step(s), loss on training batch is 0.000567701.
After 11805 training step(s), loss on training batch is 0.00049962.
After 11806 training step(s), loss on training batch is 0.00066829.
After 11807 training step(s), loss on training batch is 0.000861421.
After 11808 training step(s), loss on training batch is 0.000737423.
After 11809 training step(s), loss on training batch is 0.00053206.
After 11810 training step(s), loss on training batch is 0.000873099.
After 11811 training step(s), loss on training batch is 0.000638191.
After 11812 training step(s), loss on training batch is 0.000636699.
After 11813 training step(s), loss on training batch is 0.00052375.
After 11814 training step(s), loss on training batch is 0.000519397.
After 11815 training step(s), loss on training batch is 0.000498361.
After 11816 training step(s), loss on training batch is 0.000512845.
After 11817 training step(s), loss on training batch is 0.000525298.
After 11818 training step(s), loss on training batch is 0.000727022.
After 11819 training step(s), loss on training batch is 0.000985486.
After 11820 training step(s), loss on training batch is 0.000938289.
After 11821 training step(s), loss on training batch is 0.0012489.
After 11822 training step(s), loss on training batch is 0.000643906.
After 11823 training step(s), loss on training batch is 0.000631119.
After 11824 training step(s), loss on training batch is 0.000643141.
After 11825 training step(s), loss on training batch is 0.000705406.
After 11826 training step(s), loss on training batch is 0.000731232.
After 11827 training step(s), loss on training batch is 0.000626524.
After 11828 training step(s), loss on training batch is 0.000660245.
After 11829 training step(s), loss on training batch is 0.000681257.
After 11830 training step(s), loss on training batch is 0.000730548.
After 11831 training step(s), loss on training batch is 0.000821415.
After 11832 training step(s), loss on training batch is 0.00056967.
After 11833 training step(s), loss on training batch is 0.000594458.
After 11834 training step(s), loss on training batch is 0.00054997.
After 11835 training step(s), loss on training batch is 0.000498055.
After 11836 training step(s), loss on training batch is 0.000737022.
After 11837 training step(s), loss on training batch is 0.00116275.
After 11838 training step(s), loss on training batch is 0.000501595.
After 11839 training step(s), loss on training batch is 0.000585468.
After 11840 training step(s), loss on training batch is 0.000623713.
After 11841 training step(s), loss on training batch is 0.000576916.
After 11842 training step(s), loss on training batch is 0.000555181.
After 11843 training step(s), loss on training batch is 0.00063482.
After 11844 training step(s), loss on training batch is 0.000588224.
After 11845 training step(s), loss on training batch is 0.000625354.
After 11846 training step(s), loss on training batch is 0.000754838.
After 11847 training step(s), loss on training batch is 0.000593251.
After 11848 training step(s), loss on training batch is 0.000600063.
After 11849 training step(s), loss on training batch is 0.000647748.
After 11850 training step(s), loss on training batch is 0.000666584.
After 11851 training step(s), loss on training batch is 0.000576281.
After 11852 training step(s), loss on training batch is 0.000496471.
After 11853 training step(s), loss on training batch is 0.000734493.
After 11854 training step(s), loss on training batch is 0.000591769.
After 11855 training step(s), loss on training batch is 0.000649479.
After 11856 training step(s), loss on training batch is 0.00058788.
After 11857 training step(s), loss on training batch is 0.000532218.
After 11858 training step(s), loss on training batch is 0.00069826.
After 11859 training step(s), loss on training batch is 0.000845581.
After 11860 training step(s), loss on training batch is 0.000568098.
After 11861 training step(s), loss on training batch is 0.000504097.
After 11862 training step(s), loss on training batch is 0.000506671.
After 11863 training step(s), loss on training batch is 0.000529153.
After 11864 training step(s), loss on training batch is 0.000486942.
After 11865 training step(s), loss on training batch is 0.000667394.
After 11866 training step(s), loss on training batch is 0.00105118.
After 11867 training step(s), loss on training batch is 0.00110819.
After 11868 training step(s), loss on training batch is 0.000993725.
After 11869 training step(s), loss on training batch is 0.00102674.
After 11870 training step(s), loss on training batch is 0.00098667.
After 11871 training step(s), loss on training batch is 0.000933982.
After 11872 training step(s), loss on training batch is 0.000977705.
After 11873 training step(s), loss on training batch is 0.0013672.
After 11874 training step(s), loss on training batch is 0.00117088.
After 11875 training step(s), loss on training batch is 0.00121105.
After 11876 training step(s), loss on training batch is 0.000931695.
After 11877 training step(s), loss on training batch is 0.000904154.
After 11878 training step(s), loss on training batch is 0.00102065.
After 11879 training step(s), loss on training batch is 0.00087194.
After 11880 training step(s), loss on training batch is 0.000875039.
After 11881 training step(s), loss on training batch is 0.000829211.
After 11882 training step(s), loss on training batch is 0.000872591.
After 11883 training step(s), loss on training batch is 0.00103116.
After 11884 training step(s), loss on training batch is 0.000887472.
After 11885 training step(s), loss on training batch is 0.000861544.
After 11886 training step(s), loss on training batch is 0.00121337.
After 11887 training step(s), loss on training batch is 0.000884683.
After 11888 training step(s), loss on training batch is 0.00108025.
After 11889 training step(s), loss on training batch is 0.000895907.
After 11890 training step(s), loss on training batch is 0.000888735.
After 11891 training step(s), loss on training batch is 0.000880442.
After 11892 training step(s), loss on training batch is 0.00095549.
After 11893 training step(s), loss on training batch is 0.0008597.
After 11894 training step(s), loss on training batch is 0.000989759.
After 11895 training step(s), loss on training batch is 0.00105225.
After 11896 training step(s), loss on training batch is 0.00109795.
After 11897 training step(s), loss on training batch is 0.000921212.
After 11898 training step(s), loss on training batch is 0.000903975.
After 11899 training step(s), loss on training batch is 0.00081583.
After 11900 training step(s), loss on training batch is 0.000884032.
After 11901 training step(s), loss on training batch is 0.000788745.
After 11902 training step(s), loss on training batch is 0.00100805.
After 11903 training step(s), loss on training batch is 0.000862863.
After 11904 training step(s), loss on training batch is 0.00108736.
After 11905 training step(s), loss on training batch is 0.00112761.
After 11906 training step(s), loss on training batch is 0.00108712.
After 11907 training step(s), loss on training batch is 0.00124886.
After 11908 training step(s), loss on training batch is 0.0010936.
After 11909 training step(s), loss on training batch is 0.00642856.
After 11910 training step(s), loss on training batch is 0.001798.
After 11911 training step(s), loss on training batch is 0.00154728.
After 11912 training step(s), loss on training batch is 0.00146196.
After 11913 training step(s), loss on training batch is 0.00137041.
After 11914 training step(s), loss on training batch is 0.00136395.
After 11915 training step(s), loss on training batch is 0.0011401.
After 11916 training step(s), loss on training batch is 0.000528883.
After 11917 training step(s), loss on training batch is 0.000471141.
After 11918 training step(s), loss on training batch is 0.000444015.
After 11919 training step(s), loss on training batch is 0.000471708.
After 11920 training step(s), loss on training batch is 0.00049362.
After 11921 training step(s), loss on training batch is 0.000490549.
After 11922 training step(s), loss on training batch is 0.000498604.
After 11923 training step(s), loss on training batch is 0.000494398.
After 11924 training step(s), loss on training batch is 0.000482444.
After 11925 training step(s), loss on training batch is 0.000444475.
After 11926 training step(s), loss on training batch is 0.000871819.
After 11927 training step(s), loss on training batch is 0.000890047.
After 11928 training step(s), loss on training batch is 0.000993165.
After 11929 training step(s), loss on training batch is 0.000860778.
After 11930 training step(s), loss on training batch is 0.00182655.
After 11931 training step(s), loss on training batch is 0.00150682.
After 11932 training step(s), loss on training batch is 0.00111365.
After 11933 training step(s), loss on training batch is 0.00109582.
After 11934 training step(s), loss on training batch is 0.000939803.
After 11935 training step(s), loss on training batch is 0.000908087.
After 11936 training step(s), loss on training batch is 0.000925088.
After 11937 training step(s), loss on training batch is 0.00093511.
After 11938 training step(s), loss on training batch is 0.00105605.
After 11939 training step(s), loss on training batch is 0.000934783.
After 11940 training step(s), loss on training batch is 0.000917653.
After 11941 training step(s), loss on training batch is 0.000906768.
After 11942 training step(s), loss on training batch is 0.00154344.
After 11943 training step(s), loss on training batch is 0.000341786.
After 11944 training step(s), loss on training batch is 0.000393987.
After 11945 training step(s), loss on training batch is 0.00037015.
After 11946 training step(s), loss on training batch is 0.000467648.
After 11947 training step(s), loss on training batch is 0.000444871.
After 11948 training step(s), loss on training batch is 0.000402418.
After 11949 training step(s), loss on training batch is 0.000356772.
After 11950 training step(s), loss on training batch is 0.000324601.
After 11951 training step(s), loss on training batch is 0.000505301.
After 11952 training step(s), loss on training batch is 0.000427572.
After 11953 training step(s), loss on training batch is 0.000418117.
After 11954 training step(s), loss on training batch is 0.00036637.
After 11955 training step(s), loss on training batch is 0.000548725.
After 11956 training step(s), loss on training batch is 0.000390964.
After 11957 training step(s), loss on training batch is 0.000336051.
After 11958 training step(s), loss on training batch is 0.000336377.
After 11959 training step(s), loss on training batch is 0.000765843.
After 11960 training step(s), loss on training batch is 0.000966773.
After 11961 training step(s), loss on training batch is 0.000535982.
After 11962 training step(s), loss on training batch is 0.00041752.
After 11963 training step(s), loss on training batch is 0.000386542.
After 11964 training step(s), loss on training batch is 0.000461836.
After 11965 training step(s), loss on training batch is 0.000475827.
After 11966 training step(s), loss on training batch is 0.000425434.
After 11967 training step(s), loss on training batch is 0.000374327.
After 11968 training step(s), loss on training batch is 0.000527116.
After 11969 training step(s), loss on training batch is 0.000325896.
After 11970 training step(s), loss on training batch is 0.000408194.
After 11971 training step(s), loss on training batch is 0.00027802.
After 11972 training step(s), loss on training batch is 0.000299688.
After 11973 training step(s), loss on training batch is 0.000324181.
After 11974 training step(s), loss on training batch is 0.000320891.
After 11975 training step(s), loss on training batch is 0.000456803.
After 11976 training step(s), loss on training batch is 0.000421343.
After 11977 training step(s), loss on training batch is 0.000419708.
After 11978 training step(s), loss on training batch is 0.000330776.
After 11979 training step(s), loss on training batch is 0.000355286.
After 11980 training step(s), loss on training batch is 0.000315554.
After 11981 training step(s), loss on training batch is 0.000355911.
After 11982 training step(s), loss on training batch is 0.000328085.
After 11983 training step(s), loss on training batch is 0.000404998.
After 11984 training step(s), loss on training batch is 0.000391857.
After 11985 training step(s), loss on training batch is 0.000406778.
After 11986 training step(s), loss on training batch is 0.000536802.
After 11987 training step(s), loss on training batch is 0.000426556.
After 11988 training step(s), loss on training batch is 0.000328242.
After 11989 training step(s), loss on training batch is 0.000378848.
After 11990 training step(s), loss on training batch is 0.000347074.
After 11991 training step(s), loss on training batch is 0.000316307.
After 11992 training step(s), loss on training batch is 0.000323324.
After 11993 training step(s), loss on training batch is 0.000316936.
After 11994 training step(s), loss on training batch is 0.00027113.
After 11995 training step(s), loss on training batch is 0.000462935.
After 11996 training step(s), loss on training batch is 0.00044386.
After 11997 training step(s), loss on training batch is 0.000248626.
After 11998 training step(s), loss on training batch is 0.000264919.
After 11999 training step(s), loss on training batch is 0.000336771.
After 12000 training step(s), loss on training batch is 0.000359546.
After 12001 training step(s), loss on training batch is 0.000530447.
After 12002 training step(s), loss on training batch is 0.000605463.
After 12003 training step(s), loss on training batch is 0.000356875.
After 12004 training step(s), loss on training batch is 0.000516325.
After 12005 training step(s), loss on training batch is 0.000984897.
After 12006 training step(s), loss on training batch is 0.000691927.
After 12007 training step(s), loss on training batch is 0.00038146.
After 12008 training step(s), loss on training batch is 0.000465024.
After 12009 training step(s), loss on training batch is 0.000579459.
After 12010 training step(s), loss on training batch is 0.000603698.
After 12011 training step(s), loss on training batch is 0.000389544.
After 12012 training step(s), loss on training batch is 0.00030233.
After 12013 training step(s), loss on training batch is 0.000354601.
After 12014 training step(s), loss on training batch is 0.000379692.
After 12015 training step(s), loss on training batch is 0.000352338.
After 12016 training step(s), loss on training batch is 0.000307458.
After 12017 training step(s), loss on training batch is 0.000374224.
After 12018 training step(s), loss on training batch is 0.000346908.
After 12019 training step(s), loss on training batch is 0.000404646.
After 12020 training step(s), loss on training batch is 0.000373572.
After 12021 training step(s), loss on training batch is 0.000305088.
After 12022 training step(s), loss on training batch is 0.000360327.
After 12023 training step(s), loss on training batch is 0.000352657.
After 12024 training step(s), loss on training batch is 0.000313536.
After 12025 training step(s), loss on training batch is 0.000304774.
After 12026 training step(s), loss on training batch is 0.000362825.
After 12027 training step(s), loss on training batch is 0.000290982.
After 12028 training step(s), loss on training batch is 0.000949652.
After 12029 training step(s), loss on training batch is 0.000600773.
After 12030 training step(s), loss on training batch is 0.000529658.
After 12031 training step(s), loss on training batch is 0.000543303.
After 12032 training step(s), loss on training batch is 0.000637085.
After 12033 training step(s), loss on training batch is 0.000714567.
After 12034 training step(s), loss on training batch is 0.000614842.
After 12035 training step(s), loss on training batch is 0.000472111.
After 12036 training step(s), loss on training batch is 0.000696182.
After 12037 training step(s), loss on training batch is 0.00068401.
After 12038 training step(s), loss on training batch is 0.000495875.
After 12039 training step(s), loss on training batch is 0.000482377.
After 12040 training step(s), loss on training batch is 0.00110444.
After 12041 training step(s), loss on training batch is 0.000510487.
After 12042 training step(s), loss on training batch is 0.000577387.
After 12043 training step(s), loss on training batch is 0.000590587.
After 12044 training step(s), loss on training batch is 0.000674506.
After 12045 training step(s), loss on training batch is 0.000494412.
After 12046 training step(s), loss on training batch is 0.000662274.
After 12047 training step(s), loss on training batch is 0.000532567.
After 12048 training step(s), loss on training batch is 0.00058215.
After 12049 training step(s), loss on training batch is 0.000628244.
After 12050 training step(s), loss on training batch is 0.000610514.
After 12051 training step(s), loss on training batch is 0.000600978.
After 12052 training step(s), loss on training batch is 0.000541731.
After 12053 training step(s), loss on training batch is 0.000605895.
After 12054 training step(s), loss on training batch is 0.000440231.
After 12055 training step(s), loss on training batch is 0.000585268.
After 12056 training step(s), loss on training batch is 0.00105268.
After 12057 training step(s), loss on training batch is 0.000883983.
After 12058 training step(s), loss on training batch is 0.000942865.
After 12059 training step(s), loss on training batch is 0.000937298.
After 12060 training step(s), loss on training batch is 0.000812977.
After 12061 training step(s), loss on training batch is 0.000835741.
After 12062 training step(s), loss on training batch is 0.00103281.
After 12063 training step(s), loss on training batch is 0.000903744.
After 12064 training step(s), loss on training batch is 0.00080934.
After 12065 training step(s), loss on training batch is 0.00109914.
After 12066 training step(s), loss on training batch is 0.000798036.
After 12067 training step(s), loss on training batch is 0.000800375.
After 12068 training step(s), loss on training batch is 0.000966633.
After 12069 training step(s), loss on training batch is 0.00159583.
After 12070 training step(s), loss on training batch is 0.00310753.
After 12071 training step(s), loss on training batch is 0.00141729.
After 12072 training step(s), loss on training batch is 0.00107727.
After 12073 training step(s), loss on training batch is 0.00122786.
After 12074 training step(s), loss on training batch is 0.00106498.
After 12075 training step(s), loss on training batch is 0.00100984.
After 12076 training step(s), loss on training batch is 0.000929829.
After 12077 training step(s), loss on training batch is 0.00126505.
After 12078 training step(s), loss on training batch is 0.000883745.
After 12079 training step(s), loss on training batch is 0.000851213.
After 12080 training step(s), loss on training batch is 0.000822494.
After 12081 training step(s), loss on training batch is 0.000548974.
After 12082 training step(s), loss on training batch is 0.000445265.
After 12083 training step(s), loss on training batch is 0.000283593.
After 12084 training step(s), loss on training batch is 0.000351261.
After 12085 training step(s), loss on training batch is 0.000384136.
After 12086 training step(s), loss on training batch is 0.000531421.
After 12087 training step(s), loss on training batch is 0.000636782.
After 12088 training step(s), loss on training batch is 0.000414674.
After 12089 training step(s), loss on training batch is 0.000360197.
After 12090 training step(s), loss on training batch is 0.000329515.
After 12091 training step(s), loss on training batch is 0.000272472.
After 12092 training step(s), loss on training batch is 0.000343454.
After 12093 training step(s), loss on training batch is 0.000308412.
After 12094 training step(s), loss on training batch is 0.00031258.
After 12095 training step(s), loss on training batch is 0.000357822.
After 12096 training step(s), loss on training batch is 0.000361522.
After 12097 training step(s), loss on training batch is 0.00028272.
After 12098 training step(s), loss on training batch is 0.000267855.
After 12099 training step(s), loss on training batch is 0.000283033.
After 12100 training step(s), loss on training batch is 0.000295792.
After 12101 training step(s), loss on training batch is 0.000769395.
After 12102 training step(s), loss on training batch is 0.000520265.
After 12103 training step(s), loss on training batch is 0.000579534.
After 12104 training step(s), loss on training batch is 0.00068137.
After 12105 training step(s), loss on training batch is 0.000632351.
After 12106 training step(s), loss on training batch is 0.000617185.
After 12107 training step(s), loss on training batch is 0.000582879.
After 12108 training step(s), loss on training batch is 0.00035322.
After 12109 training step(s), loss on training batch is 0.000394127.
After 12110 training step(s), loss on training batch is 0.000411513.
After 12111 training step(s), loss on training batch is 0.000491801.
After 12112 training step(s), loss on training batch is 0.000478826.
After 12113 training step(s), loss on training batch is 0.000470776.
After 12114 training step(s), loss on training batch is 0.000611999.
After 12115 training step(s), loss on training batch is 0.00134086.
After 12116 training step(s), loss on training batch is 0.000980746.
After 12117 training step(s), loss on training batch is 0.000820053.
After 12118 training step(s), loss on training batch is 0.00063158.
After 12119 training step(s), loss on training batch is 0.000389485.
After 12120 training step(s), loss on training batch is 0.000383236.
After 12121 training step(s), loss on training batch is 0.00038981.
After 12122 training step(s), loss on training batch is 0.000343142.
After 12123 training step(s), loss on training batch is 0.000532143.
After 12124 training step(s), loss on training batch is 0.000411557.
After 12125 training step(s), loss on training batch is 0.000397652.
After 12126 training step(s), loss on training batch is 0.000391368.
After 12127 training step(s), loss on training batch is 0.000769991.
After 12128 training step(s), loss on training batch is 0.00182246.
After 12129 training step(s), loss on training batch is 0.000685951.
After 12130 training step(s), loss on training batch is 0.000523495.
After 12131 training step(s), loss on training batch is 0.000552706.
After 12132 training step(s), loss on training batch is 0.00052141.
After 12133 training step(s), loss on training batch is 0.000439464.
After 12134 training step(s), loss on training batch is 0.000351172.
After 12135 training step(s), loss on training batch is 0.000322733.
After 12136 training step(s), loss on training batch is 0.000321555.
After 12137 training step(s), loss on training batch is 0.000366328.
After 12138 training step(s), loss on training batch is 0.00036581.
After 12139 training step(s), loss on training batch is 0.000406462.
After 12140 training step(s), loss on training batch is 0.000458168.
After 12141 training step(s), loss on training batch is 0.0003638.
After 12142 training step(s), loss on training batch is 0.00044558.
After 12143 training step(s), loss on training batch is 0.000431455.
After 12144 training step(s), loss on training batch is 0.0004981.
After 12145 training step(s), loss on training batch is 0.000335002.
After 12146 training step(s), loss on training batch is 0.000452749.
After 12147 training step(s), loss on training batch is 0.000353406.
After 12148 training step(s), loss on training batch is 0.00032575.
After 12149 training step(s), loss on training batch is 0.000367667.
After 12150 training step(s), loss on training batch is 0.000485114.
After 12151 training step(s), loss on training batch is 0.000446003.
After 12152 training step(s), loss on training batch is 0.000392475.
After 12153 training step(s), loss on training batch is 0.000398426.
After 12154 training step(s), loss on training batch is 0.000369149.
After 12155 training step(s), loss on training batch is 0.000353846.
After 12156 training step(s), loss on training batch is 0.000461883.
After 12157 training step(s), loss on training batch is 0.00054472.
After 12158 training step(s), loss on training batch is 0.000351301.
After 12159 training step(s), loss on training batch is 0.000393271.
After 12160 training step(s), loss on training batch is 0.000496461.
After 12161 training step(s), loss on training batch is 0.000344332.
After 12162 training step(s), loss on training batch is 0.000316035.
After 12163 training step(s), loss on training batch is 0.000330482.
After 12164 training step(s), loss on training batch is 0.000450013.
After 12165 training step(s), loss on training batch is 0.000403765.
After 12166 training step(s), loss on training batch is 0.000398954.
After 12167 training step(s), loss on training batch is 0.000397305.
After 12168 training step(s), loss on training batch is 0.000365146.
After 12169 training step(s), loss on training batch is 0.000419967.
After 12170 training step(s), loss on training batch is 0.000352201.
After 12171 training step(s), loss on training batch is 0.000368255.
After 12172 training step(s), loss on training batch is 0.000348046.
After 12173 training step(s), loss on training batch is 0.000337744.
After 12174 training step(s), loss on training batch is 0.000317351.
After 12175 training step(s), loss on training batch is 0.000379398.
After 12176 training step(s), loss on training batch is 0.000358054.
After 12177 training step(s), loss on training batch is 0.000582655.
After 12178 training step(s), loss on training batch is 0.000376722.
After 12179 training step(s), loss on training batch is 0.000336757.
After 12180 training step(s), loss on training batch is 0.000382353.
After 12181 training step(s), loss on training batch is 0.000345758.
After 12182 training step(s), loss on training batch is 0.000377399.
After 12183 training step(s), loss on training batch is 0.000719042.
After 12184 training step(s), loss on training batch is 0.00082363.
After 12185 training step(s), loss on training batch is 0.000990423.
After 12186 training step(s), loss on training batch is 0.000632004.
After 12187 training step(s), loss on training batch is 0.000671589.
After 12188 training step(s), loss on training batch is 0.000590398.
After 12189 training step(s), loss on training batch is 0.000636016.
After 12190 training step(s), loss on training batch is 0.000599252.
After 12191 training step(s), loss on training batch is 0.000692626.
After 12192 training step(s), loss on training batch is 0.000725158.
After 12193 training step(s), loss on training batch is 0.000581475.
After 12194 training step(s), loss on training batch is 0.000575355.
After 12195 training step(s), loss on training batch is 0.000766025.
After 12196 training step(s), loss on training batch is 0.00069846.
After 12197 training step(s), loss on training batch is 0.000616417.
After 12198 training step(s), loss on training batch is 0.000649675.
After 12199 training step(s), loss on training batch is 0.000872277.
After 12200 training step(s), loss on training batch is 0.000666511.
After 12201 training step(s), loss on training batch is 0.000595787.
After 12202 training step(s), loss on training batch is 0.000711776.
After 12203 training step(s), loss on training batch is 0.000584509.
After 12204 training step(s), loss on training batch is 0.000575519.
After 12205 training step(s), loss on training batch is 0.000497872.
After 12206 training step(s), loss on training batch is 0.000665472.
After 12207 training step(s), loss on training batch is 0.000837065.
After 12208 training step(s), loss on training batch is 0.000714332.
After 12209 training step(s), loss on training batch is 0.000530341.
After 12210 training step(s), loss on training batch is 0.000840508.
After 12211 training step(s), loss on training batch is 0.000635631.
After 12212 training step(s), loss on training batch is 0.000622535.
After 12213 training step(s), loss on training batch is 0.000515277.
After 12214 training step(s), loss on training batch is 0.000516377.
After 12215 training step(s), loss on training batch is 0.000495185.
After 12216 training step(s), loss on training batch is 0.000511895.
After 12217 training step(s), loss on training batch is 0.000521885.
After 12218 training step(s), loss on training batch is 0.000710284.
After 12219 training step(s), loss on training batch is 0.000956773.
After 12220 training step(s), loss on training batch is 0.000936283.
After 12221 training step(s), loss on training batch is 0.00116769.
After 12222 training step(s), loss on training batch is 0.000637011.
After 12223 training step(s), loss on training batch is 0.000625796.
After 12224 training step(s), loss on training batch is 0.00064213.
After 12225 training step(s), loss on training batch is 0.000700471.
After 12226 training step(s), loss on training batch is 0.000723635.
After 12227 training step(s), loss on training batch is 0.000617787.
After 12228 training step(s), loss on training batch is 0.000640271.
After 12229 training step(s), loss on training batch is 0.000648306.
After 12230 training step(s), loss on training batch is 0.000722372.
After 12231 training step(s), loss on training batch is 0.00081433.
After 12232 training step(s), loss on training batch is 0.000561782.
After 12233 training step(s), loss on training batch is 0.000590369.
After 12234 training step(s), loss on training batch is 0.000546977.
After 12235 training step(s), loss on training batch is 0.000498221.
After 12236 training step(s), loss on training batch is 0.000724135.
After 12237 training step(s), loss on training batch is 0.0011555.
After 12238 training step(s), loss on training batch is 0.000496161.
After 12239 training step(s), loss on training batch is 0.000579258.
After 12240 training step(s), loss on training batch is 0.000618137.
After 12241 training step(s), loss on training batch is 0.000568824.
After 12242 training step(s), loss on training batch is 0.000550679.
After 12243 training step(s), loss on training batch is 0.000629114.
After 12244 training step(s), loss on training batch is 0.000582082.
After 12245 training step(s), loss on training batch is 0.000620163.
After 12246 training step(s), loss on training batch is 0.000750457.
After 12247 training step(s), loss on training batch is 0.000597218.
After 12248 training step(s), loss on training batch is 0.000587611.
After 12249 training step(s), loss on training batch is 0.000639729.
After 12250 training step(s), loss on training batch is 0.000655072.
After 12251 training step(s), loss on training batch is 0.000541996.
After 12252 training step(s), loss on training batch is 0.000501024.
After 12253 training step(s), loss on training batch is 0.000704592.
After 12254 training step(s), loss on training batch is 0.000584603.
After 12255 training step(s), loss on training batch is 0.000642302.
After 12256 training step(s), loss on training batch is 0.000585408.
After 12257 training step(s), loss on training batch is 0.000530963.
After 12258 training step(s), loss on training batch is 0.000673311.
After 12259 training step(s), loss on training batch is 0.00082241.
After 12260 training step(s), loss on training batch is 0.000556.
After 12261 training step(s), loss on training batch is 0.000497944.
After 12262 training step(s), loss on training batch is 0.000497622.
After 12263 training step(s), loss on training batch is 0.000523445.
After 12264 training step(s), loss on training batch is 0.000480323.
After 12265 training step(s), loss on training batch is 0.000665402.
After 12266 training step(s), loss on training batch is 0.00103813.
After 12267 training step(s), loss on training batch is 0.00108944.
After 12268 training step(s), loss on training batch is 0.000995062.
After 12269 training step(s), loss on training batch is 0.00103162.
After 12270 training step(s), loss on training batch is 0.000986604.
After 12271 training step(s), loss on training batch is 0.000907818.
After 12272 training step(s), loss on training batch is 0.000968971.
After 12273 training step(s), loss on training batch is 0.00137777.
After 12274 training step(s), loss on training batch is 0.00116875.
After 12275 training step(s), loss on training batch is 0.00120432.
After 12276 training step(s), loss on training batch is 0.000924148.
After 12277 training step(s), loss on training batch is 0.000906713.
After 12278 training step(s), loss on training batch is 0.00101154.
After 12279 training step(s), loss on training batch is 0.000834209.
After 12280 training step(s), loss on training batch is 0.000876294.
After 12281 training step(s), loss on training batch is 0.000814966.
After 12282 training step(s), loss on training batch is 0.000860445.
After 12283 training step(s), loss on training batch is 0.000991301.
After 12284 training step(s), loss on training batch is 0.000877091.
After 12285 training step(s), loss on training batch is 0.000865528.
After 12286 training step(s), loss on training batch is 0.00115752.
After 12287 training step(s), loss on training batch is 0.000888186.
After 12288 training step(s), loss on training batch is 0.00106437.
After 12289 training step(s), loss on training batch is 0.000903997.
After 12290 training step(s), loss on training batch is 0.000893982.
After 12291 training step(s), loss on training batch is 0.000891223.
After 12292 training step(s), loss on training batch is 0.000904994.
After 12293 training step(s), loss on training batch is 0.00084046.
After 12294 training step(s), loss on training batch is 0.000975344.
After 12295 training step(s), loss on training batch is 0.00101766.
After 12296 training step(s), loss on training batch is 0.00108632.
After 12297 training step(s), loss on training batch is 0.00089595.
After 12298 training step(s), loss on training batch is 0.000897336.
After 12299 training step(s), loss on training batch is 0.000812902.
After 12300 training step(s), loss on training batch is 0.000873351.
After 12301 training step(s), loss on training batch is 0.000788282.
After 12302 training step(s), loss on training batch is 0.00101067.
After 12303 training step(s), loss on training batch is 0.000855166.
After 12304 training step(s), loss on training batch is 0.00109578.
After 12305 training step(s), loss on training batch is 0.00113671.
After 12306 training step(s), loss on training batch is 0.00109748.
After 12307 training step(s), loss on training batch is 0.00124532.
After 12308 training step(s), loss on training batch is 0.00108562.
After 12309 training step(s), loss on training batch is 0.00703362.
After 12310 training step(s), loss on training batch is 0.00212552.
After 12311 training step(s), loss on training batch is 0.0018929.
After 12312 training step(s), loss on training batch is 0.00175073.
After 12313 training step(s), loss on training batch is 0.00153858.
After 12314 training step(s), loss on training batch is 0.00142192.
After 12315 training step(s), loss on training batch is 0.00122751.
After 12316 training step(s), loss on training batch is 0.00057876.
After 12317 training step(s), loss on training batch is 0.000516768.
After 12318 training step(s), loss on training batch is 0.000484216.
After 12319 training step(s), loss on training batch is 0.000501981.
After 12320 training step(s), loss on training batch is 0.000524668.
After 12321 training step(s), loss on training batch is 0.000533343.
After 12322 training step(s), loss on training batch is 0.000544173.
After 12323 training step(s), loss on training batch is 0.000538119.
After 12324 training step(s), loss on training batch is 0.000501071.
After 12325 training step(s), loss on training batch is 0.000449563.
After 12326 training step(s), loss on training batch is 0.000903319.
After 12327 training step(s), loss on training batch is 0.000925893.
After 12328 training step(s), loss on training batch is 0.00101542.
After 12329 training step(s), loss on training batch is 0.000869801.
After 12330 training step(s), loss on training batch is 0.00171521.
After 12331 training step(s), loss on training batch is 0.00147112.
After 12332 training step(s), loss on training batch is 0.00111764.
After 12333 training step(s), loss on training batch is 0.00107724.
After 12334 training step(s), loss on training batch is 0.000952963.
After 12335 training step(s), loss on training batch is 0.000901375.
After 12336 training step(s), loss on training batch is 0.000931539.
After 12337 training step(s), loss on training batch is 0.00093526.
After 12338 training step(s), loss on training batch is 0.00104845.
After 12339 training step(s), loss on training batch is 0.000932175.
After 12340 training step(s), loss on training batch is 0.000918993.
After 12341 training step(s), loss on training batch is 0.000902647.
After 12342 training step(s), loss on training batch is 0.0015313.
After 12343 training step(s), loss on training batch is 0.000346821.
After 12344 training step(s), loss on training batch is 0.000388423.
After 12345 training step(s), loss on training batch is 0.00036843.
After 12346 training step(s), loss on training batch is 0.00046276.
After 12347 training step(s), loss on training batch is 0.000429641.
After 12348 training step(s), loss on training batch is 0.000407746.
After 12349 training step(s), loss on training batch is 0.000348922.
After 12350 training step(s), loss on training batch is 0.000322133.
After 12351 training step(s), loss on training batch is 0.000505417.
After 12352 training step(s), loss on training batch is 0.000422451.
After 12353 training step(s), loss on training batch is 0.00041152.
After 12354 training step(s), loss on training batch is 0.000365176.
After 12355 training step(s), loss on training batch is 0.000534115.
After 12356 training step(s), loss on training batch is 0.000384896.
After 12357 training step(s), loss on training batch is 0.000334406.
After 12358 training step(s), loss on training batch is 0.00033358.
After 12359 training step(s), loss on training batch is 0.000746426.
After 12360 training step(s), loss on training batch is 0.000939041.
After 12361 training step(s), loss on training batch is 0.000526901.
After 12362 training step(s), loss on training batch is 0.000413541.
After 12363 training step(s), loss on training batch is 0.000380467.
After 12364 training step(s), loss on training batch is 0.000460379.
After 12365 training step(s), loss on training batch is 0.000472233.
After 12366 training step(s), loss on training batch is 0.000419995.
After 12367 training step(s), loss on training batch is 0.000370342.
After 12368 training step(s), loss on training batch is 0.000519107.
After 12369 training step(s), loss on training batch is 0.000322702.
After 12370 training step(s), loss on training batch is 0.000401854.
After 12371 training step(s), loss on training batch is 0.000274552.
After 12372 training step(s), loss on training batch is 0.000295103.
After 12373 training step(s), loss on training batch is 0.000321698.
After 12374 training step(s), loss on training batch is 0.000318774.
After 12375 training step(s), loss on training batch is 0.000457297.
After 12376 training step(s), loss on training batch is 0.000414036.
After 12377 training step(s), loss on training batch is 0.000416426.
After 12378 training step(s), loss on training batch is 0.000330289.
After 12379 training step(s), loss on training batch is 0.000350782.
After 12380 training step(s), loss on training batch is 0.000315019.
After 12381 training step(s), loss on training batch is 0.000355318.
After 12382 training step(s), loss on training batch is 0.000320691.
After 12383 training step(s), loss on training batch is 0.00038351.
After 12384 training step(s), loss on training batch is 0.000406291.
After 12385 training step(s), loss on training batch is 0.000406749.
After 12386 training step(s), loss on training batch is 0.000598582.
After 12387 training step(s), loss on training batch is 0.000416564.
After 12388 training step(s), loss on training batch is 0.000329095.
After 12389 training step(s), loss on training batch is 0.000382271.
After 12390 training step(s), loss on training batch is 0.000342054.
After 12391 training step(s), loss on training batch is 0.00031445.
After 12392 training step(s), loss on training batch is 0.000314084.
After 12393 training step(s), loss on training batch is 0.000307966.
After 12394 training step(s), loss on training batch is 0.000266338.
After 12395 training step(s), loss on training batch is 0.000468905.
After 12396 training step(s), loss on training batch is 0.000445696.
After 12397 training step(s), loss on training batch is 0.000242276.
After 12398 training step(s), loss on training batch is 0.000262291.
After 12399 training step(s), loss on training batch is 0.000346955.
After 12400 training step(s), loss on training batch is 0.000359575.
After 12401 training step(s), loss on training batch is 0.000517706.
After 12402 training step(s), loss on training batch is 0.000593627.
After 12403 training step(s), loss on training batch is 0.000369894.
After 12404 training step(s), loss on training batch is 0.000505827.
After 12405 training step(s), loss on training batch is 0.000962652.
After 12406 training step(s), loss on training batch is 0.000670362.
After 12407 training step(s), loss on training batch is 0.000377678.
After 12408 training step(s), loss on training batch is 0.000458979.
After 12409 training step(s), loss on training batch is 0.000570052.
After 12410 training step(s), loss on training batch is 0.000594162.
After 12411 training step(s), loss on training batch is 0.000388604.
After 12412 training step(s), loss on training batch is 0.000308467.
After 12413 training step(s), loss on training batch is 0.000361389.
After 12414 training step(s), loss on training batch is 0.000386789.
After 12415 training step(s), loss on training batch is 0.000361215.
After 12416 training step(s), loss on training batch is 0.000309134.
After 12417 training step(s), loss on training batch is 0.000393921.
After 12418 training step(s), loss on training batch is 0.000332634.
After 12419 training step(s), loss on training batch is 0.000418104.
After 12420 training step(s), loss on training batch is 0.00037573.
After 12421 training step(s), loss on training batch is 0.000299534.
After 12422 training step(s), loss on training batch is 0.000364573.
After 12423 training step(s), loss on training batch is 0.00034286.
After 12424 training step(s), loss on training batch is 0.000370103.
After 12425 training step(s), loss on training batch is 0.000314327.
After 12426 training step(s), loss on training batch is 0.000371054.
After 12427 training step(s), loss on training batch is 0.000281486.
After 12428 training step(s), loss on training batch is 0.000989753.
After 12429 training step(s), loss on training batch is 0.000601598.
After 12430 training step(s), loss on training batch is 0.000527318.
After 12431 training step(s), loss on training batch is 0.000536581.
After 12432 training step(s), loss on training batch is 0.000622713.
After 12433 training step(s), loss on training batch is 0.000694879.
After 12434 training step(s), loss on training batch is 0.000596476.
After 12435 training step(s), loss on training batch is 0.000470407.
After 12436 training step(s), loss on training batch is 0.000684462.
After 12437 training step(s), loss on training batch is 0.000657514.
After 12438 training step(s), loss on training batch is 0.000487029.
After 12439 training step(s), loss on training batch is 0.000481952.
After 12440 training step(s), loss on training batch is 0.00104641.
After 12441 training step(s), loss on training batch is 0.000482648.
After 12442 training step(s), loss on training batch is 0.000547312.
After 12443 training step(s), loss on training batch is 0.000569795.
After 12444 training step(s), loss on training batch is 0.000679774.
After 12445 training step(s), loss on training batch is 0.000473675.
After 12446 training step(s), loss on training batch is 0.000683352.
After 12447 training step(s), loss on training batch is 0.000523564.
After 12448 training step(s), loss on training batch is 0.000574054.
After 12449 training step(s), loss on training batch is 0.000617106.
After 12450 training step(s), loss on training batch is 0.00060675.
After 12451 training step(s), loss on training batch is 0.000598022.
After 12452 training step(s), loss on training batch is 0.000535246.
After 12453 training step(s), loss on training batch is 0.000603061.
After 12454 training step(s), loss on training batch is 0.000434275.
After 12455 training step(s), loss on training batch is 0.00057896.
After 12456 training step(s), loss on training batch is 0.00103593.
After 12457 training step(s), loss on training batch is 0.000876857.
After 12458 training step(s), loss on training batch is 0.000939848.
After 12459 training step(s), loss on training batch is 0.000931807.
After 12460 training step(s), loss on training batch is 0.000787492.
After 12461 training step(s), loss on training batch is 0.000821362.
After 12462 training step(s), loss on training batch is 0.00102095.
After 12463 training step(s), loss on training batch is 0.000888957.
After 12464 training step(s), loss on training batch is 0.00079885.
After 12465 training step(s), loss on training batch is 0.00109354.
After 12466 training step(s), loss on training batch is 0.000797423.
After 12467 training step(s), loss on training batch is 0.000811478.
After 12468 training step(s), loss on training batch is 0.000962678.
After 12469 training step(s), loss on training batch is 0.00154441.
After 12470 training step(s), loss on training batch is 0.00306472.
After 12471 training step(s), loss on training batch is 0.00141359.
After 12472 training step(s), loss on training batch is 0.00108657.
After 12473 training step(s), loss on training batch is 0.00120369.
After 12474 training step(s), loss on training batch is 0.000999882.
After 12475 training step(s), loss on training batch is 0.000962807.
After 12476 training step(s), loss on training batch is 0.000885179.
After 12477 training step(s), loss on training batch is 0.00125217.
After 12478 training step(s), loss on training batch is 0.000885709.
After 12479 training step(s), loss on training batch is 0.000867475.
After 12480 training step(s), loss on training batch is 0.0008386.
After 12481 training step(s), loss on training batch is 0.000529741.
After 12482 training step(s), loss on training batch is 0.00042663.
After 12483 training step(s), loss on training batch is 0.000280984.
After 12484 training step(s), loss on training batch is 0.000347704.
After 12485 training step(s), loss on training batch is 0.000373999.
After 12486 training step(s), loss on training batch is 0.00051414.
After 12487 training step(s), loss on training batch is 0.000621433.
After 12488 training step(s), loss on training batch is 0.000412989.
After 12489 training step(s), loss on training batch is 0.000356057.
After 12490 training step(s), loss on training batch is 0.000324834.
After 12491 training step(s), loss on training batch is 0.000272136.
After 12492 training step(s), loss on training batch is 0.00033531.
After 12493 training step(s), loss on training batch is 0.000308093.
After 12494 training step(s), loss on training batch is 0.000311434.
After 12495 training step(s), loss on training batch is 0.000353954.
After 12496 training step(s), loss on training batch is 0.000357177.
After 12497 training step(s), loss on training batch is 0.000285486.
After 12498 training step(s), loss on training batch is 0.000269755.
After 12499 training step(s), loss on training batch is 0.000283433.
After 12500 training step(s), loss on training batch is 0.000290325.
After 12501 training step(s), loss on training batch is 0.00072581.
After 12502 training step(s), loss on training batch is 0.000493735.
After 12503 training step(s), loss on training batch is 0.000535398.
After 12504 training step(s), loss on training batch is 0.000635844.
After 12505 training step(s), loss on training batch is 0.00059346.
After 12506 training step(s), loss on training batch is 0.00059825.
After 12507 training step(s), loss on training batch is 0.000586198.
After 12508 training step(s), loss on training batch is 0.000328427.
After 12509 training step(s), loss on training batch is 0.000365633.
After 12510 training step(s), loss on training batch is 0.000393347.
After 12511 training step(s), loss on training batch is 0.000466576.
After 12512 training step(s), loss on training batch is 0.000451113.
After 12513 training step(s), loss on training batch is 0.000475182.
After 12514 training step(s), loss on training batch is 0.00062473.
After 12515 training step(s), loss on training batch is 0.00136405.
After 12516 training step(s), loss on training batch is 0.000977268.
After 12517 training step(s), loss on training batch is 0.000812883.
After 12518 training step(s), loss on training batch is 0.000612213.
After 12519 training step(s), loss on training batch is 0.000385744.
After 12520 training step(s), loss on training batch is 0.000374269.
After 12521 training step(s), loss on training batch is 0.000384142.
After 12522 training step(s), loss on training batch is 0.000339808.
After 12523 training step(s), loss on training batch is 0.00052982.
After 12524 training step(s), loss on training batch is 0.000375553.
After 12525 training step(s), loss on training batch is 0.000372002.
After 12526 training step(s), loss on training batch is 0.000357274.
After 12527 training step(s), loss on training batch is 0.000900978.
After 12528 training step(s), loss on training batch is 0.00209564.
After 12529 training step(s), loss on training batch is 0.000868662.
After 12530 training step(s), loss on training batch is 0.00061982.
After 12531 training step(s), loss on training batch is 0.000534067.
After 12532 training step(s), loss on training batch is 0.000571812.
After 12533 training step(s), loss on training batch is 0.000473019.
After 12534 training step(s), loss on training batch is 0.000362537.
After 12535 training step(s), loss on training batch is 0.000352072.
After 12536 training step(s), loss on training batch is 0.000342879.
After 12537 training step(s), loss on training batch is 0.000399425.
After 12538 training step(s), loss on training batch is 0.000394043.
After 12539 training step(s), loss on training batch is 0.000436439.
After 12540 training step(s), loss on training batch is 0.000498901.
After 12541 training step(s), loss on training batch is 0.000384478.
After 12542 training step(s), loss on training batch is 0.000466904.
After 12543 training step(s), loss on training batch is 0.000452543.
After 12544 training step(s), loss on training batch is 0.000522992.
After 12545 training step(s), loss on training batch is 0.000338594.
After 12546 training step(s), loss on training batch is 0.000457781.
After 12547 training step(s), loss on training batch is 0.000355539.
After 12548 training step(s), loss on training batch is 0.000326357.
After 12549 training step(s), loss on training batch is 0.000367099.
After 12550 training step(s), loss on training batch is 0.000491418.
After 12551 training step(s), loss on training batch is 0.000448287.
After 12552 training step(s), loss on training batch is 0.000401164.
After 12553 training step(s), loss on training batch is 0.000396478.
After 12554 training step(s), loss on training batch is 0.000365411.
After 12555 training step(s), loss on training batch is 0.000357044.
After 12556 training step(s), loss on training batch is 0.000462638.
After 12557 training step(s), loss on training batch is 0.000541915.
After 12558 training step(s), loss on training batch is 0.000357655.
After 12559 training step(s), loss on training batch is 0.000399212.
After 12560 training step(s), loss on training batch is 0.000491527.
After 12561 training step(s), loss on training batch is 0.000347168.
After 12562 training step(s), loss on training batch is 0.000314134.
After 12563 training step(s), loss on training batch is 0.000328598.
After 12564 training step(s), loss on training batch is 0.000450462.
After 12565 training step(s), loss on training batch is 0.000409351.
After 12566 training step(s), loss on training batch is 0.00040309.
After 12567 training step(s), loss on training batch is 0.000395659.
After 12568 training step(s), loss on training batch is 0.000367021.
After 12569 training step(s), loss on training batch is 0.000425259.
After 12570 training step(s), loss on training batch is 0.000350596.
After 12571 training step(s), loss on training batch is 0.000359037.
After 12572 training step(s), loss on training batch is 0.000342931.
After 12573 training step(s), loss on training batch is 0.000329214.
After 12574 training step(s), loss on training batch is 0.000315568.
After 12575 training step(s), loss on training batch is 0.000380672.
After 12576 training step(s), loss on training batch is 0.000362697.
After 12577 training step(s), loss on training batch is 0.000514074.
After 12578 training step(s), loss on training batch is 0.000376948.
After 12579 training step(s), loss on training batch is 0.000342681.
After 12580 training step(s), loss on training batch is 0.000363352.
After 12581 training step(s), loss on training batch is 0.000345654.
After 12582 training step(s), loss on training batch is 0.000373707.
After 12583 training step(s), loss on training batch is 0.000678531.
After 12584 training step(s), loss on training batch is 0.000802677.
After 12585 training step(s), loss on training batch is 0.000943387.
After 12586 training step(s), loss on training batch is 0.000626127.
After 12587 training step(s), loss on training batch is 0.000661091.
After 12588 training step(s), loss on training batch is 0.000582137.
After 12589 training step(s), loss on training batch is 0.0006308.
After 12590 training step(s), loss on training batch is 0.000595283.
After 12591 training step(s), loss on training batch is 0.000691597.
After 12592 training step(s), loss on training batch is 0.000738324.
After 12593 training step(s), loss on training batch is 0.000572324.
After 12594 training step(s), loss on training batch is 0.000557038.
After 12595 training step(s), loss on training batch is 0.00074941.
After 12596 training step(s), loss on training batch is 0.000687015.
After 12597 training step(s), loss on training batch is 0.000612956.
After 12598 training step(s), loss on training batch is 0.000648973.
After 12599 training step(s), loss on training batch is 0.000868698.
After 12600 training step(s), loss on training batch is 0.000668817.
After 12601 training step(s), loss on training batch is 0.000590724.
After 12602 training step(s), loss on training batch is 0.000716221.
After 12603 training step(s), loss on training batch is 0.000585608.
After 12604 training step(s), loss on training batch is 0.000576311.
After 12605 training step(s), loss on training batch is 0.000501377.
After 12606 training step(s), loss on training batch is 0.000654254.
After 12607 training step(s), loss on training batch is 0.0008206.
After 12608 training step(s), loss on training batch is 0.000698304.
After 12609 training step(s), loss on training batch is 0.000529078.
After 12610 training step(s), loss on training batch is 0.000866265.
After 12611 training step(s), loss on training batch is 0.000619172.
After 12612 training step(s), loss on training batch is 0.00061762.
After 12613 training step(s), loss on training batch is 0.000498736.
After 12614 training step(s), loss on training batch is 0.000511495.
After 12615 training step(s), loss on training batch is 0.000484537.
After 12616 training step(s), loss on training batch is 0.000503002.
After 12617 training step(s), loss on training batch is 0.000517351.
After 12618 training step(s), loss on training batch is 0.000712821.
After 12619 training step(s), loss on training batch is 0.000957607.
After 12620 training step(s), loss on training batch is 0.00094377.
After 12621 training step(s), loss on training batch is 0.00124783.
After 12622 training step(s), loss on training batch is 0.000614142.
After 12623 training step(s), loss on training batch is 0.000600307.
After 12624 training step(s), loss on training batch is 0.000616683.
After 12625 training step(s), loss on training batch is 0.000652627.
After 12626 training step(s), loss on training batch is 0.000687492.
After 12627 training step(s), loss on training batch is 0.000588692.
After 12628 training step(s), loss on training batch is 0.000634485.
After 12629 training step(s), loss on training batch is 0.000640861.
After 12630 training step(s), loss on training batch is 0.000717316.
After 12631 training step(s), loss on training batch is 0.0008004.
After 12632 training step(s), loss on training batch is 0.000546282.
After 12633 training step(s), loss on training batch is 0.000571158.
After 12634 training step(s), loss on training batch is 0.000527798.
After 12635 training step(s), loss on training batch is 0.000478245.
After 12636 training step(s), loss on training batch is 0.000738846.
After 12637 training step(s), loss on training batch is 0.00114763.
After 12638 training step(s), loss on training batch is 0.000501846.
After 12639 training step(s), loss on training batch is 0.000575041.
After 12640 training step(s), loss on training batch is 0.000611416.
After 12641 training step(s), loss on training batch is 0.000545078.
After 12642 training step(s), loss on training batch is 0.000527682.
After 12643 training step(s), loss on training batch is 0.000597944.
After 12644 training step(s), loss on training batch is 0.000549929.
After 12645 training step(s), loss on training batch is 0.000604827.
After 12646 training step(s), loss on training batch is 0.000773792.
After 12647 training step(s), loss on training batch is 0.000574327.
After 12648 training step(s), loss on training batch is 0.000591734.
After 12649 training step(s), loss on training batch is 0.000637703.
After 12650 training step(s), loss on training batch is 0.000651246.
After 12651 training step(s), loss on training batch is 0.000566328.
After 12652 training step(s), loss on training batch is 0.000492039.
After 12653 training step(s), loss on training batch is 0.000716007.
After 12654 training step(s), loss on training batch is 0.000603988.
After 12655 training step(s), loss on training batch is 0.00065088.
After 12656 training step(s), loss on training batch is 0.000592982.
After 12657 training step(s), loss on training batch is 0.000512873.
After 12658 training step(s), loss on training batch is 0.000673569.
After 12659 training step(s), loss on training batch is 0.00085319.
After 12660 training step(s), loss on training batch is 0.000558311.
After 12661 training step(s), loss on training batch is 0.000493838.
After 12662 training step(s), loss on training batch is 0.000497983.
After 12663 training step(s), loss on training batch is 0.000523618.
After 12664 training step(s), loss on training batch is 0.00046695.
After 12665 training step(s), loss on training batch is 0.000702444.
After 12666 training step(s), loss on training batch is 0.00101952.
After 12667 training step(s), loss on training batch is 0.00107993.
After 12668 training step(s), loss on training batch is 0.000982138.
After 12669 training step(s), loss on training batch is 0.00101961.
After 12670 training step(s), loss on training batch is 0.00098335.
After 12671 training step(s), loss on training batch is 0.000924756.
After 12672 training step(s), loss on training batch is 0.000967347.
After 12673 training step(s), loss on training batch is 0.0012618.
After 12674 training step(s), loss on training batch is 0.00114609.
After 12675 training step(s), loss on training batch is 0.00117359.
After 12676 training step(s), loss on training batch is 0.0009796.
After 12677 training step(s), loss on training batch is 0.000938579.
After 12678 training step(s), loss on training batch is 0.000995393.
After 12679 training step(s), loss on training batch is 0.000878861.
After 12680 training step(s), loss on training batch is 0.000874156.
After 12681 training step(s), loss on training batch is 0.000850019.
After 12682 training step(s), loss on training batch is 0.00088131.
After 12683 training step(s), loss on training batch is 0.000970002.
After 12684 training step(s), loss on training batch is 0.00087203.
After 12685 training step(s), loss on training batch is 0.000875869.
After 12686 training step(s), loss on training batch is 0.00110607.
After 12687 training step(s), loss on training batch is 0.000889265.
After 12688 training step(s), loss on training batch is 0.00104914.
After 12689 training step(s), loss on training batch is 0.000908636.
After 12690 training step(s), loss on training batch is 0.000904644.
After 12691 training step(s), loss on training batch is 0.000898725.
After 12692 training step(s), loss on training batch is 0.000899375.
After 12693 training step(s), loss on training batch is 0.000833994.
After 12694 training step(s), loss on training batch is 0.000967005.
After 12695 training step(s), loss on training batch is 0.000986616.
After 12696 training step(s), loss on training batch is 0.0010712.
After 12697 training step(s), loss on training batch is 0.000915292.
After 12698 training step(s), loss on training batch is 0.000892972.
After 12699 training step(s), loss on training batch is 0.00085535.
After 12700 training step(s), loss on training batch is 0.00090074.
After 12701 training step(s), loss on training batch is 0.000782686.
After 12702 training step(s), loss on training batch is 0.000977188.
After 12703 training step(s), loss on training batch is 0.000861788.
After 12704 training step(s), loss on training batch is 0.0010724.
After 12705 training step(s), loss on training batch is 0.00111811.
After 12706 training step(s), loss on training batch is 0.00108458.
After 12707 training step(s), loss on training batch is 0.00125633.
After 12708 training step(s), loss on training batch is 0.00107664.
After 12709 training step(s), loss on training batch is 0.00807369.
After 12710 training step(s), loss on training batch is 0.00164411.
After 12711 training step(s), loss on training batch is 0.00152847.
After 12712 training step(s), loss on training batch is 0.00144188.
After 12713 training step(s), loss on training batch is 0.00134485.
After 12714 training step(s), loss on training batch is 0.00134561.
After 12715 training step(s), loss on training batch is 0.00114093.
After 12716 training step(s), loss on training batch is 0.00053101.
After 12717 training step(s), loss on training batch is 0.000471837.
After 12718 training step(s), loss on training batch is 0.000445683.
After 12719 training step(s), loss on training batch is 0.000450931.
After 12720 training step(s), loss on training batch is 0.000470803.
After 12721 training step(s), loss on training batch is 0.000474018.
After 12722 training step(s), loss on training batch is 0.000485762.
After 12723 training step(s), loss on training batch is 0.000483716.
After 12724 training step(s), loss on training batch is 0.00046477.
After 12725 training step(s), loss on training batch is 0.000429203.
After 12726 training step(s), loss on training batch is 0.00085163.
After 12727 training step(s), loss on training batch is 0.000882443.
After 12728 training step(s), loss on training batch is 0.000985326.
After 12729 training step(s), loss on training batch is 0.000869178.
After 12730 training step(s), loss on training batch is 0.0016764.
After 12731 training step(s), loss on training batch is 0.00144294.
After 12732 training step(s), loss on training batch is 0.00109392.
After 12733 training step(s), loss on training batch is 0.00106654.
After 12734 training step(s), loss on training batch is 0.000933525.
After 12735 training step(s), loss on training batch is 0.000889898.
After 12736 training step(s), loss on training batch is 0.00090819.
After 12737 training step(s), loss on training batch is 0.000921787.
After 12738 training step(s), loss on training batch is 0.00106183.
After 12739 training step(s), loss on training batch is 0.000919026.
After 12740 training step(s), loss on training batch is 0.00089567.
After 12741 training step(s), loss on training batch is 0.000890704.
After 12742 training step(s), loss on training batch is 0.00156548.
After 12743 training step(s), loss on training batch is 0.000339746.
After 12744 training step(s), loss on training batch is 0.000386321.
After 12745 training step(s), loss on training batch is 0.000362388.
After 12746 training step(s), loss on training batch is 0.000463185.
After 12747 training step(s), loss on training batch is 0.000420992.
After 12748 training step(s), loss on training batch is 0.000387188.
After 12749 training step(s), loss on training batch is 0.000345078.
After 12750 training step(s), loss on training batch is 0.000317577.
After 12751 training step(s), loss on training batch is 0.000494837.
After 12752 training step(s), loss on training batch is 0.000392962.
After 12753 training step(s), loss on training batch is 0.000443516.
After 12754 training step(s), loss on training batch is 0.000356759.
After 12755 training step(s), loss on training batch is 0.000537744.
After 12756 training step(s), loss on training batch is 0.000373388.
After 12757 training step(s), loss on training batch is 0.000324812.
After 12758 training step(s), loss on training batch is 0.000339692.
After 12759 training step(s), loss on training batch is 0.000742567.
After 12760 training step(s), loss on training batch is 0.000940198.
After 12761 training step(s), loss on training batch is 0.000510908.
After 12762 training step(s), loss on training batch is 0.000404323.
After 12763 training step(s), loss on training batch is 0.000369996.
After 12764 training step(s), loss on training batch is 0.000451815.
After 12765 training step(s), loss on training batch is 0.000467012.
After 12766 training step(s), loss on training batch is 0.000414378.
After 12767 training step(s), loss on training batch is 0.000367297.
After 12768 training step(s), loss on training batch is 0.000517841.
After 12769 training step(s), loss on training batch is 0.000322118.
After 12770 training step(s), loss on training batch is 0.00040178.
After 12771 training step(s), loss on training batch is 0.000271466.
After 12772 training step(s), loss on training batch is 0.00029281.
After 12773 training step(s), loss on training batch is 0.000319584.
After 12774 training step(s), loss on training batch is 0.000313858.
After 12775 training step(s), loss on training batch is 0.000470227.
After 12776 training step(s), loss on training batch is 0.000407014.
After 12777 training step(s), loss on training batch is 0.000407839.
After 12778 training step(s), loss on training batch is 0.000325702.
After 12779 training step(s), loss on training batch is 0.000355197.
After 12780 training step(s), loss on training batch is 0.000311255.
After 12781 training step(s), loss on training batch is 0.000350168.
After 12782 training step(s), loss on training batch is 0.000325694.
After 12783 training step(s), loss on training batch is 0.000399083.
After 12784 training step(s), loss on training batch is 0.000385573.
After 12785 training step(s), loss on training batch is 0.000402961.
After 12786 training step(s), loss on training batch is 0.000542473.
After 12787 training step(s), loss on training batch is 0.000410698.
After 12788 training step(s), loss on training batch is 0.000321958.
After 12789 training step(s), loss on training batch is 0.000378605.
After 12790 training step(s), loss on training batch is 0.000344921.
After 12791 training step(s), loss on training batch is 0.000310629.
After 12792 training step(s), loss on training batch is 0.000328267.
After 12793 training step(s), loss on training batch is 0.000323424.
After 12794 training step(s), loss on training batch is 0.00027398.
After 12795 training step(s), loss on training batch is 0.000435002.
After 12796 training step(s), loss on training batch is 0.000433769.
After 12797 training step(s), loss on training batch is 0.000249503.
After 12798 training step(s), loss on training batch is 0.000264726.
After 12799 training step(s), loss on training batch is 0.000337203.
After 12800 training step(s), loss on training batch is 0.000359429.
After 12801 training step(s), loss on training batch is 0.000522457.
After 12802 training step(s), loss on training batch is 0.000598096.
After 12803 training step(s), loss on training batch is 0.000378248.
After 12804 training step(s), loss on training batch is 0.000517998.
After 12805 training step(s), loss on training batch is 0.00094406.
After 12806 training step(s), loss on training batch is 0.000669229.
After 12807 training step(s), loss on training batch is 0.000369285.
After 12808 training step(s), loss on training batch is 0.00044861.
After 12809 training step(s), loss on training batch is 0.00056919.
After 12810 training step(s), loss on training batch is 0.000595625.
After 12811 training step(s), loss on training batch is 0.000388683.
After 12812 training step(s), loss on training batch is 0.000301531.
After 12813 training step(s), loss on training batch is 0.000357086.
After 12814 training step(s), loss on training batch is 0.000378245.
After 12815 training step(s), loss on training batch is 0.000350233.
After 12816 training step(s), loss on training batch is 0.000306821.
After 12817 training step(s), loss on training batch is 0.000382946.
After 12818 training step(s), loss on training batch is 0.000338031.
After 12819 training step(s), loss on training batch is 0.000404732.
After 12820 training step(s), loss on training batch is 0.000373349.
After 12821 training step(s), loss on training batch is 0.000300196.
After 12822 training step(s), loss on training batch is 0.000361475.
After 12823 training step(s), loss on training batch is 0.000341702.
After 12824 training step(s), loss on training batch is 0.00031426.
After 12825 training step(s), loss on training batch is 0.000306566.
After 12826 training step(s), loss on training batch is 0.00036521.
After 12827 training step(s), loss on training batch is 0.000294797.
After 12828 training step(s), loss on training batch is 0.000926718.
After 12829 training step(s), loss on training batch is 0.000595741.
After 12830 training step(s), loss on training batch is 0.000528758.
After 12831 training step(s), loss on training batch is 0.000537767.
After 12832 training step(s), loss on training batch is 0.000618017.
After 12833 training step(s), loss on training batch is 0.000686303.
After 12834 training step(s), loss on training batch is 0.000592301.
After 12835 training step(s), loss on training batch is 0.000475269.
After 12836 training step(s), loss on training batch is 0.000679168.
After 12837 training step(s), loss on training batch is 0.000651791.
After 12838 training step(s), loss on training batch is 0.000486334.
After 12839 training step(s), loss on training batch is 0.000482879.
After 12840 training step(s), loss on training batch is 0.00104003.
After 12841 training step(s), loss on training batch is 0.000515031.
After 12842 training step(s), loss on training batch is 0.000579949.
After 12843 training step(s), loss on training batch is 0.000586317.
After 12844 training step(s), loss on training batch is 0.00065625.
After 12845 training step(s), loss on training batch is 0.000486871.
After 12846 training step(s), loss on training batch is 0.000648711.
After 12847 training step(s), loss on training batch is 0.000520199.
After 12848 training step(s), loss on training batch is 0.000555145.
After 12849 training step(s), loss on training batch is 0.000620388.
After 12850 training step(s), loss on training batch is 0.000605963.
After 12851 training step(s), loss on training batch is 0.000594922.
After 12852 training step(s), loss on training batch is 0.000536173.
After 12853 training step(s), loss on training batch is 0.000607131.
After 12854 training step(s), loss on training batch is 0.000434238.
After 12855 training step(s), loss on training batch is 0.000570726.
After 12856 training step(s), loss on training batch is 0.00103151.
After 12857 training step(s), loss on training batch is 0.000877973.
After 12858 training step(s), loss on training batch is 0.00094024.
After 12859 training step(s), loss on training batch is 0.00092021.
After 12860 training step(s), loss on training batch is 0.000791202.
After 12861 training step(s), loss on training batch is 0.000824207.
After 12862 training step(s), loss on training batch is 0.00100207.
After 12863 training step(s), loss on training batch is 0.000895621.
After 12864 training step(s), loss on training batch is 0.000806846.
After 12865 training step(s), loss on training batch is 0.00107904.
After 12866 training step(s), loss on training batch is 0.000806476.
After 12867 training step(s), loss on training batch is 0.000824329.
After 12868 training step(s), loss on training batch is 0.000956516.
After 12869 training step(s), loss on training batch is 0.00146676.
After 12870 training step(s), loss on training batch is 0.00278943.
After 12871 training step(s), loss on training batch is 0.00140021.
After 12872 training step(s), loss on training batch is 0.00107111.
After 12873 training step(s), loss on training batch is 0.00118053.
After 12874 training step(s), loss on training batch is 0.00103021.
After 12875 training step(s), loss on training batch is 0.000967653.
After 12876 training step(s), loss on training batch is 0.000897209.
After 12877 training step(s), loss on training batch is 0.00124451.
After 12878 training step(s), loss on training batch is 0.000884229.
After 12879 training step(s), loss on training batch is 0.000834821.
After 12880 training step(s), loss on training batch is 0.000825846.
After 12881 training step(s), loss on training batch is 0.000544466.
After 12882 training step(s), loss on training batch is 0.000424128.
After 12883 training step(s), loss on training batch is 0.000278442.
After 12884 training step(s), loss on training batch is 0.000344482.
After 12885 training step(s), loss on training batch is 0.000374378.
After 12886 training step(s), loss on training batch is 0.00051482.
After 12887 training step(s), loss on training batch is 0.000629649.
After 12888 training step(s), loss on training batch is 0.000409072.
After 12889 training step(s), loss on training batch is 0.000354424.
After 12890 training step(s), loss on training batch is 0.000320712.
After 12891 training step(s), loss on training batch is 0.000266652.
After 12892 training step(s), loss on training batch is 0.00033542.
After 12893 training step(s), loss on training batch is 0.000298785.
After 12894 training step(s), loss on training batch is 0.000299558.
After 12895 training step(s), loss on training batch is 0.00034483.
After 12896 training step(s), loss on training batch is 0.000355516.
After 12897 training step(s), loss on training batch is 0.000278025.
After 12898 training step(s), loss on training batch is 0.000262829.
After 12899 training step(s), loss on training batch is 0.000280788.
After 12900 training step(s), loss on training batch is 0.000292473.
After 12901 training step(s), loss on training batch is 0.000714755.
After 12902 training step(s), loss on training batch is 0.000483236.
After 12903 training step(s), loss on training batch is 0.000518058.
After 12904 training step(s), loss on training batch is 0.000617463.
After 12905 training step(s), loss on training batch is 0.000583455.
After 12906 training step(s), loss on training batch is 0.000587787.
After 12907 training step(s), loss on training batch is 0.00056538.
After 12908 training step(s), loss on training batch is 0.000327966.
After 12909 training step(s), loss on training batch is 0.000366229.
After 12910 training step(s), loss on training batch is 0.000385343.
After 12911 training step(s), loss on training batch is 0.000457988.
After 12912 training step(s), loss on training batch is 0.000445076.
After 12913 training step(s), loss on training batch is 0.000457731.
After 12914 training step(s), loss on training batch is 0.000608488.
After 12915 training step(s), loss on training batch is 0.00134742.
After 12916 training step(s), loss on training batch is 0.000955015.
After 12917 training step(s), loss on training batch is 0.000796428.
After 12918 training step(s), loss on training batch is 0.000621077.
After 12919 training step(s), loss on training batch is 0.000376228.
After 12920 training step(s), loss on training batch is 0.000375665.
After 12921 training step(s), loss on training batch is 0.00038522.
After 12922 training step(s), loss on training batch is 0.000338917.
After 12923 training step(s), loss on training batch is 0.00052408.
After 12924 training step(s), loss on training batch is 0.000402445.
After 12925 training step(s), loss on training batch is 0.000388601.
After 12926 training step(s), loss on training batch is 0.000380207.
After 12927 training step(s), loss on training batch is 0.000753139.
After 12928 training step(s), loss on training batch is 0.00186332.
After 12929 training step(s), loss on training batch is 0.00067309.
After 12930 training step(s), loss on training batch is 0.000493322.
After 12931 training step(s), loss on training batch is 0.000568228.
After 12932 training step(s), loss on training batch is 0.000496043.
After 12933 training step(s), loss on training batch is 0.000423179.
After 12934 training step(s), loss on training batch is 0.000341916.
After 12935 training step(s), loss on training batch is 0.000304096.
After 12936 training step(s), loss on training batch is 0.00030871.
After 12937 training step(s), loss on training batch is 0.00034845.
After 12938 training step(s), loss on training batch is 0.000345796.
After 12939 training step(s), loss on training batch is 0.000388671.
After 12940 training step(s), loss on training batch is 0.000411982.
After 12941 training step(s), loss on training batch is 0.000338165.
After 12942 training step(s), loss on training batch is 0.000418424.
After 12943 training step(s), loss on training batch is 0.000400229.
After 12944 training step(s), loss on training batch is 0.000486557.
After 12945 training step(s), loss on training batch is 0.000371475.
After 12946 training step(s), loss on training batch is 0.00049731.
After 12947 training step(s), loss on training batch is 0.000355475.
After 12948 training step(s), loss on training batch is 0.00031505.
After 12949 training step(s), loss on training batch is 0.000360044.
After 12950 training step(s), loss on training batch is 0.000452418.
After 12951 training step(s), loss on training batch is 0.000426712.
After 12952 training step(s), loss on training batch is 0.000392774.
After 12953 training step(s), loss on training batch is 0.000393767.
After 12954 training step(s), loss on training batch is 0.000367405.
After 12955 training step(s), loss on training batch is 0.000353178.
After 12956 training step(s), loss on training batch is 0.000448529.
After 12957 training step(s), loss on training batch is 0.000535143.
After 12958 training step(s), loss on training batch is 0.000349717.
After 12959 training step(s), loss on training batch is 0.000390287.
After 12960 training step(s), loss on training batch is 0.000485158.
After 12961 training step(s), loss on training batch is 0.000343884.
After 12962 training step(s), loss on training batch is 0.000312292.
After 12963 training step(s), loss on training batch is 0.000325202.
After 12964 training step(s), loss on training batch is 0.000446507.
After 12965 training step(s), loss on training batch is 0.000400855.
After 12966 training step(s), loss on training batch is 0.000396498.
After 12967 training step(s), loss on training batch is 0.0003924.
After 12968 training step(s), loss on training batch is 0.00036167.
After 12969 training step(s), loss on training batch is 0.000421858.
After 12970 training step(s), loss on training batch is 0.000345083.
After 12971 training step(s), loss on training batch is 0.00045759.
After 12972 training step(s), loss on training batch is 0.000395905.
After 12973 training step(s), loss on training batch is 0.000347158.
After 12974 training step(s), loss on training batch is 0.000326786.
After 12975 training step(s), loss on training batch is 0.000373999.
After 12976 training step(s), loss on training batch is 0.000359121.
After 12977 training step(s), loss on training batch is 0.000488671.
After 12978 training step(s), loss on training batch is 0.000376379.
After 12979 training step(s), loss on training batch is 0.000339121.
After 12980 training step(s), loss on training batch is 0.000354712.
After 12981 training step(s), loss on training batch is 0.000343264.
After 12982 training step(s), loss on training batch is 0.000377408.
After 12983 training step(s), loss on training batch is 0.000751908.
After 12984 training step(s), loss on training batch is 0.00088162.
After 12985 training step(s), loss on training batch is 0.00101512.
After 12986 training step(s), loss on training batch is 0.000618783.
After 12987 training step(s), loss on training batch is 0.000684601.
After 12988 training step(s), loss on training batch is 0.000587052.
After 12989 training step(s), loss on training batch is 0.000623543.
After 12990 training step(s), loss on training batch is 0.000586959.
After 12991 training step(s), loss on training batch is 0.00067841.
After 12992 training step(s), loss on training batch is 0.000709204.
After 12993 training step(s), loss on training batch is 0.000561196.
After 12994 training step(s), loss on training batch is 0.000547432.
After 12995 training step(s), loss on training batch is 0.000729202.
After 12996 training step(s), loss on training batch is 0.000674603.
After 12997 training step(s), loss on training batch is 0.000605685.
After 12998 training step(s), loss on training batch is 0.000632755.
After 12999 training step(s), loss on training batch is 0.000849657.
After 13000 training step(s), loss on training batch is 0.000659834.
After 13001 training step(s), loss on training batch is 0.000582843.
After 13002 training step(s), loss on training batch is 0.000688558.
After 13003 training step(s), loss on training batch is 0.000588698.
After 13004 training step(s), loss on training batch is 0.000570373.
After 13005 training step(s), loss on training batch is 0.000491091.
After 13006 training step(s), loss on training batch is 0.000642299.
After 13007 training step(s), loss on training batch is 0.000809012.
After 13008 training step(s), loss on training batch is 0.000692567.
After 13009 training step(s), loss on training batch is 0.000520414.
After 13010 training step(s), loss on training batch is 0.000811245.
After 13011 training step(s), loss on training batch is 0.00062308.
After 13012 training step(s), loss on training batch is 0.000604722.
After 13013 training step(s), loss on training batch is 0.000501509.
After 13014 training step(s), loss on training batch is 0.000505622.
After 13015 training step(s), loss on training batch is 0.000479846.
After 13016 training step(s), loss on training batch is 0.000497553.
After 13017 training step(s), loss on training batch is 0.000510359.
After 13018 training step(s), loss on training batch is 0.000712915.
After 13019 training step(s), loss on training batch is 0.000951865.
After 13020 training step(s), loss on training batch is 0.000933412.
After 13021 training step(s), loss on training batch is 0.00113233.
After 13022 training step(s), loss on training batch is 0.000618704.
After 13023 training step(s), loss on training batch is 0.000607158.
After 13024 training step(s), loss on training batch is 0.000622732.
After 13025 training step(s), loss on training batch is 0.0006763.
After 13026 training step(s), loss on training batch is 0.000705684.
After 13027 training step(s), loss on training batch is 0.000603578.
After 13028 training step(s), loss on training batch is 0.000636841.
After 13029 training step(s), loss on training batch is 0.000655281.
After 13030 training step(s), loss on training batch is 0.000707273.
After 13031 training step(s), loss on training batch is 0.00077237.
After 13032 training step(s), loss on training batch is 0.000560875.
After 13033 training step(s), loss on training batch is 0.00058666.
After 13034 training step(s), loss on training batch is 0.0005409.
After 13035 training step(s), loss on training batch is 0.000496855.
After 13036 training step(s), loss on training batch is 0.000742224.
After 13037 training step(s), loss on training batch is 0.00112878.
After 13038 training step(s), loss on training batch is 0.000490127.
After 13039 training step(s), loss on training batch is 0.000570165.
After 13040 training step(s), loss on training batch is 0.000606661.
After 13041 training step(s), loss on training batch is 0.000550177.
After 13042 training step(s), loss on training batch is 0.000536043.
After 13043 training step(s), loss on training batch is 0.000603453.
After 13044 training step(s), loss on training batch is 0.000558583.
After 13045 training step(s), loss on training batch is 0.000601203.
After 13046 training step(s), loss on training batch is 0.000747065.
After 13047 training step(s), loss on training batch is 0.000571776.
After 13048 training step(s), loss on training batch is 0.000576673.
After 13049 training step(s), loss on training batch is 0.000626559.
After 13050 training step(s), loss on training batch is 0.000637827.
After 13051 training step(s), loss on training batch is 0.000549269.
After 13052 training step(s), loss on training batch is 0.000488639.
After 13053 training step(s), loss on training batch is 0.000690947.
After 13054 training step(s), loss on training batch is 0.000579615.
After 13055 training step(s), loss on training batch is 0.000631387.
After 13056 training step(s), loss on training batch is 0.000580577.
After 13057 training step(s), loss on training batch is 0.000518779.
After 13058 training step(s), loss on training batch is 0.000657205.
After 13059 training step(s), loss on training batch is 0.000804512.
After 13060 training step(s), loss on training batch is 0.000542844.
After 13061 training step(s), loss on training batch is 0.000494792.
After 13062 training step(s), loss on training batch is 0.000497399.
After 13063 training step(s), loss on training batch is 0.000519253.
After 13064 training step(s), loss on training batch is 0.000479493.
After 13065 training step(s), loss on training batch is 0.000644497.
After 13066 training step(s), loss on training batch is 0.00102197.
After 13067 training step(s), loss on training batch is 0.00106393.
After 13068 training step(s), loss on training batch is 0.000972464.
After 13069 training step(s), loss on training batch is 0.00100348.
After 13070 training step(s), loss on training batch is 0.000966302.
After 13071 training step(s), loss on training batch is 0.000908795.
After 13072 training step(s), loss on training batch is 0.000951435.
After 13073 training step(s), loss on training batch is 0.00134277.
After 13074 training step(s), loss on training batch is 0.0011592.
After 13075 training step(s), loss on training batch is 0.00117906.
After 13076 training step(s), loss on training batch is 0.000938082.
After 13077 training step(s), loss on training batch is 0.000907522.
After 13078 training step(s), loss on training batch is 0.000990341.
After 13079 training step(s), loss on training batch is 0.000870734.
After 13080 training step(s), loss on training batch is 0.000861425.
After 13081 training step(s), loss on training batch is 0.000833651.
After 13082 training step(s), loss on training batch is 0.000866963.
After 13083 training step(s), loss on training batch is 0.000960246.
After 13084 training step(s), loss on training batch is 0.000863736.
After 13085 training step(s), loss on training batch is 0.000834175.
After 13086 training step(s), loss on training batch is 0.00117929.
After 13087 training step(s), loss on training batch is 0.000855254.
After 13088 training step(s), loss on training batch is 0.00104939.
After 13089 training step(s), loss on training batch is 0.000875179.
After 13090 training step(s), loss on training batch is 0.000859289.
After 13091 training step(s), loss on training batch is 0.000872462.
After 13092 training step(s), loss on training batch is 0.000891682.
After 13093 training step(s), loss on training batch is 0.000826961.
After 13094 training step(s), loss on training batch is 0.000966856.
After 13095 training step(s), loss on training batch is 0.0010052.
After 13096 training step(s), loss on training batch is 0.00107809.
After 13097 training step(s), loss on training batch is 0.000902808.
After 13098 training step(s), loss on training batch is 0.000885659.
After 13099 training step(s), loss on training batch is 0.000836485.
After 13100 training step(s), loss on training batch is 0.00088833.
After 13101 training step(s), loss on training batch is 0.000780746.
After 13102 training step(s), loss on training batch is 0.000978546.
After 13103 training step(s), loss on training batch is 0.000842093.
After 13104 training step(s), loss on training batch is 0.00107374.
After 13105 training step(s), loss on training batch is 0.00111129.
After 13106 training step(s), loss on training batch is 0.00111207.
After 13107 training step(s), loss on training batch is 0.00125005.
After 13108 training step(s), loss on training batch is 0.00106356.
After 13109 training step(s), loss on training batch is 0.008212.
After 13110 training step(s), loss on training batch is 0.00175153.
After 13111 training step(s), loss on training batch is 0.00155524.
After 13112 training step(s), loss on training batch is 0.0014763.
After 13113 training step(s), loss on training batch is 0.00137112.
After 13114 training step(s), loss on training batch is 0.00134833.
After 13115 training step(s), loss on training batch is 0.00113227.
After 13116 training step(s), loss on training batch is 0.000527844.
After 13117 training step(s), loss on training batch is 0.000465411.
After 13118 training step(s), loss on training batch is 0.000437805.
After 13119 training step(s), loss on training batch is 0.000463095.
After 13120 training step(s), loss on training batch is 0.000482745.
After 13121 training step(s), loss on training batch is 0.000491487.
After 13122 training step(s), loss on training batch is 0.00050292.
After 13123 training step(s), loss on training batch is 0.000494614.
After 13124 training step(s), loss on training batch is 0.000477744.
After 13125 training step(s), loss on training batch is 0.00043166.
After 13126 training step(s), loss on training batch is 0.000872461.
After 13127 training step(s), loss on training batch is 0.000899459.
After 13128 training step(s), loss on training batch is 0.000986953.
After 13129 training step(s), loss on training batch is 0.000864477.
After 13130 training step(s), loss on training batch is 0.00166299.
After 13131 training step(s), loss on training batch is 0.00141782.
After 13132 training step(s), loss on training batch is 0.00108494.
After 13133 training step(s), loss on training batch is 0.00104715.
After 13134 training step(s), loss on training batch is 0.000923474.
After 13135 training step(s), loss on training batch is 0.000885227.
After 13136 training step(s), loss on training batch is 0.000903392.
After 13137 training step(s), loss on training batch is 0.000917742.
After 13138 training step(s), loss on training batch is 0.00105201.
After 13139 training step(s), loss on training batch is 0.000906251.
After 13140 training step(s), loss on training batch is 0.000889938.
After 13141 training step(s), loss on training batch is 0.000881735.
After 13142 training step(s), loss on training batch is 0.00157129.
After 13143 training step(s), loss on training batch is 0.000334109.
After 13144 training step(s), loss on training batch is 0.000381936.
After 13145 training step(s), loss on training batch is 0.000357266.
After 13146 training step(s), loss on training batch is 0.000453662.
After 13147 training step(s), loss on training batch is 0.000415642.
After 13148 training step(s), loss on training batch is 0.000386735.
After 13149 training step(s), loss on training batch is 0.000343335.
After 13150 training step(s), loss on training batch is 0.000315901.
After 13151 training step(s), loss on training batch is 0.00049232.
After 13152 training step(s), loss on training batch is 0.000412361.
After 13153 training step(s), loss on training batch is 0.000407277.
After 13154 training step(s), loss on training batch is 0.000360923.
After 13155 training step(s), loss on training batch is 0.000510949.
After 13156 training step(s), loss on training batch is 0.000382242.
After 13157 training step(s), loss on training batch is 0.000326544.
After 13158 training step(s), loss on training batch is 0.000328798.
After 13159 training step(s), loss on training batch is 0.000701976.
After 13160 training step(s), loss on training batch is 0.000896191.
After 13161 training step(s), loss on training batch is 0.000503841.
After 13162 training step(s), loss on training batch is 0.00040061.
After 13163 training step(s), loss on training batch is 0.000365985.
After 13164 training step(s), loss on training batch is 0.000446293.
After 13165 training step(s), loss on training batch is 0.000468411.
After 13166 training step(s), loss on training batch is 0.000397237.
After 13167 training step(s), loss on training batch is 0.000359542.
After 13168 training step(s), loss on training batch is 0.000495409.
After 13169 training step(s), loss on training batch is 0.000312166.
After 13170 training step(s), loss on training batch is 0.000386471.
After 13171 training step(s), loss on training batch is 0.000264746.
After 13172 training step(s), loss on training batch is 0.00028852.
After 13173 training step(s), loss on training batch is 0.000308753.
After 13174 training step(s), loss on training batch is 0.000310606.
After 13175 training step(s), loss on training batch is 0.000468367.
After 13176 training step(s), loss on training batch is 0.000410005.
After 13177 training step(s), loss on training batch is 0.000397831.
After 13178 training step(s), loss on training batch is 0.000315735.
After 13179 training step(s), loss on training batch is 0.000352082.
After 13180 training step(s), loss on training batch is 0.000308393.
After 13181 training step(s), loss on training batch is 0.000344845.
After 13182 training step(s), loss on training batch is 0.000319259.
After 13183 training step(s), loss on training batch is 0.000389702.
After 13184 training step(s), loss on training batch is 0.000380087.
After 13185 training step(s), loss on training batch is 0.000397656.
After 13186 training step(s), loss on training batch is 0.000528177.
After 13187 training step(s), loss on training batch is 0.000405297.
After 13188 training step(s), loss on training batch is 0.000321322.
After 13189 training step(s), loss on training batch is 0.000374491.
After 13190 training step(s), loss on training batch is 0.000326166.
After 13191 training step(s), loss on training batch is 0.000310915.
After 13192 training step(s), loss on training batch is 0.000307343.
After 13193 training step(s), loss on training batch is 0.000304664.
After 13194 training step(s), loss on training batch is 0.000265368.
After 13195 training step(s), loss on training batch is 0.000458734.
After 13196 training step(s), loss on training batch is 0.000440643.
After 13197 training step(s), loss on training batch is 0.000237395.
After 13198 training step(s), loss on training batch is 0.000256828.
After 13199 training step(s), loss on training batch is 0.000343105.
After 13200 training step(s), loss on training batch is 0.000350961.
After 13201 training step(s), loss on training batch is 0.000507701.
After 13202 training step(s), loss on training batch is 0.000581493.
After 13203 training step(s), loss on training batch is 0.000356004.
After 13204 training step(s), loss on training batch is 0.000497025.
After 13205 training step(s), loss on training batch is 0.000956525.
After 13206 training step(s), loss on training batch is 0.000657196.
After 13207 training step(s), loss on training batch is 0.000374334.
After 13208 training step(s), loss on training batch is 0.000453638.
After 13209 training step(s), loss on training batch is 0.000570338.
After 13210 training step(s), loss on training batch is 0.000597844.
After 13211 training step(s), loss on training batch is 0.000384786.
After 13212 training step(s), loss on training batch is 0.000295636.
After 13213 training step(s), loss on training batch is 0.000349535.
After 13214 training step(s), loss on training batch is 0.000372174.
After 13215 training step(s), loss on training batch is 0.000335767.
After 13216 training step(s), loss on training batch is 0.000309274.
After 13217 training step(s), loss on training batch is 0.000365604.
After 13218 training step(s), loss on training batch is 0.000347558.
After 13219 training step(s), loss on training batch is 0.000398447.
After 13220 training step(s), loss on training batch is 0.000376094.
After 13221 training step(s), loss on training batch is 0.000301303.
After 13222 training step(s), loss on training batch is 0.000356479.
After 13223 training step(s), loss on training batch is 0.000342145.
After 13224 training step(s), loss on training batch is 0.000315193.
After 13225 training step(s), loss on training batch is 0.000306726.
After 13226 training step(s), loss on training batch is 0.000362859.
After 13227 training step(s), loss on training batch is 0.000291804.
After 13228 training step(s), loss on training batch is 0.000888524.
After 13229 training step(s), loss on training batch is 0.000578665.
After 13230 training step(s), loss on training batch is 0.000525502.
After 13231 training step(s), loss on training batch is 0.000528899.
After 13232 training step(s), loss on training batch is 0.000609795.
After 13233 training step(s), loss on training batch is 0.000680689.
After 13234 training step(s), loss on training batch is 0.000590016.
After 13235 training step(s), loss on training batch is 0.000466245.
After 13236 training step(s), loss on training batch is 0.000673056.
After 13237 training step(s), loss on training batch is 0.000645919.
After 13238 training step(s), loss on training batch is 0.000482475.
After 13239 training step(s), loss on training batch is 0.000479551.
After 13240 training step(s), loss on training batch is 0.00101726.
After 13241 training step(s), loss on training batch is 0.000509482.
After 13242 training step(s), loss on training batch is 0.000575591.
After 13243 training step(s), loss on training batch is 0.000584116.
After 13244 training step(s), loss on training batch is 0.000650119.
After 13245 training step(s), loss on training batch is 0.000495059.
After 13246 training step(s), loss on training batch is 0.000617088.
After 13247 training step(s), loss on training batch is 0.000524413.
After 13248 training step(s), loss on training batch is 0.000567367.
After 13249 training step(s), loss on training batch is 0.000607704.
After 13250 training step(s), loss on training batch is 0.000590256.
After 13251 training step(s), loss on training batch is 0.000572677.
After 13252 training step(s), loss on training batch is 0.000525481.
After 13253 training step(s), loss on training batch is 0.000586507.
After 13254 training step(s), loss on training batch is 0.000432091.
After 13255 training step(s), loss on training batch is 0.000565992.
After 13256 training step(s), loss on training batch is 0.00102021.
After 13257 training step(s), loss on training batch is 0.000869627.
After 13258 training step(s), loss on training batch is 0.000927507.
After 13259 training step(s), loss on training batch is 0.00092672.
After 13260 training step(s), loss on training batch is 0.000801843.
After 13261 training step(s), loss on training batch is 0.000829999.
After 13262 training step(s), loss on training batch is 0.00100458.
After 13263 training step(s), loss on training batch is 0.000896607.
After 13264 training step(s), loss on training batch is 0.000796238.
After 13265 training step(s), loss on training batch is 0.00107388.
After 13266 training step(s), loss on training batch is 0.00079998.
After 13267 training step(s), loss on training batch is 0.00082115.
After 13268 training step(s), loss on training batch is 0.000946719.
After 13269 training step(s), loss on training batch is 0.00145247.
After 13270 training step(s), loss on training batch is 0.00289456.
After 13271 training step(s), loss on training batch is 0.00140446.
After 13272 training step(s), loss on training batch is 0.00104269.
After 13273 training step(s), loss on training batch is 0.00118094.
After 13274 training step(s), loss on training batch is 0.00102483.
After 13275 training step(s), loss on training batch is 0.000975337.
After 13276 training step(s), loss on training batch is 0.00090766.
After 13277 training step(s), loss on training batch is 0.00122426.
After 13278 training step(s), loss on training batch is 0.00087236.
After 13279 training step(s), loss on training batch is 0.00082702.
After 13280 training step(s), loss on training batch is 0.000812075.
After 13281 training step(s), loss on training batch is 0.00055576.
After 13282 training step(s), loss on training batch is 0.000421645.
After 13283 training step(s), loss on training batch is 0.000275399.
After 13284 training step(s), loss on training batch is 0.000341199.
After 13285 training step(s), loss on training batch is 0.000369687.
After 13286 training step(s), loss on training batch is 0.000501929.
After 13287 training step(s), loss on training batch is 0.000609237.
After 13288 training step(s), loss on training batch is 0.000405617.
After 13289 training step(s), loss on training batch is 0.000342726.
After 13290 training step(s), loss on training batch is 0.000301981.
After 13291 training step(s), loss on training batch is 0.000258677.
After 13292 training step(s), loss on training batch is 0.000331886.
After 13293 training step(s), loss on training batch is 0.000297138.
After 13294 training step(s), loss on training batch is 0.000298444.
After 13295 training step(s), loss on training batch is 0.000340887.
After 13296 training step(s), loss on training batch is 0.000351425.
After 13297 training step(s), loss on training batch is 0.000275112.
After 13298 training step(s), loss on training batch is 0.000260964.
After 13299 training step(s), loss on training batch is 0.000278749.
After 13300 training step(s), loss on training batch is 0.000287208.
After 13301 training step(s), loss on training batch is 0.000692984.
After 13302 training step(s), loss on training batch is 0.000476262.
After 13303 training step(s), loss on training batch is 0.000515958.
After 13304 training step(s), loss on training batch is 0.000614907.
After 13305 training step(s), loss on training batch is 0.000581318.
After 13306 training step(s), loss on training batch is 0.00058549.
After 13307 training step(s), loss on training batch is 0.000555282.
After 13308 training step(s), loss on training batch is 0.000327801.
After 13309 training step(s), loss on training batch is 0.000369012.
After 13310 training step(s), loss on training batch is 0.000387312.
After 13311 training step(s), loss on training batch is 0.000454213.
After 13312 training step(s), loss on training batch is 0.000427785.
After 13313 training step(s), loss on training batch is 0.000463115.
After 13314 training step(s), loss on training batch is 0.000610771.
After 13315 training step(s), loss on training batch is 0.00135186.
After 13316 training step(s), loss on training batch is 0.000953198.
After 13317 training step(s), loss on training batch is 0.000778825.
After 13318 training step(s), loss on training batch is 0.000601925.
After 13319 training step(s), loss on training batch is 0.000375711.
After 13320 training step(s), loss on training batch is 0.000355857.
After 13321 training step(s), loss on training batch is 0.000364263.
After 13322 training step(s), loss on training batch is 0.000324269.
After 13323 training step(s), loss on training batch is 0.000499667.
After 13324 training step(s), loss on training batch is 0.000390882.
After 13325 training step(s), loss on training batch is 0.000380694.
After 13326 training step(s), loss on training batch is 0.000373378.
After 13327 training step(s), loss on training batch is 0.000780866.
After 13328 training step(s), loss on training batch is 0.00188158.
After 13329 training step(s), loss on training batch is 0.000672865.
After 13330 training step(s), loss on training batch is 0.000513353.
After 13331 training step(s), loss on training batch is 0.000519159.
After 13332 training step(s), loss on training batch is 0.000508718.
After 13333 training step(s), loss on training batch is 0.00042894.
After 13334 training step(s), loss on training batch is 0.000339853.
After 13335 training step(s), loss on training batch is 0.000314535.
After 13336 training step(s), loss on training batch is 0.000313725.
After 13337 training step(s), loss on training batch is 0.000355942.
After 13338 training step(s), loss on training batch is 0.000346269.
After 13339 training step(s), loss on training batch is 0.000386157.
After 13340 training step(s), loss on training batch is 0.000432417.
After 13341 training step(s), loss on training batch is 0.000348019.
After 13342 training step(s), loss on training batch is 0.000424778.
After 13343 training step(s), loss on training batch is 0.000410034.
After 13344 training step(s), loss on training batch is 0.000488537.
After 13345 training step(s), loss on training batch is 0.000328199.
After 13346 training step(s), loss on training batch is 0.00045301.
After 13347 training step(s), loss on training batch is 0.000342392.
After 13348 training step(s), loss on training batch is 0.000318217.
After 13349 training step(s), loss on training batch is 0.000360531.
After 13350 training step(s), loss on training batch is 0.000469105.
After 13351 training step(s), loss on training batch is 0.000432461.
After 13352 training step(s), loss on training batch is 0.000381939.
After 13353 training step(s), loss on training batch is 0.000390437.
After 13354 training step(s), loss on training batch is 0.000362725.
After 13355 training step(s), loss on training batch is 0.000342994.
After 13356 training step(s), loss on training batch is 0.000450865.
After 13357 training step(s), loss on training batch is 0.000547646.
After 13358 training step(s), loss on training batch is 0.000345371.
After 13359 training step(s), loss on training batch is 0.000380452.
After 13360 training step(s), loss on training batch is 0.000490509.
After 13361 training step(s), loss on training batch is 0.000335223.
After 13362 training step(s), loss on training batch is 0.000308246.
After 13363 training step(s), loss on training batch is 0.000327098.
After 13364 training step(s), loss on training batch is 0.000449071.
After 13365 training step(s), loss on training batch is 0.000389735.
After 13366 training step(s), loss on training batch is 0.000387311.
After 13367 training step(s), loss on training batch is 0.000386582.
After 13368 training step(s), loss on training batch is 0.000366824.
After 13369 training step(s), loss on training batch is 0.00040717.
After 13370 training step(s), loss on training batch is 0.000341495.
After 13371 training step(s), loss on training batch is 0.000354824.
After 13372 training step(s), loss on training batch is 0.000362953.
After 13373 training step(s), loss on training batch is 0.000330759.
After 13374 training step(s), loss on training batch is 0.000306625.
After 13375 training step(s), loss on training batch is 0.000368967.
After 13376 training step(s), loss on training batch is 0.00035547.
After 13377 training step(s), loss on training batch is 0.000483461.
After 13378 training step(s), loss on training batch is 0.000368423.
After 13379 training step(s), loss on training batch is 0.000341167.
After 13380 training step(s), loss on training batch is 0.000358243.
After 13381 training step(s), loss on training batch is 0.000341223.
After 13382 training step(s), loss on training batch is 0.000364572.
After 13383 training step(s), loss on training batch is 0.000660828.
After 13384 training step(s), loss on training batch is 0.000765929.
After 13385 training step(s), loss on training batch is 0.000907194.
After 13386 training step(s), loss on training batch is 0.00061109.
After 13387 training step(s), loss on training batch is 0.000661725.
After 13388 training step(s), loss on training batch is 0.000578834.
After 13389 training step(s), loss on training batch is 0.000611868.
After 13390 training step(s), loss on training batch is 0.000573938.
After 13391 training step(s), loss on training batch is 0.000670533.
After 13392 training step(s), loss on training batch is 0.000711923.
After 13393 training step(s), loss on training batch is 0.00056465.
After 13394 training step(s), loss on training batch is 0.00055307.
After 13395 training step(s), loss on training batch is 0.000710011.
After 13396 training step(s), loss on training batch is 0.000665871.
After 13397 training step(s), loss on training batch is 0.000604906.
After 13398 training step(s), loss on training batch is 0.000631702.
After 13399 training step(s), loss on training batch is 0.000837209.
After 13400 training step(s), loss on training batch is 0.000654811.
After 13401 training step(s), loss on training batch is 0.00057975.
After 13402 training step(s), loss on training batch is 0.000675682.
After 13403 training step(s), loss on training batch is 0.000591425.
After 13404 training step(s), loss on training batch is 0.00057879.
After 13405 training step(s), loss on training batch is 0.000496569.
After 13406 training step(s), loss on training batch is 0.000641335.
After 13407 training step(s), loss on training batch is 0.00078971.
After 13408 training step(s), loss on training batch is 0.000670674.
After 13409 training step(s), loss on training batch is 0.000515987.
After 13410 training step(s), loss on training batch is 0.000801131.
After 13411 training step(s), loss on training batch is 0.000624198.
After 13412 training step(s), loss on training batch is 0.000597725.
After 13413 training step(s), loss on training batch is 0.000505398.
After 13414 training step(s), loss on training batch is 0.000509714.
After 13415 training step(s), loss on training batch is 0.000490047.
After 13416 training step(s), loss on training batch is 0.000506166.
After 13417 training step(s), loss on training batch is 0.000511767.
After 13418 training step(s), loss on training batch is 0.000692647.
After 13419 training step(s), loss on training batch is 0.000915947.
After 13420 training step(s), loss on training batch is 0.00091088.
After 13421 training step(s), loss on training batch is 0.00107529.
After 13422 training step(s), loss on training batch is 0.000619118.
After 13423 training step(s), loss on training batch is 0.000606588.
After 13424 training step(s), loss on training batch is 0.000607429.
After 13425 training step(s), loss on training batch is 0.000657825.
After 13426 training step(s), loss on training batch is 0.000695539.
After 13427 training step(s), loss on training batch is 0.000593334.
After 13428 training step(s), loss on training batch is 0.000628537.
After 13429 training step(s), loss on training batch is 0.000641526.
After 13430 training step(s), loss on training batch is 0.000703915.
After 13431 training step(s), loss on training batch is 0.000797255.
After 13432 training step(s), loss on training batch is 0.000542957.
After 13433 training step(s), loss on training batch is 0.000567279.
After 13434 training step(s), loss on training batch is 0.000521382.
After 13435 training step(s), loss on training batch is 0.000478228.
After 13436 training step(s), loss on training batch is 0.000712591.
After 13437 training step(s), loss on training batch is 0.00108519.
After 13438 training step(s), loss on training batch is 0.00048451.
After 13439 training step(s), loss on training batch is 0.000562392.
After 13440 training step(s), loss on training batch is 0.000600417.
After 13441 training step(s), loss on training batch is 0.000548917.
After 13442 training step(s), loss on training batch is 0.000532115.
After 13443 training step(s), loss on training batch is 0.000604563.
After 13444 training step(s), loss on training batch is 0.000562301.
After 13445 training step(s), loss on training batch is 0.000596332.
After 13446 training step(s), loss on training batch is 0.000739786.
After 13447 training step(s), loss on training batch is 0.000561481.
After 13448 training step(s), loss on training batch is 0.000577939.
After 13449 training step(s), loss on training batch is 0.000619137.
After 13450 training step(s), loss on training batch is 0.000631731.
After 13451 training step(s), loss on training batch is 0.000539353.
After 13452 training step(s), loss on training batch is 0.000483233.
After 13453 training step(s), loss on training batch is 0.000681985.
After 13454 training step(s), loss on training batch is 0.000583184.
After 13455 training step(s), loss on training batch is 0.000634977.
After 13456 training step(s), loss on training batch is 0.0005856.
After 13457 training step(s), loss on training batch is 0.000497325.
After 13458 training step(s), loss on training batch is 0.000663251.
After 13459 training step(s), loss on training batch is 0.000815894.
After 13460 training step(s), loss on training batch is 0.000543002.
After 13461 training step(s), loss on training batch is 0.000482115.
After 13462 training step(s), loss on training batch is 0.000486822.
After 13463 training step(s), loss on training batch is 0.000510943.
After 13464 training step(s), loss on training batch is 0.000468786.
After 13465 training step(s), loss on training batch is 0.000645108.
After 13466 training step(s), loss on training batch is 0.00101326.
After 13467 training step(s), loss on training batch is 0.00106065.
After 13468 training step(s), loss on training batch is 0.000966728.
After 13469 training step(s), loss on training batch is 0.000996599.
After 13470 training step(s), loss on training batch is 0.000961246.
After 13471 training step(s), loss on training batch is 0.000898923.
After 13472 training step(s), loss on training batch is 0.000948706.
After 13473 training step(s), loss on training batch is 0.00132609.
After 13474 training step(s), loss on training batch is 0.00113772.
After 13475 training step(s), loss on training batch is 0.00118739.
After 13476 training step(s), loss on training batch is 0.000928595.
After 13477 training step(s), loss on training batch is 0.000907258.
After 13478 training step(s), loss on training batch is 0.000973054.
After 13479 training step(s), loss on training batch is 0.000860136.
After 13480 training step(s), loss on training batch is 0.000855139.
After 13481 training step(s), loss on training batch is 0.000820743.
After 13482 training step(s), loss on training batch is 0.000862377.
After 13483 training step(s), loss on training batch is 0.00095758.
After 13484 training step(s), loss on training batch is 0.000857141.
After 13485 training step(s), loss on training batch is 0.000860594.
After 13486 training step(s), loss on training batch is 0.00109768.
After 13487 training step(s), loss on training batch is 0.000873227.
After 13488 training step(s), loss on training batch is 0.00102978.
After 13489 training step(s), loss on training batch is 0.000891963.
After 13490 training step(s), loss on training batch is 0.000880588.
After 13491 training step(s), loss on training batch is 0.000874357.
After 13492 training step(s), loss on training batch is 0.000882452.
After 13493 training step(s), loss on training batch is 0.000820756.
After 13494 training step(s), loss on training batch is 0.000952916.
After 13495 training step(s), loss on training batch is 0.000987181.
After 13496 training step(s), loss on training batch is 0.00106162.
After 13497 training step(s), loss on training batch is 0.000902023.
After 13498 training step(s), loss on training batch is 0.000878392.
After 13499 training step(s), loss on training batch is 0.000839482.
After 13500 training step(s), loss on training batch is 0.0008848.
After 13501 training step(s), loss on training batch is 0.000775674.
After 13502 training step(s), loss on training batch is 0.000969013.
After 13503 training step(s), loss on training batch is 0.000852814.
After 13504 training step(s), loss on training batch is 0.00106508.
After 13505 training step(s), loss on training batch is 0.00109914.
After 13506 training step(s), loss on training batch is 0.00107506.
After 13507 training step(s), loss on training batch is 0.00122301.
After 13508 training step(s), loss on training batch is 0.00105861.
After 13509 training step(s), loss on training batch is 0.00827939.
After 13510 training step(s), loss on training batch is 0.00497613.
After 13511 training step(s), loss on training batch is 0.00212922.
After 13512 training step(s), loss on training batch is 0.00175773.
After 13513 training step(s), loss on training batch is 0.00153656.
After 13514 training step(s), loss on training batch is 0.00148401.
After 13515 training step(s), loss on training batch is 0.00124362.
After 13516 training step(s), loss on training batch is 0.000602517.
After 13517 training step(s), loss on training batch is 0.000532401.
After 13518 training step(s), loss on training batch is 0.000480755.
After 13519 training step(s), loss on training batch is 0.000524619.
After 13520 training step(s), loss on training batch is 0.00051653.
After 13521 training step(s), loss on training batch is 0.000518397.
After 13522 training step(s), loss on training batch is 0.000540578.
After 13523 training step(s), loss on training batch is 0.000539196.
After 13524 training step(s), loss on training batch is 0.000489982.
After 13525 training step(s), loss on training batch is 0.000446826.
After 13526 training step(s), loss on training batch is 0.00088499.
After 13527 training step(s), loss on training batch is 0.000906166.
After 13528 training step(s), loss on training batch is 0.00102698.
After 13529 training step(s), loss on training batch is 0.00086097.
After 13530 training step(s), loss on training batch is 0.00176201.
After 13531 training step(s), loss on training batch is 0.00146035.
After 13532 training step(s), loss on training batch is 0.00108051.
After 13533 training step(s), loss on training batch is 0.00107534.
After 13534 training step(s), loss on training batch is 0.000929067.
After 13535 training step(s), loss on training batch is 0.000884841.
After 13536 training step(s), loss on training batch is 0.000914789.
After 13537 training step(s), loss on training batch is 0.000919091.
After 13538 training step(s), loss on training batch is 0.00101408.
After 13539 training step(s), loss on training batch is 0.000920213.
After 13540 training step(s), loss on training batch is 0.000904574.
After 13541 training step(s), loss on training batch is 0.000895496.
After 13542 training step(s), loss on training batch is 0.00150447.
After 13543 training step(s), loss on training batch is 0.000344096.
After 13544 training step(s), loss on training batch is 0.00038901.
After 13545 training step(s), loss on training batch is 0.00037054.
After 13546 training step(s), loss on training batch is 0.000443528.
After 13547 training step(s), loss on training batch is 0.000436289.
After 13548 training step(s), loss on training batch is 0.000419338.
After 13549 training step(s), loss on training batch is 0.000356111.
After 13550 training step(s), loss on training batch is 0.000322378.
After 13551 training step(s), loss on training batch is 0.000498511.
After 13552 training step(s), loss on training batch is 0.000431534.
After 13553 training step(s), loss on training batch is 0.000399932.
After 13554 training step(s), loss on training batch is 0.000363937.
After 13555 training step(s), loss on training batch is 0.000536089.
After 13556 training step(s), loss on training batch is 0.000389926.
After 13557 training step(s), loss on training batch is 0.000339871.
After 13558 training step(s), loss on training batch is 0.000328756.
After 13559 training step(s), loss on training batch is 0.000737083.
After 13560 training step(s), loss on training batch is 0.000903389.
After 13561 training step(s), loss on training batch is 0.000516433.
After 13562 training step(s), loss on training batch is 0.00040454.
After 13563 training step(s), loss on training batch is 0.000376081.
After 13564 training step(s), loss on training batch is 0.000446768.
After 13565 training step(s), loss on training batch is 0.000456419.
After 13566 training step(s), loss on training batch is 0.000405764.
After 13567 training step(s), loss on training batch is 0.00035566.
After 13568 training step(s), loss on training batch is 0.000518258.
After 13569 training step(s), loss on training batch is 0.000319629.
After 13570 training step(s), loss on training batch is 0.000401406.
After 13571 training step(s), loss on training batch is 0.000266645.
After 13572 training step(s), loss on training batch is 0.000280447.
After 13573 training step(s), loss on training batch is 0.000318026.
After 13574 training step(s), loss on training batch is 0.000316911.
After 13575 training step(s), loss on training batch is 0.000434806.
After 13576 training step(s), loss on training batch is 0.000405749.
After 13577 training step(s), loss on training batch is 0.000407834.
After 13578 training step(s), loss on training batch is 0.000321121.
After 13579 training step(s), loss on training batch is 0.00033033.
After 13580 training step(s), loss on training batch is 0.000310573.
After 13581 training step(s), loss on training batch is 0.000352511.
After 13582 training step(s), loss on training batch is 0.000318689.
After 13583 training step(s), loss on training batch is 0.000391519.
After 13584 training step(s), loss on training batch is 0.000390286.
After 13585 training step(s), loss on training batch is 0.000402686.
After 13586 training step(s), loss on training batch is 0.000501022.
After 13587 training step(s), loss on training batch is 0.000411022.
After 13588 training step(s), loss on training batch is 0.00032297.
After 13589 training step(s), loss on training batch is 0.000366977.
After 13590 training step(s), loss on training batch is 0.000337261.
After 13591 training step(s), loss on training batch is 0.000306925.
After 13592 training step(s), loss on training batch is 0.000313861.
After 13593 training step(s), loss on training batch is 0.000318127.
After 13594 training step(s), loss on training batch is 0.000265679.
After 13595 training step(s), loss on training batch is 0.000462594.
After 13596 training step(s), loss on training batch is 0.000444128.
After 13597 training step(s), loss on training batch is 0.000239946.
After 13598 training step(s), loss on training batch is 0.000263793.
After 13599 training step(s), loss on training batch is 0.000357432.
After 13600 training step(s), loss on training batch is 0.000364719.
After 13601 training step(s), loss on training batch is 0.000494896.
After 13602 training step(s), loss on training batch is 0.000577236.
After 13603 training step(s), loss on training batch is 0.000351818.
After 13604 training step(s), loss on training batch is 0.000523331.
After 13605 training step(s), loss on training batch is 0.000958987.
After 13606 training step(s), loss on training batch is 0.000675882.
After 13607 training step(s), loss on training batch is 0.000373866.
After 13608 training step(s), loss on training batch is 0.000459851.
After 13609 training step(s), loss on training batch is 0.000563092.
After 13610 training step(s), loss on training batch is 0.000580107.
After 13611 training step(s), loss on training batch is 0.000377923.
After 13612 training step(s), loss on training batch is 0.000301324.
After 13613 training step(s), loss on training batch is 0.000352286.
After 13614 training step(s), loss on training batch is 0.000379166.
After 13615 training step(s), loss on training batch is 0.000343772.
After 13616 training step(s), loss on training batch is 0.000305963.
After 13617 training step(s), loss on training batch is 0.000370356.
After 13618 training step(s), loss on training batch is 0.000338599.
After 13619 training step(s), loss on training batch is 0.000402728.
After 13620 training step(s), loss on training batch is 0.000369983.
After 13621 training step(s), loss on training batch is 0.000299595.
After 13622 training step(s), loss on training batch is 0.000342895.
After 13623 training step(s), loss on training batch is 0.000358134.
After 13624 training step(s), loss on training batch is 0.000326236.
After 13625 training step(s), loss on training batch is 0.000306679.
After 13626 training step(s), loss on training batch is 0.000355029.
After 13627 training step(s), loss on training batch is 0.000294285.
After 13628 training step(s), loss on training batch is 0.00084316.
After 13629 training step(s), loss on training batch is 0.000590696.
After 13630 training step(s), loss on training batch is 0.000521857.
After 13631 training step(s), loss on training batch is 0.000528676.
After 13632 training step(s), loss on training batch is 0.000609366.
After 13633 training step(s), loss on training batch is 0.000699941.
After 13634 training step(s), loss on training batch is 0.000600392.
After 13635 training step(s), loss on training batch is 0.000458158.
After 13636 training step(s), loss on training batch is 0.000671404.
After 13637 training step(s), loss on training batch is 0.000644497.
After 13638 training step(s), loss on training batch is 0.000477589.
After 13639 training step(s), loss on training batch is 0.000469074.
After 13640 training step(s), loss on training batch is 0.00101987.
After 13641 training step(s), loss on training batch is 0.000492595.
After 13642 training step(s), loss on training batch is 0.000564731.
After 13643 training step(s), loss on training batch is 0.000571714.
After 13644 training step(s), loss on training batch is 0.000659219.
After 13645 training step(s), loss on training batch is 0.000480834.
After 13646 training step(s), loss on training batch is 0.000626373.
After 13647 training step(s), loss on training batch is 0.000517147.
After 13648 training step(s), loss on training batch is 0.000563029.
After 13649 training step(s), loss on training batch is 0.000615867.
After 13650 training step(s), loss on training batch is 0.000589978.
After 13651 training step(s), loss on training batch is 0.000577836.
After 13652 training step(s), loss on training batch is 0.000528222.
After 13653 training step(s), loss on training batch is 0.000584853.
After 13654 training step(s), loss on training batch is 0.000421017.
After 13655 training step(s), loss on training batch is 0.000566042.
After 13656 training step(s), loss on training batch is 0.00101694.
After 13657 training step(s), loss on training batch is 0.000862851.
After 13658 training step(s), loss on training batch is 0.000925901.
After 13659 training step(s), loss on training batch is 0.000928284.
After 13660 training step(s), loss on training batch is 0.000807606.
After 13661 training step(s), loss on training batch is 0.000829491.
After 13662 training step(s), loss on training batch is 0.00101597.
After 13663 training step(s), loss on training batch is 0.000886503.
After 13664 training step(s), loss on training batch is 0.000803048.
After 13665 training step(s), loss on training batch is 0.00104765.
After 13666 training step(s), loss on training batch is 0.0007966.
After 13667 training step(s), loss on training batch is 0.000819393.
After 13668 training step(s), loss on training batch is 0.000921294.
After 13669 training step(s), loss on training batch is 0.0014713.
After 13670 training step(s), loss on training batch is 0.00437186.
After 13671 training step(s), loss on training batch is 0.00141056.
After 13672 training step(s), loss on training batch is 0.000966252.
After 13673 training step(s), loss on training batch is 0.00117016.
After 13674 training step(s), loss on training batch is 0.00101664.
After 13675 training step(s), loss on training batch is 0.000971128.
After 13676 training step(s), loss on training batch is 0.000901625.
After 13677 training step(s), loss on training batch is 0.00120664.
After 13678 training step(s), loss on training batch is 0.000879856.
After 13679 training step(s), loss on training batch is 0.000852657.
After 13680 training step(s), loss on training batch is 0.000800522.
After 13681 training step(s), loss on training batch is 0.000522167.
After 13682 training step(s), loss on training batch is 0.000427216.
After 13683 training step(s), loss on training batch is 0.000275558.
After 13684 training step(s), loss on training batch is 0.000355015.
After 13685 training step(s), loss on training batch is 0.000363112.
After 13686 training step(s), loss on training batch is 0.000516381.
After 13687 training step(s), loss on training batch is 0.000600782.
After 13688 training step(s), loss on training batch is 0.00039933.
After 13689 training step(s), loss on training batch is 0.000353091.
After 13690 training step(s), loss on training batch is 0.000315091.
After 13691 training step(s), loss on training batch is 0.000261303.
After 13692 training step(s), loss on training batch is 0.00033149.
After 13693 training step(s), loss on training batch is 0.000302377.
After 13694 training step(s), loss on training batch is 0.000305843.
After 13695 training step(s), loss on training batch is 0.000358547.
After 13696 training step(s), loss on training batch is 0.000352909.
After 13697 training step(s), loss on training batch is 0.000283563.
After 13698 training step(s), loss on training batch is 0.000263889.
After 13699 training step(s), loss on training batch is 0.000280236.
After 13700 training step(s), loss on training batch is 0.000280635.
After 13701 training step(s), loss on training batch is 0.000953709.
After 13702 training step(s), loss on training batch is 0.000484814.
After 13703 training step(s), loss on training batch is 0.000506996.
After 13704 training step(s), loss on training batch is 0.000602802.
After 13705 training step(s), loss on training batch is 0.000570511.
After 13706 training step(s), loss on training batch is 0.00058445.
After 13707 training step(s), loss on training batch is 0.000579725.
After 13708 training step(s), loss on training batch is 0.000325643.
After 13709 training step(s), loss on training batch is 0.000364648.
After 13710 training step(s), loss on training batch is 0.00038748.
After 13711 training step(s), loss on training batch is 0.000464514.
After 13712 training step(s), loss on training batch is 0.000461963.
After 13713 training step(s), loss on training batch is 0.000459409.
After 13714 training step(s), loss on training batch is 0.000602065.
After 13715 training step(s), loss on training batch is 0.00135376.
After 13716 training step(s), loss on training batch is 0.000945655.
After 13717 training step(s), loss on training batch is 0.000800878.
After 13718 training step(s), loss on training batch is 0.000611171.
After 13719 training step(s), loss on training batch is 0.000381305.
After 13720 training step(s), loss on training batch is 0.000374313.
After 13721 training step(s), loss on training batch is 0.000386007.
After 13722 training step(s), loss on training batch is 0.000340652.
After 13723 training step(s), loss on training batch is 0.000531477.
After 13724 training step(s), loss on training batch is 0.000407614.
After 13725 training step(s), loss on training batch is 0.000385016.
After 13726 training step(s), loss on training batch is 0.000380769.
After 13727 training step(s), loss on training batch is 0.000805661.
After 13728 training step(s), loss on training batch is 0.00183665.
After 13729 training step(s), loss on training batch is 0.000681356.
After 13730 training step(s), loss on training batch is 0.00051724.
After 13731 training step(s), loss on training batch is 0.000542026.
After 13732 training step(s), loss on training batch is 0.000500738.
After 13733 training step(s), loss on training batch is 0.000429338.
After 13734 training step(s), loss on training batch is 0.000340927.
After 13735 training step(s), loss on training batch is 0.00031952.
After 13736 training step(s), loss on training batch is 0.000317587.
After 13737 training step(s), loss on training batch is 0.000357213.
After 13738 training step(s), loss on training batch is 0.000356052.
After 13739 training step(s), loss on training batch is 0.000398473.
After 13740 training step(s), loss on training batch is 0.000449386.
After 13741 training step(s), loss on training batch is 0.000356506.
After 13742 training step(s), loss on training batch is 0.000440476.
After 13743 training step(s), loss on training batch is 0.000407451.
After 13744 training step(s), loss on training batch is 0.000483591.
After 13745 training step(s), loss on training batch is 0.000333777.
After 13746 training step(s), loss on training batch is 0.000442495.
After 13747 training step(s), loss on training batch is 0.000349037.
After 13748 training step(s), loss on training batch is 0.000319808.
After 13749 training step(s), loss on training batch is 0.00035716.
After 13750 training step(s), loss on training batch is 0.000468438.
After 13751 training step(s), loss on training batch is 0.000426981.
After 13752 training step(s), loss on training batch is 0.000411598.
After 13753 training step(s), loss on training batch is 0.00039775.
After 13754 training step(s), loss on training batch is 0.000379527.
After 13755 training step(s), loss on training batch is 0.000354533.
After 13756 training step(s), loss on training batch is 0.000451158.
After 13757 training step(s), loss on training batch is 0.000524865.
After 13758 training step(s), loss on training batch is 0.000359503.
After 13759 training step(s), loss on training batch is 0.000397864.
After 13760 training step(s), loss on training batch is 0.000483423.
After 13761 training step(s), loss on training batch is 0.000358134.
After 13762 training step(s), loss on training batch is 0.000329898.
After 13763 training step(s), loss on training batch is 0.000329711.
After 13764 training step(s), loss on training batch is 0.000433214.
After 13765 training step(s), loss on training batch is 0.000401748.
After 13766 training step(s), loss on training batch is 0.00039622.
After 13767 training step(s), loss on training batch is 0.000389353.
After 13768 training step(s), loss on training batch is 0.000361704.
After 13769 training step(s), loss on training batch is 0.000424475.
After 13770 training step(s), loss on training batch is 0.000346912.
After 13771 training step(s), loss on training batch is 0.000350141.
After 13772 training step(s), loss on training batch is 0.000336327.
After 13773 training step(s), loss on training batch is 0.000322635.
After 13774 training step(s), loss on training batch is 0.000307187.
After 13775 training step(s), loss on training batch is 0.000374642.
After 13776 training step(s), loss on training batch is 0.000366991.
After 13777 training step(s), loss on training batch is 0.000453044.
After 13778 training step(s), loss on training batch is 0.000368396.
After 13779 training step(s), loss on training batch is 0.000341072.
After 13780 training step(s), loss on training batch is 0.000349603.
After 13781 training step(s), loss on training batch is 0.000343415.
After 13782 training step(s), loss on training batch is 0.000354926.
After 13783 training step(s), loss on training batch is 0.000622456.
After 13784 training step(s), loss on training batch is 0.000755013.
After 13785 training step(s), loss on training batch is 0.000879313.
After 13786 training step(s), loss on training batch is 0.000608739.
After 13787 training step(s), loss on training batch is 0.000641032.
After 13788 training step(s), loss on training batch is 0.000569703.
After 13789 training step(s), loss on training batch is 0.000615001.
After 13790 training step(s), loss on training batch is 0.00058044.
After 13791 training step(s), loss on training batch is 0.000681463.
After 13792 training step(s), loss on training batch is 0.000687167.
After 13793 training step(s), loss on training batch is 0.000570721.
After 13794 training step(s), loss on training batch is 0.000562836.
After 13795 training step(s), loss on training batch is 0.000694571.
After 13796 training step(s), loss on training batch is 0.000657431.
After 13797 training step(s), loss on training batch is 0.000602826.
After 13798 training step(s), loss on training batch is 0.000625325.
After 13799 training step(s), loss on training batch is 0.000838026.
After 13800 training step(s), loss on training batch is 0.000641796.
After 13801 training step(s), loss on training batch is 0.000575137.
After 13802 training step(s), loss on training batch is 0.000675689.
After 13803 training step(s), loss on training batch is 0.000572743.
After 13804 training step(s), loss on training batch is 0.000561883.
After 13805 training step(s), loss on training batch is 0.000473067.
After 13806 training step(s), loss on training batch is 0.000657922.
After 13807 training step(s), loss on training batch is 0.000821133.
After 13808 training step(s), loss on training batch is 0.000709748.
After 13809 training step(s), loss on training batch is 0.000517701.
After 13810 training step(s), loss on training batch is 0.000814432.
After 13811 training step(s), loss on training batch is 0.000616136.
After 13812 training step(s), loss on training batch is 0.000608429.
After 13813 training step(s), loss on training batch is 0.000495745.
After 13814 training step(s), loss on training batch is 0.000503472.
After 13815 training step(s), loss on training batch is 0.000479665.
After 13816 training step(s), loss on training batch is 0.000502282.
After 13817 training step(s), loss on training batch is 0.000507198.
After 13818 training step(s), loss on training batch is 0.000681419.
After 13819 training step(s), loss on training batch is 0.00090522.
After 13820 training step(s), loss on training batch is 0.000979434.
After 13821 training step(s), loss on training batch is 0.0011118.
After 13822 training step(s), loss on training batch is 0.000607759.
After 13823 training step(s), loss on training batch is 0.000591827.
After 13824 training step(s), loss on training batch is 0.000613098.
After 13825 training step(s), loss on training batch is 0.000664195.
After 13826 training step(s), loss on training batch is 0.000701091.
After 13827 training step(s), loss on training batch is 0.00060051.
After 13828 training step(s), loss on training batch is 0.000632977.
After 13829 training step(s), loss on training batch is 0.000640662.
After 13830 training step(s), loss on training batch is 0.00070044.
After 13831 training step(s), loss on training batch is 0.000775966.
After 13832 training step(s), loss on training batch is 0.000549412.
After 13833 training step(s), loss on training batch is 0.000578455.
After 13834 training step(s), loss on training batch is 0.000531093.
After 13835 training step(s), loss on training batch is 0.000485099.
After 13836 training step(s), loss on training batch is 0.000728742.
After 13837 training step(s), loss on training batch is 0.00109382.
After 13838 training step(s), loss on training batch is 0.00048248.
After 13839 training step(s), loss on training batch is 0.000565565.
After 13840 training step(s), loss on training batch is 0.000599515.
After 13841 training step(s), loss on training batch is 0.000552491.
After 13842 training step(s), loss on training batch is 0.000533452.
After 13843 training step(s), loss on training batch is 0.000608863.
After 13844 training step(s), loss on training batch is 0.000563258.
After 13845 training step(s), loss on training batch is 0.000605072.
After 13846 training step(s), loss on training batch is 0.000736028.
After 13847 training step(s), loss on training batch is 0.000578003.
After 13848 training step(s), loss on training batch is 0.000567254.
After 13849 training step(s), loss on training batch is 0.00062534.
After 13850 training step(s), loss on training batch is 0.000628364.
After 13851 training step(s), loss on training batch is 0.000528132.
After 13852 training step(s), loss on training batch is 0.000493354.
After 13853 training step(s), loss on training batch is 0.000665633.
After 13854 training step(s), loss on training batch is 0.000574622.
After 13855 training step(s), loss on training batch is 0.000623236.
After 13856 training step(s), loss on training batch is 0.00056765.
After 13857 training step(s), loss on training batch is 0.000517315.
After 13858 training step(s), loss on training batch is 0.000651982.
After 13859 training step(s), loss on training batch is 0.000819181.
After 13860 training step(s), loss on training batch is 0.000539049.
After 13861 training step(s), loss on training batch is 0.000491117.
After 13862 training step(s), loss on training batch is 0.000495358.
After 13863 training step(s), loss on training batch is 0.00051642.
After 13864 training step(s), loss on training batch is 0.000466058.
After 13865 training step(s), loss on training batch is 0.000667216.
After 13866 training step(s), loss on training batch is 0.000997268.
After 13867 training step(s), loss on training batch is 0.00104914.
After 13868 training step(s), loss on training batch is 0.000955458.
After 13869 training step(s), loss on training batch is 0.00098211.
After 13870 training step(s), loss on training batch is 0.000953826.
After 13871 training step(s), loss on training batch is 0.00089195.
After 13872 training step(s), loss on training batch is 0.000944973.
After 13873 training step(s), loss on training batch is 0.00136594.
After 13874 training step(s), loss on training batch is 0.0011237.
After 13875 training step(s), loss on training batch is 0.00120268.
After 13876 training step(s), loss on training batch is 0.000927939.
After 13877 training step(s), loss on training batch is 0.0009114.
After 13878 training step(s), loss on training batch is 0.000970994.
After 13879 training step(s), loss on training batch is 0.000852736.
After 13880 training step(s), loss on training batch is 0.000854786.
After 13881 training step(s), loss on training batch is 0.000826573.
After 13882 training step(s), loss on training batch is 0.000862877.
After 13883 training step(s), loss on training batch is 0.000957699.
After 13884 training step(s), loss on training batch is 0.000853796.
After 13885 training step(s), loss on training batch is 0.000861525.
After 13886 training step(s), loss on training batch is 0.00110736.
After 13887 training step(s), loss on training batch is 0.000869238.
After 13888 training step(s), loss on training batch is 0.00103446.
After 13889 training step(s), loss on training batch is 0.000877417.
After 13890 training step(s), loss on training batch is 0.000837723.
After 13891 training step(s), loss on training batch is 0.000861259.
After 13892 training step(s), loss on training batch is 0.00089356.
After 13893 training step(s), loss on training batch is 0.000819298.
After 13894 training step(s), loss on training batch is 0.000951419.
After 13895 training step(s), loss on training batch is 0.000988771.
After 13896 training step(s), loss on training batch is 0.0010716.
After 13897 training step(s), loss on training batch is 0.000898094.
After 13898 training step(s), loss on training batch is 0.000881576.
After 13899 training step(s), loss on training batch is 0.000805525.
After 13900 training step(s), loss on training batch is 0.000870816.
After 13901 training step(s), loss on training batch is 0.000777686.
After 13902 training step(s), loss on training batch is 0.000977637.
After 13903 training step(s), loss on training batch is 0.000847541.
After 13904 training step(s), loss on training batch is 0.00106239.
After 13905 training step(s), loss on training batch is 0.00110084.
After 13906 training step(s), loss on training batch is 0.00106545.
After 13907 training step(s), loss on training batch is 0.0012574.
After 13908 training step(s), loss on training batch is 0.00105357.
After 13909 training step(s), loss on training batch is 0.00690127.
After 13910 training step(s), loss on training batch is 0.0015662.
After 13911 training step(s), loss on training batch is 0.00143137.
After 13912 training step(s), loss on training batch is 0.0013926.
After 13913 training step(s), loss on training batch is 0.00121987.
After 13914 training step(s), loss on training batch is 0.0013014.
After 13915 training step(s), loss on training batch is 0.00104575.
After 13916 training step(s), loss on training batch is 0.000484549.
After 13917 training step(s), loss on training batch is 0.000429025.
After 13918 training step(s), loss on training batch is 0.000410305.
After 13919 training step(s), loss on training batch is 0.000437389.
After 13920 training step(s), loss on training batch is 0.000458805.
After 13921 training step(s), loss on training batch is 0.000460851.
After 13922 training step(s), loss on training batch is 0.000470465.
After 13923 training step(s), loss on training batch is 0.000462764.
After 13924 training step(s), loss on training batch is 0.000450956.
After 13925 training step(s), loss on training batch is 0.000417156.
After 13926 training step(s), loss on training batch is 0.00084289.
After 13927 training step(s), loss on training batch is 0.000869487.
After 13928 training step(s), loss on training batch is 0.000971997.
After 13929 training step(s), loss on training batch is 0.000859766.
After 13930 training step(s), loss on training batch is 0.00162139.
After 13931 training step(s), loss on training batch is 0.00140153.
After 13932 training step(s), loss on training batch is 0.00107497.
After 13933 training step(s), loss on training batch is 0.00102918.
After 13934 training step(s), loss on training batch is 0.000930818.
After 13935 training step(s), loss on training batch is 0.000882562.
After 13936 training step(s), loss on training batch is 0.000894989.
After 13937 training step(s), loss on training batch is 0.000916208.
After 13938 training step(s), loss on training batch is 0.00104499.
After 13939 training step(s), loss on training batch is 0.000906255.
After 13940 training step(s), loss on training batch is 0.000892937.
After 13941 training step(s), loss on training batch is 0.000890893.
After 13942 training step(s), loss on training batch is 0.00152895.
After 13943 training step(s), loss on training batch is 0.000323583.
After 13944 training step(s), loss on training batch is 0.000378503.
After 13945 training step(s), loss on training batch is 0.000351439.
After 13946 training step(s), loss on training batch is 0.000438514.
After 13947 training step(s), loss on training batch is 0.00041234.
After 13948 training step(s), loss on training batch is 0.000384855.
After 13949 training step(s), loss on training batch is 0.000338225.
After 13950 training step(s), loss on training batch is 0.000312019.
After 13951 training step(s), loss on training batch is 0.000496838.
After 13952 training step(s), loss on training batch is 0.000405753.
After 13953 training step(s), loss on training batch is 0.000400811.
After 13954 training step(s), loss on training batch is 0.000359142.
After 13955 training step(s), loss on training batch is 0.000505793.
After 13956 training step(s), loss on training batch is 0.000380513.
After 13957 training step(s), loss on training batch is 0.000323605.
After 13958 training step(s), loss on training batch is 0.000326693.
After 13959 training step(s), loss on training batch is 0.000700043.
After 13960 training step(s), loss on training batch is 0.000886958.
After 13961 training step(s), loss on training batch is 0.000497043.
After 13962 training step(s), loss on training batch is 0.000394756.
After 13963 training step(s), loss on training batch is 0.000361678.
After 13964 training step(s), loss on training batch is 0.000433116.
After 13965 training step(s), loss on training batch is 0.000450843.
After 13966 training step(s), loss on training batch is 0.000400626.
After 13967 training step(s), loss on training batch is 0.000356539.
After 13968 training step(s), loss on training batch is 0.000507087.
After 13969 training step(s), loss on training batch is 0.00031201.
After 13970 training step(s), loss on training batch is 0.000387935.
After 13971 training step(s), loss on training batch is 0.000268696.
After 13972 training step(s), loss on training batch is 0.000281702.
After 13973 training step(s), loss on training batch is 0.000309286.
After 13974 training step(s), loss on training batch is 0.000311102.
After 13975 training step(s), loss on training batch is 0.000447652.
After 13976 training step(s), loss on training batch is 0.000398834.
After 13977 training step(s), loss on training batch is 0.000399756.
After 13978 training step(s), loss on training batch is 0.000317347.
After 13979 training step(s), loss on training batch is 0.000340895.
After 13980 training step(s), loss on training batch is 0.000305399.
After 13981 training step(s), loss on training batch is 0.000342391.
After 13982 training step(s), loss on training batch is 0.000313305.
After 13983 training step(s), loss on training batch is 0.000376507.
After 13984 training step(s), loss on training batch is 0.000379796.
After 13985 training step(s), loss on training batch is 0.000391767.
After 13986 training step(s), loss on training batch is 0.000525161.
After 13987 training step(s), loss on training batch is 0.000400291.
After 13988 training step(s), loss on training batch is 0.000324069.
After 13989 training step(s), loss on training batch is 0.000386158.
After 13990 training step(s), loss on training batch is 0.000329683.
After 13991 training step(s), loss on training batch is 0.000300252.
After 13992 training step(s), loss on training batch is 0.000311286.
After 13993 training step(s), loss on training batch is 0.000304801.
After 13994 training step(s), loss on training batch is 0.000262561.
After 13995 training step(s), loss on training batch is 0.000449861.
After 13996 training step(s), loss on training batch is 0.000433962.
After 13997 training step(s), loss on training batch is 0.000237085.
After 13998 training step(s), loss on training batch is 0.000261072.
After 13999 training step(s), loss on training batch is 0.000342011.
After 14000 training step(s), loss on training batch is 0.000356121.
After 14001 training step(s), loss on training batch is 0.000504315.
After 14002 training step(s), loss on training batch is 0.000576999.
After 14003 training step(s), loss on training batch is 0.00036193.
After 14004 training step(s), loss on training batch is 0.000502475.
After 14005 training step(s), loss on training batch is 0.000926386.
After 14006 training step(s), loss on training batch is 0.000650449.
After 14007 training step(s), loss on training batch is 0.000372278.
After 14008 training step(s), loss on training batch is 0.000444021.
After 14009 training step(s), loss on training batch is 0.00055175.
After 14010 training step(s), loss on training batch is 0.00057521.
After 14011 training step(s), loss on training batch is 0.000380957.
After 14012 training step(s), loss on training batch is 0.000299285.
After 14013 training step(s), loss on training batch is 0.000357076.
After 14014 training step(s), loss on training batch is 0.000379578.
After 14015 training step(s), loss on training batch is 0.000359588.
After 14016 training step(s), loss on training batch is 0.000307665.
After 14017 training step(s), loss on training batch is 0.000385441.
After 14018 training step(s), loss on training batch is 0.000329216.
After 14019 training step(s), loss on training batch is 0.000411578.
After 14020 training step(s), loss on training batch is 0.000372947.
After 14021 training step(s), loss on training batch is 0.000290472.
After 14022 training step(s), loss on training batch is 0.000369746.
After 14023 training step(s), loss on training batch is 0.000339225.
After 14024 training step(s), loss on training batch is 0.000304977.
After 14025 training step(s), loss on training batch is 0.00029785.
After 14026 training step(s), loss on training batch is 0.00035931.
After 14027 training step(s), loss on training batch is 0.000288597.
After 14028 training step(s), loss on training batch is 0.000873915.
After 14029 training step(s), loss on training batch is 0.000582896.
After 14030 training step(s), loss on training batch is 0.000515155.
After 14031 training step(s), loss on training batch is 0.000524144.
After 14032 training step(s), loss on training batch is 0.000603761.
After 14033 training step(s), loss on training batch is 0.000673461.
After 14034 training step(s), loss on training batch is 0.000592094.
After 14035 training step(s), loss on training batch is 0.000458127.
After 14036 training step(s), loss on training batch is 0.000669401.
After 14037 training step(s), loss on training batch is 0.000637832.
After 14038 training step(s), loss on training batch is 0.00047581.
After 14039 training step(s), loss on training batch is 0.000473072.
After 14040 training step(s), loss on training batch is 0.00101946.
After 14041 training step(s), loss on training batch is 0.000498779.
After 14042 training step(s), loss on training batch is 0.000567443.
After 14043 training step(s), loss on training batch is 0.00056995.
After 14044 training step(s), loss on training batch is 0.000647383.
After 14045 training step(s), loss on training batch is 0.000485169.
After 14046 training step(s), loss on training batch is 0.000613988.
After 14047 training step(s), loss on training batch is 0.000516963.
After 14048 training step(s), loss on training batch is 0.000557171.
After 14049 training step(s), loss on training batch is 0.000598214.
After 14050 training step(s), loss on training batch is 0.000583435.
After 14051 training step(s), loss on training batch is 0.000581215.
After 14052 training step(s), loss on training batch is 0.000530388.
After 14053 training step(s), loss on training batch is 0.000599762.
After 14054 training step(s), loss on training batch is 0.000417542.
After 14055 training step(s), loss on training batch is 0.0005588.
After 14056 training step(s), loss on training batch is 0.00100141.
After 14057 training step(s), loss on training batch is 0.000852543.
After 14058 training step(s), loss on training batch is 0.000915878.
After 14059 training step(s), loss on training batch is 0.000905737.
After 14060 training step(s), loss on training batch is 0.000794286.
After 14061 training step(s), loss on training batch is 0.000831009.
After 14062 training step(s), loss on training batch is 0.000988577.
After 14063 training step(s), loss on training batch is 0.000886321.
After 14064 training step(s), loss on training batch is 0.000804635.
After 14065 training step(s), loss on training batch is 0.0010442.
After 14066 training step(s), loss on training batch is 0.000803784.
After 14067 training step(s), loss on training batch is 0.000823024.
After 14068 training step(s), loss on training batch is 0.000931785.
After 14069 training step(s), loss on training batch is 0.00140194.
After 14070 training step(s), loss on training batch is 0.00295681.
After 14071 training step(s), loss on training batch is 0.00138713.
After 14072 training step(s), loss on training batch is 0.00104613.
After 14073 training step(s), loss on training batch is 0.0011848.
After 14074 training step(s), loss on training batch is 0.00105187.
After 14075 training step(s), loss on training batch is 0.00096496.
After 14076 training step(s), loss on training batch is 0.000870033.
After 14077 training step(s), loss on training batch is 0.00124265.
After 14078 training step(s), loss on training batch is 0.000851819.
After 14079 training step(s), loss on training batch is 0.00081453.
After 14080 training step(s), loss on training batch is 0.000806426.
After 14081 training step(s), loss on training batch is 0.000568453.
After 14082 training step(s), loss on training batch is 0.000406329.
After 14083 training step(s), loss on training batch is 0.000267924.
After 14084 training step(s), loss on training batch is 0.000342172.
After 14085 training step(s), loss on training batch is 0.000360203.
After 14086 training step(s), loss on training batch is 0.000508315.
After 14087 training step(s), loss on training batch is 0.000614492.
After 14088 training step(s), loss on training batch is 0.000396893.
After 14089 training step(s), loss on training batch is 0.000339501.
After 14090 training step(s), loss on training batch is 0.000308532.
After 14091 training step(s), loss on training batch is 0.000260557.
After 14092 training step(s), loss on training batch is 0.000316867.
After 14093 training step(s), loss on training batch is 0.00030048.
After 14094 training step(s), loss on training batch is 0.000299173.
After 14095 training step(s), loss on training batch is 0.0003354.
After 14096 training step(s), loss on training batch is 0.000345308.
After 14097 training step(s), loss on training batch is 0.000270801.
After 14098 training step(s), loss on training batch is 0.000259503.
After 14099 training step(s), loss on training batch is 0.000277665.
After 14100 training step(s), loss on training batch is 0.000279236.
After 14101 training step(s), loss on training batch is 0.00068813.
After 14102 training step(s), loss on training batch is 0.000474363.
After 14103 training step(s), loss on training batch is 0.000526847.
After 14104 training step(s), loss on training batch is 0.000612956.
After 14105 training step(s), loss on training batch is 0.000578948.
After 14106 training step(s), loss on training batch is 0.000582318.
After 14107 training step(s), loss on training batch is 0.000551412.
After 14108 training step(s), loss on training batch is 0.000322314.
After 14109 training step(s), loss on training batch is 0.000351583.
After 14110 training step(s), loss on training batch is 0.00037173.
After 14111 training step(s), loss on training batch is 0.000430294.
After 14112 training step(s), loss on training batch is 0.000427451.
After 14113 training step(s), loss on training batch is 0.000452774.
After 14114 training step(s), loss on training batch is 0.0006167.
After 14115 training step(s), loss on training batch is 0.00133676.
After 14116 training step(s), loss on training batch is 0.000938591.
After 14117 training step(s), loss on training batch is 0.000770897.
After 14118 training step(s), loss on training batch is 0.000598201.
After 14119 training step(s), loss on training batch is 0.000372628.
After 14120 training step(s), loss on training batch is 0.000369025.
After 14121 training step(s), loss on training batch is 0.000381104.
After 14122 training step(s), loss on training batch is 0.000334774.
After 14123 training step(s), loss on training batch is 0.000521153.
After 14124 training step(s), loss on training batch is 0.000402628.
After 14125 training step(s), loss on training batch is 0.000387417.
After 14126 training step(s), loss on training batch is 0.000383812.
After 14127 training step(s), loss on training batch is 0.000721502.
After 14128 training step(s), loss on training batch is 0.00173089.
After 14129 training step(s), loss on training batch is 0.00064977.
After 14130 training step(s), loss on training batch is 0.00049812.
After 14131 training step(s), loss on training batch is 0.000527067.
After 14132 training step(s), loss on training batch is 0.000493068.
After 14133 training step(s), loss on training batch is 0.000424504.
After 14134 training step(s), loss on training batch is 0.000336601.
After 14135 training step(s), loss on training batch is 0.000315509.
After 14136 training step(s), loss on training batch is 0.000310667.
After 14137 training step(s), loss on training batch is 0.00035051.
After 14138 training step(s), loss on training batch is 0.000348513.
After 14139 training step(s), loss on training batch is 0.000388037.
After 14140 training step(s), loss on training batch is 0.000440461.
After 14141 training step(s), loss on training batch is 0.000348749.
After 14142 training step(s), loss on training batch is 0.000426547.
After 14143 training step(s), loss on training batch is 0.000416905.
After 14144 training step(s), loss on training batch is 0.000481349.
After 14145 training step(s), loss on training batch is 0.000323797.
After 14146 training step(s), loss on training batch is 0.000437446.
After 14147 training step(s), loss on training batch is 0.000341802.
After 14148 training step(s), loss on training batch is 0.000318411.
After 14149 training step(s), loss on training batch is 0.000358576.
After 14150 training step(s), loss on training batch is 0.000455995.
After 14151 training step(s), loss on training batch is 0.000428149.
After 14152 training step(s), loss on training batch is 0.00038117.
After 14153 training step(s), loss on training batch is 0.000387864.
After 14154 training step(s), loss on training batch is 0.000367687.
After 14155 training step(s), loss on training batch is 0.000343025.
After 14156 training step(s), loss on training batch is 0.000443544.
After 14157 training step(s), loss on training batch is 0.000538757.
After 14158 training step(s), loss on training batch is 0.00034264.
After 14159 training step(s), loss on training batch is 0.000380481.
After 14160 training step(s), loss on training batch is 0.000483369.
After 14161 training step(s), loss on training batch is 0.000339243.
After 14162 training step(s), loss on training batch is 0.000309669.
After 14163 training step(s), loss on training batch is 0.000322599.
After 14164 training step(s), loss on training batch is 0.000442792.
After 14165 training step(s), loss on training batch is 0.00038951.
After 14166 training step(s), loss on training batch is 0.000389056.
After 14167 training step(s), loss on training batch is 0.00038655.
After 14168 training step(s), loss on training batch is 0.000361412.
After 14169 training step(s), loss on training batch is 0.000414538.
After 14170 training step(s), loss on training batch is 0.000341503.
After 14171 training step(s), loss on training batch is 0.000343324.
After 14172 training step(s), loss on training batch is 0.000334328.
After 14173 training step(s), loss on training batch is 0.000318954.
After 14174 training step(s), loss on training batch is 0.00030105.
After 14175 training step(s), loss on training batch is 0.000371372.
After 14176 training step(s), loss on training batch is 0.000361751.
After 14177 training step(s), loss on training batch is 0.000454601.
After 14178 training step(s), loss on training batch is 0.000364518.
After 14179 training step(s), loss on training batch is 0.000340918.
After 14180 training step(s), loss on training batch is 0.00035577.
After 14181 training step(s), loss on training batch is 0.00034167.
After 14182 training step(s), loss on training batch is 0.000356422.
After 14183 training step(s), loss on training batch is 0.000653998.
After 14184 training step(s), loss on training batch is 0.000751719.
After 14185 training step(s), loss on training batch is 0.00090238.
After 14186 training step(s), loss on training batch is 0.000603641.
After 14187 training step(s), loss on training batch is 0.00064776.
After 14188 training step(s), loss on training batch is 0.000573186.
After 14189 training step(s), loss on training batch is 0.000597596.
After 14190 training step(s), loss on training batch is 0.000571246.
After 14191 training step(s), loss on training batch is 0.000667627.
After 14192 training step(s), loss on training batch is 0.000698603.
After 14193 training step(s), loss on training batch is 0.00055497.
After 14194 training step(s), loss on training batch is 0.000542179.
After 14195 training step(s), loss on training batch is 0.000708721.
After 14196 training step(s), loss on training batch is 0.000662815.
After 14197 training step(s), loss on training batch is 0.000596191.
After 14198 training step(s), loss on training batch is 0.000626216.
After 14199 training step(s), loss on training batch is 0.000823532.
After 14200 training step(s), loss on training batch is 0.00065973.
After 14201 training step(s), loss on training batch is 0.000574022.
After 14202 training step(s), loss on training batch is 0.000675387.
After 14203 training step(s), loss on training batch is 0.000577042.
After 14204 training step(s), loss on training batch is 0.000565288.
After 14205 training step(s), loss on training batch is 0.000492711.
After 14206 training step(s), loss on training batch is 0.000634324.
After 14207 training step(s), loss on training batch is 0.000782374.
After 14208 training step(s), loss on training batch is 0.000666549.
After 14209 training step(s), loss on training batch is 0.00051415.
After 14210 training step(s), loss on training batch is 0.000781344.
After 14211 training step(s), loss on training batch is 0.00060475.
After 14212 training step(s), loss on training batch is 0.000596096.
After 14213 training step(s), loss on training batch is 0.00048972.
After 14214 training step(s), loss on training batch is 0.000499402.
After 14215 training step(s), loss on training batch is 0.000475597.
After 14216 training step(s), loss on training batch is 0.000494631.
After 14217 training step(s), loss on training batch is 0.000501885.
After 14218 training step(s), loss on training batch is 0.00069334.
After 14219 training step(s), loss on training batch is 0.000946681.
After 14220 training step(s), loss on training batch is 0.000940874.
After 14221 training step(s), loss on training batch is 0.00111198.
After 14222 training step(s), loss on training batch is 0.000577241.
After 14223 training step(s), loss on training batch is 0.000566351.
After 14224 training step(s), loss on training batch is 0.000584819.
After 14225 training step(s), loss on training batch is 0.000624851.
After 14226 training step(s), loss on training batch is 0.000667728.
After 14227 training step(s), loss on training batch is 0.000570251.
After 14228 training step(s), loss on training batch is 0.00061359.
After 14229 training step(s), loss on training batch is 0.000619828.
After 14230 training step(s), loss on training batch is 0.000696594.
After 14231 training step(s), loss on training batch is 0.000776589.
After 14232 training step(s), loss on training batch is 0.000541232.
After 14233 training step(s), loss on training batch is 0.000567412.
After 14234 training step(s), loss on training batch is 0.000524056.
After 14235 training step(s), loss on training batch is 0.000478219.
After 14236 training step(s), loss on training batch is 0.000706114.
After 14237 training step(s), loss on training batch is 0.00105567.
After 14238 training step(s), loss on training batch is 0.000479863.
After 14239 training step(s), loss on training batch is 0.000559817.
After 14240 training step(s), loss on training batch is 0.000593919.
After 14241 training step(s), loss on training batch is 0.000548071.
After 14242 training step(s), loss on training batch is 0.00052652.
After 14243 training step(s), loss on training batch is 0.000597894.
After 14244 training step(s), loss on training batch is 0.000556932.
After 14245 training step(s), loss on training batch is 0.000593233.
After 14246 training step(s), loss on training batch is 0.000732947.
After 14247 training step(s), loss on training batch is 0.000573789.
After 14248 training step(s), loss on training batch is 0.000566034.
After 14249 training step(s), loss on training batch is 0.000612511.
After 14250 training step(s), loss on training batch is 0.000625106.
After 14251 training step(s), loss on training batch is 0.0005171.
After 14252 training step(s), loss on training batch is 0.000487099.
After 14253 training step(s), loss on training batch is 0.000659239.
After 14254 training step(s), loss on training batch is 0.000564642.
After 14255 training step(s), loss on training batch is 0.000628258.
After 14256 training step(s), loss on training batch is 0.000570209.
After 14257 training step(s), loss on training batch is 0.000510689.
After 14258 training step(s), loss on training batch is 0.000643176.
After 14259 training step(s), loss on training batch is 0.000803007.
After 14260 training step(s), loss on training batch is 0.000533799.
After 14261 training step(s), loss on training batch is 0.000486726.
After 14262 training step(s), loss on training batch is 0.000491472.
After 14263 training step(s), loss on training batch is 0.000515351.
After 14264 training step(s), loss on training batch is 0.000474257.
After 14265 training step(s), loss on training batch is 0.000629079.
After 14266 training step(s), loss on training batch is 0.00101151.
After 14267 training step(s), loss on training batch is 0.00104959.
After 14268 training step(s), loss on training batch is 0.000947805.
After 14269 training step(s), loss on training batch is 0.000979398.
After 14270 training step(s), loss on training batch is 0.000951811.
After 14271 training step(s), loss on training batch is 0.000898445.
After 14272 training step(s), loss on training batch is 0.000943064.
After 14273 training step(s), loss on training batch is 0.00123899.
After 14274 training step(s), loss on training batch is 0.00110958.
After 14275 training step(s), loss on training batch is 0.00114718.
After 14276 training step(s), loss on training batch is 0.000940266.
After 14277 training step(s), loss on training batch is 0.000915546.
After 14278 training step(s), loss on training batch is 0.000968212.
After 14279 training step(s), loss on training batch is 0.000870326.
After 14280 training step(s), loss on training batch is 0.000859465.
After 14281 training step(s), loss on training batch is 0.000827908.
After 14282 training step(s), loss on training batch is 0.000867127.
After 14283 training step(s), loss on training batch is 0.000950656.
After 14284 training step(s), loss on training batch is 0.000850975.
After 14285 training step(s), loss on training batch is 0.000855543.
After 14286 training step(s), loss on training batch is 0.00109287.
After 14287 training step(s), loss on training batch is 0.000868292.
After 14288 training step(s), loss on training batch is 0.00103504.
After 14289 training step(s), loss on training batch is 0.000870045.
After 14290 training step(s), loss on training batch is 0.000849557.
After 14291 training step(s), loss on training batch is 0.000865913.
After 14292 training step(s), loss on training batch is 0.000874156.
After 14293 training step(s), loss on training batch is 0.000814607.
After 14294 training step(s), loss on training batch is 0.000951143.
After 14295 training step(s), loss on training batch is 0.000962569.
After 14296 training step(s), loss on training batch is 0.00106165.
After 14297 training step(s), loss on training batch is 0.000888619.
After 14298 training step(s), loss on training batch is 0.000874038.
After 14299 training step(s), loss on training batch is 0.000820583.
After 14300 training step(s), loss on training batch is 0.000877243.
After 14301 training step(s), loss on training batch is 0.00077559.
After 14302 training step(s), loss on training batch is 0.000954928.
After 14303 training step(s), loss on training batch is 0.000855536.
After 14304 training step(s), loss on training batch is 0.00104571.
After 14305 training step(s), loss on training batch is 0.00108965.
After 14306 training step(s), loss on training batch is 0.00104454.
After 14307 training step(s), loss on training batch is 0.00120758.
After 14308 training step(s), loss on training batch is 0.00105175.
After 14309 training step(s), loss on training batch is 0.00670771.
After 14310 training step(s), loss on training batch is 0.00208956.
After 14311 training step(s), loss on training batch is 0.0018721.
After 14312 training step(s), loss on training batch is 0.00173678.
After 14313 training step(s), loss on training batch is 0.00149163.
After 14314 training step(s), loss on training batch is 0.00138067.
After 14315 training step(s), loss on training batch is 0.00109832.
After 14316 training step(s), loss on training batch is 0.000527025.
After 14317 training step(s), loss on training batch is 0.000468283.
After 14318 training step(s), loss on training batch is 0.000439047.
After 14319 training step(s), loss on training batch is 0.000462667.
After 14320 training step(s), loss on training batch is 0.000479902.
After 14321 training step(s), loss on training batch is 0.000482406.
After 14322 training step(s), loss on training batch is 0.000500539.
After 14323 training step(s), loss on training batch is 0.000485906.
After 14324 training step(s), loss on training batch is 0.000462149.
After 14325 training step(s), loss on training batch is 0.000421909.
After 14326 training step(s), loss on training batch is 0.000858687.
After 14327 training step(s), loss on training batch is 0.000891715.
After 14328 training step(s), loss on training batch is 0.00097406.
After 14329 training step(s), loss on training batch is 0.000860281.
After 14330 training step(s), loss on training batch is 0.00161326.
After 14331 training step(s), loss on training batch is 0.00139517.
After 14332 training step(s), loss on training batch is 0.00107116.
After 14333 training step(s), loss on training batch is 0.00102639.
After 14334 training step(s), loss on training batch is 0.000924915.
After 14335 training step(s), loss on training batch is 0.000875167.
After 14336 training step(s), loss on training batch is 0.00089972.
After 14337 training step(s), loss on training batch is 0.00091826.
After 14338 training step(s), loss on training batch is 0.00102965.
After 14339 training step(s), loss on training batch is 0.000902793.
After 14340 training step(s), loss on training batch is 0.000889358.
After 14341 training step(s), loss on training batch is 0.00088945.
After 14342 training step(s), loss on training batch is 0.00150102.
After 14343 training step(s), loss on training batch is 0.000327182.
After 14344 training step(s), loss on training batch is 0.000385015.
After 14345 training step(s), loss on training batch is 0.000355072.
After 14346 training step(s), loss on training batch is 0.000447899.
After 14347 training step(s), loss on training batch is 0.000412509.
After 14348 training step(s), loss on training batch is 0.000388007.
After 14349 training step(s), loss on training batch is 0.000336747.
After 14350 training step(s), loss on training batch is 0.000309022.
After 14351 training step(s), loss on training batch is 0.000504307.
After 14352 training step(s), loss on training batch is 0.000398527.
After 14353 training step(s), loss on training batch is 0.000404024.
After 14354 training step(s), loss on training batch is 0.000353723.
After 14355 training step(s), loss on training batch is 0.000519116.
After 14356 training step(s), loss on training batch is 0.000369374.
After 14357 training step(s), loss on training batch is 0.000321399.
After 14358 training step(s), loss on training batch is 0.000326526.
After 14359 training step(s), loss on training batch is 0.000722108.
After 14360 training step(s), loss on training batch is 0.000894317.
After 14361 training step(s), loss on training batch is 0.000498225.
After 14362 training step(s), loss on training batch is 0.000391874.
After 14363 training step(s), loss on training batch is 0.000358027.
After 14364 training step(s), loss on training batch is 0.00043033.
After 14365 training step(s), loss on training batch is 0.000450278.
After 14366 training step(s), loss on training batch is 0.00039596.
After 14367 training step(s), loss on training batch is 0.000353405.
After 14368 training step(s), loss on training batch is 0.000505384.
After 14369 training step(s), loss on training batch is 0.000306149.
After 14370 training step(s), loss on training batch is 0.000384865.
After 14371 training step(s), loss on training batch is 0.00026377.
After 14372 training step(s), loss on training batch is 0.00027807.
After 14373 training step(s), loss on training batch is 0.00030688.
After 14374 training step(s), loss on training batch is 0.000310514.
After 14375 training step(s), loss on training batch is 0.00044312.
After 14376 training step(s), loss on training batch is 0.000424952.
After 14377 training step(s), loss on training batch is 0.000396395.
After 14378 training step(s), loss on training batch is 0.000309209.
After 14379 training step(s), loss on training batch is 0.000336519.
After 14380 training step(s), loss on training batch is 0.000303987.
After 14381 training step(s), loss on training batch is 0.000341154.
After 14382 training step(s), loss on training batch is 0.000310648.
After 14383 training step(s), loss on training batch is 0.000380799.
After 14384 training step(s), loss on training batch is 0.000378616.
After 14385 training step(s), loss on training batch is 0.000392122.
After 14386 training step(s), loss on training batch is 0.000513046.
After 14387 training step(s), loss on training batch is 0.000402716.
After 14388 training step(s), loss on training batch is 0.000319202.
After 14389 training step(s), loss on training batch is 0.00036517.
After 14390 training step(s), loss on training batch is 0.000336457.
After 14391 training step(s), loss on training batch is 0.00030992.
After 14392 training step(s), loss on training batch is 0.000313619.
After 14393 training step(s), loss on training batch is 0.000316718.
After 14394 training step(s), loss on training batch is 0.000266463.
After 14395 training step(s), loss on training batch is 0.000438388.
After 14396 training step(s), loss on training batch is 0.000430605.
After 14397 training step(s), loss on training batch is 0.000242903.
After 14398 training step(s), loss on training batch is 0.000258724.
After 14399 training step(s), loss on training batch is 0.000318614.
After 14400 training step(s), loss on training batch is 0.000346783.
After 14401 training step(s), loss on training batch is 0.000500324.
After 14402 training step(s), loss on training batch is 0.000580112.
After 14403 training step(s), loss on training batch is 0.000362433.
After 14404 training step(s), loss on training batch is 0.000510881.
After 14405 training step(s), loss on training batch is 0.000920723.
After 14406 training step(s), loss on training batch is 0.000650609.
After 14407 training step(s), loss on training batch is 0.000372078.
After 14408 training step(s), loss on training batch is 0.000453706.
After 14409 training step(s), loss on training batch is 0.000548272.
After 14410 training step(s), loss on training batch is 0.00056902.
After 14411 training step(s), loss on training batch is 0.000378204.
After 14412 training step(s), loss on training batch is 0.000307068.
After 14413 training step(s), loss on training batch is 0.000365999.
After 14414 training step(s), loss on training batch is 0.00039448.
After 14415 training step(s), loss on training batch is 0.000370768.
After 14416 training step(s), loss on training batch is 0.000305057.
After 14417 training step(s), loss on training batch is 0.000383482.
After 14418 training step(s), loss on training batch is 0.000331611.
After 14419 training step(s), loss on training batch is 0.000404544.
After 14420 training step(s), loss on training batch is 0.000371443.
After 14421 training step(s), loss on training batch is 0.000294085.
After 14422 training step(s), loss on training batch is 0.000367583.
After 14423 training step(s), loss on training batch is 0.000341905.
After 14424 training step(s), loss on training batch is 0.000304996.
After 14425 training step(s), loss on training batch is 0.000303857.
After 14426 training step(s), loss on training batch is 0.000355278.
After 14427 training step(s), loss on training batch is 0.000290841.
After 14428 training step(s), loss on training batch is 0.000838096.
After 14429 training step(s), loss on training batch is 0.000563282.
After 14430 training step(s), loss on training batch is 0.000515038.
After 14431 training step(s), loss on training batch is 0.000520204.
After 14432 training step(s), loss on training batch is 0.000593965.
After 14433 training step(s), loss on training batch is 0.000681604.
After 14434 training step(s), loss on training batch is 0.000591336.
After 14435 training step(s), loss on training batch is 0.000453533.
After 14436 training step(s), loss on training batch is 0.000673618.
After 14437 training step(s), loss on training batch is 0.000642618.
After 14438 training step(s), loss on training batch is 0.000479087.
After 14439 training step(s), loss on training batch is 0.000471727.
After 14440 training step(s), loss on training batch is 0.00105864.
After 14441 training step(s), loss on training batch is 0.000483528.
After 14442 training step(s), loss on training batch is 0.000553478.
After 14443 training step(s), loss on training batch is 0.000551403.
After 14444 training step(s), loss on training batch is 0.000659621.
After 14445 training step(s), loss on training batch is 0.00045379.
After 14446 training step(s), loss on training batch is 0.000669726.
After 14447 training step(s), loss on training batch is 0.000503261.
After 14448 training step(s), loss on training batch is 0.000541206.
After 14449 training step(s), loss on training batch is 0.000599832.
After 14450 training step(s), loss on training batch is 0.000584891.
After 14451 training step(s), loss on training batch is 0.000581658.
After 14452 training step(s), loss on training batch is 0.00052346.
After 14453 training step(s), loss on training batch is 0.000597325.
After 14454 training step(s), loss on training batch is 0.000416728.
After 14455 training step(s), loss on training batch is 0.000553255.
After 14456 training step(s), loss on training batch is 0.000996282.
After 14457 training step(s), loss on training batch is 0.000841548.
After 14458 training step(s), loss on training batch is 0.000902216.
After 14459 training step(s), loss on training batch is 0.00090255.
After 14460 training step(s), loss on training batch is 0.000785592.
After 14461 training step(s), loss on training batch is 0.000796436.
After 14462 training step(s), loss on training batch is 0.00102158.
After 14463 training step(s), loss on training batch is 0.000857606.
After 14464 training step(s), loss on training batch is 0.000754587.
After 14465 training step(s), loss on training batch is 0.00110038.
After 14466 training step(s), loss on training batch is 0.000749555.
After 14467 training step(s), loss on training batch is 0.000763504.
After 14468 training step(s), loss on training batch is 0.000924183.
After 14469 training step(s), loss on training batch is 0.00149267.
After 14470 training step(s), loss on training batch is 0.00306448.
After 14471 training step(s), loss on training batch is 0.00136567.
After 14472 training step(s), loss on training batch is 0.00100699.
After 14473 training step(s), loss on training batch is 0.00115931.
After 14474 training step(s), loss on training batch is 0.00102621.
After 14475 training step(s), loss on training batch is 0.000959596.
After 14476 training step(s), loss on training batch is 0.000899614.
After 14477 training step(s), loss on training batch is 0.00118334.
After 14478 training step(s), loss on training batch is 0.000877514.
After 14479 training step(s), loss on training batch is 0.000864113.
After 14480 training step(s), loss on training batch is 0.000836095.
After 14481 training step(s), loss on training batch is 0.00050744.
After 14482 training step(s), loss on training batch is 0.000391074.
After 14483 training step(s), loss on training batch is 0.000280086.
After 14484 training step(s), loss on training batch is 0.000340437.
After 14485 training step(s), loss on training batch is 0.000357286.
After 14486 training step(s), loss on training batch is 0.000486907.
After 14487 training step(s), loss on training batch is 0.000590784.
After 14488 training step(s), loss on training batch is 0.000394038.
After 14489 training step(s), loss on training batch is 0.00034594.
After 14490 training step(s), loss on training batch is 0.000306309.
After 14491 training step(s), loss on training batch is 0.000258489.
After 14492 training step(s), loss on training batch is 0.00031608.
After 14493 training step(s), loss on training batch is 0.000297874.
After 14494 training step(s), loss on training batch is 0.000300408.
After 14495 training step(s), loss on training batch is 0.000337561.
After 14496 training step(s), loss on training batch is 0.000343142.
After 14497 training step(s), loss on training batch is 0.000271639.
After 14498 training step(s), loss on training batch is 0.000255362.
After 14499 training step(s), loss on training batch is 0.000273483.
After 14500 training step(s), loss on training batch is 0.000277925.
After 14501 training step(s), loss on training batch is 0.000704832.
After 14502 training step(s), loss on training batch is 0.000473439.
After 14503 training step(s), loss on training batch is 0.00053031.
After 14504 training step(s), loss on training batch is 0.000624266.
After 14505 training step(s), loss on training batch is 0.00058902.
After 14506 training step(s), loss on training batch is 0.000582103.
After 14507 training step(s), loss on training batch is 0.000543711.
After 14508 training step(s), loss on training batch is 0.000327599.
After 14509 training step(s), loss on training batch is 0.000367753.
After 14510 training step(s), loss on training batch is 0.000387497.
After 14511 training step(s), loss on training batch is 0.000462647.
After 14512 training step(s), loss on training batch is 0.00045875.
After 14513 training step(s), loss on training batch is 0.000451272.
After 14514 training step(s), loss on training batch is 0.000543774.
After 14515 training step(s), loss on training batch is 0.00129365.
After 14516 training step(s), loss on training batch is 0.000927641.
After 14517 training step(s), loss on training batch is 0.000778938.
After 14518 training step(s), loss on training batch is 0.000602161.
After 14519 training step(s), loss on training batch is 0.000377343.
After 14520 training step(s), loss on training batch is 0.000363081.
After 14521 training step(s), loss on training batch is 0.000374601.
After 14522 training step(s), loss on training batch is 0.000329422.
After 14523 training step(s), loss on training batch is 0.000509036.
After 14524 training step(s), loss on training batch is 0.000394025.
After 14525 training step(s), loss on training batch is 0.000378338.
After 14526 training step(s), loss on training batch is 0.000374668.
After 14527 training step(s), loss on training batch is 0.00073118.
After 14528 training step(s), loss on training batch is 0.00171445.
After 14529 training step(s), loss on training batch is 0.000641653.
After 14530 training step(s), loss on training batch is 0.00048712.
After 14531 training step(s), loss on training batch is 0.000537509.
After 14532 training step(s), loss on training batch is 0.000488932.
After 14533 training step(s), loss on training batch is 0.000419471.
After 14534 training step(s), loss on training batch is 0.000334117.
After 14535 training step(s), loss on training batch is 0.000309407.
After 14536 training step(s), loss on training batch is 0.00030702.
After 14537 training step(s), loss on training batch is 0.000346569.
After 14538 training step(s), loss on training batch is 0.000338871.
After 14539 training step(s), loss on training batch is 0.000382852.
After 14540 training step(s), loss on training batch is 0.000429906.
After 14541 training step(s), loss on training batch is 0.000345659.
After 14542 training step(s), loss on training batch is 0.000422822.
After 14543 training step(s), loss on training batch is 0.000415765.
After 14544 training step(s), loss on training batch is 0.000476931.
After 14545 training step(s), loss on training batch is 0.000325599.
After 14546 training step(s), loss on training batch is 0.000436977.
After 14547 training step(s), loss on training batch is 0.000341627.
After 14548 training step(s), loss on training batch is 0.000316019.
After 14549 training step(s), loss on training batch is 0.000356531.
After 14550 training step(s), loss on training batch is 0.000451834.
After 14551 training step(s), loss on training batch is 0.000423001.
After 14552 training step(s), loss on training batch is 0.000384181.
After 14553 training step(s), loss on training batch is 0.000388789.
After 14554 training step(s), loss on training batch is 0.000365934.
After 14555 training step(s), loss on training batch is 0.000344006.
After 14556 training step(s), loss on training batch is 0.000441807.
After 14557 training step(s), loss on training batch is 0.000518064.
After 14558 training step(s), loss on training batch is 0.000350186.
After 14559 training step(s), loss on training batch is 0.0003918.
After 14560 training step(s), loss on training batch is 0.000479659.
After 14561 training step(s), loss on training batch is 0.000338796.
After 14562 training step(s), loss on training batch is 0.000310607.
After 14563 training step(s), loss on training batch is 0.000320139.
After 14564 training step(s), loss on training batch is 0.000439274.
After 14565 training step(s), loss on training batch is 0.000392216.
After 14566 training step(s), loss on training batch is 0.000391874.
After 14567 training step(s), loss on training batch is 0.000384409.
After 14568 training step(s), loss on training batch is 0.000358081.
After 14569 training step(s), loss on training batch is 0.000415778.
After 14570 training step(s), loss on training batch is 0.00034189.
After 14571 training step(s), loss on training batch is 0.000341847.
After 14572 training step(s), loss on training batch is 0.000334818.
After 14573 training step(s), loss on training batch is 0.000319018.
After 14574 training step(s), loss on training batch is 0.000300486.
After 14575 training step(s), loss on training batch is 0.000370822.
After 14576 training step(s), loss on training batch is 0.000360161.
After 14577 training step(s), loss on training batch is 0.000454687.
After 14578 training step(s), loss on training batch is 0.000360923.
After 14579 training step(s), loss on training batch is 0.000330571.
After 14580 training step(s), loss on training batch is 0.000355136.
After 14581 training step(s), loss on training batch is 0.000337913.
After 14582 training step(s), loss on training batch is 0.000356535.
After 14583 training step(s), loss on training batch is 0.00066861.
After 14584 training step(s), loss on training batch is 0.000757481.
After 14585 training step(s), loss on training batch is 0.000886319.
After 14586 training step(s), loss on training batch is 0.000599359.
After 14587 training step(s), loss on training batch is 0.00064844.
After 14588 training step(s), loss on training batch is 0.000565408.
After 14589 training step(s), loss on training batch is 0.000601384.
After 14590 training step(s), loss on training batch is 0.000571898.
After 14591 training step(s), loss on training batch is 0.000667956.
After 14592 training step(s), loss on training batch is 0.000702472.
After 14593 training step(s), loss on training batch is 0.000558444.
After 14594 training step(s), loss on training batch is 0.000543196.
After 14595 training step(s), loss on training batch is 0.000707622.
After 14596 training step(s), loss on training batch is 0.000656521.
After 14597 training step(s), loss on training batch is 0.000591951.
After 14598 training step(s), loss on training batch is 0.000624845.
After 14599 training step(s), loss on training batch is 0.000823724.
After 14600 training step(s), loss on training batch is 0.000647214.
After 14601 training step(s), loss on training batch is 0.000570864.
After 14602 training step(s), loss on training batch is 0.000676849.
After 14603 training step(s), loss on training batch is 0.000573993.
After 14604 training step(s), loss on training batch is 0.000564302.
After 14605 training step(s), loss on training batch is 0.000482135.
After 14606 training step(s), loss on training batch is 0.00063472.
After 14607 training step(s), loss on training batch is 0.000778888.
After 14608 training step(s), loss on training batch is 0.000665916.
After 14609 training step(s), loss on training batch is 0.000512388.
After 14610 training step(s), loss on training batch is 0.00078107.
After 14611 training step(s), loss on training batch is 0.000614065.
After 14612 training step(s), loss on training batch is 0.000589987.
After 14613 training step(s), loss on training batch is 0.000495079.
After 14614 training step(s), loss on training batch is 0.000504339.
After 14615 training step(s), loss on training batch is 0.000479895.
After 14616 training step(s), loss on training batch is 0.000499166.
After 14617 training step(s), loss on training batch is 0.000504954.
After 14618 training step(s), loss on training batch is 0.000675712.
After 14619 training step(s), loss on training batch is 0.000892209.
After 14620 training step(s), loss on training batch is 0.000916689.
After 14621 training step(s), loss on training batch is 0.00106697.
After 14622 training step(s), loss on training batch is 0.000600573.
After 14623 training step(s), loss on training batch is 0.000590251.
After 14624 training step(s), loss on training batch is 0.000607986.
After 14625 training step(s), loss on training batch is 0.000660661.
After 14626 training step(s), loss on training batch is 0.000689064.
After 14627 training step(s), loss on training batch is 0.000582274.
After 14628 training step(s), loss on training batch is 0.000617661.
After 14629 training step(s), loss on training batch is 0.000631782.
After 14630 training step(s), loss on training batch is 0.00068978.
After 14631 training step(s), loss on training batch is 0.000758327.
After 14632 training step(s), loss on training batch is 0.000547911.
After 14633 training step(s), loss on training batch is 0.000557193.
After 14634 training step(s), loss on training batch is 0.00051119.
After 14635 training step(s), loss on training batch is 0.000471451.
After 14636 training step(s), loss on training batch is 0.000695908.
After 14637 training step(s), loss on training batch is 0.00106919.
After 14638 training step(s), loss on training batch is 0.000478074.
After 14639 training step(s), loss on training batch is 0.000557339.
After 14640 training step(s), loss on training batch is 0.000586692.
After 14641 training step(s), loss on training batch is 0.000539844.
After 14642 training step(s), loss on training batch is 0.000523221.
After 14643 training step(s), loss on training batch is 0.000596762.
After 14644 training step(s), loss on training batch is 0.000555411.
After 14645 training step(s), loss on training batch is 0.000585833.
After 14646 training step(s), loss on training batch is 0.000732857.
After 14647 training step(s), loss on training batch is 0.000564634.
After 14648 training step(s), loss on training batch is 0.000565192.
After 14649 training step(s), loss on training batch is 0.00061217.
After 14650 training step(s), loss on training batch is 0.000622995.
After 14651 training step(s), loss on training batch is 0.000532286.
After 14652 training step(s), loss on training batch is 0.000479556.
After 14653 training step(s), loss on training batch is 0.000662697.
After 14654 training step(s), loss on training batch is 0.00056549.
After 14655 training step(s), loss on training batch is 0.000620865.
After 14656 training step(s), loss on training batch is 0.000570365.
After 14657 training step(s), loss on training batch is 0.000507527.
After 14658 training step(s), loss on training batch is 0.000639273.
After 14659 training step(s), loss on training batch is 0.0007858.
After 14660 training step(s), loss on training batch is 0.000528777.
After 14661 training step(s), loss on training batch is 0.000484084.
After 14662 training step(s), loss on training batch is 0.000489782.
After 14663 training step(s), loss on training batch is 0.000512937.
After 14664 training step(s), loss on training batch is 0.00046268.
After 14665 training step(s), loss on training batch is 0.000631339.
After 14666 training step(s), loss on training batch is 0.000998265.
After 14667 training step(s), loss on training batch is 0.00104596.
After 14668 training step(s), loss on training batch is 0.00094033.
After 14669 training step(s), loss on training batch is 0.000966817.
After 14670 training step(s), loss on training batch is 0.00094719.
After 14671 training step(s), loss on training batch is 0.000890759.
After 14672 training step(s), loss on training batch is 0.000939401.
After 14673 training step(s), loss on training batch is 0.00121933.
After 14674 training step(s), loss on training batch is 0.001101.
After 14675 training step(s), loss on training batch is 0.00113258.
After 14676 training step(s), loss on training batch is 0.000941181.
After 14677 training step(s), loss on training batch is 0.000911066.
After 14678 training step(s), loss on training batch is 0.000963233.
After 14679 training step(s), loss on training batch is 0.000856178.
After 14680 training step(s), loss on training batch is 0.00084856.
After 14681 training step(s), loss on training batch is 0.000808164.
After 14682 training step(s), loss on training batch is 0.000850763.
After 14683 training step(s), loss on training batch is 0.000954594.
After 14684 training step(s), loss on training batch is 0.000846109.
After 14685 training step(s), loss on training batch is 0.000845141.
After 14686 training step(s), loss on training batch is 0.00110105.
After 14687 training step(s), loss on training batch is 0.000846046.
After 14688 training step(s), loss on training batch is 0.0010275.
After 14689 training step(s), loss on training batch is 0.000866707.
After 14690 training step(s), loss on training batch is 0.000852479.
After 14691 training step(s), loss on training batch is 0.000858304.
After 14692 training step(s), loss on training batch is 0.000883162.
After 14693 training step(s), loss on training batch is 0.000813347.
After 14694 training step(s), loss on training batch is 0.000950478.
After 14695 training step(s), loss on training batch is 0.000984199.
After 14696 training step(s), loss on training batch is 0.00104826.
After 14697 training step(s), loss on training batch is 0.000870048.
After 14698 training step(s), loss on training batch is 0.00087263.
After 14699 training step(s), loss on training batch is 0.000807613.
After 14700 training step(s), loss on training batch is 0.000872512.
After 14701 training step(s), loss on training batch is 0.000767104.
After 14702 training step(s), loss on training batch is 0.00096175.
After 14703 training step(s), loss on training batch is 0.000847261.
After 14704 training step(s), loss on training batch is 0.0010483.
After 14705 training step(s), loss on training batch is 0.00107698.
After 14706 training step(s), loss on training batch is 0.00104562.
After 14707 training step(s), loss on training batch is 0.0012072.
After 14708 training step(s), loss on training batch is 0.00105214.
After 14709 training step(s), loss on training batch is 0.0069562.
After 14710 training step(s), loss on training batch is 0.00373392.
After 14711 training step(s), loss on training batch is 0.0025719.
After 14712 training step(s), loss on training batch is 0.00221202.
After 14713 training step(s), loss on training batch is 0.00170894.
After 14714 training step(s), loss on training batch is 0.00153921.
After 14715 training step(s), loss on training batch is 0.00129083.
After 14716 training step(s), loss on training batch is 0.000636144.
After 14717 training step(s), loss on training batch is 0.000559338.
After 14718 training step(s), loss on training batch is 0.000510347.
After 14719 training step(s), loss on training batch is 0.000526104.
After 14720 training step(s), loss on training batch is 0.000541591.
After 14721 training step(s), loss on training batch is 0.000565582.
After 14722 training step(s), loss on training batch is 0.000577952.
After 14723 training step(s), loss on training batch is 0.000570894.
After 14724 training step(s), loss on training batch is 0.000515489.
After 14725 training step(s), loss on training batch is 0.000456484.
After 14726 training step(s), loss on training batch is 0.000937208.
After 14727 training step(s), loss on training batch is 0.000942162.
After 14728 training step(s), loss on training batch is 0.00103554.
After 14729 training step(s), loss on training batch is 0.000886864.
After 14730 training step(s), loss on training batch is 0.00156913.
After 14731 training step(s), loss on training batch is 0.00140269.
After 14732 training step(s), loss on training batch is 0.00109233.
After 14733 training step(s), loss on training batch is 0.00104882.
After 14734 training step(s), loss on training batch is 0.000941781.
After 14735 training step(s), loss on training batch is 0.000884425.
After 14736 training step(s), loss on training batch is 0.000918012.
After 14737 training step(s), loss on training batch is 0.000921493.
After 14738 training step(s), loss on training batch is 0.0010171.
After 14739 training step(s), loss on training batch is 0.000913894.
After 14740 training step(s), loss on training batch is 0.000904153.
After 14741 training step(s), loss on training batch is 0.000897569.
After 14742 training step(s), loss on training batch is 0.00147872.
After 14743 training step(s), loss on training batch is 0.000336289.
After 14744 training step(s), loss on training batch is 0.00038129.
After 14745 training step(s), loss on training batch is 0.000364342.
After 14746 training step(s), loss on training batch is 0.000446362.
After 14747 training step(s), loss on training batch is 0.000427284.
After 14748 training step(s), loss on training batch is 0.000408564.
After 14749 training step(s), loss on training batch is 0.00034286.
After 14750 training step(s), loss on training batch is 0.000313283.
After 14751 training step(s), loss on training batch is 0.000512859.
After 14752 training step(s), loss on training batch is 0.000409947.
After 14753 training step(s), loss on training batch is 0.000404499.
After 14754 training step(s), loss on training batch is 0.000358574.
After 14755 training step(s), loss on training batch is 0.000522127.
After 14756 training step(s), loss on training batch is 0.000374849.
After 14757 training step(s), loss on training batch is 0.000330944.
After 14758 training step(s), loss on training batch is 0.00032397.
After 14759 training step(s), loss on training batch is 0.000713803.
After 14760 training step(s), loss on training batch is 0.000867326.
After 14761 training step(s), loss on training batch is 0.000504374.
After 14762 training step(s), loss on training batch is 0.000387403.
After 14763 training step(s), loss on training batch is 0.000359431.
After 14764 training step(s), loss on training batch is 0.000434893.
After 14765 training step(s), loss on training batch is 0.0004497.
After 14766 training step(s), loss on training batch is 0.000399246.
After 14767 training step(s), loss on training batch is 0.000349403.
After 14768 training step(s), loss on training batch is 0.000498546.
After 14769 training step(s), loss on training batch is 0.000304077.
After 14770 training step(s), loss on training batch is 0.000381381.
After 14771 training step(s), loss on training batch is 0.0002579.
After 14772 training step(s), loss on training batch is 0.000275739.
After 14773 training step(s), loss on training batch is 0.000307972.
After 14774 training step(s), loss on training batch is 0.000311582.
After 14775 training step(s), loss on training batch is 0.000443397.
After 14776 training step(s), loss on training batch is 0.000399519.
After 14777 training step(s), loss on training batch is 0.000401089.
After 14778 training step(s), loss on training batch is 0.000314579.
After 14779 training step(s), loss on training batch is 0.00032792.
After 14780 training step(s), loss on training batch is 0.000306744.
After 14781 training step(s), loss on training batch is 0.000348482.
After 14782 training step(s), loss on training batch is 0.000312225.
After 14783 training step(s), loss on training batch is 0.000380119.
After 14784 training step(s), loss on training batch is 0.000382108.
After 14785 training step(s), loss on training batch is 0.000398135.
After 14786 training step(s), loss on training batch is 0.000497192.
After 14787 training step(s), loss on training batch is 0.000401546.
After 14788 training step(s), loss on training batch is 0.000319339.
After 14789 training step(s), loss on training batch is 0.000362548.
After 14790 training step(s), loss on training batch is 0.000336331.
After 14791 training step(s), loss on training batch is 0.000306645.
After 14792 training step(s), loss on training batch is 0.000311992.
After 14793 training step(s), loss on training batch is 0.000318947.
After 14794 training step(s), loss on training batch is 0.000266602.
After 14795 training step(s), loss on training batch is 0.000438355.
After 14796 training step(s), loss on training batch is 0.000433669.
After 14797 training step(s), loss on training batch is 0.000243084.
After 14798 training step(s), loss on training batch is 0.000261375.
After 14799 training step(s), loss on training batch is 0.000320577.
After 14800 training step(s), loss on training batch is 0.000350383.
After 14801 training step(s), loss on training batch is 0.000490243.
After 14802 training step(s), loss on training batch is 0.000564458.
After 14803 training step(s), loss on training batch is 0.000347164.
After 14804 training step(s), loss on training batch is 0.000503041.
After 14805 training step(s), loss on training batch is 0.000913174.
After 14806 training step(s), loss on training batch is 0.000649319.
After 14807 training step(s), loss on training batch is 0.000362141.
After 14808 training step(s), loss on training batch is 0.00044846.
After 14809 training step(s), loss on training batch is 0.000542485.
After 14810 training step(s), loss on training batch is 0.00056279.
After 14811 training step(s), loss on training batch is 0.000368964.
After 14812 training step(s), loss on training batch is 0.000297155.
After 14813 training step(s), loss on training batch is 0.000349384.
After 14814 training step(s), loss on training batch is 0.000378327.
After 14815 training step(s), loss on training batch is 0.000358708.
After 14816 training step(s), loss on training batch is 0.000306321.
After 14817 training step(s), loss on training batch is 0.00038658.
After 14818 training step(s), loss on training batch is 0.000329008.
After 14819 training step(s), loss on training batch is 0.000409285.
After 14820 training step(s), loss on training batch is 0.000366699.
After 14821 training step(s), loss on training batch is 0.000292263.
After 14822 training step(s), loss on training batch is 0.000361187.
After 14823 training step(s), loss on training batch is 0.00033949.
After 14824 training step(s), loss on training batch is 0.000304801.
After 14825 training step(s), loss on training batch is 0.000299788.
After 14826 training step(s), loss on training batch is 0.000347023.
After 14827 training step(s), loss on training batch is 0.000295287.
After 14828 training step(s), loss on training batch is 0.000787056.
After 14829 training step(s), loss on training batch is 0.000564323.
After 14830 training step(s), loss on training batch is 0.000516761.
After 14831 training step(s), loss on training batch is 0.000520462.
After 14832 training step(s), loss on training batch is 0.000589352.
After 14833 training step(s), loss on training batch is 0.000676232.
After 14834 training step(s), loss on training batch is 0.000580045.
After 14835 training step(s), loss on training batch is 0.000453363.
After 14836 training step(s), loss on training batch is 0.000661736.
After 14837 training step(s), loss on training batch is 0.000634084.
After 14838 training step(s), loss on training batch is 0.000472351.
After 14839 training step(s), loss on training batch is 0.000463638.
After 14840 training step(s), loss on training batch is 0.00105204.
After 14841 training step(s), loss on training batch is 0.000476065.
After 14842 training step(s), loss on training batch is 0.000538476.
After 14843 training step(s), loss on training batch is 0.000553823.
After 14844 training step(s), loss on training batch is 0.000650339.
After 14845 training step(s), loss on training batch is 0.000461139.
After 14846 training step(s), loss on training batch is 0.000639455.
After 14847 training step(s), loss on training batch is 0.000500304.
After 14848 training step(s), loss on training batch is 0.000541447.
After 14849 training step(s), loss on training batch is 0.00060495.
After 14850 training step(s), loss on training batch is 0.000581498.
After 14851 training step(s), loss on training batch is 0.000573867.
After 14852 training step(s), loss on training batch is 0.00051915.
After 14853 training step(s), loss on training batch is 0.000573088.
After 14854 training step(s), loss on training batch is 0.000408439.
After 14855 training step(s), loss on training batch is 0.000551959.
After 14856 training step(s), loss on training batch is 0.000988153.
After 14857 training step(s), loss on training batch is 0.000836638.
After 14858 training step(s), loss on training batch is 0.0008883.
After 14859 training step(s), loss on training batch is 0.000892462.
After 14860 training step(s), loss on training batch is 0.000768153.
After 14861 training step(s), loss on training batch is 0.000810712.
After 14862 training step(s), loss on training batch is 0.00100329.
After 14863 training step(s), loss on training batch is 0.000872776.
After 14864 training step(s), loss on training batch is 0.000785084.
After 14865 training step(s), loss on training batch is 0.00103634.
After 14866 training step(s), loss on training batch is 0.000769068.
After 14867 training step(s), loss on training batch is 0.000792412.
After 14868 training step(s), loss on training batch is 0.000910145.
After 14869 training step(s), loss on training batch is 0.00139536.
After 14870 training step(s), loss on training batch is 0.00331196.
After 14871 training step(s), loss on training batch is 0.00135686.
After 14872 training step(s), loss on training batch is 0.000985356.
After 14873 training step(s), loss on training batch is 0.00113028.
After 14874 training step(s), loss on training batch is 0.000974098.
After 14875 training step(s), loss on training batch is 0.000934079.
After 14876 training step(s), loss on training batch is 0.000865611.
After 14877 training step(s), loss on training batch is 0.00119051.
After 14878 training step(s), loss on training batch is 0.000853726.
After 14879 training step(s), loss on training batch is 0.000833909.
After 14880 training step(s), loss on training batch is 0.000803166.
After 14881 training step(s), loss on training batch is 0.00051051.
After 14882 training step(s), loss on training batch is 0.000423226.
After 14883 training step(s), loss on training batch is 0.00026294.
After 14884 training step(s), loss on training batch is 0.000340705.
After 14885 training step(s), loss on training batch is 0.000356574.
After 14886 training step(s), loss on training batch is 0.000526488.
After 14887 training step(s), loss on training batch is 0.000619749.
After 14888 training step(s), loss on training batch is 0.000381724.
After 14889 training step(s), loss on training batch is 0.000331753.
After 14890 training step(s), loss on training batch is 0.00030025.
After 14891 training step(s), loss on training batch is 0.000253407.
After 14892 training step(s), loss on training batch is 0.000318426.
After 14893 training step(s), loss on training batch is 0.000296406.
After 14894 training step(s), loss on training batch is 0.000298578.
After 14895 training step(s), loss on training batch is 0.000339707.
After 14896 training step(s), loss on training batch is 0.000344278.
After 14897 training step(s), loss on training batch is 0.000275292.
After 14898 training step(s), loss on training batch is 0.000260666.
After 14899 training step(s), loss on training batch is 0.00027612.
After 14900 training step(s), loss on training batch is 0.000272105.
After 14901 training step(s), loss on training batch is 0.000683621.
After 14902 training step(s), loss on training batch is 0.000473524.
After 14903 training step(s), loss on training batch is 0.000523034.
After 14904 training step(s), loss on training batch is 0.000611538.
After 14905 training step(s), loss on training batch is 0.000577239.
After 14906 training step(s), loss on training batch is 0.00057893.
After 14907 training step(s), loss on training batch is 0.000553404.
After 14908 training step(s), loss on training batch is 0.000323778.
After 14909 training step(s), loss on training batch is 0.00036526.
After 14910 training step(s), loss on training batch is 0.00038255.
After 14911 training step(s), loss on training batch is 0.000457033.
After 14912 training step(s), loss on training batch is 0.000453355.
After 14913 training step(s), loss on training batch is 0.00045248.
After 14914 training step(s), loss on training batch is 0.000547356.
After 14915 training step(s), loss on training batch is 0.00129838.
After 14916 training step(s), loss on training batch is 0.000925694.
After 14917 training step(s), loss on training batch is 0.00075083.
After 14918 training step(s), loss on training batch is 0.0005458.
After 14919 training step(s), loss on training batch is 0.000395107.
After 14920 training step(s), loss on training batch is 0.000347574.
After 14921 training step(s), loss on training batch is 0.000353584.
After 14922 training step(s), loss on training batch is 0.000329603.
After 14923 training step(s), loss on training batch is 0.000485956.
After 14924 training step(s), loss on training batch is 0.000382515.
After 14925 training step(s), loss on training batch is 0.000372612.
After 14926 training step(s), loss on training batch is 0.000365336.
After 14927 training step(s), loss on training batch is 0.00075524.
After 14928 training step(s), loss on training batch is 0.00174524.
After 14929 training step(s), loss on training batch is 0.000633309.
After 14930 training step(s), loss on training batch is 0.000487582.
After 14931 training step(s), loss on training batch is 0.000530686.
After 14932 training step(s), loss on training batch is 0.00049121.
After 14933 training step(s), loss on training batch is 0.000418403.
After 14934 training step(s), loss on training batch is 0.000335008.
After 14935 training step(s), loss on training batch is 0.000308031.
After 14936 training step(s), loss on training batch is 0.000306762.
After 14937 training step(s), loss on training batch is 0.000350242.
After 14938 training step(s), loss on training batch is 0.00034781.
After 14939 training step(s), loss on training batch is 0.000386946.
After 14940 training step(s), loss on training batch is 0.000437952.
After 14941 training step(s), loss on training batch is 0.000350089.
After 14942 training step(s), loss on training batch is 0.000435054.
After 14943 training step(s), loss on training batch is 0.0004285.
After 14944 training step(s), loss on training batch is 0.000474783.
After 14945 training step(s), loss on training batch is 0.000327323.
After 14946 training step(s), loss on training batch is 0.000426283.
After 14947 training step(s), loss on training batch is 0.000344109.
After 14948 training step(s), loss on training batch is 0.000317978.
After 14949 training step(s), loss on training batch is 0.000357036.
After 14950 training step(s), loss on training batch is 0.000469109.
After 14951 training step(s), loss on training batch is 0.000436904.
After 14952 training step(s), loss on training batch is 0.000382844.
After 14953 training step(s), loss on training batch is 0.000390632.
After 14954 training step(s), loss on training batch is 0.00036285.
After 14955 training step(s), loss on training batch is 0.000343942.
After 14956 training step(s), loss on training batch is 0.00044863.
After 14957 training step(s), loss on training batch is 0.000544894.
After 14958 training step(s), loss on training batch is 0.000347447.
After 14959 training step(s), loss on training batch is 0.000379251.
After 14960 training step(s), loss on training batch is 0.000488342.
After 14961 training step(s), loss on training batch is 0.000335888.
After 14962 training step(s), loss on training batch is 0.000311216.
After 14963 training step(s), loss on training batch is 0.000320687.
After 14964 training step(s), loss on training batch is 0.000433281.
After 14965 training step(s), loss on training batch is 0.000393588.
After 14966 training step(s), loss on training batch is 0.000385402.
After 14967 training step(s), loss on training batch is 0.000387253.
After 14968 training step(s), loss on training batch is 0.000396962.
After 14969 training step(s), loss on training batch is 0.000414015.
After 14970 training step(s), loss on training batch is 0.000346705.
After 14971 training step(s), loss on training batch is 0.000399758.
After 14972 training step(s), loss on training batch is 0.000357318.
After 14973 training step(s), loss on training batch is 0.000337031.
After 14974 training step(s), loss on training batch is 0.000314164.
After 14975 training step(s), loss on training batch is 0.000369633.
After 14976 training step(s), loss on training batch is 0.000354743.
After 14977 training step(s), loss on training batch is 0.000467452.
After 14978 training step(s), loss on training batch is 0.000366114.
After 14979 training step(s), loss on training batch is 0.00031444.
After 14980 training step(s), loss on training batch is 0.000356943.
After 14981 training step(s), loss on training batch is 0.000333283.
After 14982 training step(s), loss on training batch is 0.000356678.
After 14983 training step(s), loss on training batch is 0.000675522.
After 14984 training step(s), loss on training batch is 0.000760765.
After 14985 training step(s), loss on training batch is 0.000908508.
After 14986 training step(s), loss on training batch is 0.000597347.
After 14987 training step(s), loss on training batch is 0.000642315.
After 14988 training step(s), loss on training batch is 0.000561046.
After 14989 training step(s), loss on training batch is 0.000599552.
After 14990 training step(s), loss on training batch is 0.00055782.
After 14991 training step(s), loss on training batch is 0.000654087.
After 14992 training step(s), loss on training batch is 0.000703473.
After 14993 training step(s), loss on training batch is 0.000550085.
After 14994 training step(s), loss on training batch is 0.000540178.
After 14995 training step(s), loss on training batch is 0.000707288.
After 14996 training step(s), loss on training batch is 0.0006526.
After 14997 training step(s), loss on training batch is 0.000589415.
After 14998 training step(s), loss on training batch is 0.000619179.
After 14999 training step(s), loss on training batch is 0.000825828.
After 15000 training step(s), loss on training batch is 0.000643376.
After 15001 training step(s), loss on training batch is 0.000570342.
After 15002 training step(s), loss on training batch is 0.000667565.
After 15003 training step(s), loss on training batch is 0.000581431.
After 15004 training step(s), loss on training batch is 0.000564029.
After 15005 training step(s), loss on training batch is 0.000486754.
After 15006 training step(s), loss on training batch is 0.0006317.
After 15007 training step(s), loss on training batch is 0.000775431.
After 15008 training step(s), loss on training batch is 0.000666326.
After 15009 training step(s), loss on training batch is 0.000509537.
After 15010 training step(s), loss on training batch is 0.000787575.
After 15011 training step(s), loss on training batch is 0.000610583.
After 15012 training step(s), loss on training batch is 0.000591583.
After 15013 training step(s), loss on training batch is 0.00047859.
After 15014 training step(s), loss on training batch is 0.00049639.
After 15015 training step(s), loss on training batch is 0.000464183.
After 15016 training step(s), loss on training batch is 0.000488395.
After 15017 training step(s), loss on training batch is 0.000500416.
After 15018 training step(s), loss on training batch is 0.000674648.
After 15019 training step(s), loss on training batch is 0.000909784.
After 15020 training step(s), loss on training batch is 0.000940079.
After 15021 training step(s), loss on training batch is 0.00110017.
After 15022 training step(s), loss on training batch is 0.000580296.
After 15023 training step(s), loss on training batch is 0.000550592.
After 15024 training step(s), loss on training batch is 0.000572594.
After 15025 training step(s), loss on training batch is 0.00059648.
After 15026 training step(s), loss on training batch is 0.000642928.
After 15027 training step(s), loss on training batch is 0.000544557.
After 15028 training step(s), loss on training batch is 0.000605042.
After 15029 training step(s), loss on training batch is 0.000597228.
After 15030 training step(s), loss on training batch is 0.00069602.
After 15031 training step(s), loss on training batch is 0.000786024.
After 15032 training step(s), loss on training batch is 0.00052362.
After 15033 training step(s), loss on training batch is 0.000549732.
After 15034 training step(s), loss on training batch is 0.0005014.
After 15035 training step(s), loss on training batch is 0.000463329.
After 15036 training step(s), loss on training batch is 0.00071123.
After 15037 training step(s), loss on training batch is 0.0011079.
After 15038 training step(s), loss on training batch is 0.000480591.
After 15039 training step(s), loss on training batch is 0.000559364.
After 15040 training step(s), loss on training batch is 0.000579785.
After 15041 training step(s), loss on training batch is 0.000540121.
After 15042 training step(s), loss on training batch is 0.00052097.
After 15043 training step(s), loss on training batch is 0.00059884.
After 15044 training step(s), loss on training batch is 0.000548441.
After 15045 training step(s), loss on training batch is 0.000586338.
After 15046 training step(s), loss on training batch is 0.000729827.
After 15047 training step(s), loss on training batch is 0.000566589.
After 15048 training step(s), loss on training batch is 0.000566419.
After 15049 training step(s), loss on training batch is 0.000613143.
After 15050 training step(s), loss on training batch is 0.00061892.
After 15051 training step(s), loss on training batch is 0.000518898.
After 15052 training step(s), loss on training batch is 0.000482414.
After 15053 training step(s), loss on training batch is 0.000656685.
After 15054 training step(s), loss on training batch is 0.000562944.
After 15055 training step(s), loss on training batch is 0.000619332.
After 15056 training step(s), loss on training batch is 0.000567331.
After 15057 training step(s), loss on training batch is 0.000509759.
After 15058 training step(s), loss on training batch is 0.000633316.
After 15059 training step(s), loss on training batch is 0.000780269.
After 15060 training step(s), loss on training batch is 0.000524345.
After 15061 training step(s), loss on training batch is 0.000484545.
After 15062 training step(s), loss on training batch is 0.000488219.
After 15063 training step(s), loss on training batch is 0.000509044.
After 15064 training step(s), loss on training batch is 0.000467523.
After 15065 training step(s), loss on training batch is 0.000621945.
After 15066 training step(s), loss on training batch is 0.0010041.
After 15067 training step(s), loss on training batch is 0.00104901.
After 15068 training step(s), loss on training batch is 0.000936191.
After 15069 training step(s), loss on training batch is 0.000955973.
After 15070 training step(s), loss on training batch is 0.000950883.
After 15071 training step(s), loss on training batch is 0.000885668.
After 15072 training step(s), loss on training batch is 0.00093861.
After 15073 training step(s), loss on training batch is 0.00152485.
After 15074 training step(s), loss on training batch is 0.0011377.
After 15075 training step(s), loss on training batch is 0.00119852.
After 15076 training step(s), loss on training batch is 0.000898708.
After 15077 training step(s), loss on training batch is 0.0008846.
After 15078 training step(s), loss on training batch is 0.000955018.
After 15079 training step(s), loss on training batch is 0.000844196.
After 15080 training step(s), loss on training batch is 0.000845021.
After 15081 training step(s), loss on training batch is 0.000813551.
After 15082 training step(s), loss on training batch is 0.000857635.
After 15083 training step(s), loss on training batch is 0.000944311.
After 15084 training step(s), loss on training batch is 0.00084148.
After 15085 training step(s), loss on training batch is 0.000849283.
After 15086 training step(s), loss on training batch is 0.00114666.
After 15087 training step(s), loss on training batch is 0.000842522.
After 15088 training step(s), loss on training batch is 0.00102131.
After 15089 training step(s), loss on training batch is 0.000865404.
After 15090 training step(s), loss on training batch is 0.000857272.
After 15091 training step(s), loss on training batch is 0.000871975.
After 15092 training step(s), loss on training batch is 0.000870951.
After 15093 training step(s), loss on training batch is 0.000808245.
After 15094 training step(s), loss on training batch is 0.000946541.
After 15095 training step(s), loss on training batch is 0.000969907.
After 15096 training step(s), loss on training batch is 0.00105072.
After 15097 training step(s), loss on training batch is 0.000884367.
After 15098 training step(s), loss on training batch is 0.000872538.
After 15099 training step(s), loss on training batch is 0.000829657.
After 15100 training step(s), loss on training batch is 0.000878358.
After 15101 training step(s), loss on training batch is 0.000775599.
After 15102 training step(s), loss on training batch is 0.000949374.
After 15103 training step(s), loss on training batch is 0.000833205.
After 15104 training step(s), loss on training batch is 0.00105836.
After 15105 training step(s), loss on training batch is 0.00109475.
After 15106 training step(s), loss on training batch is 0.0010521.
After 15107 training step(s), loss on training batch is 0.00121432.
After 15108 training step(s), loss on training batch is 0.00104182.
After 15109 training step(s), loss on training batch is 0.00660894.
After 15110 training step(s), loss on training batch is 0.00154313.
After 15111 training step(s), loss on training batch is 0.00138709.
After 15112 training step(s), loss on training batch is 0.00132044.
After 15113 training step(s), loss on training batch is 0.00125771.
After 15114 training step(s), loss on training batch is 0.001281.
After 15115 training step(s), loss on training batch is 0.00108609.
After 15116 training step(s), loss on training batch is 0.000505395.
After 15117 training step(s), loss on training batch is 0.000451659.
After 15118 training step(s), loss on training batch is 0.000430336.
After 15119 training step(s), loss on training batch is 0.000450459.
After 15120 training step(s), loss on training batch is 0.000469085.
After 15121 training step(s), loss on training batch is 0.000477532.
After 15122 training step(s), loss on training batch is 0.000488297.
After 15123 training step(s), loss on training batch is 0.000482655.
After 15124 training step(s), loss on training batch is 0.000460538.
After 15125 training step(s), loss on training batch is 0.000419533.
After 15126 training step(s), loss on training batch is 0.000862395.
After 15127 training step(s), loss on training batch is 0.000893603.
After 15128 training step(s), loss on training batch is 0.000972521.
After 15129 training step(s), loss on training batch is 0.000865591.
After 15130 training step(s), loss on training batch is 0.00160954.
After 15131 training step(s), loss on training batch is 0.00137887.
After 15132 training step(s), loss on training batch is 0.00104438.
After 15133 training step(s), loss on training batch is 0.00101962.
After 15134 training step(s), loss on training batch is 0.000893072.
After 15135 training step(s), loss on training batch is 0.000861042.
After 15136 training step(s), loss on training batch is 0.000883785.
After 15137 training step(s), loss on training batch is 0.000901058.
After 15138 training step(s), loss on training batch is 0.00102835.
After 15139 training step(s), loss on training batch is 0.000888952.
After 15140 training step(s), loss on training batch is 0.000878928.
After 15141 training step(s), loss on training batch is 0.00087516.
After 15142 training step(s), loss on training batch is 0.00151618.
After 15143 training step(s), loss on training batch is 0.000328143.
After 15144 training step(s), loss on training batch is 0.000377671.
After 15145 training step(s), loss on training batch is 0.000341422.
After 15146 training step(s), loss on training batch is 0.000434402.
After 15147 training step(s), loss on training batch is 0.000397493.
After 15148 training step(s), loss on training batch is 0.000372164.
After 15149 training step(s), loss on training batch is 0.00033259.
After 15150 training step(s), loss on training batch is 0.000306063.
After 15151 training step(s), loss on training batch is 0.000477888.
After 15152 training step(s), loss on training batch is 0.000396202.
After 15153 training step(s), loss on training batch is 0.000395797.
After 15154 training step(s), loss on training batch is 0.000351608.
After 15155 training step(s), loss on training batch is 0.00050715.
After 15156 training step(s), loss on training batch is 0.000356498.
After 15157 training step(s), loss on training batch is 0.000313523.
After 15158 training step(s), loss on training batch is 0.00032235.
After 15159 training step(s), loss on training batch is 0.000686389.
After 15160 training step(s), loss on training batch is 0.000866329.
After 15161 training step(s), loss on training batch is 0.000478304.
After 15162 training step(s), loss on training batch is 0.000381222.
After 15163 training step(s), loss on training batch is 0.000348151.
After 15164 training step(s), loss on training batch is 0.000418785.
After 15165 training step(s), loss on training batch is 0.000448106.
After 15166 training step(s), loss on training batch is 0.000387971.
After 15167 training step(s), loss on training batch is 0.000350627.
After 15168 training step(s), loss on training batch is 0.000498185.
After 15169 training step(s), loss on training batch is 0.000307124.
After 15170 training step(s), loss on training batch is 0.000381067.
After 15171 training step(s), loss on training batch is 0.000265884.
After 15172 training step(s), loss on training batch is 0.000276019.
After 15173 training step(s), loss on training batch is 0.000308767.
After 15174 training step(s), loss on training batch is 0.00030927.
After 15175 training step(s), loss on training batch is 0.000434514.
After 15176 training step(s), loss on training batch is 0.000380465.
After 15177 training step(s), loss on training batch is 0.000392168.
After 15178 training step(s), loss on training batch is 0.000316431.
After 15179 training step(s), loss on training batch is 0.000333965.
After 15180 training step(s), loss on training batch is 0.000305078.
After 15181 training step(s), loss on training batch is 0.000340165.
After 15182 training step(s), loss on training batch is 0.000315268.
After 15183 training step(s), loss on training batch is 0.000384857.
After 15184 training step(s), loss on training batch is 0.000372285.
After 15185 training step(s), loss on training batch is 0.000388081.
After 15186 training step(s), loss on training batch is 0.000526934.
After 15187 training step(s), loss on training batch is 0.000386072.
After 15188 training step(s), loss on training batch is 0.000312874.
After 15189 training step(s), loss on training batch is 0.000364424.
After 15190 training step(s), loss on training batch is 0.000330774.
After 15191 training step(s), loss on training batch is 0.000301533.
After 15192 training step(s), loss on training batch is 0.000309981.
After 15193 training step(s), loss on training batch is 0.000310693.
After 15194 training step(s), loss on training batch is 0.000264155.
After 15195 training step(s), loss on training batch is 0.00042362.
After 15196 training step(s), loss on training batch is 0.000419813.
After 15197 training step(s), loss on training batch is 0.000238685.
After 15198 training step(s), loss on training batch is 0.000256037.
After 15199 training step(s), loss on training batch is 0.000321628.
After 15200 training step(s), loss on training batch is 0.000339276.
After 15201 training step(s), loss on training batch is 0.000497558.
After 15202 training step(s), loss on training batch is 0.000574455.
After 15203 training step(s), loss on training batch is 0.000358816.
After 15204 training step(s), loss on training batch is 0.000495111.
After 15205 training step(s), loss on training batch is 0.000900467.
After 15206 training step(s), loss on training batch is 0.000637147.
After 15207 training step(s), loss on training batch is 0.000357711.
After 15208 training step(s), loss on training batch is 0.000436674.
After 15209 training step(s), loss on training batch is 0.000536774.
After 15210 training step(s), loss on training batch is 0.000580346.
After 15211 training step(s), loss on training batch is 0.000379739.
After 15212 training step(s), loss on training batch is 0.000285652.
After 15213 training step(s), loss on training batch is 0.000339981.
After 15214 training step(s), loss on training batch is 0.000353777.
After 15215 training step(s), loss on training batch is 0.000339746.
After 15216 training step(s), loss on training batch is 0.000300421.
After 15217 training step(s), loss on training batch is 0.000363947.
After 15218 training step(s), loss on training batch is 0.000325798.
After 15219 training step(s), loss on training batch is 0.000397177.
After 15220 training step(s), loss on training batch is 0.0003667.
After 15221 training step(s), loss on training batch is 0.000287321.
After 15222 training step(s), loss on training batch is 0.000361027.
After 15223 training step(s), loss on training batch is 0.000334262.
After 15224 training step(s), loss on training batch is 0.000308586.
After 15225 training step(s), loss on training batch is 0.000298376.
After 15226 training step(s), loss on training batch is 0.000355908.
After 15227 training step(s), loss on training batch is 0.000284771.
After 15228 training step(s), loss on training batch is 0.000857162.
After 15229 training step(s), loss on training batch is 0.00058174.
After 15230 training step(s), loss on training batch is 0.000508172.
After 15231 training step(s), loss on training batch is 0.000517085.
After 15232 training step(s), loss on training batch is 0.000596498.
After 15233 training step(s), loss on training batch is 0.000691038.
After 15234 training step(s), loss on training batch is 0.00059078.
After 15235 training step(s), loss on training batch is 0.000451383.
After 15236 training step(s), loss on training batch is 0.000674963.
After 15237 training step(s), loss on training batch is 0.000639125.
After 15238 training step(s), loss on training batch is 0.000475706.
After 15239 training step(s), loss on training batch is 0.000467619.
After 15240 training step(s), loss on training batch is 0.00102049.
After 15241 training step(s), loss on training batch is 0.000487359.
After 15242 training step(s), loss on training batch is 0.000553734.
After 15243 training step(s), loss on training batch is 0.000560972.
After 15244 training step(s), loss on training batch is 0.000630265.
After 15245 training step(s), loss on training batch is 0.000478337.
After 15246 training step(s), loss on training batch is 0.00059624.
After 15247 training step(s), loss on training batch is 0.000504422.
After 15248 training step(s), loss on training batch is 0.000544942.
After 15249 training step(s), loss on training batch is 0.000587712.
After 15250 training step(s), loss on training batch is 0.00057572.
After 15251 training step(s), loss on training batch is 0.000562036.
After 15252 training step(s), loss on training batch is 0.000515303.
After 15253 training step(s), loss on training batch is 0.000575485.
After 15254 training step(s), loss on training batch is 0.000421713.
After 15255 training step(s), loss on training batch is 0.000544688.
After 15256 training step(s), loss on training batch is 0.000986144.
After 15257 training step(s), loss on training batch is 0.000837894.
After 15258 training step(s), loss on training batch is 0.000892524.
After 15259 training step(s), loss on training batch is 0.000905791.
After 15260 training step(s), loss on training batch is 0.000758423.
After 15261 training step(s), loss on training batch is 0.000795713.
After 15262 training step(s), loss on training batch is 0.00098306.
After 15263 training step(s), loss on training batch is 0.000860684.
After 15264 training step(s), loss on training batch is 0.000783404.
After 15265 training step(s), loss on training batch is 0.00103727.
After 15266 training step(s), loss on training batch is 0.000765026.
After 15267 training step(s), loss on training batch is 0.000767047.
After 15268 training step(s), loss on training batch is 0.000927525.
After 15269 training step(s), loss on training batch is 0.00152424.
After 15270 training step(s), loss on training batch is 0.00314602.
After 15271 training step(s), loss on training batch is 0.0013591.
After 15272 training step(s), loss on training batch is 0.000980329.
After 15273 training step(s), loss on training batch is 0.00113998.
After 15274 training step(s), loss on training batch is 0.000992624.
After 15275 training step(s), loss on training batch is 0.000946578.
After 15276 training step(s), loss on training batch is 0.000877154.
After 15277 training step(s), loss on training batch is 0.00118104.
After 15278 training step(s), loss on training batch is 0.000868209.
After 15279 training step(s), loss on training batch is 0.000821653.
After 15280 training step(s), loss on training batch is 0.00080752.
After 15281 training step(s), loss on training batch is 0.000538664.
After 15282 training step(s), loss on training batch is 0.000394965.
After 15283 training step(s), loss on training batch is 0.000268947.
After 15284 training step(s), loss on training batch is 0.000331322.
After 15285 training step(s), loss on training batch is 0.000349558.
After 15286 training step(s), loss on training batch is 0.000492518.
After 15287 training step(s), loss on training batch is 0.000594831.
After 15288 training step(s), loss on training batch is 0.000390807.
After 15289 training step(s), loss on training batch is 0.000342996.
After 15290 training step(s), loss on training batch is 0.000308876.
After 15291 training step(s), loss on training batch is 0.000261436.
After 15292 training step(s), loss on training batch is 0.000311893.
After 15293 training step(s), loss on training batch is 0.000302611.
After 15294 training step(s), loss on training batch is 0.000301603.
After 15295 training step(s), loss on training batch is 0.000333791.
After 15296 training step(s), loss on training batch is 0.000342889.
After 15297 training step(s), loss on training batch is 0.000275221.
After 15298 training step(s), loss on training batch is 0.000262095.
After 15299 training step(s), loss on training batch is 0.000275947.
After 15300 training step(s), loss on training batch is 0.000272615.
After 15301 training step(s), loss on training batch is 0.000636759.
After 15302 training step(s), loss on training batch is 0.000454077.
After 15303 training step(s), loss on training batch is 0.000497794.
After 15304 training step(s), loss on training batch is 0.00059298.
After 15305 training step(s), loss on training batch is 0.000563319.
After 15306 training step(s), loss on training batch is 0.000567177.
After 15307 training step(s), loss on training batch is 0.000537271.
After 15308 training step(s), loss on training batch is 0.000316373.
After 15309 training step(s), loss on training batch is 0.000353012.
After 15310 training step(s), loss on training batch is 0.000371515.
After 15311 training step(s), loss on training batch is 0.000421728.
After 15312 training step(s), loss on training batch is 0.000427986.
After 15313 training step(s), loss on training batch is 0.000438736.
After 15314 training step(s), loss on training batch is 0.000584873.
After 15315 training step(s), loss on training batch is 0.0012885.
After 15316 training step(s), loss on training batch is 0.000925055.
After 15317 training step(s), loss on training batch is 0.000758338.
After 15318 training step(s), loss on training batch is 0.000584063.
After 15319 training step(s), loss on training batch is 0.00037035.
After 15320 training step(s), loss on training batch is 0.000353853.
After 15321 training step(s), loss on training batch is 0.000363724.
After 15322 training step(s), loss on training batch is 0.000325842.
After 15323 training step(s), loss on training batch is 0.000502707.
After 15324 training step(s), loss on training batch is 0.000365277.
After 15325 training step(s), loss on training batch is 0.000360016.
After 15326 training step(s), loss on training batch is 0.000348229.
After 15327 training step(s), loss on training batch is 0.000790638.
After 15328 training step(s), loss on training batch is 0.0018772.
After 15329 training step(s), loss on training batch is 0.000661721.
After 15330 training step(s), loss on training batch is 0.000499304.
After 15331 training step(s), loss on training batch is 0.000515852.
After 15332 training step(s), loss on training batch is 0.000491802.
After 15333 training step(s), loss on training batch is 0.000422404.
After 15334 training step(s), loss on training batch is 0.000333138.
After 15335 training step(s), loss on training batch is 0.000311983.
After 15336 training step(s), loss on training batch is 0.000310403.
After 15337 training step(s), loss on training batch is 0.000352452.
After 15338 training step(s), loss on training batch is 0.000353885.
After 15339 training step(s), loss on training batch is 0.000394952.
After 15340 training step(s), loss on training batch is 0.000451909.
After 15341 training step(s), loss on training batch is 0.000341774.
After 15342 training step(s), loss on training batch is 0.000421021.
After 15343 training step(s), loss on training batch is 0.000406704.
After 15344 training step(s), loss on training batch is 0.000478122.
After 15345 training step(s), loss on training batch is 0.000327499.
After 15346 training step(s), loss on training batch is 0.000427253.
After 15347 training step(s), loss on training batch is 0.000341811.
After 15348 training step(s), loss on training batch is 0.000314766.
After 15349 training step(s), loss on training batch is 0.000353112.
After 15350 training step(s), loss on training batch is 0.000464876.
After 15351 training step(s), loss on training batch is 0.000430539.
After 15352 training step(s), loss on training batch is 0.000382589.
After 15353 training step(s), loss on training batch is 0.000386785.
After 15354 training step(s), loss on training batch is 0.000365711.
After 15355 training step(s), loss on training batch is 0.000341342.
After 15356 training step(s), loss on training batch is 0.00044657.
After 15357 training step(s), loss on training batch is 0.000536924.
After 15358 training step(s), loss on training batch is 0.000345818.
After 15359 training step(s), loss on training batch is 0.000380154.
After 15360 training step(s), loss on training batch is 0.000484409.
After 15361 training step(s), loss on training batch is 0.000337607.
After 15362 training step(s), loss on training batch is 0.000309267.
After 15363 training step(s), loss on training batch is 0.000317692.
After 15364 training step(s), loss on training batch is 0.000433758.
After 15365 training step(s), loss on training batch is 0.000393538.
After 15366 training step(s), loss on training batch is 0.000388439.
After 15367 training step(s), loss on training batch is 0.000382826.
After 15368 training step(s), loss on training batch is 0.000362514.
After 15369 training step(s), loss on training batch is 0.000415406.
After 15370 training step(s), loss on training batch is 0.000340817.
After 15371 training step(s), loss on training batch is 0.000341077.
After 15372 training step(s), loss on training batch is 0.000329376.
After 15373 training step(s), loss on training batch is 0.00031787.
After 15374 training step(s), loss on training batch is 0.000300937.
After 15375 training step(s), loss on training batch is 0.000368284.
After 15376 training step(s), loss on training batch is 0.000360829.
After 15377 training step(s), loss on training batch is 0.000455347.
After 15378 training step(s), loss on training batch is 0.000357018.
After 15379 training step(s), loss on training batch is 0.000328228.
After 15380 training step(s), loss on training batch is 0.00034646.
After 15381 training step(s), loss on training batch is 0.00033564.
After 15382 training step(s), loss on training batch is 0.000348846.
After 15383 training step(s), loss on training batch is 0.000627311.
After 15384 training step(s), loss on training batch is 0.000740453.
After 15385 training step(s), loss on training batch is 0.000868829.
After 15386 training step(s), loss on training batch is 0.000592626.
After 15387 training step(s), loss on training batch is 0.000636798.
After 15388 training step(s), loss on training batch is 0.000561492.
After 15389 training step(s), loss on training batch is 0.000571612.
After 15390 training step(s), loss on training batch is 0.000555296.
After 15391 training step(s), loss on training batch is 0.000653707.
After 15392 training step(s), loss on training batch is 0.000692218.
After 15393 training step(s), loss on training batch is 0.000550183.
After 15394 training step(s), loss on training batch is 0.000535841.
After 15395 training step(s), loss on training batch is 0.000681443.
After 15396 training step(s), loss on training batch is 0.000647467.
After 15397 training step(s), loss on training batch is 0.000585976.
After 15398 training step(s), loss on training batch is 0.000621004.
After 15399 training step(s), loss on training batch is 0.000804952.
After 15400 training step(s), loss on training batch is 0.000634907.
After 15401 training step(s), loss on training batch is 0.000562069.
After 15402 training step(s), loss on training batch is 0.000705524.
After 15403 training step(s), loss on training batch is 0.000531062.
After 15404 training step(s), loss on training batch is 0.000527456.
After 15405 training step(s), loss on training batch is 0.000464297.
After 15406 training step(s), loss on training batch is 0.000632069.
After 15407 training step(s), loss on training batch is 0.000783114.
After 15408 training step(s), loss on training batch is 0.000674659.
After 15409 training step(s), loss on training batch is 0.000503066.
After 15410 training step(s), loss on training batch is 0.000783263.
After 15411 training step(s), loss on training batch is 0.000594056.
After 15412 training step(s), loss on training batch is 0.000588059.
After 15413 training step(s), loss on training batch is 0.000483506.
After 15414 training step(s), loss on training batch is 0.000495485.
After 15415 training step(s), loss on training batch is 0.000470694.
After 15416 training step(s), loss on training batch is 0.00049523.
After 15417 training step(s), loss on training batch is 0.000499134.
After 15418 training step(s), loss on training batch is 0.000659978.
After 15419 training step(s), loss on training batch is 0.000890439.
After 15420 training step(s), loss on training batch is 0.000902582.
After 15421 training step(s), loss on training batch is 0.00101812.
After 15422 training step(s), loss on training batch is 0.000572748.
After 15423 training step(s), loss on training batch is 0.000562636.
After 15424 training step(s), loss on training batch is 0.000561523.
After 15425 training step(s), loss on training batch is 0.000607243.
After 15426 training step(s), loss on training batch is 0.000645625.
After 15427 training step(s), loss on training batch is 0.000519383.
After 15428 training step(s), loss on training batch is 0.000599302.
After 15429 training step(s), loss on training batch is 0.000582516.
After 15430 training step(s), loss on training batch is 0.000698555.
After 15431 training step(s), loss on training batch is 0.000802858.
After 15432 training step(s), loss on training batch is 0.000515436.
After 15433 training step(s), loss on training batch is 0.000549452.
After 15434 training step(s), loss on training batch is 0.000505347.
After 15435 training step(s), loss on training batch is 0.000470782.
After 15436 training step(s), loss on training batch is 0.000687139.
After 15437 training step(s), loss on training batch is 0.00106597.
After 15438 training step(s), loss on training batch is 0.000474211.
After 15439 training step(s), loss on training batch is 0.000551469.
After 15440 training step(s), loss on training batch is 0.000581246.
After 15441 training step(s), loss on training batch is 0.000532518.
After 15442 training step(s), loss on training batch is 0.000517936.
After 15443 training step(s), loss on training batch is 0.000589829.
After 15444 training step(s), loss on training batch is 0.000549703.
After 15445 training step(s), loss on training batch is 0.000583669.
After 15446 training step(s), loss on training batch is 0.000727762.
After 15447 training step(s), loss on training batch is 0.000562073.
After 15448 training step(s), loss on training batch is 0.000557847.
After 15449 training step(s), loss on training batch is 0.000605253.
After 15450 training step(s), loss on training batch is 0.00061122.
After 15451 training step(s), loss on training batch is 0.000513174.
After 15452 training step(s), loss on training batch is 0.000479132.
After 15453 training step(s), loss on training batch is 0.000645345.
After 15454 training step(s), loss on training batch is 0.000565184.
After 15455 training step(s), loss on training batch is 0.000618928.
After 15456 training step(s), loss on training batch is 0.000570584.
After 15457 training step(s), loss on training batch is 0.000505194.
After 15458 training step(s), loss on training batch is 0.000628166.
After 15459 training step(s), loss on training batch is 0.000788953.
After 15460 training step(s), loss on training batch is 0.000526958.
After 15461 training step(s), loss on training batch is 0.00047864.
After 15462 training step(s), loss on training batch is 0.000474673.
After 15463 training step(s), loss on training batch is 0.000494731.
After 15464 training step(s), loss on training batch is 0.000463409.
After 15465 training step(s), loss on training batch is 0.000620017.
After 15466 training step(s), loss on training batch is 0.000993501.
After 15467 training step(s), loss on training batch is 0.00103587.
After 15468 training step(s), loss on training batch is 0.000951441.
After 15469 training step(s), loss on training batch is 0.000944451.
After 15470 training step(s), loss on training batch is 0.000934084.
After 15471 training step(s), loss on training batch is 0.000868159.
After 15472 training step(s), loss on training batch is 0.00093131.
After 15473 training step(s), loss on training batch is 0.00128258.
After 15474 training step(s), loss on training batch is 0.00112251.
After 15475 training step(s), loss on training batch is 0.00115742.
After 15476 training step(s), loss on training batch is 0.000891293.
After 15477 training step(s), loss on training batch is 0.000885308.
After 15478 training step(s), loss on training batch is 0.000946392.
After 15479 training step(s), loss on training batch is 0.000850933.
After 15480 training step(s), loss on training batch is 0.000844216.
After 15481 training step(s), loss on training batch is 0.000820708.
After 15482 training step(s), loss on training batch is 0.0008565.
After 15483 training step(s), loss on training batch is 0.000934664.
After 15484 training step(s), loss on training batch is 0.00083655.
After 15485 training step(s), loss on training batch is 0.000830498.
After 15486 training step(s), loss on training batch is 0.00110546.
After 15487 training step(s), loss on training batch is 0.000836889.
After 15488 training step(s), loss on training batch is 0.00101167.
After 15489 training step(s), loss on training batch is 0.000868235.
After 15490 training step(s), loss on training batch is 0.000856609.
After 15491 training step(s), loss on training batch is 0.000870299.
After 15492 training step(s), loss on training batch is 0.000868133.
After 15493 training step(s), loss on training batch is 0.000806858.
After 15494 training step(s), loss on training batch is 0.000937024.
After 15495 training step(s), loss on training batch is 0.000942312.
After 15496 training step(s), loss on training batch is 0.00107693.
After 15497 training step(s), loss on training batch is 0.000841789.
After 15498 training step(s), loss on training batch is 0.000868908.
After 15499 training step(s), loss on training batch is 0.000778409.
After 15500 training step(s), loss on training batch is 0.000850814.
After 15501 training step(s), loss on training batch is 0.00076794.
After 15502 training step(s), loss on training batch is 0.00095052.
After 15503 training step(s), loss on training batch is 0.000804134.
After 15504 training step(s), loss on training batch is 0.00107108.
After 15505 training step(s), loss on training batch is 0.00111218.
After 15506 training step(s), loss on training batch is 0.00105678.
After 15507 training step(s), loss on training batch is 0.00121189.
After 15508 training step(s), loss on training batch is 0.00103833.
After 15509 training step(s), loss on training batch is 0.00650589.
After 15510 training step(s), loss on training batch is 0.00156995.
After 15511 training step(s), loss on training batch is 0.00134251.
After 15512 training step(s), loss on training batch is 0.00130266.
After 15513 training step(s), loss on training batch is 0.00122708.
After 15514 training step(s), loss on training batch is 0.0012686.
After 15515 training step(s), loss on training batch is 0.00105227.
After 15516 training step(s), loss on training batch is 0.00049624.
After 15517 training step(s), loss on training batch is 0.000443578.
After 15518 training step(s), loss on training batch is 0.000405725.
After 15519 training step(s), loss on training batch is 0.00042956.
After 15520 training step(s), loss on training batch is 0.000453811.
After 15521 training step(s), loss on training batch is 0.000455896.
After 15522 training step(s), loss on training batch is 0.000465602.
After 15523 training step(s), loss on training batch is 0.000454649.
After 15524 training step(s), loss on training batch is 0.000440735.
After 15525 training step(s), loss on training batch is 0.000409473.
After 15526 training step(s), loss on training batch is 0.000832311.
After 15527 training step(s), loss on training batch is 0.000866496.
After 15528 training step(s), loss on training batch is 0.000956352.
After 15529 training step(s), loss on training batch is 0.000852053.
After 15530 training step(s), loss on training batch is 0.00154453.
After 15531 training step(s), loss on training batch is 0.00139018.
After 15532 training step(s), loss on training batch is 0.00101712.
After 15533 training step(s), loss on training batch is 0.00106707.
After 15534 training step(s), loss on training batch is 0.000891772.
After 15535 training step(s), loss on training batch is 0.000865106.
After 15536 training step(s), loss on training batch is 0.000879869.
After 15537 training step(s), loss on training batch is 0.000888641.
After 15538 training step(s), loss on training batch is 0.00102613.
After 15539 training step(s), loss on training batch is 0.000888449.
After 15540 training step(s), loss on training batch is 0.000870362.
After 15541 training step(s), loss on training batch is 0.000870995.
After 15542 training step(s), loss on training batch is 0.00155002.
After 15543 training step(s), loss on training batch is 0.000328705.
After 15544 training step(s), loss on training batch is 0.000376329.
After 15545 training step(s), loss on training batch is 0.000351096.
After 15546 training step(s), loss on training batch is 0.000434467.
After 15547 training step(s), loss on training batch is 0.000399336.
After 15548 training step(s), loss on training batch is 0.000383554.
After 15549 training step(s), loss on training batch is 0.000331689.
After 15550 training step(s), loss on training batch is 0.000303634.
After 15551 training step(s), loss on training batch is 0.000487299.
After 15552 training step(s), loss on training batch is 0.000396701.
After 15553 training step(s), loss on training batch is 0.000391744.
After 15554 training step(s), loss on training batch is 0.000349279.
After 15555 training step(s), loss on training batch is 0.000500642.
After 15556 training step(s), loss on training batch is 0.000365599.
After 15557 training step(s), loss on training batch is 0.000315005.
After 15558 training step(s), loss on training batch is 0.000320089.
After 15559 training step(s), loss on training batch is 0.000697075.
After 15560 training step(s), loss on training batch is 0.00086019.
After 15561 training step(s), loss on training batch is 0.000478429.
After 15562 training step(s), loss on training batch is 0.000379467.
After 15563 training step(s), loss on training batch is 0.000345933.
After 15564 training step(s), loss on training batch is 0.000421605.
After 15565 training step(s), loss on training batch is 0.000439446.
After 15566 training step(s), loss on training batch is 0.000391342.
After 15567 training step(s), loss on training batch is 0.000349607.
After 15568 training step(s), loss on training batch is 0.000492396.
After 15569 training step(s), loss on training batch is 0.000300653.
After 15570 training step(s), loss on training batch is 0.000365528.
After 15571 training step(s), loss on training batch is 0.000257112.
After 15572 training step(s), loss on training batch is 0.000276334.
After 15573 training step(s), loss on training batch is 0.000300471.
After 15574 training step(s), loss on training batch is 0.000305582.
After 15575 training step(s), loss on training batch is 0.000444951.
After 15576 training step(s), loss on training batch is 0.000400523.
After 15577 training step(s), loss on training batch is 0.00038612.
After 15578 training step(s), loss on training batch is 0.00030651.
After 15579 training step(s), loss on training batch is 0.000337251.
After 15580 training step(s), loss on training batch is 0.000299799.
After 15581 training step(s), loss on training batch is 0.000336853.
After 15582 training step(s), loss on training batch is 0.000310032.
After 15583 training step(s), loss on training batch is 0.00037785.
After 15584 training step(s), loss on training batch is 0.000365488.
After 15585 training step(s), loss on training batch is 0.000388976.
After 15586 training step(s), loss on training batch is 0.000502215.
After 15587 training step(s), loss on training batch is 0.000390385.
After 15588 training step(s), loss on training batch is 0.000313711.
After 15589 training step(s), loss on training batch is 0.000358559.
After 15590 training step(s), loss on training batch is 0.000333237.
After 15591 training step(s), loss on training batch is 0.000303068.
After 15592 training step(s), loss on training batch is 0.000310977.
After 15593 training step(s), loss on training batch is 0.000315359.
After 15594 training step(s), loss on training batch is 0.000263926.
After 15595 training step(s), loss on training batch is 0.000423557.
After 15596 training step(s), loss on training batch is 0.000421525.
After 15597 training step(s), loss on training batch is 0.000239526.
After 15598 training step(s), loss on training batch is 0.000253622.
After 15599 training step(s), loss on training batch is 0.000322402.
After 15600 training step(s), loss on training batch is 0.000337747.
After 15601 training step(s), loss on training batch is 0.000481945.
After 15602 training step(s), loss on training batch is 0.000563718.
After 15603 training step(s), loss on training batch is 0.000350547.
After 15604 training step(s), loss on training batch is 0.000497564.
After 15605 training step(s), loss on training batch is 0.000916652.
After 15606 training step(s), loss on training batch is 0.00064281.
After 15607 training step(s), loss on training batch is 0.00035578.
After 15608 training step(s), loss on training batch is 0.000431898.
After 15609 training step(s), loss on training batch is 0.000544822.
After 15610 training step(s), loss on training batch is 0.000560427.
After 15611 training step(s), loss on training batch is 0.000370533.
After 15612 training step(s), loss on training batch is 0.000300034.
After 15613 training step(s), loss on training batch is 0.000362134.
After 15614 training step(s), loss on training batch is 0.00038336.
After 15615 training step(s), loss on training batch is 0.000358869.
After 15616 training step(s), loss on training batch is 0.000309578.
After 15617 training step(s), loss on training batch is 0.00039495.
After 15618 training step(s), loss on training batch is 0.000330903.
After 15619 training step(s), loss on training batch is 0.000415949.
After 15620 training step(s), loss on training batch is 0.000372367.
After 15621 training step(s), loss on training batch is 0.000286616.
After 15622 training step(s), loss on training batch is 0.000375263.
After 15623 training step(s), loss on training batch is 0.000344976.
After 15624 training step(s), loss on training batch is 0.000299457.
After 15625 training step(s), loss on training batch is 0.000297588.
After 15626 training step(s), loss on training batch is 0.000350147.
After 15627 training step(s), loss on training batch is 0.00028712.
After 15628 training step(s), loss on training batch is 0.000805269.
After 15629 training step(s), loss on training batch is 0.000556738.
After 15630 training step(s), loss on training batch is 0.000506343.
After 15631 training step(s), loss on training batch is 0.000510389.
After 15632 training step(s), loss on training batch is 0.000585615.
After 15633 training step(s), loss on training batch is 0.000660256.
After 15634 training step(s), loss on training batch is 0.000570853.
After 15635 training step(s), loss on training batch is 0.000452054.
After 15636 training step(s), loss on training batch is 0.000653919.
After 15637 training step(s), loss on training batch is 0.000620459.
After 15638 training step(s), loss on training batch is 0.000468603.
After 15639 training step(s), loss on training batch is 0.000461822.
After 15640 training step(s), loss on training batch is 0.00102551.
After 15641 training step(s), loss on training batch is 0.000477307.
After 15642 training step(s), loss on training batch is 0.000545195.
After 15643 training step(s), loss on training batch is 0.000549159.
After 15644 training step(s), loss on training batch is 0.000630769.
After 15645 training step(s), loss on training batch is 0.000469762.
After 15646 training step(s), loss on training batch is 0.000606721.
After 15647 training step(s), loss on training batch is 0.000499794.
After 15648 training step(s), loss on training batch is 0.000534294.
After 15649 training step(s), loss on training batch is 0.000586264.
After 15650 training step(s), loss on training batch is 0.000567866.
After 15651 training step(s), loss on training batch is 0.000554155.
After 15652 training step(s), loss on training batch is 0.000538543.
After 15653 training step(s), loss on training batch is 0.0005778.
After 15654 training step(s), loss on training batch is 0.000408482.
After 15655 training step(s), loss on training batch is 0.000546406.
After 15656 training step(s), loss on training batch is 0.000974245.
After 15657 training step(s), loss on training batch is 0.00083033.
After 15658 training step(s), loss on training batch is 0.000896661.
After 15659 training step(s), loss on training batch is 0.000893834.
After 15660 training step(s), loss on training batch is 0.000787616.
After 15661 training step(s), loss on training batch is 0.000815949.
After 15662 training step(s), loss on training batch is 0.00097871.
After 15663 training step(s), loss on training batch is 0.000866427.
After 15664 training step(s), loss on training batch is 0.000755809.
After 15665 training step(s), loss on training batch is 0.00106402.
After 15666 training step(s), loss on training batch is 0.000757707.
After 15667 training step(s), loss on training batch is 0.00078087.
After 15668 training step(s), loss on training batch is 0.000901255.
After 15669 training step(s), loss on training batch is 0.00141398.
After 15670 training step(s), loss on training batch is 0.00317612.
After 15671 training step(s), loss on training batch is 0.00138346.
After 15672 training step(s), loss on training batch is 0.00091793.
After 15673 training step(s), loss on training batch is 0.00110691.
After 15674 training step(s), loss on training batch is 0.000943974.
After 15675 training step(s), loss on training batch is 0.000911498.
After 15676 training step(s), loss on training batch is 0.000844157.
After 15677 training step(s), loss on training batch is 0.00117245.
After 15678 training step(s), loss on training batch is 0.000846551.
After 15679 training step(s), loss on training batch is 0.000820098.
After 15680 training step(s), loss on training batch is 0.000797419.
After 15681 training step(s), loss on training batch is 0.000531278.
After 15682 training step(s), loss on training batch is 0.000391547.
After 15683 training step(s), loss on training batch is 0.000267771.
After 15684 training step(s), loss on training batch is 0.000327183.
After 15685 training step(s), loss on training batch is 0.000347639.
After 15686 training step(s), loss on training batch is 0.000483725.
After 15687 training step(s), loss on training batch is 0.000601656.
After 15688 training step(s), loss on training batch is 0.000380908.
After 15689 training step(s), loss on training batch is 0.000327792.
After 15690 training step(s), loss on training batch is 0.000291146.
After 15691 training step(s), loss on training batch is 0.000249981.
After 15692 training step(s), loss on training batch is 0.000313299.
After 15693 training step(s), loss on training batch is 0.000295475.
After 15694 training step(s), loss on training batch is 0.000292978.
After 15695 training step(s), loss on training batch is 0.000326772.
After 15696 training step(s), loss on training batch is 0.000336687.
After 15697 training step(s), loss on training batch is 0.000267093.
After 15698 training step(s), loss on training batch is 0.000251061.
After 15699 training step(s), loss on training batch is 0.000269727.
After 15700 training step(s), loss on training batch is 0.000272829.
After 15701 training step(s), loss on training batch is 0.00066706.
After 15702 training step(s), loss on training batch is 0.000454316.
After 15703 training step(s), loss on training batch is 0.000491714.
After 15704 training step(s), loss on training batch is 0.000584031.
After 15705 training step(s), loss on training batch is 0.000548289.
After 15706 training step(s), loss on training batch is 0.000561024.
After 15707 training step(s), loss on training batch is 0.000543482.
After 15708 training step(s), loss on training batch is 0.000314732.
After 15709 training step(s), loss on training batch is 0.000352362.
After 15710 training step(s), loss on training batch is 0.000371859.
After 15711 training step(s), loss on training batch is 0.000442101.
After 15712 training step(s), loss on training batch is 0.000437219.
After 15713 training step(s), loss on training batch is 0.000435544.
After 15714 training step(s), loss on training batch is 0.000543142.
After 15715 training step(s), loss on training batch is 0.00126534.
After 15716 training step(s), loss on training batch is 0.000907825.
After 15717 training step(s), loss on training batch is 0.000754018.
After 15718 training step(s), loss on training batch is 0.000590044.
After 15719 training step(s), loss on training batch is 0.000365459.
After 15720 training step(s), loss on training batch is 0.000362053.
After 15721 training step(s), loss on training batch is 0.000372797.
After 15722 training step(s), loss on training batch is 0.000331252.
After 15723 training step(s), loss on training batch is 0.000517284.
After 15724 training step(s), loss on training batch is 0.0003926.
After 15725 training step(s), loss on training batch is 0.000382663.
After 15726 training step(s), loss on training batch is 0.000370106.
After 15727 training step(s), loss on training batch is 0.000689308.
After 15728 training step(s), loss on training batch is 0.00166018.
After 15729 training step(s), loss on training batch is 0.000618582.
After 15730 training step(s), loss on training batch is 0.000471417.
After 15731 training step(s), loss on training batch is 0.000533165.
After 15732 training step(s), loss on training batch is 0.000482446.
After 15733 training step(s), loss on training batch is 0.000412783.
After 15734 training step(s), loss on training batch is 0.00033245.
After 15735 training step(s), loss on training batch is 0.000301844.
After 15736 training step(s), loss on training batch is 0.00030265.
After 15737 training step(s), loss on training batch is 0.000340449.
After 15738 training step(s), loss on training batch is 0.000334911.
After 15739 training step(s), loss on training batch is 0.000378881.
After 15740 training step(s), loss on training batch is 0.000429191.
After 15741 training step(s), loss on training batch is 0.000343515.
After 15742 training step(s), loss on training batch is 0.000421136.
After 15743 training step(s), loss on training batch is 0.00041099.
After 15744 training step(s), loss on training batch is 0.000474268.
After 15745 training step(s), loss on training batch is 0.000324738.
After 15746 training step(s), loss on training batch is 0.000481845.
After 15747 training step(s), loss on training batch is 0.000351202.
After 15748 training step(s), loss on training batch is 0.000308387.
After 15749 training step(s), loss on training batch is 0.000356108.
After 15750 training step(s), loss on training batch is 0.000440588.
After 15751 training step(s), loss on training batch is 0.000418755.
After 15752 training step(s), loss on training batch is 0.000385788.
After 15753 training step(s), loss on training batch is 0.000387254.
After 15754 training step(s), loss on training batch is 0.000363148.
After 15755 training step(s), loss on training batch is 0.000339077.
After 15756 training step(s), loss on training batch is 0.000441287.
After 15757 training step(s), loss on training batch is 0.000535425.
After 15758 training step(s), loss on training batch is 0.000350774.
After 15759 training step(s), loss on training batch is 0.000390636.
After 15760 training step(s), loss on training batch is 0.000472572.
After 15761 training step(s), loss on training batch is 0.000337543.
After 15762 training step(s), loss on training batch is 0.000308647.
After 15763 training step(s), loss on training batch is 0.000316168.
After 15764 training step(s), loss on training batch is 0.000434342.
After 15765 training step(s), loss on training batch is 0.000387502.
After 15766 training step(s), loss on training batch is 0.00038631.
After 15767 training step(s), loss on training batch is 0.000378331.
After 15768 training step(s), loss on training batch is 0.000388601.
After 15769 training step(s), loss on training batch is 0.000409368.
After 15770 training step(s), loss on training batch is 0.000339831.
After 15771 training step(s), loss on training batch is 0.000341722.
After 15772 training step(s), loss on training batch is 0.000337929.
After 15773 training step(s), loss on training batch is 0.000319709.
After 15774 training step(s), loss on training batch is 0.000297948.
After 15775 training step(s), loss on training batch is 0.00036636.
After 15776 training step(s), loss on training batch is 0.000355358.
After 15777 training step(s), loss on training batch is 0.000456869.
After 15778 training step(s), loss on training batch is 0.000356457.
After 15779 training step(s), loss on training batch is 0.000325789.
After 15780 training step(s), loss on training batch is 0.000346012.
After 15781 training step(s), loss on training batch is 0.000335514.
After 15782 training step(s), loss on training batch is 0.000351514.
After 15783 training step(s), loss on training batch is 0.000614306.
After 15784 training step(s), loss on training batch is 0.000722676.
After 15785 training step(s), loss on training batch is 0.000839835.
After 15786 training step(s), loss on training batch is 0.000589107.
After 15787 training step(s), loss on training batch is 0.000641035.
After 15788 training step(s), loss on training batch is 0.000557148.
After 15789 training step(s), loss on training batch is 0.000588311.
After 15790 training step(s), loss on training batch is 0.00056214.
After 15791 training step(s), loss on training batch is 0.000655478.
After 15792 training step(s), loss on training batch is 0.000679072.
After 15793 training step(s), loss on training batch is 0.000554175.
After 15794 training step(s), loss on training batch is 0.00053731.
After 15795 training step(s), loss on training batch is 0.000667812.
After 15796 training step(s), loss on training batch is 0.000638807.
After 15797 training step(s), loss on training batch is 0.000583081.
After 15798 training step(s), loss on training batch is 0.000612545.
After 15799 training step(s), loss on training batch is 0.00079577.
After 15800 training step(s), loss on training batch is 0.000633287.
After 15801 training step(s), loss on training batch is 0.000558012.
After 15802 training step(s), loss on training batch is 0.000660574.
After 15803 training step(s), loss on training batch is 0.000565515.
After 15804 training step(s), loss on training batch is 0.000548224.
After 15805 training step(s), loss on training batch is 0.000476497.
After 15806 training step(s), loss on training batch is 0.000624427.
After 15807 training step(s), loss on training batch is 0.000759488.
After 15808 training step(s), loss on training batch is 0.000650455.
After 15809 training step(s), loss on training batch is 0.000503885.
After 15810 training step(s), loss on training batch is 0.000751979.
After 15811 training step(s), loss on training batch is 0.000600808.
After 15812 training step(s), loss on training batch is 0.000578517.
After 15813 training step(s), loss on training batch is 0.000486313.
After 15814 training step(s), loss on training batch is 0.000494008.
After 15815 training step(s), loss on training batch is 0.000468577.
After 15816 training step(s), loss on training batch is 0.000489534.
After 15817 training step(s), loss on training batch is 0.000495494.
After 15818 training step(s), loss on training batch is 0.00066611.
After 15819 training step(s), loss on training batch is 0.000883009.
After 15820 training step(s), loss on training batch is 0.000902562.
After 15821 training step(s), loss on training batch is 0.00102154.
After 15822 training step(s), loss on training batch is 0.000581065.
After 15823 training step(s), loss on training batch is 0.000496914.
After 15824 training step(s), loss on training batch is 0.000534958.
After 15825 training step(s), loss on training batch is 0.000566698.
After 15826 training step(s), loss on training batch is 0.000624607.
After 15827 training step(s), loss on training batch is 0.000509988.
After 15828 training step(s), loss on training batch is 0.000587068.
After 15829 training step(s), loss on training batch is 0.000577224.
After 15830 training step(s), loss on training batch is 0.000684843.
After 15831 training step(s), loss on training batch is 0.000800826.
After 15832 training step(s), loss on training batch is 0.000505332.
After 15833 training step(s), loss on training batch is 0.000546016.
After 15834 training step(s), loss on training batch is 0.000500806.
After 15835 training step(s), loss on training batch is 0.000466751.
After 15836 training step(s), loss on training batch is 0.000684187.
After 15837 training step(s), loss on training batch is 0.00108736.
After 15838 training step(s), loss on training batch is 0.000470064.
After 15839 training step(s), loss on training batch is 0.000545979.
After 15840 training step(s), loss on training batch is 0.000575013.
After 15841 training step(s), loss on training batch is 0.000517033.
After 15842 training step(s), loss on training batch is 0.000507526.
After 15843 training step(s), loss on training batch is 0.000569768.
After 15844 training step(s), loss on training batch is 0.000530897.
After 15845 training step(s), loss on training batch is 0.000569492.
After 15846 training step(s), loss on training batch is 0.000727926.
After 15847 training step(s), loss on training batch is 0.000551538.
After 15848 training step(s), loss on training batch is 0.000552386.
After 15849 training step(s), loss on training batch is 0.000596398.
After 15850 training step(s), loss on training batch is 0.000605782.
After 15851 training step(s), loss on training batch is 0.000510335.
After 15852 training step(s), loss on training batch is 0.000475913.
After 15853 training step(s), loss on training batch is 0.000636221.
After 15854 training step(s), loss on training batch is 0.00055328.
After 15855 training step(s), loss on training batch is 0.000606248.
After 15856 training step(s), loss on training batch is 0.000550038.
After 15857 training step(s), loss on training batch is 0.000499914.
After 15858 training step(s), loss on training batch is 0.000632095.
After 15859 training step(s), loss on training batch is 0.000806051.
After 15860 training step(s), loss on training batch is 0.000527607.
After 15861 training step(s), loss on training batch is 0.000472755.
After 15862 training step(s), loss on training batch is 0.000479.
After 15863 training step(s), loss on training batch is 0.000501837.
After 15864 training step(s), loss on training batch is 0.000464039.
After 15865 training step(s), loss on training batch is 0.000619051.
After 15866 training step(s), loss on training batch is 0.00097195.
After 15867 training step(s), loss on training batch is 0.0010225.
After 15868 training step(s), loss on training batch is 0.000938852.
After 15869 training step(s), loss on training batch is 0.000975974.
After 15870 training step(s), loss on training batch is 0.000935859.
After 15871 training step(s), loss on training batch is 0.000895382.
After 15872 training step(s), loss on training batch is 0.000928718.
After 15873 training step(s), loss on training batch is 0.00115237.
After 15874 training step(s), loss on training batch is 0.00108746.
After 15875 training step(s), loss on training batch is 0.00111296.
After 15876 training step(s), loss on training batch is 0.000931653.
After 15877 training step(s), loss on training batch is 0.00090711.
After 15878 training step(s), loss on training batch is 0.000944597.
After 15879 training step(s), loss on training batch is 0.000864939.
After 15880 training step(s), loss on training batch is 0.000845975.
After 15881 training step(s), loss on training batch is 0.000819401.
After 15882 training step(s), loss on training batch is 0.000865275.
After 15883 training step(s), loss on training batch is 0.000922195.
After 15884 training step(s), loss on training batch is 0.00083734.
After 15885 training step(s), loss on training batch is 0.000853005.
After 15886 training step(s), loss on training batch is 0.00106731.
After 15887 training step(s), loss on training batch is 0.000834824.
After 15888 training step(s), loss on training batch is 0.00100387.
After 15889 training step(s), loss on training batch is 0.000857241.
After 15890 training step(s), loss on training batch is 0.000843707.
After 15891 training step(s), loss on training batch is 0.00086415.
After 15892 training step(s), loss on training batch is 0.000852167.
After 15893 training step(s), loss on training batch is 0.000799177.
After 15894 training step(s), loss on training batch is 0.000934878.
After 15895 training step(s), loss on training batch is 0.000937217.
After 15896 training step(s), loss on training batch is 0.00103052.
After 15897 training step(s), loss on training batch is 0.000871752.
After 15898 training step(s), loss on training batch is 0.000861918.
After 15899 training step(s), loss on training batch is 0.000795629.
After 15900 training step(s), loss on training batch is 0.000856081.
After 15901 training step(s), loss on training batch is 0.000765011.
After 15902 training step(s), loss on training batch is 0.000944033.
After 15903 training step(s), loss on training batch is 0.000841873.
After 15904 training step(s), loss on training batch is 0.00107393.
After 15905 training step(s), loss on training batch is 0.00113235.
After 15906 training step(s), loss on training batch is 0.00108239.
After 15907 training step(s), loss on training batch is 0.00124846.
After 15908 training step(s), loss on training batch is 0.00103285.
After 15909 training step(s), loss on training batch is 0.00702088.
After 15910 training step(s), loss on training batch is 0.00184954.
After 15911 training step(s), loss on training batch is 0.00170592.
After 15912 training step(s), loss on training batch is 0.00161779.
After 15913 training step(s), loss on training batch is 0.00145045.
After 15914 training step(s), loss on training batch is 0.00133395.
After 15915 training step(s), loss on training batch is 0.00107966.
After 15916 training step(s), loss on training batch is 0.000504858.
After 15917 training step(s), loss on training batch is 0.000453415.
After 15918 training step(s), loss on training batch is 0.000429111.
After 15919 training step(s), loss on training batch is 0.00045174.
After 15920 training step(s), loss on training batch is 0.000474289.
After 15921 training step(s), loss on training batch is 0.00048452.
After 15922 training step(s), loss on training batch is 0.000499649.
After 15923 training step(s), loss on training batch is 0.000478762.
After 15924 training step(s), loss on training batch is 0.000456656.
After 15925 training step(s), loss on training batch is 0.000417179.
After 15926 training step(s), loss on training batch is 0.000856346.
After 15927 training step(s), loss on training batch is 0.000882997.
After 15928 training step(s), loss on training batch is 0.000963142.
After 15929 training step(s), loss on training batch is 0.0008362.
After 15930 training step(s), loss on training batch is 0.00156979.
After 15931 training step(s), loss on training batch is 0.00133818.
After 15932 training step(s), loss on training batch is 0.00102973.
After 15933 training step(s), loss on training batch is 0.00101629.
After 15934 training step(s), loss on training batch is 0.000897828.
After 15935 training step(s), loss on training batch is 0.000852916.
After 15936 training step(s), loss on training batch is 0.000887146.
After 15937 training step(s), loss on training batch is 0.00090499.
After 15938 training step(s), loss on training batch is 0.00101502.
After 15939 training step(s), loss on training batch is 0.000884646.
After 15940 training step(s), loss on training batch is 0.000879316.
After 15941 training step(s), loss on training batch is 0.000867755.
After 15942 training step(s), loss on training batch is 0.00150291.
After 15943 training step(s), loss on training batch is 0.000323414.
After 15944 training step(s), loss on training batch is 0.000358752.
After 15945 training step(s), loss on training batch is 0.000335658.
After 15946 training step(s), loss on training batch is 0.000428446.
After 15947 training step(s), loss on training batch is 0.000396963.
After 15948 training step(s), loss on training batch is 0.000377914.
After 15949 training step(s), loss on training batch is 0.000331046.
After 15950 training step(s), loss on training batch is 0.000306134.
After 15951 training step(s), loss on training batch is 0.000468097.
After 15952 training step(s), loss on training batch is 0.000401308.
After 15953 training step(s), loss on training batch is 0.000388791.
After 15954 training step(s), loss on training batch is 0.000349086.
After 15955 training step(s), loss on training batch is 0.000479897.
After 15956 training step(s), loss on training batch is 0.000350002.
After 15957 training step(s), loss on training batch is 0.000312667.
After 15958 training step(s), loss on training batch is 0.000321508.
After 15959 training step(s), loss on training batch is 0.000717845.
After 15960 training step(s), loss on training batch is 0.000880646.
After 15961 training step(s), loss on training batch is 0.000466291.
After 15962 training step(s), loss on training batch is 0.000359844.
After 15963 training step(s), loss on training batch is 0.000322635.
After 15964 training step(s), loss on training batch is 0.000400116.
After 15965 training step(s), loss on training batch is 0.000459788.
After 15966 training step(s), loss on training batch is 0.000367396.
After 15967 training step(s), loss on training batch is 0.000343492.
After 15968 training step(s), loss on training batch is 0.00046723.
After 15969 training step(s), loss on training batch is 0.000294485.
After 15970 training step(s), loss on training batch is 0.000362464.
After 15971 training step(s), loss on training batch is 0.000250677.
After 15972 training step(s), loss on training batch is 0.000272869.
After 15973 training step(s), loss on training batch is 0.00029888.
After 15974 training step(s), loss on training batch is 0.000309835.
After 15975 training step(s), loss on training batch is 0.000435355.
After 15976 training step(s), loss on training batch is 0.000396021.
After 15977 training step(s), loss on training batch is 0.000381847.
After 15978 training step(s), loss on training batch is 0.00030731.
After 15979 training step(s), loss on training batch is 0.0003341.
After 15980 training step(s), loss on training batch is 0.00030294.
After 15981 training step(s), loss on training batch is 0.000332748.
After 15982 training step(s), loss on training batch is 0.000308915.
After 15983 training step(s), loss on training batch is 0.000371047.
After 15984 training step(s), loss on training batch is 0.000362619.
After 15985 training step(s), loss on training batch is 0.000387008.
After 15986 training step(s), loss on training batch is 0.000500354.
After 15987 training step(s), loss on training batch is 0.000384306.
After 15988 training step(s), loss on training batch is 0.000313318.
After 15989 training step(s), loss on training batch is 0.000358542.
After 15990 training step(s), loss on training batch is 0.000331821.
After 15991 training step(s), loss on training batch is 0.000301338.
After 15992 training step(s), loss on training batch is 0.000311695.
After 15993 training step(s), loss on training batch is 0.000315686.
After 15994 training step(s), loss on training batch is 0.000262244.
After 15995 training step(s), loss on training batch is 0.000413509.
After 15996 training step(s), loss on training batch is 0.000416419.
After 15997 training step(s), loss on training batch is 0.00023196.
After 15998 training step(s), loss on training batch is 0.000255389.
After 15999 training step(s), loss on training batch is 0.000319764.
After 16000 training step(s), loss on training batch is 0.000333391.
After 16001 training step(s), loss on training batch is 0.000484245.
After 16002 training step(s), loss on training batch is 0.000564137.
After 16003 training step(s), loss on training batch is 0.000355986.
After 16004 training step(s), loss on training batch is 0.000491995.
After 16005 training step(s), loss on training batch is 0.000879304.
After 16006 training step(s), loss on training batch is 0.00062994.
After 16007 training step(s), loss on training batch is 0.000360966.
After 16008 training step(s), loss on training batch is 0.000438073.
After 16009 training step(s), loss on training batch is 0.000530245.
After 16010 training step(s), loss on training batch is 0.00055088.
After 16011 training step(s), loss on training batch is 0.000367846.
After 16012 training step(s), loss on training batch is 0.000299285.
After 16013 training step(s), loss on training batch is 0.000355171.
After 16014 training step(s), loss on training batch is 0.000376966.
After 16015 training step(s), loss on training batch is 0.000355719.
After 16016 training step(s), loss on training batch is 0.000307938.
After 16017 training step(s), loss on training batch is 0.000389766.
After 16018 training step(s), loss on training batch is 0.000330541.
After 16019 training step(s), loss on training batch is 0.000406849.
After 16020 training step(s), loss on training batch is 0.00036979.
After 16021 training step(s), loss on training batch is 0.000288005.
After 16022 training step(s), loss on training batch is 0.000360938.
After 16023 training step(s), loss on training batch is 0.000346477.
After 16024 training step(s), loss on training batch is 0.000300278.
After 16025 training step(s), loss on training batch is 0.000297796.
After 16026 training step(s), loss on training batch is 0.000347643.
After 16027 training step(s), loss on training batch is 0.000285364.
After 16028 training step(s), loss on training batch is 0.000803323.
After 16029 training step(s), loss on training batch is 0.000552704.
After 16030 training step(s), loss on training batch is 0.000503094.
After 16031 training step(s), loss on training batch is 0.000509109.
After 16032 training step(s), loss on training batch is 0.000572713.
After 16033 training step(s), loss on training batch is 0.000655121.
After 16034 training step(s), loss on training batch is 0.000565374.
After 16035 training step(s), loss on training batch is 0.00045165.
After 16036 training step(s), loss on training batch is 0.000644851.
After 16037 training step(s), loss on training batch is 0.000615283.
After 16038 training step(s), loss on training batch is 0.00046678.
After 16039 training step(s), loss on training batch is 0.00046349.
After 16040 training step(s), loss on training batch is 0.000965133.
After 16041 training step(s), loss on training batch is 0.000486272.
After 16042 training step(s), loss on training batch is 0.000554241.
After 16043 training step(s), loss on training batch is 0.000553538.
After 16044 training step(s), loss on training batch is 0.000627787.
After 16045 training step(s), loss on training batch is 0.00047373.
After 16046 training step(s), loss on training batch is 0.000588058.
After 16047 training step(s), loss on training batch is 0.000500645.
After 16048 training step(s), loss on training batch is 0.000538491.
After 16049 training step(s), loss on training batch is 0.000582751.
After 16050 training step(s), loss on training batch is 0.00056482.
After 16051 training step(s), loss on training batch is 0.000551526.
After 16052 training step(s), loss on training batch is 0.000510691.
After 16053 training step(s), loss on training batch is 0.000557368.
After 16054 training step(s), loss on training batch is 0.000420561.
After 16055 training step(s), loss on training batch is 0.000543548.
After 16056 training step(s), loss on training batch is 0.000975363.
After 16057 training step(s), loss on training batch is 0.000835966.
After 16058 training step(s), loss on training batch is 0.000892285.
After 16059 training step(s), loss on training batch is 0.000885591.
After 16060 training step(s), loss on training batch is 0.000763322.
After 16061 training step(s), loss on training batch is 0.000802845.
After 16062 training step(s), loss on training batch is 0.00097638.
After 16063 training step(s), loss on training batch is 0.000859424.
After 16064 training step(s), loss on training batch is 0.000781607.
After 16065 training step(s), loss on training batch is 0.00101492.
After 16066 training step(s), loss on training batch is 0.00075837.
After 16067 training step(s), loss on training batch is 0.00077854.
After 16068 training step(s), loss on training batch is 0.0009075.
After 16069 training step(s), loss on training batch is 0.00138904.
After 16070 training step(s), loss on training batch is 0.00313553.
After 16071 training step(s), loss on training batch is 0.00135496.
After 16072 training step(s), loss on training batch is 0.000899001.
After 16073 training step(s), loss on training batch is 0.00109336.
After 16074 training step(s), loss on training batch is 0.000914978.
After 16075 training step(s), loss on training batch is 0.000898897.
After 16076 training step(s), loss on training batch is 0.000816037.
After 16077 training step(s), loss on training batch is 0.00117707.
After 16078 training step(s), loss on training batch is 0.000832994.
After 16079 training step(s), loss on training batch is 0.000786485.
After 16080 training step(s), loss on training batch is 0.00077268.
After 16081 training step(s), loss on training batch is 0.000614576.
After 16082 training step(s), loss on training batch is 0.000385227.
After 16083 training step(s), loss on training batch is 0.000268149.
After 16084 training step(s), loss on training batch is 0.000326382.
After 16085 training step(s), loss on training batch is 0.000341716.
After 16086 training step(s), loss on training batch is 0.000478372.
After 16087 training step(s), loss on training batch is 0.00057068.
After 16088 training step(s), loss on training batch is 0.000392754.
After 16089 training step(s), loss on training batch is 0.000346428.
After 16090 training step(s), loss on training batch is 0.000309746.
After 16091 training step(s), loss on training batch is 0.000261972.
After 16092 training step(s), loss on training batch is 0.0003036.
After 16093 training step(s), loss on training batch is 0.000300451.
After 16094 training step(s), loss on training batch is 0.000298783.
After 16095 training step(s), loss on training batch is 0.000329704.
After 16096 training step(s), loss on training batch is 0.000338052.
After 16097 training step(s), loss on training batch is 0.000274798.
After 16098 training step(s), loss on training batch is 0.000257881.
After 16099 training step(s), loss on training batch is 0.000272979.
After 16100 training step(s), loss on training batch is 0.000272017.
After 16101 training step(s), loss on training batch is 0.000628377.
After 16102 training step(s), loss on training batch is 0.000444716.
After 16103 training step(s), loss on training batch is 0.000493225.
After 16104 training step(s), loss on training batch is 0.000586373.
After 16105 training step(s), loss on training batch is 0.000555438.
After 16106 training step(s), loss on training batch is 0.000559213.
After 16107 training step(s), loss on training batch is 0.00053042.
After 16108 training step(s), loss on training batch is 0.000312654.
After 16109 training step(s), loss on training batch is 0.000349174.
After 16110 training step(s), loss on training batch is 0.000361934.
After 16111 training step(s), loss on training batch is 0.000423067.
After 16112 training step(s), loss on training batch is 0.000429247.
After 16113 training step(s), loss on training batch is 0.000435693.
After 16114 training step(s), loss on training batch is 0.00059719.
After 16115 training step(s), loss on training batch is 0.00127926.
After 16116 training step(s), loss on training batch is 0.000910709.
After 16117 training step(s), loss on training batch is 0.000738023.
After 16118 training step(s), loss on training batch is 0.00057666.
After 16119 training step(s), loss on training batch is 0.00036819.
After 16120 training step(s), loss on training batch is 0.000355775.
After 16121 training step(s), loss on training batch is 0.00036534.
After 16122 training step(s), loss on training batch is 0.000327974.
After 16123 training step(s), loss on training batch is 0.000506666.
After 16124 training step(s), loss on training batch is 0.000385485.
After 16125 training step(s), loss on training batch is 0.000375015.
After 16126 training step(s), loss on training batch is 0.000370697.
After 16127 training step(s), loss on training batch is 0.000683217.
After 16128 training step(s), loss on training batch is 0.00169455.
After 16129 training step(s), loss on training batch is 0.000631855.
After 16130 training step(s), loss on training batch is 0.000483864.
After 16131 training step(s), loss on training batch is 0.00052739.
After 16132 training step(s), loss on training batch is 0.00047688.
After 16133 training step(s), loss on training batch is 0.000417281.
After 16134 training step(s), loss on training batch is 0.000328281.
After 16135 training step(s), loss on training batch is 0.00030421.
After 16136 training step(s), loss on training batch is 0.000305238.
After 16137 training step(s), loss on training batch is 0.000342619.
After 16138 training step(s), loss on training batch is 0.000342478.
After 16139 training step(s), loss on training batch is 0.000383837.
After 16140 training step(s), loss on training batch is 0.000436162.
After 16141 training step(s), loss on training batch is 0.000349643.
After 16142 training step(s), loss on training batch is 0.000427698.
After 16143 training step(s), loss on training batch is 0.00041856.
After 16144 training step(s), loss on training batch is 0.000468178.
After 16145 training step(s), loss on training batch is 0.000319994.
After 16146 training step(s), loss on training batch is 0.000424301.
After 16147 training step(s), loss on training batch is 0.000336126.
After 16148 training step(s), loss on training batch is 0.00031457.
After 16149 training step(s), loss on training batch is 0.000354674.
After 16150 training step(s), loss on training batch is 0.000466598.
After 16151 training step(s), loss on training batch is 0.000421637.
After 16152 training step(s), loss on training batch is 0.000374159.
After 16153 training step(s), loss on training batch is 0.000381483.
After 16154 training step(s), loss on training batch is 0.000365363.
After 16155 training step(s), loss on training batch is 0.000336223.
After 16156 training step(s), loss on training batch is 0.000439757.
After 16157 training step(s), loss on training batch is 0.000531159.
After 16158 training step(s), loss on training batch is 0.000338501.
After 16159 training step(s), loss on training batch is 0.000372905.
After 16160 training step(s), loss on training batch is 0.000482666.
After 16161 training step(s), loss on training batch is 0.000328956.
After 16162 training step(s), loss on training batch is 0.000302743.
After 16163 training step(s), loss on training batch is 0.000311813.
After 16164 training step(s), loss on training batch is 0.000433241.
After 16165 training step(s), loss on training batch is 0.000387602.
After 16166 training step(s), loss on training batch is 0.000383136.
After 16167 training step(s), loss on training batch is 0.000378819.
After 16168 training step(s), loss on training batch is 0.00035162.
After 16169 training step(s), loss on training batch is 0.000410404.
After 16170 training step(s), loss on training batch is 0.000335584.
After 16171 training step(s), loss on training batch is 0.000339768.
After 16172 training step(s), loss on training batch is 0.000334999.
After 16173 training step(s), loss on training batch is 0.000316525.
After 16174 training step(s), loss on training batch is 0.000295882.
After 16175 training step(s), loss on training batch is 0.000360534.
After 16176 training step(s), loss on training batch is 0.000351902.
After 16177 training step(s), loss on training batch is 0.000455237.
After 16178 training step(s), loss on training batch is 0.000353552.
After 16179 training step(s), loss on training batch is 0.000326727.
After 16180 training step(s), loss on training batch is 0.000346572.
After 16181 training step(s), loss on training batch is 0.000334606.
After 16182 training step(s), loss on training batch is 0.00034756.
After 16183 training step(s), loss on training batch is 0.000610502.
After 16184 training step(s), loss on training batch is 0.000744769.
After 16185 training step(s), loss on training batch is 0.000846654.
After 16186 training step(s), loss on training batch is 0.000583293.
After 16187 training step(s), loss on training batch is 0.000665687.
After 16188 training step(s), loss on training batch is 0.000555025.
After 16189 training step(s), loss on training batch is 0.000572635.
After 16190 training step(s), loss on training batch is 0.000552073.
After 16191 training step(s), loss on training batch is 0.000648042.
After 16192 training step(s), loss on training batch is 0.000673063.
After 16193 training step(s), loss on training batch is 0.000548542.
After 16194 training step(s), loss on training batch is 0.0005329.
After 16195 training step(s), loss on training batch is 0.000659358.
After 16196 training step(s), loss on training batch is 0.000635442.
After 16197 training step(s), loss on training batch is 0.000579673.
After 16198 training step(s), loss on training batch is 0.000608847.
After 16199 training step(s), loss on training batch is 0.000777288.
After 16200 training step(s), loss on training batch is 0.000636009.
After 16201 training step(s), loss on training batch is 0.000553619.
After 16202 training step(s), loss on training batch is 0.00064869.
After 16203 training step(s), loss on training batch is 0.000559641.
After 16204 training step(s), loss on training batch is 0.000549975.
After 16205 training step(s), loss on training batch is 0.000477725.
After 16206 training step(s), loss on training batch is 0.000616908.
After 16207 training step(s), loss on training batch is 0.00074622.
After 16208 training step(s), loss on training batch is 0.000630268.
After 16209 training step(s), loss on training batch is 0.000501823.
After 16210 training step(s), loss on training batch is 0.00072696.
After 16211 training step(s), loss on training batch is 0.000594341.
After 16212 training step(s), loss on training batch is 0.000573748.
After 16213 training step(s), loss on training batch is 0.000477736.
After 16214 training step(s), loss on training batch is 0.000490305.
After 16215 training step(s), loss on training batch is 0.000453064.
After 16216 training step(s), loss on training batch is 0.000481452.
After 16217 training step(s), loss on training batch is 0.000488925.
After 16218 training step(s), loss on training batch is 0.000707944.
After 16219 training step(s), loss on training batch is 0.00094135.
After 16220 training step(s), loss on training batch is 0.000929736.
After 16221 training step(s), loss on training batch is 0.00104679.
After 16222 training step(s), loss on training batch is 0.000566385.
After 16223 training step(s), loss on training batch is 0.000557626.
After 16224 training step(s), loss on training batch is 0.000581167.
After 16225 training step(s), loss on training batch is 0.000620777.
After 16226 training step(s), loss on training batch is 0.000661448.
After 16227 training step(s), loss on training batch is 0.000565833.
After 16228 training step(s), loss on training batch is 0.000601402.
After 16229 training step(s), loss on training batch is 0.000618673.
After 16230 training step(s), loss on training batch is 0.000670774.
After 16231 training step(s), loss on training batch is 0.000730976.
After 16232 training step(s), loss on training batch is 0.000527281.
After 16233 training step(s), loss on training batch is 0.000555834.
After 16234 training step(s), loss on training batch is 0.000507881.
After 16235 training step(s), loss on training batch is 0.000471681.
After 16236 training step(s), loss on training batch is 0.000688681.
After 16237 training step(s), loss on training batch is 0.000999428.
After 16238 training step(s), loss on training batch is 0.000468152.
After 16239 training step(s), loss on training batch is 0.000546249.
After 16240 training step(s), loss on training batch is 0.000572422.
After 16241 training step(s), loss on training batch is 0.000511661.
After 16242 training step(s), loss on training batch is 0.000501537.
After 16243 training step(s), loss on training batch is 0.000570459.
After 16244 training step(s), loss on training batch is 0.000530475.
After 16245 training step(s), loss on training batch is 0.000569197.
After 16246 training step(s), loss on training batch is 0.000725966.
After 16247 training step(s), loss on training batch is 0.000545413.
After 16248 training step(s), loss on training batch is 0.000547512.
After 16249 training step(s), loss on training batch is 0.000593097.
After 16250 training step(s), loss on training batch is 0.000599971.
After 16251 training step(s), loss on training batch is 0.000519527.
After 16252 training step(s), loss on training batch is 0.000471311.
After 16253 training step(s), loss on training batch is 0.000643306.
After 16254 training step(s), loss on training batch is 0.000557994.
After 16255 training step(s), loss on training batch is 0.000609927.
After 16256 training step(s), loss on training batch is 0.000560255.
After 16257 training step(s), loss on training batch is 0.000492633.
After 16258 training step(s), loss on training batch is 0.000619179.
After 16259 training step(s), loss on training batch is 0.000774996.
After 16260 training step(s), loss on training batch is 0.000523771.
After 16261 training step(s), loss on training batch is 0.000468705.
After 16262 training step(s), loss on training batch is 0.000474661.
After 16263 training step(s), loss on training batch is 0.000499154.
After 16264 training step(s), loss on training batch is 0.000455507.
After 16265 training step(s), loss on training batch is 0.000616244.
After 16266 training step(s), loss on training batch is 0.000970796.
After 16267 training step(s), loss on training batch is 0.00101506.
After 16268 training step(s), loss on training batch is 0.000930754.
After 16269 training step(s), loss on training batch is 0.000958174.
After 16270 training step(s), loss on training batch is 0.000926339.
After 16271 training step(s), loss on training batch is 0.000878094.
After 16272 training step(s), loss on training batch is 0.000917288.
After 16273 training step(s), loss on training batch is 0.00117494.
After 16274 training step(s), loss on training batch is 0.00109165.
After 16275 training step(s), loss on training batch is 0.00111757.
After 16276 training step(s), loss on training batch is 0.000880183.
After 16277 training step(s), loss on training batch is 0.00087236.
After 16278 training step(s), loss on training batch is 0.000931255.
After 16279 training step(s), loss on training batch is 0.00083566.
After 16280 training step(s), loss on training batch is 0.00083815.
After 16281 training step(s), loss on training batch is 0.000806178.
After 16282 training step(s), loss on training batch is 0.000846495.
After 16283 training step(s), loss on training batch is 0.000938002.
After 16284 training step(s), loss on training batch is 0.000829313.
After 16285 training step(s), loss on training batch is 0.000831499.
After 16286 training step(s), loss on training batch is 0.00107236.
After 16287 training step(s), loss on training batch is 0.000838962.
After 16288 training step(s), loss on training batch is 0.000993431.
After 16289 training step(s), loss on training batch is 0.000856.
After 16290 training step(s), loss on training batch is 0.000845379.
After 16291 training step(s), loss on training batch is 0.000861827.
After 16292 training step(s), loss on training batch is 0.000849353.
After 16293 training step(s), loss on training batch is 0.000795894.
After 16294 training step(s), loss on training batch is 0.000927013.
After 16295 training step(s), loss on training batch is 0.000932948.
After 16296 training step(s), loss on training batch is 0.0010362.
After 16297 training step(s), loss on training batch is 0.000859617.
After 16298 training step(s), loss on training batch is 0.000857368.
After 16299 training step(s), loss on training batch is 0.000800017.
After 16300 training step(s), loss on training batch is 0.000860282.
After 16301 training step(s), loss on training batch is 0.000765111.
After 16302 training step(s), loss on training batch is 0.00093706.
After 16303 training step(s), loss on training batch is 0.000838943.
After 16304 training step(s), loss on training batch is 0.0011472.
After 16305 training step(s), loss on training batch is 0.00113929.
After 16306 training step(s), loss on training batch is 0.00109996.
After 16307 training step(s), loss on training batch is 0.00128842.
After 16308 training step(s), loss on training batch is 0.00102837.
After 16309 training step(s), loss on training batch is 0.00724733.
After 16310 training step(s), loss on training batch is 0.00179856.
After 16311 training step(s), loss on training batch is 0.00165617.
After 16312 training step(s), loss on training batch is 0.00155609.
After 16313 training step(s), loss on training batch is 0.00144911.
After 16314 training step(s), loss on training batch is 0.00134775.
After 16315 training step(s), loss on training batch is 0.0011696.
After 16316 training step(s), loss on training batch is 0.000556073.
After 16317 training step(s), loss on training batch is 0.000496081.
After 16318 training step(s), loss on training batch is 0.000467508.
After 16319 training step(s), loss on training batch is 0.000482718.
After 16320 training step(s), loss on training batch is 0.000499029.
After 16321 training step(s), loss on training batch is 0.000515628.
After 16322 training step(s), loss on training batch is 0.000532013.
After 16323 training step(s), loss on training batch is 0.000516212.
After 16324 training step(s), loss on training batch is 0.000482561.
After 16325 training step(s), loss on training batch is 0.000432815.
After 16326 training step(s), loss on training batch is 0.000895055.
After 16327 training step(s), loss on training batch is 0.000917697.
After 16328 training step(s), loss on training batch is 0.000992453.
After 16329 training step(s), loss on training batch is 0.00085146.
After 16330 training step(s), loss on training batch is 0.00150279.
After 16331 training step(s), loss on training batch is 0.00130984.
After 16332 training step(s), loss on training batch is 0.00102403.
After 16333 training step(s), loss on training batch is 0.000994736.
After 16334 training step(s), loss on training batch is 0.000899521.
After 16335 training step(s), loss on training batch is 0.000851862.
After 16336 training step(s), loss on training batch is 0.000869212.
After 16337 training step(s), loss on training batch is 0.000879969.
After 16338 training step(s), loss on training batch is 0.00101955.
After 16339 training step(s), loss on training batch is 0.000877041.
After 16340 training step(s), loss on training batch is 0.000858933.
After 16341 training step(s), loss on training batch is 0.000855152.
After 16342 training step(s), loss on training batch is 0.00154643.
After 16343 training step(s), loss on training batch is 0.000329147.
After 16344 training step(s), loss on training batch is 0.000365923.
After 16345 training step(s), loss on training batch is 0.000345314.
After 16346 training step(s), loss on training batch is 0.000429972.
After 16347 training step(s), loss on training batch is 0.000387028.
After 16348 training step(s), loss on training batch is 0.000384898.
After 16349 training step(s), loss on training batch is 0.000325001.
After 16350 training step(s), loss on training batch is 0.000300715.
After 16351 training step(s), loss on training batch is 0.000493033.
After 16352 training step(s), loss on training batch is 0.000389741.
After 16353 training step(s), loss on training batch is 0.000390064.
After 16354 training step(s), loss on training batch is 0.00034829.
After 16355 training step(s), loss on training batch is 0.000487073.
After 16356 training step(s), loss on training batch is 0.000366999.
After 16357 training step(s), loss on training batch is 0.00031409.
After 16358 training step(s), loss on training batch is 0.000312865.
After 16359 training step(s), loss on training batch is 0.000656442.
After 16360 training step(s), loss on training batch is 0.000833601.
After 16361 training step(s), loss on training batch is 0.000467691.
After 16362 training step(s), loss on training batch is 0.000377209.
After 16363 training step(s), loss on training batch is 0.000341242.
After 16364 training step(s), loss on training batch is 0.000417575.
After 16365 training step(s), loss on training batch is 0.000435181.
After 16366 training step(s), loss on training batch is 0.000387703.
After 16367 training step(s), loss on training batch is 0.000345384.
After 16368 training step(s), loss on training batch is 0.000496851.
After 16369 training step(s), loss on training batch is 0.000301352.
After 16370 training step(s), loss on training batch is 0.000375465.
After 16371 training step(s), loss on training batch is 0.000260302.
After 16372 training step(s), loss on training batch is 0.000270788.
After 16373 training step(s), loss on training batch is 0.000304292.
After 16374 training step(s), loss on training batch is 0.000305013.
After 16375 training step(s), loss on training batch is 0.000433631.
After 16376 training step(s), loss on training batch is 0.000371271.
After 16377 training step(s), loss on training batch is 0.000379741.
After 16378 training step(s), loss on training batch is 0.000305566.
After 16379 training step(s), loss on training batch is 0.000332137.
After 16380 training step(s), loss on training batch is 0.000298234.
After 16381 training step(s), loss on training batch is 0.000335928.
After 16382 training step(s), loss on training batch is 0.000307314.
After 16383 training step(s), loss on training batch is 0.000372493.
After 16384 training step(s), loss on training batch is 0.000361404.
After 16385 training step(s), loss on training batch is 0.000388379.
After 16386 training step(s), loss on training batch is 0.000495848.
After 16387 training step(s), loss on training batch is 0.000380119.
After 16388 training step(s), loss on training batch is 0.000311025.
After 16389 training step(s), loss on training batch is 0.000357999.
After 16390 training step(s), loss on training batch is 0.000330281.
After 16391 training step(s), loss on training batch is 0.000300396.
After 16392 training step(s), loss on training batch is 0.000308236.
After 16393 training step(s), loss on training batch is 0.000311082.
After 16394 training step(s), loss on training batch is 0.000260652.
After 16395 training step(s), loss on training batch is 0.000421749.
After 16396 training step(s), loss on training batch is 0.000416785.
After 16397 training step(s), loss on training batch is 0.000237649.
After 16398 training step(s), loss on training batch is 0.000254152.
After 16399 training step(s), loss on training batch is 0.000310574.
After 16400 training step(s), loss on training batch is 0.000327064.
After 16401 training step(s), loss on training batch is 0.000487751.
After 16402 training step(s), loss on training batch is 0.000561905.
After 16403 training step(s), loss on training batch is 0.00035439.
After 16404 training step(s), loss on training batch is 0.000497161.
After 16405 training step(s), loss on training batch is 0.00088629.
After 16406 training step(s), loss on training batch is 0.000631685.
After 16407 training step(s), loss on training batch is 0.000364188.
After 16408 training step(s), loss on training batch is 0.000445021.
After 16409 training step(s), loss on training batch is 0.00052761.
After 16410 training step(s), loss on training batch is 0.000550035.
After 16411 training step(s), loss on training batch is 0.000371219.
After 16412 training step(s), loss on training batch is 0.000300397.
After 16413 training step(s), loss on training batch is 0.000362397.
After 16414 training step(s), loss on training batch is 0.000389194.
After 16415 training step(s), loss on training batch is 0.00036598.
After 16416 training step(s), loss on training batch is 0.000305564.
After 16417 training step(s), loss on training batch is 0.000379253.
After 16418 training step(s), loss on training batch is 0.000329889.
After 16419 training step(s), loss on training batch is 0.000397159.
After 16420 training step(s), loss on training batch is 0.000365529.
After 16421 training step(s), loss on training batch is 0.000286029.
After 16422 training step(s), loss on training batch is 0.000359453.
After 16423 training step(s), loss on training batch is 0.000347453.
After 16424 training step(s), loss on training batch is 0.000299131.
After 16425 training step(s), loss on training batch is 0.000293724.
After 16426 training step(s), loss on training batch is 0.00034758.
After 16427 training step(s), loss on training batch is 0.000287768.
After 16428 training step(s), loss on training batch is 0.000777376.
After 16429 training step(s), loss on training batch is 0.000561355.
After 16430 training step(s), loss on training batch is 0.000499855.
After 16431 training step(s), loss on training batch is 0.000509639.
After 16432 training step(s), loss on training batch is 0.00057671.
After 16433 training step(s), loss on training batch is 0.000648294.
After 16434 training step(s), loss on training batch is 0.000560726.
After 16435 training step(s), loss on training batch is 0.000449356.
After 16436 training step(s), loss on training batch is 0.000636722.
After 16437 training step(s), loss on training batch is 0.000606726.
After 16438 training step(s), loss on training batch is 0.000462845.
After 16439 training step(s), loss on training batch is 0.000458544.
After 16440 training step(s), loss on training batch is 0.000975049.
After 16441 training step(s), loss on training batch is 0.000473437.
After 16442 training step(s), loss on training batch is 0.00053573.
After 16443 training step(s), loss on training batch is 0.000545552.
After 16444 training step(s), loss on training batch is 0.000621002.
After 16445 training step(s), loss on training batch is 0.000467478.
After 16446 training step(s), loss on training batch is 0.000583471.
After 16447 training step(s), loss on training batch is 0.000496649.
After 16448 training step(s), loss on training batch is 0.000532254.
After 16449 training step(s), loss on training batch is 0.0005785.
After 16450 training step(s), loss on training batch is 0.000559568.
After 16451 training step(s), loss on training batch is 0.000540284.
After 16452 training step(s), loss on training batch is 0.000505567.
After 16453 training step(s), loss on training batch is 0.000552482.
After 16454 training step(s), loss on training batch is 0.000418229.
After 16455 training step(s), loss on training batch is 0.00053761.
After 16456 training step(s), loss on training batch is 0.000972232.
After 16457 training step(s), loss on training batch is 0.000832407.
After 16458 training step(s), loss on training batch is 0.000891149.
After 16459 training step(s), loss on training batch is 0.000888773.
After 16460 training step(s), loss on training batch is 0.000780222.
After 16461 training step(s), loss on training batch is 0.000808232.
After 16462 training step(s), loss on training batch is 0.000965803.
After 16463 training step(s), loss on training batch is 0.000855288.
After 16464 training step(s), loss on training batch is 0.000773156.
After 16465 training step(s), loss on training batch is 0.00101121.
After 16466 training step(s), loss on training batch is 0.000767574.
After 16467 training step(s), loss on training batch is 0.000789126.
After 16468 training step(s), loss on training batch is 0.000907213.
After 16469 training step(s), loss on training batch is 0.00137747.
After 16470 training step(s), loss on training batch is 0.00312818.
After 16471 training step(s), loss on training batch is 0.00136014.
After 16472 training step(s), loss on training batch is 0.000963597.
After 16473 training step(s), loss on training batch is 0.0011282.
After 16474 training step(s), loss on training batch is 0.000991585.
After 16475 training step(s), loss on training batch is 0.00094722.
After 16476 training step(s), loss on training batch is 0.000893501.
After 16477 training step(s), loss on training batch is 0.00113526.
After 16478 training step(s), loss on training batch is 0.000856588.
After 16479 training step(s), loss on training batch is 0.000841066.
After 16480 training step(s), loss on training batch is 0.000817982.
After 16481 training step(s), loss on training batch is 0.000519413.
After 16482 training step(s), loss on training batch is 0.000373424.
After 16483 training step(s), loss on training batch is 0.000273399.
After 16484 training step(s), loss on training batch is 0.000329024.
After 16485 training step(s), loss on training batch is 0.000344201.
After 16486 training step(s), loss on training batch is 0.000468351.
After 16487 training step(s), loss on training batch is 0.00055473.
After 16488 training step(s), loss on training batch is 0.000382759.
After 16489 training step(s), loss on training batch is 0.000339859.
After 16490 training step(s), loss on training batch is 0.000304468.
After 16491 training step(s), loss on training batch is 0.000256317.
After 16492 training step(s), loss on training batch is 0.000301037.
After 16493 training step(s), loss on training batch is 0.000296904.
After 16494 training step(s), loss on training batch is 0.000297436.
After 16495 training step(s), loss on training batch is 0.000328935.
After 16496 training step(s), loss on training batch is 0.000336996.
After 16497 training step(s), loss on training batch is 0.000274159.
After 16498 training step(s), loss on training batch is 0.000260135.
After 16499 training step(s), loss on training batch is 0.000273053.
After 16500 training step(s), loss on training batch is 0.000267318.
After 16501 training step(s), loss on training batch is 0.000609408.
After 16502 training step(s), loss on training batch is 0.000441846.
After 16503 training step(s), loss on training batch is 0.000489973.
After 16504 training step(s), loss on training batch is 0.000576651.
After 16505 training step(s), loss on training batch is 0.000551082.
After 16506 training step(s), loss on training batch is 0.000558362.
After 16507 training step(s), loss on training batch is 0.000525904.
After 16508 training step(s), loss on training batch is 0.000309436.
After 16509 training step(s), loss on training batch is 0.000345812.
After 16510 training step(s), loss on training batch is 0.00036551.
After 16511 training step(s), loss on training batch is 0.000428287.
After 16512 training step(s), loss on training batch is 0.000430873.
After 16513 training step(s), loss on training batch is 0.000434597.
After 16514 training step(s), loss on training batch is 0.000531702.
After 16515 training step(s), loss on training batch is 0.00128012.
After 16516 training step(s), loss on training batch is 0.00090495.
After 16517 training step(s), loss on training batch is 0.000730269.
After 16518 training step(s), loss on training batch is 0.000562971.
After 16519 training step(s), loss on training batch is 0.000366191.
After 16520 training step(s), loss on training batch is 0.000349361.
After 16521 training step(s), loss on training batch is 0.000358741.
After 16522 training step(s), loss on training batch is 0.000319936.
After 16523 training step(s), loss on training batch is 0.000481245.
After 16524 training step(s), loss on training batch is 0.000377827.
After 16525 training step(s), loss on training batch is 0.000366775.
After 16526 training step(s), loss on training batch is 0.000355761.
After 16527 training step(s), loss on training batch is 0.000713938.
After 16528 training step(s), loss on training batch is 0.00171249.
After 16529 training step(s), loss on training batch is 0.000610493.
After 16530 training step(s), loss on training batch is 0.000472818.
After 16531 training step(s), loss on training batch is 0.000525216.
After 16532 training step(s), loss on training batch is 0.000470313.
After 16533 training step(s), loss on training batch is 0.000401984.
After 16534 training step(s), loss on training batch is 0.000327227.
After 16535 training step(s), loss on training batch is 0.00029306.
After 16536 training step(s), loss on training batch is 0.000297583.
After 16537 training step(s), loss on training batch is 0.000334329.
After 16538 training step(s), loss on training batch is 0.000326543.
After 16539 training step(s), loss on training batch is 0.000371634.
After 16540 training step(s), loss on training batch is 0.00041499.
After 16541 training step(s), loss on training batch is 0.000336231.
After 16542 training step(s), loss on training batch is 0.000413693.
After 16543 training step(s), loss on training batch is 0.00040609.
After 16544 training step(s), loss on training batch is 0.000474007.
After 16545 training step(s), loss on training batch is 0.000319812.
After 16546 training step(s), loss on training batch is 0.000428672.
After 16547 training step(s), loss on training batch is 0.000335264.
After 16548 training step(s), loss on training batch is 0.000311795.
After 16549 training step(s), loss on training batch is 0.000352068.
After 16550 training step(s), loss on training batch is 0.000466663.
After 16551 training step(s), loss on training batch is 0.00043396.
After 16552 training step(s), loss on training batch is 0.000375182.
After 16553 training step(s), loss on training batch is 0.000380023.
After 16554 training step(s), loss on training batch is 0.000356418.
After 16555 training step(s), loss on training batch is 0.000333766.
After 16556 training step(s), loss on training batch is 0.000442771.
After 16557 training step(s), loss on training batch is 0.000535498.
After 16558 training step(s), loss on training batch is 0.000340964.
After 16559 training step(s), loss on training batch is 0.000368463.
After 16560 training step(s), loss on training batch is 0.000482905.
After 16561 training step(s), loss on training batch is 0.000328297.
After 16562 training step(s), loss on training batch is 0.000304329.
After 16563 training step(s), loss on training batch is 0.000313093.
After 16564 training step(s), loss on training batch is 0.00043146.
After 16565 training step(s), loss on training batch is 0.00038658.
After 16566 training step(s), loss on training batch is 0.000384466.
After 16567 training step(s), loss on training batch is 0.000373623.
After 16568 training step(s), loss on training batch is 0.000353486.
After 16569 training step(s), loss on training batch is 0.000411655.
After 16570 training step(s), loss on training batch is 0.000337619.
After 16571 training step(s), loss on training batch is 0.000330593.
After 16572 training step(s), loss on training batch is 0.000326313.
After 16573 training step(s), loss on training batch is 0.000314939.
After 16574 training step(s), loss on training batch is 0.000295153.
After 16575 training step(s), loss on training batch is 0.000362428.
After 16576 training step(s), loss on training batch is 0.000356063.
After 16577 training step(s), loss on training batch is 0.00044753.
After 16578 training step(s), loss on training batch is 0.000354482.
After 16579 training step(s), loss on training batch is 0.000322638.
After 16580 training step(s), loss on training batch is 0.000342975.
After 16581 training step(s), loss on training batch is 0.00033208.
After 16582 training step(s), loss on training batch is 0.000348047.
After 16583 training step(s), loss on training batch is 0.000611954.
After 16584 training step(s), loss on training batch is 0.000722675.
After 16585 training step(s), loss on training batch is 0.000844704.
After 16586 training step(s), loss on training batch is 0.000577363.
After 16587 training step(s), loss on training batch is 0.000634817.
After 16588 training step(s), loss on training batch is 0.000551913.
After 16589 training step(s), loss on training batch is 0.000581909.
After 16590 training step(s), loss on training batch is 0.000556123.
After 16591 training step(s), loss on training batch is 0.000647651.
After 16592 training step(s), loss on training batch is 0.000669598.
After 16593 training step(s), loss on training batch is 0.000547056.
After 16594 training step(s), loss on training batch is 0.000532425.
After 16595 training step(s), loss on training batch is 0.000647431.
After 16596 training step(s), loss on training batch is 0.000627148.
After 16597 training step(s), loss on training batch is 0.000576837.
After 16598 training step(s), loss on training batch is 0.000607298.
After 16599 training step(s), loss on training batch is 0.000770595.
After 16600 training step(s), loss on training batch is 0.000625109.
After 16601 training step(s), loss on training batch is 0.000548744.
After 16602 training step(s), loss on training batch is 0.000640005.
After 16603 training step(s), loss on training batch is 0.000561544.
After 16604 training step(s), loss on training batch is 0.000554628.
After 16605 training step(s), loss on training batch is 0.000481868.
After 16606 training step(s), loss on training batch is 0.000618589.
After 16607 training step(s), loss on training batch is 0.000738231.
After 16608 training step(s), loss on training batch is 0.000625711.
After 16609 training step(s), loss on training batch is 0.000502213.
After 16610 training step(s), loss on training batch is 0.000716787.
After 16611 training step(s), loss on training batch is 0.000601436.
After 16612 training step(s), loss on training batch is 0.00056863.
After 16613 training step(s), loss on training batch is 0.000485138.
After 16614 training step(s), loss on training batch is 0.000494649.
After 16615 training step(s), loss on training batch is 0.00047291.
After 16616 training step(s), loss on training batch is 0.000494852.
After 16617 training step(s), loss on training batch is 0.000490939.
After 16618 training step(s), loss on training batch is 0.0006572.
After 16619 training step(s), loss on training batch is 0.000860148.
After 16620 training step(s), loss on training batch is 0.000909205.
After 16621 training step(s), loss on training batch is 0.000967473.
After 16622 training step(s), loss on training batch is 0.000574012.
After 16623 training step(s), loss on training batch is 0.000564911.
After 16624 training step(s), loss on training batch is 0.000585979.
After 16625 training step(s), loss on training batch is 0.0006269.
After 16626 training step(s), loss on training batch is 0.000665131.
After 16627 training step(s), loss on training batch is 0.000561817.
After 16628 training step(s), loss on training batch is 0.000596206.
After 16629 training step(s), loss on training batch is 0.000609218.
After 16630 training step(s), loss on training batch is 0.000666362.
After 16631 training step(s), loss on training batch is 0.000731596.
After 16632 training step(s), loss on training batch is 0.000526525.
After 16633 training step(s), loss on training batch is 0.000558586.
After 16634 training step(s), loss on training batch is 0.000514848.
After 16635 training step(s), loss on training batch is 0.000477661.
After 16636 training step(s), loss on training batch is 0.000676726.
After 16637 training step(s), loss on training batch is 0.00100988.
After 16638 training step(s), loss on training batch is 0.000464279.
After 16639 training step(s), loss on training batch is 0.000542774.
After 16640 training step(s), loss on training batch is 0.000570919.
After 16641 training step(s), loss on training batch is 0.000530632.
After 16642 training step(s), loss on training batch is 0.000515468.
After 16643 training step(s), loss on training batch is 0.000565201.
After 16644 training step(s), loss on training batch is 0.000524835.
After 16645 training step(s), loss on training batch is 0.000560589.
After 16646 training step(s), loss on training batch is 0.000765927.
After 16647 training step(s), loss on training batch is 0.000529155.
After 16648 training step(s), loss on training batch is 0.000562303.
After 16649 training step(s), loss on training batch is 0.000590693.
After 16650 training step(s), loss on training batch is 0.00059983.
After 16651 training step(s), loss on training batch is 0.000523611.
After 16652 training step(s), loss on training batch is 0.000472827.
After 16653 training step(s), loss on training batch is 0.000628322.
After 16654 training step(s), loss on training batch is 0.000552028.
After 16655 training step(s), loss on training batch is 0.000605613.
After 16656 training step(s), loss on training batch is 0.000551769.
After 16657 training step(s), loss on training batch is 0.000492039.
After 16658 training step(s), loss on training batch is 0.000614267.
After 16659 training step(s), loss on training batch is 0.00077682.
After 16660 training step(s), loss on training batch is 0.000517367.
After 16661 training step(s), loss on training batch is 0.000472929.
After 16662 training step(s), loss on training batch is 0.000477622.
After 16663 training step(s), loss on training batch is 0.000499966.
After 16664 training step(s), loss on training batch is 0.000458104.
After 16665 training step(s), loss on training batch is 0.000604258.
After 16666 training step(s), loss on training batch is 0.000974555.
After 16667 training step(s), loss on training batch is 0.00101295.
After 16668 training step(s), loss on training batch is 0.000928384.
After 16669 training step(s), loss on training batch is 0.000966774.
After 16670 training step(s), loss on training batch is 0.000924652.
After 16671 training step(s), loss on training batch is 0.000883316.
After 16672 training step(s), loss on training batch is 0.000915508.
After 16673 training step(s), loss on training batch is 0.00115621.
After 16674 training step(s), loss on training batch is 0.0010797.
After 16675 training step(s), loss on training batch is 0.00111413.
After 16676 training step(s), loss on training batch is 0.00091316.
After 16677 training step(s), loss on training batch is 0.000900741.
After 16678 training step(s), loss on training batch is 0.000929553.
After 16679 training step(s), loss on training batch is 0.00081754.
After 16680 training step(s), loss on training batch is 0.000833433.
After 16681 training step(s), loss on training batch is 0.00079541.
After 16682 training step(s), loss on training batch is 0.000814774.
After 16683 training step(s), loss on training batch is 0.00096067.
After 16684 training step(s), loss on training batch is 0.000834896.
After 16685 training step(s), loss on training batch is 0.000818622.
After 16686 training step(s), loss on training batch is 0.00110911.
After 16687 training step(s), loss on training batch is 0.000818588.
After 16688 training step(s), loss on training batch is 0.000997198.
After 16689 training step(s), loss on training batch is 0.000844961.
After 16690 training step(s), loss on training batch is 0.000837257.
After 16691 training step(s), loss on training batch is 0.000858806.
After 16692 training step(s), loss on training batch is 0.000847884.
After 16693 training step(s), loss on training batch is 0.000794259.
After 16694 training step(s), loss on training batch is 0.000930951.
After 16695 training step(s), loss on training batch is 0.000929915.
After 16696 training step(s), loss on training batch is 0.00103053.
After 16697 training step(s), loss on training batch is 0.000865317.
After 16698 training step(s), loss on training batch is 0.000859915.
After 16699 training step(s), loss on training batch is 0.000813837.
After 16700 training step(s), loss on training batch is 0.000870428.
After 16701 training step(s), loss on training batch is 0.000770899.
After 16702 training step(s), loss on training batch is 0.000935184.
After 16703 training step(s), loss on training batch is 0.000844253.
After 16704 training step(s), loss on training batch is 0.00101569.
After 16705 training step(s), loss on training batch is 0.00107366.
After 16706 training step(s), loss on training batch is 0.00103305.
After 16707 training step(s), loss on training batch is 0.00121368.
After 16708 training step(s), loss on training batch is 0.0010283.
After 16709 training step(s), loss on training batch is 0.00733158.
After 16710 training step(s), loss on training batch is 0.00197343.
After 16711 training step(s), loss on training batch is 0.00181876.
After 16712 training step(s), loss on training batch is 0.0016881.
After 16713 training step(s), loss on training batch is 0.00151509.
After 16714 training step(s), loss on training batch is 0.00138194.
After 16715 training step(s), loss on training batch is 0.00113007.
After 16716 training step(s), loss on training batch is 0.00054421.
After 16717 training step(s), loss on training batch is 0.000486448.
After 16718 training step(s), loss on training batch is 0.000448259.
After 16719 training step(s), loss on training batch is 0.000468813.
After 16720 training step(s), loss on training batch is 0.000489031.
After 16721 training step(s), loss on training batch is 0.00050092.
After 16722 training step(s), loss on training batch is 0.000519903.
After 16723 training step(s), loss on training batch is 0.000514709.
After 16724 training step(s), loss on training batch is 0.000481076.
After 16725 training step(s), loss on training batch is 0.000431416.
After 16726 training step(s), loss on training batch is 0.000896361.
After 16727 training step(s), loss on training batch is 0.000917133.
After 16728 training step(s), loss on training batch is 0.000990881.
After 16729 training step(s), loss on training batch is 0.000862538.
After 16730 training step(s), loss on training batch is 0.00146966.
After 16731 training step(s), loss on training batch is 0.00130432.
After 16732 training step(s), loss on training batch is 0.00102923.
After 16733 training step(s), loss on training batch is 0.000992681.
After 16734 training step(s), loss on training batch is 0.000905517.
After 16735 training step(s), loss on training batch is 0.000854305.
After 16736 training step(s), loss on training batch is 0.000891695.
After 16737 training step(s), loss on training batch is 0.000870896.
After 16738 training step(s), loss on training batch is 0.00100229.
After 16739 training step(s), loss on training batch is 0.000872176.
After 16740 training step(s), loss on training batch is 0.000858585.
After 16741 training step(s), loss on training batch is 0.000854847.
After 16742 training step(s), loss on training batch is 0.00157204.
After 16743 training step(s), loss on training batch is 0.000327697.
After 16744 training step(s), loss on training batch is 0.000380511.
After 16745 training step(s), loss on training batch is 0.000345608.
After 16746 training step(s), loss on training batch is 0.000427672.
After 16747 training step(s), loss on training batch is 0.000382755.
After 16748 training step(s), loss on training batch is 0.000391413.
After 16749 training step(s), loss on training batch is 0.000322195.
After 16750 training step(s), loss on training batch is 0.000302602.
After 16751 training step(s), loss on training batch is 0.000491039.
After 16752 training step(s), loss on training batch is 0.000388934.
After 16753 training step(s), loss on training batch is 0.000389619.
After 16754 training step(s), loss on training batch is 0.000347399.
After 16755 training step(s), loss on training batch is 0.000486024.
After 16756 training step(s), loss on training batch is 0.000366628.
After 16757 training step(s), loss on training batch is 0.000315894.
After 16758 training step(s), loss on training batch is 0.000310542.
After 16759 training step(s), loss on training batch is 0.000653455.
After 16760 training step(s), loss on training batch is 0.00082584.
After 16761 training step(s), loss on training batch is 0.000468809.
After 16762 training step(s), loss on training batch is 0.000374949.
After 16763 training step(s), loss on training batch is 0.000341498.
After 16764 training step(s), loss on training batch is 0.000414602.
After 16765 training step(s), loss on training batch is 0.000429467.
After 16766 training step(s), loss on training batch is 0.000387474.
After 16767 training step(s), loss on training batch is 0.00034073.
After 16768 training step(s), loss on training batch is 0.000491422.
After 16769 training step(s), loss on training batch is 0.000297723.
After 16770 training step(s), loss on training batch is 0.000375544.
After 16771 training step(s), loss on training batch is 0.000258852.
After 16772 training step(s), loss on training batch is 0.000265236.
After 16773 training step(s), loss on training batch is 0.000305733.
After 16774 training step(s), loss on training batch is 0.000307874.
After 16775 training step(s), loss on training batch is 0.000410538.
After 16776 training step(s), loss on training batch is 0.000363846.
After 16777 training step(s), loss on training batch is 0.000378769.
After 16778 training step(s), loss on training batch is 0.000304388.
After 16779 training step(s), loss on training batch is 0.000324045.
After 16780 training step(s), loss on training batch is 0.000299879.
After 16781 training step(s), loss on training batch is 0.000336761.
After 16782 training step(s), loss on training batch is 0.000306984.
After 16783 training step(s), loss on training batch is 0.000371385.
After 16784 training step(s), loss on training batch is 0.000362692.
After 16785 training step(s), loss on training batch is 0.000387104.
After 16786 training step(s), loss on training batch is 0.000487505.
After 16787 training step(s), loss on training batch is 0.000379977.
After 16788 training step(s), loss on training batch is 0.000310654.
After 16789 training step(s), loss on training batch is 0.000355342.
After 16790 training step(s), loss on training batch is 0.000327057.
After 16791 training step(s), loss on training batch is 0.00029915.
After 16792 training step(s), loss on training batch is 0.000301833.
After 16793 training step(s), loss on training batch is 0.000309884.
After 16794 training step(s), loss on training batch is 0.000258444.
After 16795 training step(s), loss on training batch is 0.000423413.
After 16796 training step(s), loss on training batch is 0.000419898.
After 16797 training step(s), loss on training batch is 0.000236659.
After 16798 training step(s), loss on training batch is 0.000254178.
After 16799 training step(s), loss on training batch is 0.00030902.
After 16800 training step(s), loss on training batch is 0.000322616.
After 16801 training step(s), loss on training batch is 0.000484106.
After 16802 training step(s), loss on training batch is 0.000560676.
After 16803 training step(s), loss on training batch is 0.000352505.
After 16804 training step(s), loss on training batch is 0.000496619.
After 16805 training step(s), loss on training batch is 0.000881099.
After 16806 training step(s), loss on training batch is 0.000632292.
After 16807 training step(s), loss on training batch is 0.000356866.
After 16808 training step(s), loss on training batch is 0.000437605.
After 16809 training step(s), loss on training batch is 0.000527153.
After 16810 training step(s), loss on training batch is 0.000549126.
After 16811 training step(s), loss on training batch is 0.000361269.
After 16812 training step(s), loss on training batch is 0.000291121.
After 16813 training step(s), loss on training batch is 0.000348331.
After 16814 training step(s), loss on training batch is 0.000372908.
After 16815 training step(s), loss on training batch is 0.000342391.
After 16816 training step(s), loss on training batch is 0.000303871.
After 16817 training step(s), loss on training batch is 0.000378849.
After 16818 training step(s), loss on training batch is 0.000332497.
After 16819 training step(s), loss on training batch is 0.000402224.
After 16820 training step(s), loss on training batch is 0.000365164.
After 16821 training step(s), loss on training batch is 0.000286515.
After 16822 training step(s), loss on training batch is 0.000362538.
After 16823 training step(s), loss on training batch is 0.000349369.
After 16824 training step(s), loss on training batch is 0.000296802.
After 16825 training step(s), loss on training batch is 0.000294802.
After 16826 training step(s), loss on training batch is 0.000342523.
After 16827 training step(s), loss on training batch is 0.000288904.
After 16828 training step(s), loss on training batch is 0.000753967.
After 16829 training step(s), loss on training batch is 0.000543409.
After 16830 training step(s), loss on training batch is 0.000499451.
After 16831 training step(s), loss on training batch is 0.00050477.
After 16832 training step(s), loss on training batch is 0.000575451.
After 16833 training step(s), loss on training batch is 0.000662301.
After 16834 training step(s), loss on training batch is 0.000569659.
After 16835 training step(s), loss on training batch is 0.000441391.
After 16836 training step(s), loss on training batch is 0.000667135.
After 16837 training step(s), loss on training batch is 0.000626183.
After 16838 training step(s), loss on training batch is 0.000464845.
After 16839 training step(s), loss on training batch is 0.000454249.
After 16840 training step(s), loss on training batch is 0.000999024.
After 16841 training step(s), loss on training batch is 0.000460787.
After 16842 training step(s), loss on training batch is 0.000529177.
After 16843 training step(s), loss on training batch is 0.00053831.
After 16844 training step(s), loss on training batch is 0.000622936.
After 16845 training step(s), loss on training batch is 0.000457868.
After 16846 training step(s), loss on training batch is 0.000590264.
After 16847 training step(s), loss on training batch is 0.000482472.
After 16848 training step(s), loss on training batch is 0.000504171.
After 16849 training step(s), loss on training batch is 0.000598921.
After 16850 training step(s), loss on training batch is 0.000567207.
After 16851 training step(s), loss on training batch is 0.000583272.
After 16852 training step(s), loss on training batch is 0.000527886.
After 16853 training step(s), loss on training batch is 0.000573646.
After 16854 training step(s), loss on training batch is 0.000402226.
After 16855 training step(s), loss on training batch is 0.000531705.
After 16856 training step(s), loss on training batch is 0.000951125.
After 16857 training step(s), loss on training batch is 0.000811006.
After 16858 training step(s), loss on training batch is 0.000873838.
After 16859 training step(s), loss on training batch is 0.000876878.
After 16860 training step(s), loss on training batch is 0.000770478.
After 16861 training step(s), loss on training batch is 0.000805787.
After 16862 training step(s), loss on training batch is 0.000963974.
After 16863 training step(s), loss on training batch is 0.000839757.
After 16864 training step(s), loss on training batch is 0.00076498.
After 16865 training step(s), loss on training batch is 0.000998012.
After 16866 training step(s), loss on training batch is 0.000762144.
After 16867 training step(s), loss on training batch is 0.000769479.
After 16868 training step(s), loss on training batch is 0.000897925.
After 16869 training step(s), loss on training batch is 0.00137248.
After 16870 training step(s), loss on training batch is 0.00309922.
After 16871 training step(s), loss on training batch is 0.00134358.
After 16872 training step(s), loss on training batch is 0.000954018.
After 16873 training step(s), loss on training batch is 0.00112258.
After 16874 training step(s), loss on training batch is 0.000998109.
After 16875 training step(s), loss on training batch is 0.000946671.
After 16876 training step(s), loss on training batch is 0.000873899.
After 16877 training step(s), loss on training batch is 0.00112099.
After 16878 training step(s), loss on training batch is 0.00084344.
After 16879 training step(s), loss on training batch is 0.000833327.
After 16880 training step(s), loss on training batch is 0.000797014.
After 16881 training step(s), loss on training batch is 0.000525871.
After 16882 training step(s), loss on training batch is 0.00037151.
After 16883 training step(s), loss on training batch is 0.000268095.
After 16884 training step(s), loss on training batch is 0.000327932.
After 16885 training step(s), loss on training batch is 0.00033959.
After 16886 training step(s), loss on training batch is 0.000473188.
After 16887 training step(s), loss on training batch is 0.000551175.
After 16888 training step(s), loss on training batch is 0.00037543.
After 16889 training step(s), loss on training batch is 0.000335865.
After 16890 training step(s), loss on training batch is 0.00030068.
After 16891 training step(s), loss on training batch is 0.000242242.
After 16892 training step(s), loss on training batch is 0.000310836.
After 16893 training step(s), loss on training batch is 0.000289612.
After 16894 training step(s), loss on training batch is 0.00027927.
After 16895 training step(s), loss on training batch is 0.000316326.
After 16896 training step(s), loss on training batch is 0.000328762.
After 16897 training step(s), loss on training batch is 0.000261402.
After 16898 training step(s), loss on training batch is 0.00024411.
After 16899 training step(s), loss on training batch is 0.000266178.
After 16900 training step(s), loss on training batch is 0.000274422.
After 16901 training step(s), loss on training batch is 0.000665178.
After 16902 training step(s), loss on training batch is 0.000444103.
After 16903 training step(s), loss on training batch is 0.000483621.
After 16904 training step(s), loss on training batch is 0.000569855.
After 16905 training step(s), loss on training batch is 0.000545416.
After 16906 training step(s), loss on training batch is 0.000559577.
After 16907 training step(s), loss on training batch is 0.00052881.
After 16908 training step(s), loss on training batch is 0.00031172.
After 16909 training step(s), loss on training batch is 0.000347556.
After 16910 training step(s), loss on training batch is 0.000369246.
After 16911 training step(s), loss on training batch is 0.000426775.
After 16912 training step(s), loss on training batch is 0.000436467.
After 16913 training step(s), loss on training batch is 0.000444441.
After 16914 training step(s), loss on training batch is 0.000540967.
After 16915 training step(s), loss on training batch is 0.00127506.
After 16916 training step(s), loss on training batch is 0.000916235.
After 16917 training step(s), loss on training batch is 0.00073904.
After 16918 training step(s), loss on training batch is 0.000567073.
After 16919 training step(s), loss on training batch is 0.000371915.
After 16920 training step(s), loss on training batch is 0.000344585.
After 16921 training step(s), loss on training batch is 0.000356209.
After 16922 training step(s), loss on training batch is 0.000322294.
After 16923 training step(s), loss on training batch is 0.000491541.
After 16924 training step(s), loss on training batch is 0.000384757.
After 16925 training step(s), loss on training batch is 0.000370766.
After 16926 training step(s), loss on training batch is 0.000363509.
After 16927 training step(s), loss on training batch is 0.000706477.
After 16928 training step(s), loss on training batch is 0.00167249.
After 16929 training step(s), loss on training batch is 0.000612178.
After 16930 training step(s), loss on training batch is 0.000476204.
After 16931 training step(s), loss on training batch is 0.000534925.
After 16932 training step(s), loss on training batch is 0.000471213.
After 16933 training step(s), loss on training batch is 0.000408479.
After 16934 training step(s), loss on training batch is 0.000325424.
After 16935 training step(s), loss on training batch is 0.000294989.
After 16936 training step(s), loss on training batch is 0.000297811.
After 16937 training step(s), loss on training batch is 0.000334869.
After 16938 training step(s), loss on training batch is 0.000327946.
After 16939 training step(s), loss on training batch is 0.000372013.
After 16940 training step(s), loss on training batch is 0.000412411.
After 16941 training step(s), loss on training batch is 0.000334504.
After 16942 training step(s), loss on training batch is 0.000412541.
After 16943 training step(s), loss on training batch is 0.000405344.
After 16944 training step(s), loss on training batch is 0.000467397.
After 16945 training step(s), loss on training batch is 0.000319976.
After 16946 training step(s), loss on training batch is 0.000428192.
After 16947 training step(s), loss on training batch is 0.000335108.
After 16948 training step(s), loss on training batch is 0.000311408.
After 16949 training step(s), loss on training batch is 0.000353321.
After 16950 training step(s), loss on training batch is 0.000461857.
After 16951 training step(s), loss on training batch is 0.000430676.
After 16952 training step(s), loss on training batch is 0.000375151.
After 16953 training step(s), loss on training batch is 0.000382928.
After 16954 training step(s), loss on training batch is 0.000362454.
After 16955 training step(s), loss on training batch is 0.000335555.
After 16956 training step(s), loss on training batch is 0.000439827.
After 16957 training step(s), loss on training batch is 0.000534432.
After 16958 training step(s), loss on training batch is 0.000340069.
After 16959 training step(s), loss on training batch is 0.000371717.
After 16960 training step(s), loss on training batch is 0.000481153.
After 16961 training step(s), loss on training batch is 0.000331753.
After 16962 training step(s), loss on training batch is 0.000305355.
After 16963 training step(s), loss on training batch is 0.000312683.
After 16964 training step(s), loss on training batch is 0.000428383.
After 16965 training step(s), loss on training batch is 0.000384682.
After 16966 training step(s), loss on training batch is 0.000383512.
After 16967 training step(s), loss on training batch is 0.000374603.
After 16968 training step(s), loss on training batch is 0.000359106.
After 16969 training step(s), loss on training batch is 0.000408686.
After 16970 training step(s), loss on training batch is 0.000336355.
After 16971 training step(s), loss on training batch is 0.000333423.
After 16972 training step(s), loss on training batch is 0.000336673.
After 16973 training step(s), loss on training batch is 0.000319841.
After 16974 training step(s), loss on training batch is 0.000296949.
After 16975 training step(s), loss on training batch is 0.000360348.
After 16976 training step(s), loss on training batch is 0.000346441.
After 16977 training step(s), loss on training batch is 0.000457195.
After 16978 training step(s), loss on training batch is 0.000351382.
After 16979 training step(s), loss on training batch is 0.000321049.
After 16980 training step(s), loss on training batch is 0.000343699.
After 16981 training step(s), loss on training batch is 0.000333132.
After 16982 training step(s), loss on training batch is 0.000345126.
After 16983 training step(s), loss on training batch is 0.000611018.
After 16984 training step(s), loss on training batch is 0.000720408.
After 16985 training step(s), loss on training batch is 0.000823639.
After 16986 training step(s), loss on training batch is 0.000573373.
After 16987 training step(s), loss on training batch is 0.000633293.
After 16988 training step(s), loss on training batch is 0.000552994.
After 16989 training step(s), loss on training batch is 0.000581018.
After 16990 training step(s), loss on training batch is 0.000554203.
After 16991 training step(s), loss on training batch is 0.000649666.
After 16992 training step(s), loss on training batch is 0.000655909.
After 16993 training step(s), loss on training batch is 0.000550234.
After 16994 training step(s), loss on training batch is 0.000513849.
After 16995 training step(s), loss on training batch is 0.000668068.
After 16996 training step(s), loss on training batch is 0.000625932.
After 16997 training step(s), loss on training batch is 0.000569362.
After 16998 training step(s), loss on training batch is 0.000604106.
After 16999 training step(s), loss on training batch is 0.000779363.
After 17000 training step(s), loss on training batch is 0.000631598.
After 17001 training step(s), loss on training batch is 0.000547944.
After 17002 training step(s), loss on training batch is 0.000648377.
After 17003 training step(s), loss on training batch is 0.000556751.
After 17004 training step(s), loss on training batch is 0.000549984.
After 17005 training step(s), loss on training batch is 0.000469641.
After 17006 training step(s), loss on training batch is 0.000617988.
After 17007 training step(s), loss on training batch is 0.000749169.
After 17008 training step(s), loss on training batch is 0.000649558.
After 17009 training step(s), loss on training batch is 0.000499256.
After 17010 training step(s), loss on training batch is 0.000739233.
After 17011 training step(s), loss on training batch is 0.00058352.
After 17012 training step(s), loss on training batch is 0.000568537.
After 17013 training step(s), loss on training batch is 0.000471222.
After 17014 training step(s), loss on training batch is 0.000488002.
After 17015 training step(s), loss on training batch is 0.000461399.
After 17016 training step(s), loss on training batch is 0.000483711.
After 17017 training step(s), loss on training batch is 0.000491736.
After 17018 training step(s), loss on training batch is 0.000652778.
After 17019 training step(s), loss on training batch is 0.000853532.
After 17020 training step(s), loss on training batch is 0.000932874.
After 17021 training step(s), loss on training batch is 0.000974528.
After 17022 training step(s), loss on training batch is 0.000552541.
After 17023 training step(s), loss on training batch is 0.000542814.
After 17024 training step(s), loss on training batch is 0.000566206.
After 17025 training step(s), loss on training batch is 0.000600034.
After 17026 training step(s), loss on training batch is 0.000643586.
After 17027 training step(s), loss on training batch is 0.000550819.
After 17028 training step(s), loss on training batch is 0.000588048.
After 17029 training step(s), loss on training batch is 0.000581492.
After 17030 training step(s), loss on training batch is 0.000669675.
After 17031 training step(s), loss on training batch is 0.000749528.
After 17032 training step(s), loss on training batch is 0.00051734.
After 17033 training step(s), loss on training batch is 0.000549872.
After 17034 training step(s), loss on training batch is 0.000506678.
After 17035 training step(s), loss on training batch is 0.000468655.
After 17036 training step(s), loss on training batch is 0.00069081.
After 17037 training step(s), loss on training batch is 0.00102621.
After 17038 training step(s), loss on training batch is 0.000466812.
After 17039 training step(s), loss on training batch is 0.000544951.
After 17040 training step(s), loss on training batch is 0.000565413.
After 17041 training step(s), loss on training batch is 0.000523323.
After 17042 training step(s), loss on training batch is 0.000509683.
After 17043 training step(s), loss on training batch is 0.000578239.
After 17044 training step(s), loss on training batch is 0.000534417.
After 17045 training step(s), loss on training batch is 0.000566722.
After 17046 training step(s), loss on training batch is 0.000712175.
After 17047 training step(s), loss on training batch is 0.000547812.
After 17048 training step(s), loss on training batch is 0.00054165.
After 17049 training step(s), loss on training batch is 0.000587233.
After 17050 training step(s), loss on training batch is 0.000591215.
After 17051 training step(s), loss on training batch is 0.000510091.
After 17052 training step(s), loss on training batch is 0.000471313.
After 17053 training step(s), loss on training batch is 0.000615955.
After 17054 training step(s), loss on training batch is 0.000548064.
After 17055 training step(s), loss on training batch is 0.000600065.
After 17056 training step(s), loss on training batch is 0.000553371.
After 17057 training step(s), loss on training batch is 0.000494354.
After 17058 training step(s), loss on training batch is 0.000608184.
After 17059 training step(s), loss on training batch is 0.000770197.
After 17060 training step(s), loss on training batch is 0.000511192.
After 17061 training step(s), loss on training batch is 0.000470652.
After 17062 training step(s), loss on training batch is 0.000476245.
After 17063 training step(s), loss on training batch is 0.000495407.
After 17064 training step(s), loss on training batch is 0.000457366.
After 17065 training step(s), loss on training batch is 0.000602492.
After 17066 training step(s), loss on training batch is 0.000972424.
After 17067 training step(s), loss on training batch is 0.00101173.
After 17068 training step(s), loss on training batch is 0.000932214.
After 17069 training step(s), loss on training batch is 0.000969124.
After 17070 training step(s), loss on training batch is 0.000924693.
After 17071 training step(s), loss on training batch is 0.000877451.
After 17072 training step(s), loss on training batch is 0.000912804.
After 17073 training step(s), loss on training batch is 0.00115726.
After 17074 training step(s), loss on training batch is 0.00107631.
After 17075 training step(s), loss on training batch is 0.001112.
After 17076 training step(s), loss on training batch is 0.000908238.
After 17077 training step(s), loss on training batch is 0.000889365.
After 17078 training step(s), loss on training batch is 0.00092588.
After 17079 training step(s), loss on training batch is 0.000838561.
After 17080 training step(s), loss on training batch is 0.000839266.
After 17081 training step(s), loss on training batch is 0.000781686.
After 17082 training step(s), loss on training batch is 0.000813564.
After 17083 training step(s), loss on training batch is 0.000949905.
After 17084 training step(s), loss on training batch is 0.000823427.
After 17085 training step(s), loss on training batch is 0.000825772.
After 17086 training step(s), loss on training batch is 0.00106812.
After 17087 training step(s), loss on training batch is 0.000833468.
After 17088 training step(s), loss on training batch is 0.000983755.
After 17089 training step(s), loss on training batch is 0.000853089.
After 17090 training step(s), loss on training batch is 0.000822717.
After 17091 training step(s), loss on training batch is 0.000849429.
After 17092 training step(s), loss on training batch is 0.000842667.
After 17093 training step(s), loss on training batch is 0.000790373.
After 17094 training step(s), loss on training batch is 0.000920998.
After 17095 training step(s), loss on training batch is 0.000919568.
After 17096 training step(s), loss on training batch is 0.00102395.
After 17097 training step(s), loss on training batch is 0.000868582.
After 17098 training step(s), loss on training batch is 0.000859997.
After 17099 training step(s), loss on training batch is 0.000778196.
After 17100 training step(s), loss on training batch is 0.000841561.
After 17101 training step(s), loss on training batch is 0.000760259.
After 17102 training step(s), loss on training batch is 0.00094076.
After 17103 training step(s), loss on training batch is 0.000829328.
After 17104 training step(s), loss on training batch is 0.001013.
After 17105 training step(s), loss on training batch is 0.00104573.
After 17106 training step(s), loss on training batch is 0.00101515.
After 17107 training step(s), loss on training batch is 0.00121289.
After 17108 training step(s), loss on training batch is 0.00102246.
After 17109 training step(s), loss on training batch is 0.00675443.
After 17110 training step(s), loss on training batch is 0.00168861.
After 17111 training step(s), loss on training batch is 0.0015947.
After 17112 training step(s), loss on training batch is 0.00152157.
After 17113 training step(s), loss on training batch is 0.00137917.
After 17114 training step(s), loss on training batch is 0.00130392.
After 17115 training step(s), loss on training batch is 0.00114226.
After 17116 training step(s), loss on training batch is 0.000543495.
After 17117 training step(s), loss on training batch is 0.000488596.
After 17118 training step(s), loss on training batch is 0.000459891.
After 17119 training step(s), loss on training batch is 0.000473796.
After 17120 training step(s), loss on training batch is 0.000491305.
After 17121 training step(s), loss on training batch is 0.000507009.
After 17122 training step(s), loss on training batch is 0.000521919.
After 17123 training step(s), loss on training batch is 0.000512528.
After 17124 training step(s), loss on training batch is 0.000481658.
After 17125 training step(s), loss on training batch is 0.000434017.
After 17126 training step(s), loss on training batch is 0.00089938.
After 17127 training step(s), loss on training batch is 0.000911539.
After 17128 training step(s), loss on training batch is 0.000987361.
After 17129 training step(s), loss on training batch is 0.000877197.
After 17130 training step(s), loss on training batch is 0.00141819.
After 17131 training step(s), loss on training batch is 0.00127234.
After 17132 training step(s), loss on training batch is 0.0010225.
After 17133 training step(s), loss on training batch is 0.000975787.
After 17134 training step(s), loss on training batch is 0.000896523.
After 17135 training step(s), loss on training batch is 0.000851183.
After 17136 training step(s), loss on training batch is 0.000879804.
After 17137 training step(s), loss on training batch is 0.000896583.
After 17138 training step(s), loss on training batch is 0.00101479.
After 17139 training step(s), loss on training batch is 0.000873791.
After 17140 training step(s), loss on training batch is 0.000867313.
After 17141 training step(s), loss on training batch is 0.000851994.
After 17142 training step(s), loss on training batch is 0.00151455.
After 17143 training step(s), loss on training batch is 0.000312905.
After 17144 training step(s), loss on training batch is 0.00036242.
After 17145 training step(s), loss on training batch is 0.000331105.
After 17146 training step(s), loss on training batch is 0.000419659.
After 17147 training step(s), loss on training batch is 0.000383515.
After 17148 training step(s), loss on training batch is 0.000367143.
After 17149 training step(s), loss on training batch is 0.000321267.
After 17150 training step(s), loss on training batch is 0.000300096.
After 17151 training step(s), loss on training batch is 0.000468032.
After 17152 training step(s), loss on training batch is 0.000388984.
After 17153 training step(s), loss on training batch is 0.000379513.
After 17154 training step(s), loss on training batch is 0.000347438.
After 17155 training step(s), loss on training batch is 0.000467459.
After 17156 training step(s), loss on training batch is 0.000366035.
After 17157 training step(s), loss on training batch is 0.000310831.
After 17158 training step(s), loss on training batch is 0.000308466.
After 17159 training step(s), loss on training batch is 0.000631441.
After 17160 training step(s), loss on training batch is 0.000812586.
After 17161 training step(s), loss on training batch is 0.000461364.
After 17162 training step(s), loss on training batch is 0.000374752.
After 17163 training step(s), loss on training batch is 0.000335123.
After 17164 training step(s), loss on training batch is 0.000405598.
After 17165 training step(s), loss on training batch is 0.000429233.
After 17166 training step(s), loss on training batch is 0.000379494.
After 17167 training step(s), loss on training batch is 0.00034099.
After 17168 training step(s), loss on training batch is 0.000494771.
After 17169 training step(s), loss on training batch is 0.000297596.
After 17170 training step(s), loss on training batch is 0.000372058.
After 17171 training step(s), loss on training batch is 0.000261066.
After 17172 training step(s), loss on training batch is 0.000263861.
After 17173 training step(s), loss on training batch is 0.000305117.
After 17174 training step(s), loss on training batch is 0.000308446.
After 17175 training step(s), loss on training batch is 0.000421791.
After 17176 training step(s), loss on training batch is 0.000376142.
After 17177 training step(s), loss on training batch is 0.000369491.
After 17178 training step(s), loss on training batch is 0.00030103.
After 17179 training step(s), loss on training batch is 0.000327555.
After 17180 training step(s), loss on training batch is 0.000299256.
After 17181 training step(s), loss on training batch is 0.000330196.
After 17182 training step(s), loss on training batch is 0.000304313.
After 17183 training step(s), loss on training batch is 0.000373349.
After 17184 training step(s), loss on training batch is 0.000357974.
After 17185 training step(s), loss on training batch is 0.000379801.
After 17186 training step(s), loss on training batch is 0.000496069.
After 17187 training step(s), loss on training batch is 0.000379129.
After 17188 training step(s), loss on training batch is 0.000309389.
After 17189 training step(s), loss on training batch is 0.000353187.
After 17190 training step(s), loss on training batch is 0.000328225.
After 17191 training step(s), loss on training batch is 0.000299967.
After 17192 training step(s), loss on training batch is 0.000303787.
After 17193 training step(s), loss on training batch is 0.000309155.
After 17194 training step(s), loss on training batch is 0.000258226.
After 17195 training step(s), loss on training batch is 0.000407872.
After 17196 training step(s), loss on training batch is 0.000412483.
After 17197 training step(s), loss on training batch is 0.000234467.
After 17198 training step(s), loss on training batch is 0.00025336.
After 17199 training step(s), loss on training batch is 0.00030849.
After 17200 training step(s), loss on training batch is 0.000333849.
After 17201 training step(s), loss on training batch is 0.000470484.
After 17202 training step(s), loss on training batch is 0.00054874.
After 17203 training step(s), loss on training batch is 0.000343489.
After 17204 training step(s), loss on training batch is 0.000475763.
After 17205 training step(s), loss on training batch is 0.000872741.
After 17206 training step(s), loss on training batch is 0.000617054.
After 17207 training step(s), loss on training batch is 0.00035022.
After 17208 training step(s), loss on training batch is 0.000428012.
After 17209 training step(s), loss on training batch is 0.000517323.
After 17210 training step(s), loss on training batch is 0.000539951.
After 17211 training step(s), loss on training batch is 0.00036163.
After 17212 training step(s), loss on training batch is 0.000290849.
After 17213 training step(s), loss on training batch is 0.00034875.
After 17214 training step(s), loss on training batch is 0.00037181.
After 17215 training step(s), loss on training batch is 0.000352554.
After 17216 training step(s), loss on training batch is 0.000303859.
After 17217 training step(s), loss on training batch is 0.000386005.
After 17218 training step(s), loss on training batch is 0.000322593.
After 17219 training step(s), loss on training batch is 0.000406347.
After 17220 training step(s), loss on training batch is 0.000364631.
After 17221 training step(s), loss on training batch is 0.000281111.
After 17222 training step(s), loss on training batch is 0.000360498.
After 17223 training step(s), loss on training batch is 0.00033205.
After 17224 training step(s), loss on training batch is 0.000296046.
After 17225 training step(s), loss on training batch is 0.000295219.
After 17226 training step(s), loss on training batch is 0.000344082.
After 17227 training step(s), loss on training batch is 0.000285225.
After 17228 training step(s), loss on training batch is 0.000767277.
After 17229 training step(s), loss on training batch is 0.000549025.
After 17230 training step(s), loss on training batch is 0.00049667.
After 17231 training step(s), loss on training batch is 0.000502556.
After 17232 training step(s), loss on training batch is 0.000560299.
After 17233 training step(s), loss on training batch is 0.000637905.
After 17234 training step(s), loss on training batch is 0.000553791.
After 17235 training step(s), loss on training batch is 0.000441076.
After 17236 training step(s), loss on training batch is 0.000636736.
After 17237 training step(s), loss on training batch is 0.000603735.
After 17238 training step(s), loss on training batch is 0.000458816.
After 17239 training step(s), loss on training batch is 0.00045308.
After 17240 training step(s), loss on training batch is 0.00096419.
After 17241 training step(s), loss on training batch is 0.000468771.
After 17242 training step(s), loss on training batch is 0.000535217.
After 17243 training step(s), loss on training batch is 0.000539931.
After 17244 training step(s), loss on training batch is 0.000614676.
After 17245 training step(s), loss on training batch is 0.000460837.
After 17246 training step(s), loss on training batch is 0.000581352.
After 17247 training step(s), loss on training batch is 0.000488711.
After 17248 training step(s), loss on training batch is 0.00052246.
After 17249 training step(s), loss on training batch is 0.000571242.
After 17250 training step(s), loss on training batch is 0.000552951.
After 17251 training step(s), loss on training batch is 0.000546966.
After 17252 training step(s), loss on training batch is 0.000507131.
After 17253 training step(s), loss on training batch is 0.000552778.
After 17254 training step(s), loss on training batch is 0.00040783.
After 17255 training step(s), loss on training batch is 0.000528745.
After 17256 training step(s), loss on training batch is 0.000946197.
After 17257 training step(s), loss on training batch is 0.000809645.
After 17258 training step(s), loss on training batch is 0.000875555.
After 17259 training step(s), loss on training batch is 0.000869532.
After 17260 training step(s), loss on training batch is 0.000764095.
After 17261 training step(s), loss on training batch is 0.000792602.
After 17262 training step(s), loss on training batch is 0.000964253.
After 17263 training step(s), loss on training batch is 0.000841741.
After 17264 training step(s), loss on training batch is 0.000743963.
After 17265 training step(s), loss on training batch is 0.00102459.
After 17266 training step(s), loss on training batch is 0.000748746.
After 17267 training step(s), loss on training batch is 0.000772079.
After 17268 training step(s), loss on training batch is 0.000890265.
After 17269 training step(s), loss on training batch is 0.0014598.
After 17270 training step(s), loss on training batch is 0.00318663.
After 17271 training step(s), loss on training batch is 0.00133935.
After 17272 training step(s), loss on training batch is 0.000944007.
After 17273 training step(s), loss on training batch is 0.00111413.
After 17274 training step(s), loss on training batch is 0.000964921.
After 17275 training step(s), loss on training batch is 0.000921473.
After 17276 training step(s), loss on training batch is 0.000855641.
After 17277 training step(s), loss on training batch is 0.00113082.
After 17278 training step(s), loss on training batch is 0.000828028.
After 17279 training step(s), loss on training batch is 0.000804048.
After 17280 training step(s), loss on training batch is 0.000771495.
After 17281 training step(s), loss on training batch is 0.000585154.
After 17282 training step(s), loss on training batch is 0.000372368.
After 17283 training step(s), loss on training batch is 0.00026797.
After 17284 training step(s), loss on training batch is 0.000325946.
After 17285 training step(s), loss on training batch is 0.00033709.
After 17286 training step(s), loss on training batch is 0.000469275.
After 17287 training step(s), loss on training batch is 0.000552323.
After 17288 training step(s), loss on training batch is 0.000375035.
After 17289 training step(s), loss on training batch is 0.000339185.
After 17290 training step(s), loss on training batch is 0.000298344.
After 17291 training step(s), loss on training batch is 0.000251114.
After 17292 training step(s), loss on training batch is 0.000301735.
After 17293 training step(s), loss on training batch is 0.000293944.
After 17294 training step(s), loss on training batch is 0.000291258.
After 17295 training step(s), loss on training batch is 0.000321107.
After 17296 training step(s), loss on training batch is 0.00033398.
After 17297 training step(s), loss on training batch is 0.000269658.
After 17298 training step(s), loss on training batch is 0.000252938.
After 17299 training step(s), loss on training batch is 0.000268148.
After 17300 training step(s), loss on training batch is 0.000266443.
After 17301 training step(s), loss on training batch is 0.000602602.
After 17302 training step(s), loss on training batch is 0.00044042.
After 17303 training step(s), loss on training batch is 0.000480484.
After 17304 training step(s), loss on training batch is 0.000564433.
After 17305 training step(s), loss on training batch is 0.000541076.
After 17306 training step(s), loss on training batch is 0.000552171.
After 17307 training step(s), loss on training batch is 0.000523799.
After 17308 training step(s), loss on training batch is 0.000307308.
After 17309 training step(s), loss on training batch is 0.000343124.
After 17310 training step(s), loss on training batch is 0.000365671.
After 17311 training step(s), loss on training batch is 0.000428679.
After 17312 training step(s), loss on training batch is 0.000429008.
After 17313 training step(s), loss on training batch is 0.000433088.
After 17314 training step(s), loss on training batch is 0.000551192.
After 17315 training step(s), loss on training batch is 0.00125794.
After 17316 training step(s), loss on training batch is 0.000898262.
After 17317 training step(s), loss on training batch is 0.000727642.
After 17318 training step(s), loss on training batch is 0.000564726.
After 17319 training step(s), loss on training batch is 0.000368348.
After 17320 training step(s), loss on training batch is 0.000349002.
After 17321 training step(s), loss on training batch is 0.000360574.
After 17322 training step(s), loss on training batch is 0.000321028.
After 17323 training step(s), loss on training batch is 0.000497386.
After 17324 training step(s), loss on training batch is 0.000382621.
After 17325 training step(s), loss on training batch is 0.000370783.
After 17326 training step(s), loss on training batch is 0.000367398.
After 17327 training step(s), loss on training batch is 0.000664132.
After 17328 training step(s), loss on training batch is 0.00156956.
After 17329 training step(s), loss on training batch is 0.000601934.
After 17330 training step(s), loss on training batch is 0.000466427.
After 17331 training step(s), loss on training batch is 0.0005236.
After 17332 training step(s), loss on training batch is 0.000471337.
After 17333 training step(s), loss on training batch is 0.000406727.
After 17334 training step(s), loss on training batch is 0.000325791.
After 17335 training step(s), loss on training batch is 0.000296869.
After 17336 training step(s), loss on training batch is 0.000297125.
After 17337 training step(s), loss on training batch is 0.000338002.
After 17338 training step(s), loss on training batch is 0.000333239.
After 17339 training step(s), loss on training batch is 0.000376135.
After 17340 training step(s), loss on training batch is 0.000422301.
After 17341 training step(s), loss on training batch is 0.000339893.
After 17342 training step(s), loss on training batch is 0.000418649.
After 17343 training step(s), loss on training batch is 0.000404412.
After 17344 training step(s), loss on training batch is 0.000468949.
After 17345 training step(s), loss on training batch is 0.000322814.
After 17346 training step(s), loss on training batch is 0.000423711.
After 17347 training step(s), loss on training batch is 0.000333131.
After 17348 training step(s), loss on training batch is 0.000306252.
After 17349 training step(s), loss on training batch is 0.00035501.
After 17350 training step(s), loss on training batch is 0.000431109.
After 17351 training step(s), loss on training batch is 0.000415766.
After 17352 training step(s), loss on training batch is 0.000399348.
After 17353 training step(s), loss on training batch is 0.000389256.
After 17354 training step(s), loss on training batch is 0.000374734.
After 17355 training step(s), loss on training batch is 0.000337641.
After 17356 training step(s), loss on training batch is 0.000436808.
After 17357 training step(s), loss on training batch is 0.000526626.
After 17358 training step(s), loss on training batch is 0.000342551.
After 17359 training step(s), loss on training batch is 0.000377117.
After 17360 training step(s), loss on training batch is 0.000474239.
After 17361 training step(s), loss on training batch is 0.000331165.
After 17362 training step(s), loss on training batch is 0.000303389.
After 17363 training step(s), loss on training batch is 0.000311674.
After 17364 training step(s), loss on training batch is 0.000430568.
After 17365 training step(s), loss on training batch is 0.000387865.
After 17366 training step(s), loss on training batch is 0.000381315.
After 17367 training step(s), loss on training batch is 0.000372081.
After 17368 training step(s), loss on training batch is 0.000356459.
After 17369 training step(s), loss on training batch is 0.000408401.
After 17370 training step(s), loss on training batch is 0.000337582.
After 17371 training step(s), loss on training batch is 0.000329423.
After 17372 training step(s), loss on training batch is 0.000324982.
After 17373 training step(s), loss on training batch is 0.000313777.
After 17374 training step(s), loss on training batch is 0.000294277.
After 17375 training step(s), loss on training batch is 0.000363053.
After 17376 training step(s), loss on training batch is 0.000353108.
After 17377 training step(s), loss on training batch is 0.000443662.
After 17378 training step(s), loss on training batch is 0.000350179.
After 17379 training step(s), loss on training batch is 0.000321197.
After 17380 training step(s), loss on training batch is 0.000342481.
After 17381 training step(s), loss on training batch is 0.000332486.
After 17382 training step(s), loss on training batch is 0.000344642.
After 17383 training step(s), loss on training batch is 0.000593419.
After 17384 training step(s), loss on training batch is 0.000703228.
After 17385 training step(s), loss on training batch is 0.000803212.
After 17386 training step(s), loss on training batch is 0.000574115.
After 17387 training step(s), loss on training batch is 0.000623552.
After 17388 training step(s), loss on training batch is 0.000546103.
After 17389 training step(s), loss on training batch is 0.000574985.
After 17390 training step(s), loss on training batch is 0.000549032.
After 17391 training step(s), loss on training batch is 0.000644616.
After 17392 training step(s), loss on training batch is 0.000652782.
After 17393 training step(s), loss on training batch is 0.000544154.
After 17394 training step(s), loss on training batch is 0.000525648.
After 17395 training step(s), loss on training batch is 0.000646298.
After 17396 training step(s), loss on training batch is 0.000623031.
After 17397 training step(s), loss on training batch is 0.000571598.
After 17398 training step(s), loss on training batch is 0.000601233.
After 17399 training step(s), loss on training batch is 0.000763504.
After 17400 training step(s), loss on training batch is 0.000622933.
After 17401 training step(s), loss on training batch is 0.000545893.
After 17402 training step(s), loss on training batch is 0.000644431.
After 17403 training step(s), loss on training batch is 0.000551343.
After 17404 training step(s), loss on training batch is 0.000544525.
After 17405 training step(s), loss on training batch is 0.000470564.
After 17406 training step(s), loss on training batch is 0.000611846.
After 17407 training step(s), loss on training batch is 0.000737876.
After 17408 training step(s), loss on training batch is 0.000636669.
After 17409 training step(s), loss on training batch is 0.000492442.
After 17410 training step(s), loss on training batch is 0.000740096.
After 17411 training step(s), loss on training batch is 0.000572662.
After 17412 training step(s), loss on training batch is 0.000574364.
After 17413 training step(s), loss on training batch is 0.000466824.
After 17414 training step(s), loss on training batch is 0.000482305.
After 17415 training step(s), loss on training batch is 0.000455204.
After 17416 training step(s), loss on training batch is 0.000479863.
After 17417 training step(s), loss on training batch is 0.000485147.
After 17418 training step(s), loss on training batch is 0.000653501.
After 17419 training step(s), loss on training batch is 0.000927482.
After 17420 training step(s), loss on training batch is 0.000910344.
After 17421 training step(s), loss on training batch is 0.00102452.
After 17422 training step(s), loss on training batch is 0.0005555.
After 17423 training step(s), loss on training batch is 0.000545392.
After 17424 training step(s), loss on training batch is 0.00056414.
After 17425 training step(s), loss on training batch is 0.000608315.
After 17426 training step(s), loss on training batch is 0.00065323.
After 17427 training step(s), loss on training batch is 0.000554.
After 17428 training step(s), loss on training batch is 0.000589215.
After 17429 training step(s), loss on training batch is 0.000606563.
After 17430 training step(s), loss on training batch is 0.000662199.
After 17431 training step(s), loss on training batch is 0.000742022.
After 17432 training step(s), loss on training batch is 0.000509267.
After 17433 training step(s), loss on training batch is 0.000543535.
After 17434 training step(s), loss on training batch is 0.000485486.
After 17435 training step(s), loss on training batch is 0.000452538.
After 17436 training step(s), loss on training batch is 0.000685863.
After 17437 training step(s), loss on training batch is 0.00108388.
After 17438 training step(s), loss on training batch is 0.000465956.
After 17439 training step(s), loss on training batch is 0.00054143.
After 17440 training step(s), loss on training batch is 0.000560231.
After 17441 training step(s), loss on training batch is 0.000512724.
After 17442 training step(s), loss on training batch is 0.000500283.
After 17443 training step(s), loss on training batch is 0.000565795.
After 17444 training step(s), loss on training batch is 0.000527691.
After 17445 training step(s), loss on training batch is 0.000560781.
After 17446 training step(s), loss on training batch is 0.000715259.
After 17447 training step(s), loss on training batch is 0.000540112.
After 17448 training step(s), loss on training batch is 0.000543242.
After 17449 training step(s), loss on training batch is 0.000584832.
After 17450 training step(s), loss on training batch is 0.000590166.
After 17451 training step(s), loss on training batch is 0.000502296.
After 17452 training step(s), loss on training batch is 0.000468709.
After 17453 training step(s), loss on training batch is 0.000612744.
After 17454 training step(s), loss on training batch is 0.000543691.
After 17455 training step(s), loss on training batch is 0.000595597.
After 17456 training step(s), loss on training batch is 0.00054861.
After 17457 training step(s), loss on training batch is 0.000494193.
After 17458 training step(s), loss on training batch is 0.000604839.
After 17459 training step(s), loss on training batch is 0.000765018.
After 17460 training step(s), loss on training batch is 0.00051465.
After 17461 training step(s), loss on training batch is 0.000463666.
After 17462 training step(s), loss on training batch is 0.000469678.
After 17463 training step(s), loss on training batch is 0.00048711.
After 17464 training step(s), loss on training batch is 0.00045162.
After 17465 training step(s), loss on training batch is 0.000602187.
After 17466 training step(s), loss on training batch is 0.000954843.
After 17467 training step(s), loss on training batch is 0.00099968.
After 17468 training step(s), loss on training batch is 0.000916393.
After 17469 training step(s), loss on training batch is 0.00094384.
After 17470 training step(s), loss on training batch is 0.000913932.
After 17471 training step(s), loss on training batch is 0.000864967.
After 17472 training step(s), loss on training batch is 0.000907412.
After 17473 training step(s), loss on training batch is 0.00115049.
After 17474 training step(s), loss on training batch is 0.00108377.
After 17475 training step(s), loss on training batch is 0.00109791.
After 17476 training step(s), loss on training batch is 0.000879629.
After 17477 training step(s), loss on training batch is 0.000875289.
After 17478 training step(s), loss on training batch is 0.000924619.
After 17479 training step(s), loss on training batch is 0.000839852.
After 17480 training step(s), loss on training batch is 0.000830617.
After 17481 training step(s), loss on training batch is 0.000793706.
After 17482 training step(s), loss on training batch is 0.000837377.
After 17483 training step(s), loss on training batch is 0.000911653.
After 17484 training step(s), loss on training batch is 0.000820161.
After 17485 training step(s), loss on training batch is 0.000831798.
After 17486 training step(s), loss on training batch is 0.00104672.
After 17487 training step(s), loss on training batch is 0.000835764.
After 17488 training step(s), loss on training batch is 0.000981371.
After 17489 training step(s), loss on training batch is 0.000847098.
After 17490 training step(s), loss on training batch is 0.000837319.
After 17491 training step(s), loss on training batch is 0.000857701.
After 17492 training step(s), loss on training batch is 0.000839836.
After 17493 training step(s), loss on training batch is 0.000790256.
After 17494 training step(s), loss on training batch is 0.000921254.
After 17495 training step(s), loss on training batch is 0.000921742.
After 17496 training step(s), loss on training batch is 0.00102273.
After 17497 training step(s), loss on training batch is 0.000848302.
After 17498 training step(s), loss on training batch is 0.000847682.
After 17499 training step(s), loss on training batch is 0.000794765.
After 17500 training step(s), loss on training batch is 0.000856303.
After 17501 training step(s), loss on training batch is 0.000756528.
After 17502 training step(s), loss on training batch is 0.000946714.
After 17503 training step(s), loss on training batch is 0.000822016.
After 17504 training step(s), loss on training batch is 0.00101582.
After 17505 training step(s), loss on training batch is 0.00107112.
After 17506 training step(s), loss on training batch is 0.00102451.
After 17507 training step(s), loss on training batch is 0.0011958.
After 17508 training step(s), loss on training batch is 0.00102793.
After 17509 training step(s), loss on training batch is 0.00713084.
After 17510 training step(s), loss on training batch is 0.0143186.
After 17511 training step(s), loss on training batch is 0.00329331.
After 17512 training step(s), loss on training batch is 0.00270332.
After 17513 training step(s), loss on training batch is 0.00209811.
After 17514 training step(s), loss on training batch is 0.00187153.
After 17515 training step(s), loss on training batch is 0.00149961.
After 17516 training step(s), loss on training batch is 0.000810315.
After 17517 training step(s), loss on training batch is 0.000688213.
After 17518 training step(s), loss on training batch is 0.000595671.
After 17519 training step(s), loss on training batch is 0.000628423.
After 17520 training step(s), loss on training batch is 0.000643435.
After 17521 training step(s), loss on training batch is 0.000668942.
After 17522 training step(s), loss on training batch is 0.000715938.
After 17523 training step(s), loss on training batch is 0.000701732.
After 17524 training step(s), loss on training batch is 0.000613799.
After 17525 training step(s), loss on training batch is 0.000538324.
After 17526 training step(s), loss on training batch is 0.00109409.
After 17527 training step(s), loss on training batch is 0.00110629.
After 17528 training step(s), loss on training batch is 0.0011553.
After 17529 training step(s), loss on training batch is 0.00103465.
After 17530 training step(s), loss on training batch is 0.0015356.
After 17531 training step(s), loss on training batch is 0.00139801.
After 17532 training step(s), loss on training batch is 0.00114572.
After 17533 training step(s), loss on training batch is 0.00108892.
After 17534 training step(s), loss on training batch is 0.00103095.
After 17535 training step(s), loss on training batch is 0.000960886.
After 17536 training step(s), loss on training batch is 0.00102089.
After 17537 training step(s), loss on training batch is 0.00100117.
After 17538 training step(s), loss on training batch is 0.00107472.
After 17539 training step(s), loss on training batch is 0.000973812.
After 17540 training step(s), loss on training batch is 0.000965221.
After 17541 training step(s), loss on training batch is 0.000968249.
After 17542 training step(s), loss on training batch is 0.00146924.
After 17543 training step(s), loss on training batch is 0.000340103.
After 17544 training step(s), loss on training batch is 0.000388315.
After 17545 training step(s), loss on training batch is 0.000407046.
After 17546 training step(s), loss on training batch is 0.000473188.
After 17547 training step(s), loss on training batch is 0.000467581.
After 17548 training step(s), loss on training batch is 0.000453142.
After 17549 training step(s), loss on training batch is 0.000382436.
After 17550 training step(s), loss on training batch is 0.00034429.
After 17551 training step(s), loss on training batch is 0.000520339.
After 17552 training step(s), loss on training batch is 0.000452225.
After 17553 training step(s), loss on training batch is 0.000436543.
After 17554 training step(s), loss on training batch is 0.000379503.
After 17555 training step(s), loss on training batch is 0.000546301.
After 17556 training step(s), loss on training batch is 0.000391034.
After 17557 training step(s), loss on training batch is 0.000361978.
After 17558 training step(s), loss on training batch is 0.00035056.
After 17559 training step(s), loss on training batch is 0.000701838.
After 17560 training step(s), loss on training batch is 0.000838283.
After 17561 training step(s), loss on training batch is 0.000514054.
After 17562 training step(s), loss on training batch is 0.000398746.
After 17563 training step(s), loss on training batch is 0.00037386.
After 17564 training step(s), loss on training batch is 0.000441594.
After 17565 training step(s), loss on training batch is 0.000441457.
After 17566 training step(s), loss on training batch is 0.000414028.
After 17567 training step(s), loss on training batch is 0.000350621.
After 17568 training step(s), loss on training batch is 0.000537825.
After 17569 training step(s), loss on training batch is 0.000321022.
After 17570 training step(s), loss on training batch is 0.000411441.
After 17571 training step(s), loss on training batch is 0.000265092.
After 17572 training step(s), loss on training batch is 0.000276293.
After 17573 training step(s), loss on training batch is 0.000326522.
After 17574 training step(s), loss on training batch is 0.000321379.
After 17575 training step(s), loss on training batch is 0.000433278.
After 17576 training step(s), loss on training batch is 0.000376287.
After 17577 training step(s), loss on training batch is 0.000400109.
After 17578 training step(s), loss on training batch is 0.000317063.
After 17579 training step(s), loss on training batch is 0.000323198.
After 17580 training step(s), loss on training batch is 0.000313969.
After 17581 training step(s), loss on training batch is 0.000371472.
After 17582 training step(s), loss on training batch is 0.000320228.
After 17583 training step(s), loss on training batch is 0.000397017.
After 17584 training step(s), loss on training batch is 0.000386739.
After 17585 training step(s), loss on training batch is 0.00041537.
After 17586 training step(s), loss on training batch is 0.000478564.
After 17587 training step(s), loss on training batch is 0.000402814.
After 17588 training step(s), loss on training batch is 0.000323457.
After 17589 training step(s), loss on training batch is 0.000359316.
After 17590 training step(s), loss on training batch is 0.000339278.
After 17591 training step(s), loss on training batch is 0.000310215.
After 17592 training step(s), loss on training batch is 0.000306034.
After 17593 training step(s), loss on training batch is 0.000321602.
After 17594 training step(s), loss on training batch is 0.000266991.
After 17595 training step(s), loss on training batch is 0.000461487.
After 17596 training step(s), loss on training batch is 0.000451166.
After 17597 training step(s), loss on training batch is 0.000241969.
After 17598 training step(s), loss on training batch is 0.000257645.
After 17599 training step(s), loss on training batch is 0.000324117.
After 17600 training step(s), loss on training batch is 0.000333385.
After 17601 training step(s), loss on training batch is 0.000489613.
After 17602 training step(s), loss on training batch is 0.000568969.
After 17603 training step(s), loss on training batch is 0.00035206.
After 17604 training step(s), loss on training batch is 0.000562242.
After 17605 training step(s), loss on training batch is 0.000920514.
After 17606 training step(s), loss on training batch is 0.000670167.
After 17607 training step(s), loss on training batch is 0.000372159.
After 17608 training step(s), loss on training batch is 0.000472169.
After 17609 training step(s), loss on training batch is 0.000546306.
After 17610 training step(s), loss on training batch is 0.000566861.
After 17611 training step(s), loss on training batch is 0.000364226.
After 17612 training step(s), loss on training batch is 0.000298876.
After 17613 training step(s), loss on training batch is 0.000362831.
After 17614 training step(s), loss on training batch is 0.000404769.
After 17615 training step(s), loss on training batch is 0.000363016.
After 17616 training step(s), loss on training batch is 0.000311745.
After 17617 training step(s), loss on training batch is 0.000398246.
After 17618 training step(s), loss on training batch is 0.000338292.
After 17619 training step(s), loss on training batch is 0.000413818.
After 17620 training step(s), loss on training batch is 0.000368141.
After 17621 training step(s), loss on training batch is 0.000287169.
After 17622 training step(s), loss on training batch is 0.000377729.
After 17623 training step(s), loss on training batch is 0.000354039.
After 17624 training step(s), loss on training batch is 0.000299062.
After 17625 training step(s), loss on training batch is 0.000297887.
After 17626 training step(s), loss on training batch is 0.000340249.
After 17627 training step(s), loss on training batch is 0.000297463.
After 17628 training step(s), loss on training batch is 0.000705132.
After 17629 training step(s), loss on training batch is 0.000561983.
After 17630 training step(s), loss on training batch is 0.000510213.
After 17631 training step(s), loss on training batch is 0.00051402.
After 17632 training step(s), loss on training batch is 0.000576392.
After 17633 training step(s), loss on training batch is 0.000671891.
After 17634 training step(s), loss on training batch is 0.000574702.
After 17635 training step(s), loss on training batch is 0.000443608.
After 17636 training step(s), loss on training batch is 0.000629774.
After 17637 training step(s), loss on training batch is 0.000603454.
After 17638 training step(s), loss on training batch is 0.000460207.
After 17639 training step(s), loss on training batch is 0.000447493.
After 17640 training step(s), loss on training batch is 0.000959906.
After 17641 training step(s), loss on training batch is 0.000468722.
After 17642 training step(s), loss on training batch is 0.00054895.
After 17643 training step(s), loss on training batch is 0.000551767.
After 17644 training step(s), loss on training batch is 0.00063647.
After 17645 training step(s), loss on training batch is 0.000457868.
After 17646 training step(s), loss on training batch is 0.000602271.
After 17647 training step(s), loss on training batch is 0.000493453.
After 17648 training step(s), loss on training batch is 0.000529534.
After 17649 training step(s), loss on training batch is 0.000598172.
After 17650 training step(s), loss on training batch is 0.000555984.
After 17651 training step(s), loss on training batch is 0.000541839.
After 17652 training step(s), loss on training batch is 0.000506232.
After 17653 training step(s), loss on training batch is 0.000536047.
After 17654 training step(s), loss on training batch is 0.00040561.
After 17655 training step(s), loss on training batch is 0.000536323.
After 17656 training step(s), loss on training batch is 0.00097218.
After 17657 training step(s), loss on training batch is 0.000829036.
After 17658 training step(s), loss on training batch is 0.000879442.
After 17659 training step(s), loss on training batch is 0.0008926.
After 17660 training step(s), loss on training batch is 0.000788071.
After 17661 training step(s), loss on training batch is 0.000827232.
After 17662 training step(s), loss on training batch is 0.000979813.
After 17663 training step(s), loss on training batch is 0.000837293.
After 17664 training step(s), loss on training batch is 0.000765467.
After 17665 training step(s), loss on training batch is 0.000980278.
After 17666 training step(s), loss on training batch is 0.000759467.
After 17667 training step(s), loss on training batch is 0.000773117.
After 17668 training step(s), loss on training batch is 0.000880069.
After 17669 training step(s), loss on training batch is 0.0012756.
After 17670 training step(s), loss on training batch is 0.00362685.
After 17671 training step(s), loss on training batch is 0.00135393.
After 17672 training step(s), loss on training batch is 0.00089455.
After 17673 training step(s), loss on training batch is 0.00112085.
After 17674 training step(s), loss on training batch is 0.000951816.
After 17675 training step(s), loss on training batch is 0.000917091.
After 17676 training step(s), loss on training batch is 0.000835609.
After 17677 training step(s), loss on training batch is 0.00116812.
After 17678 training step(s), loss on training batch is 0.000812734.
After 17679 training step(s), loss on training batch is 0.000785045.
After 17680 training step(s), loss on training batch is 0.000761844.
After 17681 training step(s), loss on training batch is 0.000596912.
After 17682 training step(s), loss on training batch is 0.00040453.
After 17683 training step(s), loss on training batch is 0.000273693.
After 17684 training step(s), loss on training batch is 0.000348117.
After 17685 training step(s), loss on training batch is 0.000341383.
After 17686 training step(s), loss on training batch is 0.000487825.
After 17687 training step(s), loss on training batch is 0.000550131.
After 17688 training step(s), loss on training batch is 0.000371148.
After 17689 training step(s), loss on training batch is 0.000342076.
After 17690 training step(s), loss on training batch is 0.000306378.
After 17691 training step(s), loss on training batch is 0.000256592.
After 17692 training step(s), loss on training batch is 0.000313849.
After 17693 training step(s), loss on training batch is 0.000302446.
After 17694 training step(s), loss on training batch is 0.000298292.
After 17695 training step(s), loss on training batch is 0.00034576.
After 17696 training step(s), loss on training batch is 0.000345439.
After 17697 training step(s), loss on training batch is 0.000280574.
After 17698 training step(s), loss on training batch is 0.00026132.
After 17699 training step(s), loss on training batch is 0.000278203.
After 17700 training step(s), loss on training batch is 0.000266767.
After 17701 training step(s), loss on training batch is 0.000910055.
After 17702 training step(s), loss on training batch is 0.000460169.
After 17703 training step(s), loss on training batch is 0.000487371.
After 17704 training step(s), loss on training batch is 0.000566408.
After 17705 training step(s), loss on training batch is 0.000539214.
After 17706 training step(s), loss on training batch is 0.000572182.
After 17707 training step(s), loss on training batch is 0.000565342.
After 17708 training step(s), loss on training batch is 0.000317848.
After 17709 training step(s), loss on training batch is 0.000355629.
After 17710 training step(s), loss on training batch is 0.000384478.
After 17711 training step(s), loss on training batch is 0.000458587.
After 17712 training step(s), loss on training batch is 0.000456121.
After 17713 training step(s), loss on training batch is 0.000459068.
After 17714 training step(s), loss on training batch is 0.000547654.
After 17715 training step(s), loss on training batch is 0.00132962.
After 17716 training step(s), loss on training batch is 0.000922307.
After 17717 training step(s), loss on training batch is 0.000773821.
After 17718 training step(s), loss on training batch is 0.000579562.
After 17719 training step(s), loss on training batch is 0.000373028.
After 17720 training step(s), loss on training batch is 0.000354796.
After 17721 training step(s), loss on training batch is 0.000362629.
After 17722 training step(s), loss on training batch is 0.000328023.
After 17723 training step(s), loss on training batch is 0.000493006.
After 17724 training step(s), loss on training batch is 0.000386114.
After 17725 training step(s), loss on training batch is 0.000375625.
After 17726 training step(s), loss on training batch is 0.000369876.
After 17727 training step(s), loss on training batch is 0.000745178.
After 17728 training step(s), loss on training batch is 0.00158359.
After 17729 training step(s), loss on training batch is 0.000601898.
After 17730 training step(s), loss on training batch is 0.000463651.
After 17731 training step(s), loss on training batch is 0.000562353.
After 17732 training step(s), loss on training batch is 0.000484699.
After 17733 training step(s), loss on training batch is 0.000405477.
After 17734 training step(s), loss on training batch is 0.000325419.
After 17735 training step(s), loss on training batch is 0.000295802.
After 17736 training step(s), loss on training batch is 0.000300576.
After 17737 training step(s), loss on training batch is 0.000338767.
After 17738 training step(s), loss on training batch is 0.000329317.
After 17739 training step(s), loss on training batch is 0.000380667.
After 17740 training step(s), loss on training batch is 0.000430133.
After 17741 training step(s), loss on training batch is 0.000341539.
After 17742 training step(s), loss on training batch is 0.000438853.
After 17743 training step(s), loss on training batch is 0.000427198.
After 17744 training step(s), loss on training batch is 0.000470035.
After 17745 training step(s), loss on training batch is 0.00032928.
After 17746 training step(s), loss on training batch is 0.000409847.
After 17747 training step(s), loss on training batch is 0.000342149.
After 17748 training step(s), loss on training batch is 0.000311086.
After 17749 training step(s), loss on training batch is 0.000355377.
After 17750 training step(s), loss on training batch is 0.000464844.
After 17751 training step(s), loss on training batch is 0.000434448.
After 17752 training step(s), loss on training batch is 0.000387572.
After 17753 training step(s), loss on training batch is 0.000390324.
After 17754 training step(s), loss on training batch is 0.000362839.
After 17755 training step(s), loss on training batch is 0.000342616.
After 17756 training step(s), loss on training batch is 0.000442403.
After 17757 training step(s), loss on training batch is 0.000547551.
After 17758 training step(s), loss on training batch is 0.000348611.
After 17759 training step(s), loss on training batch is 0.000410417.
After 17760 training step(s), loss on training batch is 0.000474531.
After 17761 training step(s), loss on training batch is 0.000348942.
After 17762 training step(s), loss on training batch is 0.000323337.
After 17763 training step(s), loss on training batch is 0.00031984.
After 17764 training step(s), loss on training batch is 0.000424331.
After 17765 training step(s), loss on training batch is 0.000397353.
After 17766 training step(s), loss on training batch is 0.000389786.
After 17767 training step(s), loss on training batch is 0.00037504.
After 17768 training step(s), loss on training batch is 0.000373113.
After 17769 training step(s), loss on training batch is 0.00041863.
After 17770 training step(s), loss on training batch is 0.000342491.
After 17771 training step(s), loss on training batch is 0.000341626.
After 17772 training step(s), loss on training batch is 0.00033517.
After 17773 training step(s), loss on training batch is 0.00032563.
After 17774 training step(s), loss on training batch is 0.000298081.
After 17775 training step(s), loss on training batch is 0.000369504.
After 17776 training step(s), loss on training batch is 0.000357688.
After 17777 training step(s), loss on training batch is 0.000439797.
After 17778 training step(s), loss on training batch is 0.000361742.
After 17779 training step(s), loss on training batch is 0.000323501.
After 17780 training step(s), loss on training batch is 0.000338452.
After 17781 training step(s), loss on training batch is 0.000339639.
After 17782 training step(s), loss on training batch is 0.00034525.
After 17783 training step(s), loss on training batch is 0.00058053.
After 17784 training step(s), loss on training batch is 0.000732523.
After 17785 training step(s), loss on training batch is 0.000835499.
After 17786 training step(s), loss on training batch is 0.000574519.
After 17787 training step(s), loss on training batch is 0.000621753.
After 17788 training step(s), loss on training batch is 0.000542313.
After 17789 training step(s), loss on training batch is 0.000573278.
After 17790 training step(s), loss on training batch is 0.00055208.
After 17791 training step(s), loss on training batch is 0.000654432.
After 17792 training step(s), loss on training batch is 0.000660163.
After 17793 training step(s), loss on training batch is 0.00055545.
After 17794 training step(s), loss on training batch is 0.000540281.
After 17795 training step(s), loss on training batch is 0.000649342.
After 17796 training step(s), loss on training batch is 0.000622378.
After 17797 training step(s), loss on training batch is 0.00057327.
After 17798 training step(s), loss on training batch is 0.000608412.
After 17799 training step(s), loss on training batch is 0.000797784.
After 17800 training step(s), loss on training batch is 0.000626398.
After 17801 training step(s), loss on training batch is 0.0005505.
After 17802 training step(s), loss on training batch is 0.000664048.
After 17803 training step(s), loss on training batch is 0.000555953.
After 17804 training step(s), loss on training batch is 0.000548125.
After 17805 training step(s), loss on training batch is 0.000474599.
After 17806 training step(s), loss on training batch is 0.00062218.
After 17807 training step(s), loss on training batch is 0.000741408.
After 17808 training step(s), loss on training batch is 0.000639394.
After 17809 training step(s), loss on training batch is 0.000508692.
After 17810 training step(s), loss on training batch is 0.000721115.
After 17811 training step(s), loss on training batch is 0.000607235.
After 17812 training step(s), loss on training batch is 0.000565101.
After 17813 training step(s), loss on training batch is 0.000479198.
After 17814 training step(s), loss on training batch is 0.000489699.
After 17815 training step(s), loss on training batch is 0.000452026.
After 17816 training step(s), loss on training batch is 0.000481641.
After 17817 training step(s), loss on training batch is 0.000491299.
After 17818 training step(s), loss on training batch is 0.000664658.
After 17819 training step(s), loss on training batch is 0.000864992.
After 17820 training step(s), loss on training batch is 0.00100658.
After 17821 training step(s), loss on training batch is 0.00098064.
After 17822 training step(s), loss on training batch is 0.000561432.
After 17823 training step(s), loss on training batch is 0.000546018.
After 17824 training step(s), loss on training batch is 0.000565955.
After 17825 training step(s), loss on training batch is 0.000612398.
After 17826 training step(s), loss on training batch is 0.000656131.
After 17827 training step(s), loss on training batch is 0.000559927.
After 17828 training step(s), loss on training batch is 0.000599137.
After 17829 training step(s), loss on training batch is 0.000613955.
After 17830 training step(s), loss on training batch is 0.000674838.
After 17831 training step(s), loss on training batch is 0.000739091.
After 17832 training step(s), loss on training batch is 0.000525615.
After 17833 training step(s), loss on training batch is 0.000561543.
After 17834 training step(s), loss on training batch is 0.000496067.
After 17835 training step(s), loss on training batch is 0.000459041.
After 17836 training step(s), loss on training batch is 0.000709485.
After 17837 training step(s), loss on training batch is 0.00105868.
After 17838 training step(s), loss on training batch is 0.000468597.
After 17839 training step(s), loss on training batch is 0.000547213.
After 17840 training step(s), loss on training batch is 0.000565594.
After 17841 training step(s), loss on training batch is 0.00052401.
After 17842 training step(s), loss on training batch is 0.000507.
After 17843 training step(s), loss on training batch is 0.000579921.
After 17844 training step(s), loss on training batch is 0.000539127.
After 17845 training step(s), loss on training batch is 0.000568638.
After 17846 training step(s), loss on training batch is 0.000748571.
After 17847 training step(s), loss on training batch is 0.000521855.
After 17848 training step(s), loss on training batch is 0.000562099.
After 17849 training step(s), loss on training batch is 0.00060066.
After 17850 training step(s), loss on training batch is 0.000594529.
After 17851 training step(s), loss on training batch is 0.000542784.
After 17852 training step(s), loss on training batch is 0.00047102.
After 17853 training step(s), loss on training batch is 0.000652077.
After 17854 training step(s), loss on training batch is 0.000600528.
After 17855 training step(s), loss on training batch is 0.000634478.
After 17856 training step(s), loss on training batch is 0.000547964.
After 17857 training step(s), loss on training batch is 0.000484058.
After 17858 training step(s), loss on training batch is 0.000625969.
After 17859 training step(s), loss on training batch is 0.000811259.
After 17860 training step(s), loss on training batch is 0.000516494.
After 17861 training step(s), loss on training batch is 0.000467897.
After 17862 training step(s), loss on training batch is 0.000471424.
After 17863 training step(s), loss on training batch is 0.000493808.
After 17864 training step(s), loss on training batch is 0.000461303.
After 17865 training step(s), loss on training batch is 0.000602341.
After 17866 training step(s), loss on training batch is 0.00097181.
After 17867 training step(s), loss on training batch is 0.00100999.
After 17868 training step(s), loss on training batch is 0.000924766.
After 17869 training step(s), loss on training batch is 0.000964685.
After 17870 training step(s), loss on training batch is 0.000914293.
After 17871 training step(s), loss on training batch is 0.000858766.
After 17872 training step(s), loss on training batch is 0.0009062.
After 17873 training step(s), loss on training batch is 0.0012347.
After 17874 training step(s), loss on training batch is 0.00106881.
After 17875 training step(s), loss on training batch is 0.0011461.
After 17876 training step(s), loss on training batch is 0.0008751.
After 17877 training step(s), loss on training batch is 0.00087653.
After 17878 training step(s), loss on training batch is 0.000939903.
After 17879 training step(s), loss on training batch is 0.000837207.
After 17880 training step(s), loss on training batch is 0.000840352.
After 17881 training step(s), loss on training batch is 0.00080263.
After 17882 training step(s), loss on training batch is 0.000854315.
After 17883 training step(s), loss on training batch is 0.000911453.
After 17884 training step(s), loss on training batch is 0.000823235.
After 17885 training step(s), loss on training batch is 0.000846459.
After 17886 training step(s), loss on training batch is 0.00105705.
After 17887 training step(s), loss on training batch is 0.000839024.
After 17888 training step(s), loss on training batch is 0.000992212.
After 17889 training step(s), loss on training batch is 0.000853842.
After 17890 training step(s), loss on training batch is 0.000839841.
After 17891 training step(s), loss on training batch is 0.000862172.
After 17892 training step(s), loss on training batch is 0.000842092.
After 17893 training step(s), loss on training batch is 0.000797399.
After 17894 training step(s), loss on training batch is 0.000933649.
After 17895 training step(s), loss on training batch is 0.000939444.
After 17896 training step(s), loss on training batch is 0.00103911.
After 17897 training step(s), loss on training batch is 0.000857401.
After 17898 training step(s), loss on training batch is 0.000864014.
After 17899 training step(s), loss on training batch is 0.00080808.
After 17900 training step(s), loss on training batch is 0.000867226.
After 17901 training step(s), loss on training batch is 0.000768771.
After 17902 training step(s), loss on training batch is 0.00094674.
After 17903 training step(s), loss on training batch is 0.000845571.
After 17904 training step(s), loss on training batch is 0.00101416.
After 17905 training step(s), loss on training batch is 0.00107739.
After 17906 training step(s), loss on training batch is 0.00102875.
After 17907 training step(s), loss on training batch is 0.00120382.
After 17908 training step(s), loss on training batch is 0.0010283.
After 17909 training step(s), loss on training batch is 0.00603902.
After 17910 training step(s), loss on training batch is 0.00146374.
After 17911 training step(s), loss on training batch is 0.00130949.
After 17912 training step(s), loss on training batch is 0.00129078.
After 17913 training step(s), loss on training batch is 0.00123545.
After 17914 training step(s), loss on training batch is 0.00122222.
After 17915 training step(s), loss on training batch is 0.00104509.
After 17916 training step(s), loss on training batch is 0.000490996.
After 17917 training step(s), loss on training batch is 0.000439661.
After 17918 training step(s), loss on training batch is 0.000404789.
After 17919 training step(s), loss on training batch is 0.000427491.
After 17920 training step(s), loss on training batch is 0.000445028.
After 17921 training step(s), loss on training batch is 0.000455595.
After 17922 training step(s), loss on training batch is 0.000470201.
After 17923 training step(s), loss on training batch is 0.000458941.
After 17924 training step(s), loss on training batch is 0.000446331.
After 17925 training step(s), loss on training batch is 0.000407921.
After 17926 training step(s), loss on training batch is 0.000850698.
After 17927 training step(s), loss on training batch is 0.000834305.
After 17928 training step(s), loss on training batch is 0.000988108.
After 17929 training step(s), loss on training batch is 0.00081355.
After 17930 training step(s), loss on training batch is 0.00160054.
After 17931 training step(s), loss on training batch is 0.00131116.
After 17932 training step(s), loss on training batch is 0.00100225.
After 17933 training step(s), loss on training batch is 0.000982958.
After 17934 training step(s), loss on training batch is 0.000882533.
After 17935 training step(s), loss on training batch is 0.000839621.
After 17936 training step(s), loss on training batch is 0.000868323.
After 17937 training step(s), loss on training batch is 0.000903217.
After 17938 training step(s), loss on training batch is 0.00100587.
After 17939 training step(s), loss on training batch is 0.000863807.
After 17940 training step(s), loss on training batch is 0.000863732.
After 17941 training step(s), loss on training batch is 0.000856634.
After 17942 training step(s), loss on training batch is 0.00155651.
After 17943 training step(s), loss on training batch is 0.000307598.
After 17944 training step(s), loss on training batch is 0.000361788.
After 17945 training step(s), loss on training batch is 0.000325913.
After 17946 training step(s), loss on training batch is 0.000419293.
After 17947 training step(s), loss on training batch is 0.000381949.
After 17948 training step(s), loss on training batch is 0.000362102.
After 17949 training step(s), loss on training batch is 0.00031864.
After 17950 training step(s), loss on training batch is 0.000303198.
After 17951 training step(s), loss on training batch is 0.000463919.
After 17952 training step(s), loss on training batch is 0.000386975.
After 17953 training step(s), loss on training batch is 0.000384136.
After 17954 training step(s), loss on training batch is 0.000345901.
After 17955 training step(s), loss on training batch is 0.000474984.
After 17956 training step(s), loss on training batch is 0.000361157.
After 17957 training step(s), loss on training batch is 0.000307363.
After 17958 training step(s), loss on training batch is 0.000308213.
After 17959 training step(s), loss on training batch is 0.000625305.
After 17960 training step(s), loss on training batch is 0.000821258.
After 17961 training step(s), loss on training batch is 0.000452246.
After 17962 training step(s), loss on training batch is 0.000366174.
After 17963 training step(s), loss on training batch is 0.000334965.
After 17964 training step(s), loss on training batch is 0.000394792.
After 17965 training step(s), loss on training batch is 0.000431491.
After 17966 training step(s), loss on training batch is 0.000368514.
After 17967 training step(s), loss on training batch is 0.000339224.
After 17968 training step(s), loss on training batch is 0.000488119.
After 17969 training step(s), loss on training batch is 0.000299375.
After 17970 training step(s), loss on training batch is 0.000367708.
After 17971 training step(s), loss on training batch is 0.000256825.
After 17972 training step(s), loss on training batch is 0.000262422.
After 17973 training step(s), loss on training batch is 0.000300883.
After 17974 training step(s), loss on training batch is 0.000306792.
After 17975 training step(s), loss on training batch is 0.000408501.
After 17976 training step(s), loss on training batch is 0.00036436.
After 17977 training step(s), loss on training batch is 0.00037248.
After 17978 training step(s), loss on training batch is 0.000301461.
After 17979 training step(s), loss on training batch is 0.000320765.
After 17980 training step(s), loss on training batch is 0.000298593.
After 17981 training step(s), loss on training batch is 0.000328564.
After 17982 training step(s), loss on training batch is 0.000304384.
After 17983 training step(s), loss on training batch is 0.000374761.
After 17984 training step(s), loss on training batch is 0.000357741.
After 17985 training step(s), loss on training batch is 0.000379742.
After 17986 training step(s), loss on training batch is 0.000486327.
After 17987 training step(s), loss on training batch is 0.000375739.
After 17988 training step(s), loss on training batch is 0.000309896.
After 17989 training step(s), loss on training batch is 0.000352741.
After 17990 training step(s), loss on training batch is 0.000327264.
After 17991 training step(s), loss on training batch is 0.00030215.
After 17992 training step(s), loss on training batch is 0.000305531.
After 17993 training step(s), loss on training batch is 0.000304894.
After 17994 training step(s), loss on training batch is 0.000256245.
After 17995 training step(s), loss on training batch is 0.000410338.
After 17996 training step(s), loss on training batch is 0.000417509.
After 17997 training step(s), loss on training batch is 0.000232987.
After 17998 training step(s), loss on training batch is 0.000249465.
After 17999 training step(s), loss on training batch is 0.000334395.
After 18000 training step(s), loss on training batch is 0.000338757.
After 18001 training step(s), loss on training batch is 0.000460214.
After 18002 training step(s), loss on training batch is 0.000548291.
After 18003 training step(s), loss on training batch is 0.000338295.
After 18004 training step(s), loss on training batch is 0.000487416.
After 18005 training step(s), loss on training batch is 0.00090912.
After 18006 training step(s), loss on training batch is 0.000625595.
After 18007 training step(s), loss on training batch is 0.000354651.
After 18008 training step(s), loss on training batch is 0.000433434.
After 18009 training step(s), loss on training batch is 0.000526123.
After 18010 training step(s), loss on training batch is 0.000542405.
After 18011 training step(s), loss on training batch is 0.000360733.
After 18012 training step(s), loss on training batch is 0.000291806.
After 18013 training step(s), loss on training batch is 0.00034965.
After 18014 training step(s), loss on training batch is 0.000366667.
After 18015 training step(s), loss on training batch is 0.000344166.
After 18016 training step(s), loss on training batch is 0.000305693.
After 18017 training step(s), loss on training batch is 0.000382548.
After 18018 training step(s), loss on training batch is 0.000326672.
After 18019 training step(s), loss on training batch is 0.000403826.
After 18020 training step(s), loss on training batch is 0.000367869.
After 18021 training step(s), loss on training batch is 0.000284325.
After 18022 training step(s), loss on training batch is 0.000367725.
After 18023 training step(s), loss on training batch is 0.00034415.
After 18024 training step(s), loss on training batch is 0.00029424.
After 18025 training step(s), loss on training batch is 0.000295687.
After 18026 training step(s), loss on training batch is 0.000348503.
After 18027 training step(s), loss on training batch is 0.000277683.
After 18028 training step(s), loss on training batch is 0.000799106.
After 18029 training step(s), loss on training batch is 0.000561888.
After 18030 training step(s), loss on training batch is 0.000490951.
After 18031 training step(s), loss on training batch is 0.000496038.
After 18032 training step(s), loss on training batch is 0.000572767.
After 18033 training step(s), loss on training batch is 0.000646341.
After 18034 training step(s), loss on training batch is 0.000554476.
After 18035 training step(s), loss on training batch is 0.000440956.
After 18036 training step(s), loss on training batch is 0.00063555.
After 18037 training step(s), loss on training batch is 0.00059892.
After 18038 training step(s), loss on training batch is 0.00045809.
After 18039 training step(s), loss on training batch is 0.000455162.
After 18040 training step(s), loss on training batch is 0.000966936.
After 18041 training step(s), loss on training batch is 0.000465706.
After 18042 training step(s), loss on training batch is 0.000536296.
After 18043 training step(s), loss on training batch is 0.000540596.
After 18044 training step(s), loss on training batch is 0.000619241.
After 18045 training step(s), loss on training batch is 0.000463856.
After 18046 training step(s), loss on training batch is 0.000582226.
After 18047 training step(s), loss on training batch is 0.000489116.
After 18048 training step(s), loss on training batch is 0.000520424.
After 18049 training step(s), loss on training batch is 0.000582265.
After 18050 training step(s), loss on training batch is 0.000558691.
After 18051 training step(s), loss on training batch is 0.000547233.
After 18052 training step(s), loss on training batch is 0.000504277.
After 18053 training step(s), loss on training batch is 0.000562466.
After 18054 training step(s), loss on training batch is 0.000406599.
After 18055 training step(s), loss on training batch is 0.00053066.
After 18056 training step(s), loss on training batch is 0.000951815.
After 18057 training step(s), loss on training batch is 0.000814893.
After 18058 training step(s), loss on training batch is 0.000875595.
After 18059 training step(s), loss on training batch is 0.000875329.
After 18060 training step(s), loss on training batch is 0.000770526.
After 18061 training step(s), loss on training batch is 0.000803565.
After 18062 training step(s), loss on training batch is 0.000954762.
After 18063 training step(s), loss on training batch is 0.00085412.
After 18064 training step(s), loss on training batch is 0.000778109.
After 18065 training step(s), loss on training batch is 0.000980148.
After 18066 training step(s), loss on training batch is 0.000772636.
After 18067 training step(s), loss on training batch is 0.000793449.
After 18068 training step(s), loss on training batch is 0.000892718.
After 18069 training step(s), loss on training batch is 0.00127693.
After 18070 training step(s), loss on training batch is 0.0029452.
After 18071 training step(s), loss on training batch is 0.00131117.
After 18072 training step(s), loss on training batch is 0.00105745.
After 18073 training step(s), loss on training batch is 0.00117602.
After 18074 training step(s), loss on training batch is 0.00107335.
After 18075 training step(s), loss on training batch is 0.00101916.
After 18076 training step(s), loss on training batch is 0.000966412.
After 18077 training step(s), loss on training batch is 0.00112853.
After 18078 training step(s), loss on training batch is 0.000864806.
After 18079 training step(s), loss on training batch is 0.000857723.
After 18080 training step(s), loss on training batch is 0.000837507.
After 18081 training step(s), loss on training batch is 0.000511659.
After 18082 training step(s), loss on training batch is 0.000375197.
After 18083 training step(s), loss on training batch is 0.000267382.
After 18084 training step(s), loss on training batch is 0.000322709.
After 18085 training step(s), loss on training batch is 0.000335403.
After 18086 training step(s), loss on training batch is 0.000463732.
After 18087 training step(s), loss on training batch is 0.000554956.
After 18088 training step(s), loss on training batch is 0.000376341.
After 18089 training step(s), loss on training batch is 0.000330262.
After 18090 training step(s), loss on training batch is 0.000297361.
After 18091 training step(s), loss on training batch is 0.00025256.
After 18092 training step(s), loss on training batch is 0.000296488.
After 18093 training step(s), loss on training batch is 0.000294161.
After 18094 training step(s), loss on training batch is 0.000285619.
After 18095 training step(s), loss on training batch is 0.000316964.
After 18096 training step(s), loss on training batch is 0.000331364.
After 18097 training step(s), loss on training batch is 0.000268154.
After 18098 training step(s), loss on training batch is 0.00025409.
After 18099 training step(s), loss on training batch is 0.00026895.
After 18100 training step(s), loss on training batch is 0.000268999.
After 18101 training step(s), loss on training batch is 0.000642291.
After 18102 training step(s), loss on training batch is 0.00043738.
After 18103 training step(s), loss on training batch is 0.000483354.
After 18104 training step(s), loss on training batch is 0.000571186.
After 18105 training step(s), loss on training batch is 0.000541756.
After 18106 training step(s), loss on training batch is 0.000555144.
After 18107 training step(s), loss on training batch is 0.000527477.
After 18108 training step(s), loss on training batch is 0.000309162.
After 18109 training step(s), loss on training batch is 0.00034116.
After 18110 training step(s), loss on training batch is 0.000366917.
After 18111 training step(s), loss on training batch is 0.000425875.
After 18112 training step(s), loss on training batch is 0.000433795.
After 18113 training step(s), loss on training batch is 0.000436332.
After 18114 training step(s), loss on training batch is 0.000541167.
After 18115 training step(s), loss on training batch is 0.00127282.
After 18116 training step(s), loss on training batch is 0.000906684.
After 18117 training step(s), loss on training batch is 0.000743818.
After 18118 training step(s), loss on training batch is 0.000569355.
After 18119 training step(s), loss on training batch is 0.000367428.
After 18120 training step(s), loss on training batch is 0.000352686.
After 18121 training step(s), loss on training batch is 0.000365949.
After 18122 training step(s), loss on training batch is 0.000317747.
After 18123 training step(s), loss on training batch is 0.000460681.
After 18124 training step(s), loss on training batch is 0.000371692.
After 18125 training step(s), loss on training batch is 0.000355731.
After 18126 training step(s), loss on training batch is 0.00034934.
After 18127 training step(s), loss on training batch is 0.000710632.
After 18128 training step(s), loss on training batch is 0.0017301.
After 18129 training step(s), loss on training batch is 0.000563793.
After 18130 training step(s), loss on training batch is 0.000445874.
After 18131 training step(s), loss on training batch is 0.000552114.
After 18132 training step(s), loss on training batch is 0.000467631.
After 18133 training step(s), loss on training batch is 0.000398829.
After 18134 training step(s), loss on training batch is 0.000323415.
After 18135 training step(s), loss on training batch is 0.000294243.
After 18136 training step(s), loss on training batch is 0.000296322.
After 18137 training step(s), loss on training batch is 0.000332522.
After 18138 training step(s), loss on training batch is 0.00033084.
After 18139 training step(s), loss on training batch is 0.000374519.
After 18140 training step(s), loss on training batch is 0.000423574.
After 18141 training step(s), loss on training batch is 0.000340096.
After 18142 training step(s), loss on training batch is 0.000416836.
After 18143 training step(s), loss on training batch is 0.000409481.
After 18144 training step(s), loss on training batch is 0.000470232.
After 18145 training step(s), loss on training batch is 0.00031778.
After 18146 training step(s), loss on training batch is 0.000415442.
After 18147 training step(s), loss on training batch is 0.000329597.
After 18148 training step(s), loss on training batch is 0.000307676.
After 18149 training step(s), loss on training batch is 0.000352785.
After 18150 training step(s), loss on training batch is 0.000463228.
After 18151 training step(s), loss on training batch is 0.000419385.
After 18152 training step(s), loss on training batch is 0.00038481.
After 18153 training step(s), loss on training batch is 0.000383093.
After 18154 training step(s), loss on training batch is 0.000378334.
After 18155 training step(s), loss on training batch is 0.000336055.
After 18156 training step(s), loss on training batch is 0.00043358.
After 18157 training step(s), loss on training batch is 0.000519039.
After 18158 training step(s), loss on training batch is 0.000341559.
After 18159 training step(s), loss on training batch is 0.000370773.
After 18160 training step(s), loss on training batch is 0.000472235.
After 18161 training step(s), loss on training batch is 0.000331015.
After 18162 training step(s), loss on training batch is 0.000307255.
After 18163 training step(s), loss on training batch is 0.000310638.
After 18164 training step(s), loss on training batch is 0.000425335.
After 18165 training step(s), loss on training batch is 0.000385775.
After 18166 training step(s), loss on training batch is 0.000383176.
After 18167 training step(s), loss on training batch is 0.000370026.
After 18168 training step(s), loss on training batch is 0.000358196.
After 18169 training step(s), loss on training batch is 0.000406126.
After 18170 training step(s), loss on training batch is 0.000333653.
After 18171 training step(s), loss on training batch is 0.000331452.
After 18172 training step(s), loss on training batch is 0.00032761.
After 18173 training step(s), loss on training batch is 0.000314079.
After 18174 training step(s), loss on training batch is 0.000292771.
After 18175 training step(s), loss on training batch is 0.000359623.
After 18176 training step(s), loss on training batch is 0.000351022.
After 18177 training step(s), loss on training batch is 0.000439538.
After 18178 training step(s), loss on training batch is 0.00035551.
After 18179 training step(s), loss on training batch is 0.000320364.
After 18180 training step(s), loss on training batch is 0.000342439.
After 18181 training step(s), loss on training batch is 0.000333129.
After 18182 training step(s), loss on training batch is 0.000339205.
After 18183 training step(s), loss on training batch is 0.000599578.
After 18184 training step(s), loss on training batch is 0.000721999.
After 18185 training step(s), loss on training batch is 0.000833156.
After 18186 training step(s), loss on training batch is 0.00057225.
After 18187 training step(s), loss on training batch is 0.0006454.
After 18188 training step(s), loss on training batch is 0.000550253.
After 18189 training step(s), loss on training batch is 0.000560442.
After 18190 training step(s), loss on training batch is 0.000543548.
After 18191 training step(s), loss on training batch is 0.000632188.
After 18192 training step(s), loss on training batch is 0.000668675.
After 18193 training step(s), loss on training batch is 0.000542633.
After 18194 training step(s), loss on training batch is 0.000528212.
After 18195 training step(s), loss on training batch is 0.000637103.
After 18196 training step(s), loss on training batch is 0.000625431.
After 18197 training step(s), loss on training batch is 0.000572281.
After 18198 training step(s), loss on training batch is 0.00060425.
After 18199 training step(s), loss on training batch is 0.00077434.
After 18200 training step(s), loss on training batch is 0.000624244.
After 18201 training step(s), loss on training batch is 0.000544272.
After 18202 training step(s), loss on training batch is 0.000639682.
After 18203 training step(s), loss on training batch is 0.000547494.
After 18204 training step(s), loss on training batch is 0.000541632.
After 18205 training step(s), loss on training batch is 0.000470152.
After 18206 training step(s), loss on training batch is 0.000612804.
After 18207 training step(s), loss on training batch is 0.000738573.
After 18208 training step(s), loss on training batch is 0.000626996.
After 18209 training step(s), loss on training batch is 0.000496768.
After 18210 training step(s), loss on training batch is 0.000718068.
After 18211 training step(s), loss on training batch is 0.000586458.
After 18212 training step(s), loss on training batch is 0.000557631.
After 18213 training step(s), loss on training batch is 0.000474194.
After 18214 training step(s), loss on training batch is 0.000489142.
After 18215 training step(s), loss on training batch is 0.000462104.
After 18216 training step(s), loss on training batch is 0.000483779.
After 18217 training step(s), loss on training batch is 0.000486346.
After 18218 training step(s), loss on training batch is 0.000642108.
After 18219 training step(s), loss on training batch is 0.000876853.
After 18220 training step(s), loss on training batch is 0.000914707.
After 18221 training step(s), loss on training batch is 0.000945689.
After 18222 training step(s), loss on training batch is 0.000563028.
After 18223 training step(s), loss on training batch is 0.000554951.
After 18224 training step(s), loss on training batch is 0.00057681.
After 18225 training step(s), loss on training batch is 0.000624249.
After 18226 training step(s), loss on training batch is 0.000659496.
After 18227 training step(s), loss on training batch is 0.000565417.
After 18228 training step(s), loss on training batch is 0.000592589.
After 18229 training step(s), loss on training batch is 0.000594669.
After 18230 training step(s), loss on training batch is 0.000662145.
After 18231 training step(s), loss on training batch is 0.000732436.
After 18232 training step(s), loss on training batch is 0.000515145.
After 18233 training step(s), loss on training batch is 0.000537708.
After 18234 training step(s), loss on training batch is 0.000496714.
After 18235 training step(s), loss on training batch is 0.000461221.
After 18236 training step(s), loss on training batch is 0.000689899.
After 18237 training step(s), loss on training batch is 0.000985915.
After 18238 training step(s), loss on training batch is 0.000464359.
After 18239 training step(s), loss on training batch is 0.000541253.
After 18240 training step(s), loss on training batch is 0.000559492.
After 18241 training step(s), loss on training batch is 0.000515071.
After 18242 training step(s), loss on training batch is 0.000502719.
After 18243 training step(s), loss on training batch is 0.000573178.
After 18244 training step(s), loss on training batch is 0.000536798.
After 18245 training step(s), loss on training batch is 0.000566053.
After 18246 training step(s), loss on training batch is 0.000706469.
After 18247 training step(s), loss on training batch is 0.000535589.
After 18248 training step(s), loss on training batch is 0.000545395.
After 18249 training step(s), loss on training batch is 0.000586167.
After 18250 training step(s), loss on training batch is 0.0005894.
After 18251 training step(s), loss on training batch is 0.000502081.
After 18252 training step(s), loss on training batch is 0.000468238.
After 18253 training step(s), loss on training batch is 0.000617138.
After 18254 training step(s), loss on training batch is 0.000546159.
After 18255 training step(s), loss on training batch is 0.000602175.
After 18256 training step(s), loss on training batch is 0.000552386.
After 18257 training step(s), loss on training batch is 0.000492365.
After 18258 training step(s), loss on training batch is 0.000607344.
After 18259 training step(s), loss on training batch is 0.000794425.
After 18260 training step(s), loss on training batch is 0.000515308.
After 18261 training step(s), loss on training batch is 0.000462549.
After 18262 training step(s), loss on training batch is 0.000467285.
After 18263 training step(s), loss on training batch is 0.000491404.
After 18264 training step(s), loss on training batch is 0.000453297.
After 18265 training step(s), loss on training batch is 0.000604803.
After 18266 training step(s), loss on training batch is 0.000967982.
After 18267 training step(s), loss on training batch is 0.00100342.
After 18268 training step(s), loss on training batch is 0.000928547.
After 18269 training step(s), loss on training batch is 0.000966351.
After 18270 training step(s), loss on training batch is 0.000928045.
After 18271 training step(s), loss on training batch is 0.000870896.
After 18272 training step(s), loss on training batch is 0.000914452.
After 18273 training step(s), loss on training batch is 0.00113618.
After 18274 training step(s), loss on training batch is 0.00107881.
After 18275 training step(s), loss on training batch is 0.00109626.
After 18276 training step(s), loss on training batch is 0.000885256.
After 18277 training step(s), loss on training batch is 0.000880461.
After 18278 training step(s), loss on training batch is 0.000922264.
After 18279 training step(s), loss on training batch is 0.00082507.
After 18280 training step(s), loss on training batch is 0.000826737.
After 18281 training step(s), loss on training batch is 0.000786567.
After 18282 training step(s), loss on training batch is 0.000833099.
After 18283 training step(s), loss on training batch is 0.000919244.
After 18284 training step(s), loss on training batch is 0.000815144.
After 18285 training step(s), loss on training batch is 0.000817936.
After 18286 training step(s), loss on training batch is 0.00106685.
After 18287 training step(s), loss on training batch is 0.000815655.
After 18288 training step(s), loss on training batch is 0.000985321.
After 18289 training step(s), loss on training batch is 0.000834112.
After 18290 training step(s), loss on training batch is 0.000822439.
After 18291 training step(s), loss on training batch is 0.000849474.
After 18292 training step(s), loss on training batch is 0.000852453.
After 18293 training step(s), loss on training batch is 0.000787026.
After 18294 training step(s), loss on training batch is 0.00091707.
After 18295 training step(s), loss on training batch is 0.000940762.
After 18296 training step(s), loss on training batch is 0.00109841.
After 18297 training step(s), loss on training batch is 0.000829002.
After 18298 training step(s), loss on training batch is 0.000847763.
After 18299 training step(s), loss on training batch is 0.000770414.
After 18300 training step(s), loss on training batch is 0.000838375.
After 18301 training step(s), loss on training batch is 0.000756912.
After 18302 training step(s), loss on training batch is 0.000953294.
After 18303 training step(s), loss on training batch is 0.000812283.
After 18304 training step(s), loss on training batch is 0.00105877.
After 18305 training step(s), loss on training batch is 0.00109137.
After 18306 training step(s), loss on training batch is 0.00103158.
After 18307 training step(s), loss on training batch is 0.00119831.
After 18308 training step(s), loss on training batch is 0.00101542.
After 18309 training step(s), loss on training batch is 0.00631547.
After 18310 training step(s), loss on training batch is 0.00142473.
After 18311 training step(s), loss on training batch is 0.00126744.
After 18312 training step(s), loss on training batch is 0.0012456.
After 18313 training step(s), loss on training batch is 0.00119438.
After 18314 training step(s), loss on training batch is 0.00121969.
After 18315 training step(s), loss on training batch is 0.00101152.
After 18316 training step(s), loss on training batch is 0.000464398.
After 18317 training step(s), loss on training batch is 0.000416932.
After 18318 training step(s), loss on training batch is 0.000402278.
After 18319 training step(s), loss on training batch is 0.000424315.
After 18320 training step(s), loss on training batch is 0.000451592.
After 18321 training step(s), loss on training batch is 0.000451191.
After 18322 training step(s), loss on training batch is 0.00046481.
After 18323 training step(s), loss on training batch is 0.000462183.
After 18324 training step(s), loss on training batch is 0.000448446.
After 18325 training step(s), loss on training batch is 0.000410996.
After 18326 training step(s), loss on training batch is 0.000841227.
After 18327 training step(s), loss on training batch is 0.000871734.
After 18328 training step(s), loss on training batch is 0.000934144.
After 18329 training step(s), loss on training batch is 0.000824457.
After 18330 training step(s), loss on training batch is 0.00149124.
After 18331 training step(s), loss on training batch is 0.00129875.
After 18332 training step(s), loss on training batch is 0.00100305.
After 18333 training step(s), loss on training batch is 0.000989912.
After 18334 training step(s), loss on training batch is 0.000888911.
After 18335 training step(s), loss on training batch is 0.000841773.
After 18336 training step(s), loss on training batch is 0.000860024.
After 18337 training step(s), loss on training batch is 0.000880548.
After 18338 training step(s), loss on training batch is 0.000998121.
After 18339 training step(s), loss on training batch is 0.000862254.
After 18340 training step(s), loss on training batch is 0.000860063.
After 18341 training step(s), loss on training batch is 0.000849794.
After 18342 training step(s), loss on training batch is 0.0015123.
After 18343 training step(s), loss on training batch is 0.000316525.
After 18344 training step(s), loss on training batch is 0.00036233.
After 18345 training step(s), loss on training batch is 0.000329575.
After 18346 training step(s), loss on training batch is 0.000421031.
After 18347 training step(s), loss on training batch is 0.000373155.
After 18348 training step(s), loss on training batch is 0.000362214.
After 18349 training step(s), loss on training batch is 0.000314747.
After 18350 training step(s), loss on training batch is 0.000298055.
After 18351 training step(s), loss on training batch is 0.000456493.
After 18352 training step(s), loss on training batch is 0.000380391.
After 18353 training step(s), loss on training batch is 0.000384445.
After 18354 training step(s), loss on training batch is 0.00034251.
After 18355 training step(s), loss on training batch is 0.000462684.
After 18356 training step(s), loss on training batch is 0.000352434.
After 18357 training step(s), loss on training batch is 0.000304198.
After 18358 training step(s), loss on training batch is 0.000301351.
After 18359 training step(s), loss on training batch is 0.000641886.
After 18360 training step(s), loss on training batch is 0.000818192.
After 18361 training step(s), loss on training batch is 0.000447689.
After 18362 training step(s), loss on training batch is 0.000361112.
After 18363 training step(s), loss on training batch is 0.000325333.
After 18364 training step(s), loss on training batch is 0.000395692.
After 18365 training step(s), loss on training batch is 0.000428342.
After 18366 training step(s), loss on training batch is 0.000372896.
After 18367 training step(s), loss on training batch is 0.000343395.
After 18368 training step(s), loss on training batch is 0.000483659.
After 18369 training step(s), loss on training batch is 0.000295859.
After 18370 training step(s), loss on training batch is 0.000368435.
After 18371 training step(s), loss on training batch is 0.000259182.
After 18372 training step(s), loss on training batch is 0.000265133.
After 18373 training step(s), loss on training batch is 0.0003007.
After 18374 training step(s), loss on training batch is 0.000304161.
After 18375 training step(s), loss on training batch is 0.000423194.
After 18376 training step(s), loss on training batch is 0.00036239.
After 18377 training step(s), loss on training batch is 0.00037349.
After 18378 training step(s), loss on training batch is 0.000301847.
After 18379 training step(s), loss on training batch is 0.000325767.
After 18380 training step(s), loss on training batch is 0.000297561.
After 18381 training step(s), loss on training batch is 0.000326077.
After 18382 training step(s), loss on training batch is 0.000301796.
After 18383 training step(s), loss on training batch is 0.000368664.
After 18384 training step(s), loss on training batch is 0.000348946.
After 18385 training step(s), loss on training batch is 0.000377842.
After 18386 training step(s), loss on training batch is 0.000489387.
After 18387 training step(s), loss on training batch is 0.000373081.
After 18388 training step(s), loss on training batch is 0.000306278.
After 18389 training step(s), loss on training batch is 0.000352284.
After 18390 training step(s), loss on training batch is 0.000324499.
After 18391 training step(s), loss on training batch is 0.000297601.
After 18392 training step(s), loss on training batch is 0.000304596.
After 18393 training step(s), loss on training batch is 0.00030969.
After 18394 training step(s), loss on training batch is 0.000260224.
After 18395 training step(s), loss on training batch is 0.000404705.
After 18396 training step(s), loss on training batch is 0.000410936.
After 18397 training step(s), loss on training batch is 0.000232815.
After 18398 training step(s), loss on training batch is 0.000250708.
After 18399 training step(s), loss on training batch is 0.000310455.
After 18400 training step(s), loss on training batch is 0.000326663.
After 18401 training step(s), loss on training batch is 0.000478307.
After 18402 training step(s), loss on training batch is 0.000557886.
After 18403 training step(s), loss on training batch is 0.000350805.
After 18404 training step(s), loss on training batch is 0.000495023.
After 18405 training step(s), loss on training batch is 0.000868344.
After 18406 training step(s), loss on training batch is 0.000623943.
After 18407 training step(s), loss on training batch is 0.000361571.
After 18408 training step(s), loss on training batch is 0.000441681.
After 18409 training step(s), loss on training batch is 0.000521471.
After 18410 training step(s), loss on training batch is 0.000544643.
After 18411 training step(s), loss on training batch is 0.000365883.
After 18412 training step(s), loss on training batch is 0.000299485.
After 18413 training step(s), loss on training batch is 0.000364867.
After 18414 training step(s), loss on training batch is 0.000392968.
After 18415 training step(s), loss on training batch is 0.000363813.
After 18416 training step(s), loss on training batch is 0.00030985.
After 18417 training step(s), loss on training batch is 0.000396739.
After 18418 training step(s), loss on training batch is 0.000328287.
After 18419 training step(s), loss on training batch is 0.000416007.
After 18420 training step(s), loss on training batch is 0.00037252.
After 18421 training step(s), loss on training batch is 0.000280985.
After 18422 training step(s), loss on training batch is 0.000375799.
After 18423 training step(s), loss on training batch is 0.000340844.
After 18424 training step(s), loss on training batch is 0.00029803.
After 18425 training step(s), loss on training batch is 0.000298612.
After 18426 training step(s), loss on training batch is 0.000349527.
After 18427 training step(s), loss on training batch is 0.000288894.
After 18428 training step(s), loss on training batch is 0.000750201.
After 18429 training step(s), loss on training batch is 0.000549994.
After 18430 training step(s), loss on training batch is 0.000497111.
After 18431 training step(s), loss on training batch is 0.000502026.
After 18432 training step(s), loss on training batch is 0.000568646.
After 18433 training step(s), loss on training batch is 0.000640442.
After 18434 training step(s), loss on training batch is 0.000552279.
After 18435 training step(s), loss on training batch is 0.000443898.
After 18436 training step(s), loss on training batch is 0.000642823.
After 18437 training step(s), loss on training batch is 0.000604683.
After 18438 training step(s), loss on training batch is 0.000465279.
After 18439 training step(s), loss on training batch is 0.000455843.
After 18440 training step(s), loss on training batch is 0.00099895.
After 18441 training step(s), loss on training batch is 0.000459447.
After 18442 training step(s), loss on training batch is 0.000528298.
After 18443 training step(s), loss on training batch is 0.000537541.
After 18444 training step(s), loss on training batch is 0.00061563.
After 18445 training step(s), loss on training batch is 0.000460578.
After 18446 training step(s), loss on training batch is 0.00060353.
After 18447 training step(s), loss on training batch is 0.000485804.
After 18448 training step(s), loss on training batch is 0.000519942.
After 18449 training step(s), loss on training batch is 0.00057863.
After 18450 training step(s), loss on training batch is 0.000559125.
After 18451 training step(s), loss on training batch is 0.000546555.
After 18452 training step(s), loss on training batch is 0.000506564.
After 18453 training step(s), loss on training batch is 0.000558359.
After 18454 training step(s), loss on training batch is 0.000410438.
After 18455 training step(s), loss on training batch is 0.000530022.
After 18456 training step(s), loss on training batch is 0.000958825.
After 18457 training step(s), loss on training batch is 0.000823683.
After 18458 training step(s), loss on training batch is 0.00088076.
After 18459 training step(s), loss on training batch is 0.000875707.
After 18460 training step(s), loss on training batch is 0.000762936.
After 18461 training step(s), loss on training batch is 0.00079143.
After 18462 training step(s), loss on training batch is 0.000951842.
After 18463 training step(s), loss on training batch is 0.00084267.
After 18464 training step(s), loss on training batch is 0.00076951.
After 18465 training step(s), loss on training batch is 0.000985152.
After 18466 training step(s), loss on training batch is 0.000764784.
After 18467 training step(s), loss on training batch is 0.000780227.
After 18468 training step(s), loss on training batch is 0.000895641.
After 18469 training step(s), loss on training batch is 0.00132382.
After 18470 training step(s), loss on training batch is 0.00290366.
After 18471 training step(s), loss on training batch is 0.00132511.
After 18472 training step(s), loss on training batch is 0.000920173.
After 18473 training step(s), loss on training batch is 0.00108472.
After 18474 training step(s), loss on training batch is 0.000964514.
After 18475 training step(s), loss on training batch is 0.000926788.
After 18476 training step(s), loss on training batch is 0.000856059.
After 18477 training step(s), loss on training batch is 0.00112557.
After 18478 training step(s), loss on training batch is 0.000842127.
After 18479 training step(s), loss on training batch is 0.000816023.
After 18480 training step(s), loss on training batch is 0.000796001.
After 18481 training step(s), loss on training batch is 0.000537024.
After 18482 training step(s), loss on training batch is 0.000377248.
After 18483 training step(s), loss on training batch is 0.000264033.
After 18484 training step(s), loss on training batch is 0.000319115.
After 18485 training step(s), loss on training batch is 0.000337076.
After 18486 training step(s), loss on training batch is 0.000469171.
After 18487 training step(s), loss on training batch is 0.000567517.
After 18488 training step(s), loss on training batch is 0.000369479.
After 18489 training step(s), loss on training batch is 0.000324599.
After 18490 training step(s), loss on training batch is 0.000285086.
After 18491 training step(s), loss on training batch is 0.00024674.
After 18492 training step(s), loss on training batch is 0.000297906.
After 18493 training step(s), loss on training batch is 0.000289965.
After 18494 training step(s), loss on training batch is 0.000276528.
After 18495 training step(s), loss on training batch is 0.000308578.
After 18496 training step(s), loss on training batch is 0.000330195.
After 18497 training step(s), loss on training batch is 0.000260814.
After 18498 training step(s), loss on training batch is 0.000246112.
After 18499 training step(s), loss on training batch is 0.000265829.
After 18500 training step(s), loss on training batch is 0.000270365.
After 18501 training step(s), loss on training batch is 0.000619339.
After 18502 training step(s), loss on training batch is 0.000435453.
After 18503 training step(s), loss on training batch is 0.000473937.
After 18504 training step(s), loss on training batch is 0.000558788.
After 18505 training step(s), loss on training batch is 0.000535662.
After 18506 training step(s), loss on training batch is 0.000551816.
After 18507 training step(s), loss on training batch is 0.000520441.
After 18508 training step(s), loss on training batch is 0.000304239.
After 18509 training step(s), loss on training batch is 0.000335872.
After 18510 training step(s), loss on training batch is 0.000361825.
After 18511 training step(s), loss on training batch is 0.000416023.
After 18512 training step(s), loss on training batch is 0.000421144.
After 18513 training step(s), loss on training batch is 0.000434004.
After 18514 training step(s), loss on training batch is 0.000529493.
After 18515 training step(s), loss on training batch is 0.00125553.
After 18516 training step(s), loss on training batch is 0.000903044.
After 18517 training step(s), loss on training batch is 0.000733825.
After 18518 training step(s), loss on training batch is 0.000576083.
After 18519 training step(s), loss on training batch is 0.00036323.
After 18520 training step(s), loss on training batch is 0.000349482.
After 18521 training step(s), loss on training batch is 0.000363428.
After 18522 training step(s), loss on training batch is 0.000321544.
After 18523 training step(s), loss on training batch is 0.000500106.
After 18524 training step(s), loss on training batch is 0.000381073.
After 18525 training step(s), loss on training batch is 0.000366749.
After 18526 training step(s), loss on training batch is 0.000364612.
After 18527 training step(s), loss on training batch is 0.000648503.
After 18528 training step(s), loss on training batch is 0.00154736.
After 18529 training step(s), loss on training batch is 0.000579721.
After 18530 training step(s), loss on training batch is 0.000452882.
After 18531 training step(s), loss on training batch is 0.000526863.
After 18532 training step(s), loss on training batch is 0.000461972.
After 18533 training step(s), loss on training batch is 0.000398128.
After 18534 training step(s), loss on training batch is 0.000321937.
After 18535 training step(s), loss on training batch is 0.000291414.
After 18536 training step(s), loss on training batch is 0.000294757.
After 18537 training step(s), loss on training batch is 0.000332342.
After 18538 training step(s), loss on training batch is 0.00032982.
After 18539 training step(s), loss on training batch is 0.00037267.
After 18540 training step(s), loss on training batch is 0.00041661.
After 18541 training step(s), loss on training batch is 0.000335997.
After 18542 training step(s), loss on training batch is 0.000414968.
After 18543 training step(s), loss on training batch is 0.00040774.
After 18544 training step(s), loss on training batch is 0.00046929.
After 18545 training step(s), loss on training batch is 0.000318138.
After 18546 training step(s), loss on training batch is 0.000415112.
After 18547 training step(s), loss on training batch is 0.000328763.
After 18548 training step(s), loss on training batch is 0.00030545.
After 18549 training step(s), loss on training batch is 0.000351362.
After 18550 training step(s), loss on training batch is 0.000455962.
After 18551 training step(s), loss on training batch is 0.000425993.
After 18552 training step(s), loss on training batch is 0.000373262.
After 18553 training step(s), loss on training batch is 0.000377388.
After 18554 training step(s), loss on training batch is 0.000354431.
After 18555 training step(s), loss on training batch is 0.00032927.
After 18556 training step(s), loss on training batch is 0.000437066.
After 18557 training step(s), loss on training batch is 0.000531109.
After 18558 training step(s), loss on training batch is 0.000339877.
After 18559 training step(s), loss on training batch is 0.00036528.
After 18560 training step(s), loss on training batch is 0.00046709.
After 18561 training step(s), loss on training batch is 0.000330007.
After 18562 training step(s), loss on training batch is 0.000305243.
After 18563 training step(s), loss on training batch is 0.000311416.
After 18564 training step(s), loss on training batch is 0.000430149.
After 18565 training step(s), loss on training batch is 0.00038235.
After 18566 training step(s), loss on training batch is 0.000380411.
After 18567 training step(s), loss on training batch is 0.0003715.
After 18568 training step(s), loss on training batch is 0.000349939.
After 18569 training step(s), loss on training batch is 0.000405217.
After 18570 training step(s), loss on training batch is 0.000334287.
After 18571 training step(s), loss on training batch is 0.000327072.
After 18572 training step(s), loss on training batch is 0.000324086.
After 18573 training step(s), loss on training batch is 0.000313642.
After 18574 training step(s), loss on training batch is 0.00029265.
After 18575 training step(s), loss on training batch is 0.000360057.
After 18576 training step(s), loss on training batch is 0.000352846.
After 18577 training step(s), loss on training batch is 0.000439068.
After 18578 training step(s), loss on training batch is 0.000350876.
After 18579 training step(s), loss on training batch is 0.000319155.
After 18580 training step(s), loss on training batch is 0.000340456.
After 18581 training step(s), loss on training batch is 0.000330908.
After 18582 training step(s), loss on training batch is 0.000339276.
After 18583 training step(s), loss on training batch is 0.000591171.
After 18584 training step(s), loss on training batch is 0.000698558.
After 18585 training step(s), loss on training batch is 0.000832808.
After 18586 training step(s), loss on training batch is 0.000568987.
After 18587 training step(s), loss on training batch is 0.00066057.
After 18588 training step(s), loss on training batch is 0.000548648.
After 18589 training step(s), loss on training batch is 0.000556499.
After 18590 training step(s), loss on training batch is 0.000541952.
After 18591 training step(s), loss on training batch is 0.000631712.
After 18592 training step(s), loss on training batch is 0.000660051.
After 18593 training step(s), loss on training batch is 0.00053894.
After 18594 training step(s), loss on training batch is 0.000521131.
After 18595 training step(s), loss on training batch is 0.000678196.
After 18596 training step(s), loss on training batch is 0.000638255.
After 18597 training step(s), loss on training batch is 0.000556886.
After 18598 training step(s), loss on training batch is 0.000628635.
After 18599 training step(s), loss on training batch is 0.000784703.
After 18600 training step(s), loss on training batch is 0.000656349.
After 18601 training step(s), loss on training batch is 0.000548394.
After 18602 training step(s), loss on training batch is 0.000663034.
After 18603 training step(s), loss on training batch is 0.000529381.
After 18604 training step(s), loss on training batch is 0.000527135.
After 18605 training step(s), loss on training batch is 0.000458194.
After 18606 training step(s), loss on training batch is 0.000611747.
After 18607 training step(s), loss on training batch is 0.000733486.
After 18608 training step(s), loss on training batch is 0.000633172.
After 18609 training step(s), loss on training batch is 0.000492028.
After 18610 training step(s), loss on training batch is 0.000715921.
After 18611 training step(s), loss on training batch is 0.000573347.
After 18612 training step(s), loss on training batch is 0.000559245.
After 18613 training step(s), loss on training batch is 0.00046523.
After 18614 training step(s), loss on training batch is 0.000481969.
After 18615 training step(s), loss on training batch is 0.000455354.
After 18616 training step(s), loss on training batch is 0.00047989.
After 18617 training step(s), loss on training batch is 0.000480136.
After 18618 training step(s), loss on training batch is 0.000650567.
After 18619 training step(s), loss on training batch is 0.000878636.
After 18620 training step(s), loss on training batch is 0.000897542.
After 18621 training step(s), loss on training batch is 0.00096977.
After 18622 training step(s), loss on training batch is 0.000551951.
After 18623 training step(s), loss on training batch is 0.000545417.
After 18624 training step(s), loss on training batch is 0.000564642.
After 18625 training step(s), loss on training batch is 0.000614512.
After 18626 training step(s), loss on training batch is 0.000654332.
After 18627 training step(s), loss on training batch is 0.000555087.
After 18628 training step(s), loss on training batch is 0.000586801.
After 18629 training step(s), loss on training batch is 0.000599387.
After 18630 training step(s), loss on training batch is 0.000657486.
After 18631 training step(s), loss on training batch is 0.00070758.
After 18632 training step(s), loss on training batch is 0.000520165.
After 18633 training step(s), loss on training batch is 0.000550489.
After 18634 training step(s), loss on training batch is 0.000509204.
After 18635 training step(s), loss on training batch is 0.000471618.
After 18636 training step(s), loss on training batch is 0.000675984.
After 18637 training step(s), loss on training batch is 0.000941628.
After 18638 training step(s), loss on training batch is 0.000465119.
After 18639 training step(s), loss on training batch is 0.000538531.
After 18640 training step(s), loss on training batch is 0.000555041.
After 18641 training step(s), loss on training batch is 0.000509954.
After 18642 training step(s), loss on training batch is 0.000498206.
After 18643 training step(s), loss on training batch is 0.000564234.
After 18644 training step(s), loss on training batch is 0.000528052.
After 18645 training step(s), loss on training batch is 0.000557064.
After 18646 training step(s), loss on training batch is 0.000705187.
After 18647 training step(s), loss on training batch is 0.000535884.
After 18648 training step(s), loss on training batch is 0.00053875.
After 18649 training step(s), loss on training batch is 0.00058034.
After 18650 training step(s), loss on training batch is 0.000584757.
After 18651 training step(s), loss on training batch is 0.000494185.
After 18652 training step(s), loss on training batch is 0.000466353.
After 18653 training step(s), loss on training batch is 0.000608416.
After 18654 training step(s), loss on training batch is 0.000541677.
After 18655 training step(s), loss on training batch is 0.000598133.
After 18656 training step(s), loss on training batch is 0.000549267.
After 18657 training step(s), loss on training batch is 0.000484323.
After 18658 training step(s), loss on training batch is 0.000605651.
After 18659 training step(s), loss on training batch is 0.00076756.
After 18660 training step(s), loss on training batch is 0.000507407.
After 18661 training step(s), loss on training batch is 0.000458375.
After 18662 training step(s), loss on training batch is 0.000462914.
After 18663 training step(s), loss on training batch is 0.000484747.
After 18664 training step(s), loss on training batch is 0.000452664.
After 18665 training step(s), loss on training batch is 0.00059424.
After 18666 training step(s), loss on training batch is 0.000956952.
After 18667 training step(s), loss on training batch is 0.000997754.
After 18668 training step(s), loss on training batch is 0.000920943.
After 18669 training step(s), loss on training batch is 0.00095812.
After 18670 training step(s), loss on training batch is 0.000913943.
After 18671 training step(s), loss on training batch is 0.000866377.
After 18672 training step(s), loss on training batch is 0.000905373.
After 18673 training step(s), loss on training batch is 0.00110969.
After 18674 training step(s), loss on training batch is 0.00106259.
After 18675 training step(s), loss on training batch is 0.00107854.
After 18676 training step(s), loss on training batch is 0.000875187.
After 18677 training step(s), loss on training batch is 0.00085617.
After 18678 training step(s), loss on training batch is 0.000915045.
After 18679 training step(s), loss on training batch is 0.000822532.
After 18680 training step(s), loss on training batch is 0.000826147.
After 18681 training step(s), loss on training batch is 0.00079226.
After 18682 training step(s), loss on training batch is 0.000835207.
After 18683 training step(s), loss on training batch is 0.000904306.
After 18684 training step(s), loss on training batch is 0.000812786.
After 18685 training step(s), loss on training batch is 0.000823564.
After 18686 training step(s), loss on training batch is 0.00103898.
After 18687 training step(s), loss on training batch is 0.000824796.
After 18688 training step(s), loss on training batch is 0.000972632.
After 18689 training step(s), loss on training batch is 0.000847474.
After 18690 training step(s), loss on training batch is 0.000839816.
After 18691 training step(s), loss on training batch is 0.000857895.
After 18692 training step(s), loss on training batch is 0.000841455.
After 18693 training step(s), loss on training batch is 0.000782754.
After 18694 training step(s), loss on training batch is 0.000913036.
After 18695 training step(s), loss on training batch is 0.000916627.
After 18696 training step(s), loss on training batch is 0.00101082.
After 18697 training step(s), loss on training batch is 0.000838756.
After 18698 training step(s), loss on training batch is 0.000850459.
After 18699 training step(s), loss on training batch is 0.000769954.
After 18700 training step(s), loss on training batch is 0.000838634.
After 18701 training step(s), loss on training batch is 0.000754235.
After 18702 training step(s), loss on training batch is 0.000931468.
After 18703 training step(s), loss on training batch is 0.000810162.
After 18704 training step(s), loss on training batch is 0.001028.
After 18705 training step(s), loss on training batch is 0.00105396.
After 18706 training step(s), loss on training batch is 0.00101307.
After 18707 training step(s), loss on training batch is 0.00121181.
After 18708 training step(s), loss on training batch is 0.00101569.
After 18709 training step(s), loss on training batch is 0.00636525.
After 18710 training step(s), loss on training batch is 0.00140315.
After 18711 training step(s), loss on training batch is 0.00126306.
After 18712 training step(s), loss on training batch is 0.00124548.
After 18713 training step(s), loss on training batch is 0.00116926.
After 18714 training step(s), loss on training batch is 0.00121055.
After 18715 training step(s), loss on training batch is 0.00102136.
After 18716 training step(s), loss on training batch is 0.000478888.
After 18717 training step(s), loss on training batch is 0.000428155.
After 18718 training step(s), loss on training batch is 0.000410676.
After 18719 training step(s), loss on training batch is 0.000425448.
After 18720 training step(s), loss on training batch is 0.000444454.
After 18721 training step(s), loss on training batch is 0.000453537.
After 18722 training step(s), loss on training batch is 0.000468936.
After 18723 training step(s), loss on training batch is 0.000463493.
After 18724 training step(s), loss on training batch is 0.000448445.
After 18725 training step(s), loss on training batch is 0.000410642.
After 18726 training step(s), loss on training batch is 0.000857733.
After 18727 training step(s), loss on training batch is 0.000885213.
After 18728 training step(s), loss on training batch is 0.000955614.
After 18729 training step(s), loss on training batch is 0.000861271.
After 18730 training step(s), loss on training batch is 0.00141271.
After 18731 training step(s), loss on training batch is 0.00126123.
After 18732 training step(s), loss on training batch is 0.0010062.
After 18733 training step(s), loss on training batch is 0.000967709.
After 18734 training step(s), loss on training batch is 0.000894269.
After 18735 training step(s), loss on training batch is 0.000853555.
After 18736 training step(s), loss on training batch is 0.000878536.
After 18737 training step(s), loss on training batch is 0.000901449.
After 18738 training step(s), loss on training batch is 0.00101573.
After 18739 training step(s), loss on training batch is 0.000874658.
After 18740 training step(s), loss on training batch is 0.000862805.
After 18741 training step(s), loss on training batch is 0.000865402.
After 18742 training step(s), loss on training batch is 0.00152685.
After 18743 training step(s), loss on training batch is 0.00030327.
After 18744 training step(s), loss on training batch is 0.000353528.
After 18745 training step(s), loss on training batch is 0.00032259.
After 18746 training step(s), loss on training batch is 0.000412324.
After 18747 training step(s), loss on training batch is 0.000376342.
After 18748 training step(s), loss on training batch is 0.000356408.
After 18749 training step(s), loss on training batch is 0.000316609.
After 18750 training step(s), loss on training batch is 0.00029694.
After 18751 training step(s), loss on training batch is 0.00045449.
After 18752 training step(s), loss on training batch is 0.000377947.
After 18753 training step(s), loss on training batch is 0.000379501.
After 18754 training step(s), loss on training batch is 0.000342663.
After 18755 training step(s), loss on training batch is 0.000458077.
After 18756 training step(s), loss on training batch is 0.000348873.
After 18757 training step(s), loss on training batch is 0.000302266.
After 18758 training step(s), loss on training batch is 0.000304443.
After 18759 training step(s), loss on training batch is 0.00063584.
After 18760 training step(s), loss on training batch is 0.000812061.
After 18761 training step(s), loss on training batch is 0.000441954.
After 18762 training step(s), loss on training batch is 0.000344882.
After 18763 training step(s), loss on training batch is 0.000313826.
After 18764 training step(s), loss on training batch is 0.000387915.
After 18765 training step(s), loss on training batch is 0.000428111.
After 18766 training step(s), loss on training batch is 0.00036414.
After 18767 training step(s), loss on training batch is 0.000341724.
After 18768 training step(s), loss on training batch is 0.000474704.
After 18769 training step(s), loss on training batch is 0.000289187.
After 18770 training step(s), loss on training batch is 0.000356091.
After 18771 training step(s), loss on training batch is 0.000255047.
After 18772 training step(s), loss on training batch is 0.000262569.
After 18773 training step(s), loss on training batch is 0.000295135.
After 18774 training step(s), loss on training batch is 0.000301081.
After 18775 training step(s), loss on training batch is 0.000417426.
After 18776 training step(s), loss on training batch is 0.000365005.
After 18777 training step(s), loss on training batch is 0.000367668.
After 18778 training step(s), loss on training batch is 0.000295932.
After 18779 training step(s), loss on training batch is 0.000326165.
After 18780 training step(s), loss on training batch is 0.000295469.
After 18781 training step(s), loss on training batch is 0.000321472.
After 18782 training step(s), loss on training batch is 0.000298943.
After 18783 training step(s), loss on training batch is 0.000365185.
After 18784 training step(s), loss on training batch is 0.000350058.
After 18785 training step(s), loss on training batch is 0.000374117.
After 18786 training step(s), loss on training batch is 0.000494154.
After 18787 training step(s), loss on training batch is 0.000372226.
After 18788 training step(s), loss on training batch is 0.000305811.
After 18789 training step(s), loss on training batch is 0.000348786.
After 18790 training step(s), loss on training batch is 0.000325113.
After 18791 training step(s), loss on training batch is 0.000299903.
After 18792 training step(s), loss on training batch is 0.000304917.
After 18793 training step(s), loss on training batch is 0.000310368.
After 18794 training step(s), loss on training batch is 0.000257037.
After 18795 training step(s), loss on training batch is 0.000402675.
After 18796 training step(s), loss on training batch is 0.000410408.
After 18797 training step(s), loss on training batch is 0.000230539.
After 18798 training step(s), loss on training batch is 0.000249226.
After 18799 training step(s), loss on training batch is 0.000309243.
After 18800 training step(s), loss on training batch is 0.000323432.
After 18801 training step(s), loss on training batch is 0.000474331.
After 18802 training step(s), loss on training batch is 0.000552425.
After 18803 training step(s), loss on training batch is 0.000350918.
After 18804 training step(s), loss on training batch is 0.000486001.
After 18805 training step(s), loss on training batch is 0.000849816.
After 18806 training step(s), loss on training batch is 0.000613207.
After 18807 training step(s), loss on training batch is 0.000359295.
After 18808 training step(s), loss on training batch is 0.000436748.
After 18809 training step(s), loss on training batch is 0.000513586.
After 18810 training step(s), loss on training batch is 0.00053785.
After 18811 training step(s), loss on training batch is 0.000361761.
After 18812 training step(s), loss on training batch is 0.000294157.
After 18813 training step(s), loss on training batch is 0.000359071.
After 18814 training step(s), loss on training batch is 0.000373042.
After 18815 training step(s), loss on training batch is 0.000351912.
After 18816 training step(s), loss on training batch is 0.000304215.
After 18817 training step(s), loss on training batch is 0.000385762.
After 18818 training step(s), loss on training batch is 0.000324699.
After 18819 training step(s), loss on training batch is 0.000404307.
After 18820 training step(s), loss on training batch is 0.000366579.
After 18821 training step(s), loss on training batch is 0.000279158.
After 18822 training step(s), loss on training batch is 0.000369542.
After 18823 training step(s), loss on training batch is 0.000338327.
After 18824 training step(s), loss on training batch is 0.000294832.
After 18825 training step(s), loss on training batch is 0.000296579.
After 18826 training step(s), loss on training batch is 0.000346969.
After 18827 training step(s), loss on training batch is 0.000285751.
After 18828 training step(s), loss on training batch is 0.000742059.
After 18829 training step(s), loss on training batch is 0.000541888.
After 18830 training step(s), loss on training batch is 0.000491896.
After 18831 training step(s), loss on training batch is 0.000495292.
After 18832 training step(s), loss on training batch is 0.000567047.
After 18833 training step(s), loss on training batch is 0.00063417.
After 18834 training step(s), loss on training batch is 0.000561273.
After 18835 training step(s), loss on training batch is 0.000436718.
After 18836 training step(s), loss on training batch is 0.000638934.
After 18837 training step(s), loss on training batch is 0.00060179.
After 18838 training step(s), loss on training batch is 0.000461718.
After 18839 training step(s), loss on training batch is 0.000450651.
After 18840 training step(s), loss on training batch is 0.000972385.
After 18841 training step(s), loss on training batch is 0.000463617.
After 18842 training step(s), loss on training batch is 0.000530959.
After 18843 training step(s), loss on training batch is 0.000537774.
After 18844 training step(s), loss on training batch is 0.000606277.
After 18845 training step(s), loss on training batch is 0.000464789.
After 18846 training step(s), loss on training batch is 0.000569474.
After 18847 training step(s), loss on training batch is 0.000485695.
After 18848 training step(s), loss on training batch is 0.000519971.
After 18849 training step(s), loss on training batch is 0.000567148.
After 18850 training step(s), loss on training batch is 0.000548726.
After 18851 training step(s), loss on training batch is 0.000530879.
After 18852 training step(s), loss on training batch is 0.000498744.
After 18853 training step(s), loss on training batch is 0.000544493.
After 18854 training step(s), loss on training batch is 0.000412302.
After 18855 training step(s), loss on training batch is 0.000523762.
After 18856 training step(s), loss on training batch is 0.000947228.
After 18857 training step(s), loss on training batch is 0.00081403.
After 18858 training step(s), loss on training batch is 0.000879087.
After 18859 training step(s), loss on training batch is 0.000872038.
After 18860 training step(s), loss on training batch is 0.000767409.
After 18861 training step(s), loss on training batch is 0.000804714.
After 18862 training step(s), loss on training batch is 0.000945607.
After 18863 training step(s), loss on training batch is 0.000848396.
After 18864 training step(s), loss on training batch is 0.000772346.
After 18865 training step(s), loss on training batch is 0.000979968.
After 18866 training step(s), loss on training batch is 0.000758244.
After 18867 training step(s), loss on training batch is 0.000781797.
After 18868 training step(s), loss on training batch is 0.000887908.
After 18869 training step(s), loss on training batch is 0.00128619.
After 18870 training step(s), loss on training batch is 0.00283466.
After 18871 training step(s), loss on training batch is 0.0013286.
After 18872 training step(s), loss on training batch is 0.000907399.
After 18873 training step(s), loss on training batch is 0.00107563.
After 18874 training step(s), loss on training batch is 0.000944299.
After 18875 training step(s), loss on training batch is 0.000901841.
After 18876 training step(s), loss on training batch is 0.000846431.
After 18877 training step(s), loss on training batch is 0.00112185.
After 18878 training step(s), loss on training batch is 0.000823127.
After 18879 training step(s), loss on training batch is 0.000804341.
After 18880 training step(s), loss on training batch is 0.00078687.
After 18881 training step(s), loss on training batch is 0.000585379.
After 18882 training step(s), loss on training batch is 0.000369641.
After 18883 training step(s), loss on training batch is 0.000299712.
After 18884 training step(s), loss on training batch is 0.000344957.
After 18885 training step(s), loss on training batch is 0.000343957.
After 18886 training step(s), loss on training batch is 0.000438011.
After 18887 training step(s), loss on training batch is 0.000514607.
After 18888 training step(s), loss on training batch is 0.000385879.
After 18889 training step(s), loss on training batch is 0.00036235.
After 18890 training step(s), loss on training batch is 0.000319998.
After 18891 training step(s), loss on training batch is 0.000265874.
After 18892 training step(s), loss on training batch is 0.000297448.
After 18893 training step(s), loss on training batch is 0.000305854.
After 18894 training step(s), loss on training batch is 0.000305284.
After 18895 training step(s), loss on training batch is 0.000332571.
After 18896 training step(s), loss on training batch is 0.000337743.
After 18897 training step(s), loss on training batch is 0.000279095.
After 18898 training step(s), loss on training batch is 0.000262335.
After 18899 training step(s), loss on training batch is 0.000272766.
After 18900 training step(s), loss on training batch is 0.000265023.
After 18901 training step(s), loss on training batch is 0.000555855.
After 18902 training step(s), loss on training batch is 0.000430008.
After 18903 training step(s), loss on training batch is 0.000482922.
After 18904 training step(s), loss on training batch is 0.000572086.
After 18905 training step(s), loss on training batch is 0.000548672.
After 18906 training step(s), loss on training batch is 0.000547851.
After 18907 training step(s), loss on training batch is 0.000510113.
After 18908 training step(s), loss on training batch is 0.000308867.
After 18909 training step(s), loss on training batch is 0.000343838.
After 18910 training step(s), loss on training batch is 0.000366428.
After 18911 training step(s), loss on training batch is 0.000431253.
After 18912 training step(s), loss on training batch is 0.000435149.
After 18913 training step(s), loss on training batch is 0.000449294.
After 18914 training step(s), loss on training batch is 0.000593782.
After 18915 training step(s), loss on training batch is 0.00127363.
After 18916 training step(s), loss on training batch is 0.000915041.
After 18917 training step(s), loss on training batch is 0.000716715.
After 18918 training step(s), loss on training batch is 0.000552982.
After 18919 training step(s), loss on training batch is 0.000363683.
After 18920 training step(s), loss on training batch is 0.000342459.
After 18921 training step(s), loss on training batch is 0.000355033.
After 18922 training step(s), loss on training batch is 0.000316179.
After 18923 training step(s), loss on training batch is 0.000490528.
After 18924 training step(s), loss on training batch is 0.000378559.
After 18925 training step(s), loss on training batch is 0.000364305.
After 18926 training step(s), loss on training batch is 0.000362106.
After 18927 training step(s), loss on training batch is 0.000640233.
After 18928 training step(s), loss on training batch is 0.00152756.
After 18929 training step(s), loss on training batch is 0.000581739.
After 18930 training step(s), loss on training batch is 0.000453297.
After 18931 training step(s), loss on training batch is 0.00052485.
After 18932 training step(s), loss on training batch is 0.000456556.
After 18933 training step(s), loss on training batch is 0.000402615.
After 18934 training step(s), loss on training batch is 0.00031879.
After 18935 training step(s), loss on training batch is 0.000294439.
After 18936 training step(s), loss on training batch is 0.00029605.
After 18937 training step(s), loss on training batch is 0.000332956.
After 18938 training step(s), loss on training batch is 0.000333196.
After 18939 training step(s), loss on training batch is 0.000374655.
After 18940 training step(s), loss on training batch is 0.000426679.
After 18941 training step(s), loss on training batch is 0.000341105.
After 18942 training step(s), loss on training batch is 0.000420792.
After 18943 training step(s), loss on training batch is 0.000410264.
After 18944 training step(s), loss on training batch is 0.000462182.
After 18945 training step(s), loss on training batch is 0.000316802.
After 18946 training step(s), loss on training batch is 0.000413845.
After 18947 training step(s), loss on training batch is 0.000328658.
After 18948 training step(s), loss on training batch is 0.000305385.
After 18949 training step(s), loss on training batch is 0.000350187.
After 18950 training step(s), loss on training batch is 0.000454623.
After 18951 training step(s), loss on training batch is 0.000425147.
After 18952 training step(s), loss on training batch is 0.000371216.
After 18953 training step(s), loss on training batch is 0.00037678.
After 18954 training step(s), loss on training batch is 0.000353956.
After 18955 training step(s), loss on training batch is 0.000328784.
After 18956 training step(s), loss on training batch is 0.000436433.
After 18957 training step(s), loss on training batch is 0.000531942.
After 18958 training step(s), loss on training batch is 0.000338402.
After 18959 training step(s), loss on training batch is 0.000367761.
After 18960 training step(s), loss on training batch is 0.000469284.
After 18961 training step(s), loss on training batch is 0.000324754.
After 18962 training step(s), loss on training batch is 0.000302672.
After 18963 training step(s), loss on training batch is 0.000308663.
After 18964 training step(s), loss on training batch is 0.000424896.
After 18965 training step(s), loss on training batch is 0.000383063.
After 18966 training step(s), loss on training batch is 0.000379623.
After 18967 training step(s), loss on training batch is 0.000368593.
After 18968 training step(s), loss on training batch is 0.000349232.
After 18969 training step(s), loss on training batch is 0.000407964.
After 18970 training step(s), loss on training batch is 0.000333083.
After 18971 training step(s), loss on training batch is 0.000325929.
After 18972 training step(s), loss on training batch is 0.000327334.
After 18973 training step(s), loss on training batch is 0.000311791.
After 18974 training step(s), loss on training batch is 0.000291835.
After 18975 training step(s), loss on training batch is 0.000356772.
After 18976 training step(s), loss on training batch is 0.000344899.
After 18977 training step(s), loss on training batch is 0.000441603.
After 18978 training step(s), loss on training batch is 0.000351142.
After 18979 training step(s), loss on training batch is 0.000305552.
After 18980 training step(s), loss on training batch is 0.000341395.
After 18981 training step(s), loss on training batch is 0.000325353.
After 18982 training step(s), loss on training batch is 0.000342948.
After 18983 training step(s), loss on training batch is 0.000622949.
After 18984 training step(s), loss on training batch is 0.000727169.
After 18985 training step(s), loss on training batch is 0.000838833.
After 18986 training step(s), loss on training batch is 0.000566044.
After 18987 training step(s), loss on training batch is 0.000640075.
After 18988 training step(s), loss on training batch is 0.000542827.
After 18989 training step(s), loss on training batch is 0.000559934.
After 18990 training step(s), loss on training batch is 0.000541011.
After 18991 training step(s), loss on training batch is 0.000631153.
After 18992 training step(s), loss on training batch is 0.000657005.
After 18993 training step(s), loss on training batch is 0.00053445.
After 18994 training step(s), loss on training batch is 0.000521494.
After 18995 training step(s), loss on training batch is 0.000626974.
After 18996 training step(s), loss on training batch is 0.000615253.
After 18997 training step(s), loss on training batch is 0.000564999.
After 18998 training step(s), loss on training batch is 0.000596092.
After 18999 training step(s), loss on training batch is 0.000756393.
After 19000 training step(s), loss on training batch is 0.000617866.
After 19001 training step(s), loss on training batch is 0.000540646.
After 19002 training step(s), loss on training batch is 0.000629068.
After 19003 training step(s), loss on training batch is 0.000545899.
After 19004 training step(s), loss on training batch is 0.000540267.
After 19005 training step(s), loss on training batch is 0.000469492.
After 19006 training step(s), loss on training batch is 0.000606035.
After 19007 training step(s), loss on training batch is 0.000719727.
After 19008 training step(s), loss on training batch is 0.000613826.
After 19009 training step(s), loss on training batch is 0.000492261.
After 19010 training step(s), loss on training batch is 0.000700261.
After 19011 training step(s), loss on training batch is 0.000581906.
After 19012 training step(s), loss on training batch is 0.000553149.
After 19013 training step(s), loss on training batch is 0.000468961.
After 19014 training step(s), loss on training batch is 0.000483898.
After 19015 training step(s), loss on training batch is 0.000457456.
After 19016 training step(s), loss on training batch is 0.000483138.
After 19017 training step(s), loss on training batch is 0.000485551.
After 19018 training step(s), loss on training batch is 0.000626625.
After 19019 training step(s), loss on training batch is 0.000841214.
After 19020 training step(s), loss on training batch is 0.000879487.
After 19021 training step(s), loss on training batch is 0.000925079.
After 19022 training step(s), loss on training batch is 0.00055367.
After 19023 training step(s), loss on training batch is 0.000526502.
After 19024 training step(s), loss on training batch is 0.000542713.
After 19025 training step(s), loss on training batch is 0.00057924.
After 19026 training step(s), loss on training batch is 0.000625655.
After 19027 training step(s), loss on training batch is 0.000534403.
After 19028 training step(s), loss on training batch is 0.000577391.
After 19029 training step(s), loss on training batch is 0.000591556.
After 19030 training step(s), loss on training batch is 0.000654167.
After 19031 training step(s), loss on training batch is 0.000718422.
After 19032 training step(s), loss on training batch is 0.000510051.
After 19033 training step(s), loss on training batch is 0.000539624.
After 19034 training step(s), loss on training batch is 0.000498975.
After 19035 training step(s), loss on training batch is 0.000463753.
After 19036 training step(s), loss on training batch is 0.00066895.
After 19037 training step(s), loss on training batch is 0.000940985.
After 19038 training step(s), loss on training batch is 0.000457514.
After 19039 training step(s), loss on training batch is 0.000533939.
After 19040 training step(s), loss on training batch is 0.000551757.
After 19041 training step(s), loss on training batch is 0.000513749.
After 19042 training step(s), loss on training batch is 0.000501369.
After 19043 training step(s), loss on training batch is 0.000569172.
After 19044 training step(s), loss on training batch is 0.000535517.
After 19045 training step(s), loss on training batch is 0.000559405.
After 19046 training step(s), loss on training batch is 0.000689627.
After 19047 training step(s), loss on training batch is 0.000542606.
After 19048 training step(s), loss on training batch is 0.00053386.
After 19049 training step(s), loss on training batch is 0.000577745.
After 19050 training step(s), loss on training batch is 0.000582641.
After 19051 training step(s), loss on training batch is 0.000492361.
After 19052 training step(s), loss on training batch is 0.000464765.
After 19053 training step(s), loss on training batch is 0.00060634.
After 19054 training step(s), loss on training batch is 0.000539087.
After 19055 training step(s), loss on training batch is 0.00059684.
After 19056 training step(s), loss on training batch is 0.000545255.
After 19057 training step(s), loss on training batch is 0.000486305.
After 19058 training step(s), loss on training batch is 0.000595827.
After 19059 training step(s), loss on training batch is 0.000778396.
After 19060 training step(s), loss on training batch is 0.000512375.
After 19061 training step(s), loss on training batch is 0.000457204.
After 19062 training step(s), loss on training batch is 0.000455856.
After 19063 training step(s), loss on training batch is 0.000477865.
After 19064 training step(s), loss on training batch is 0.000446964.
After 19065 training step(s), loss on training batch is 0.000604493.
After 19066 training step(s), loss on training batch is 0.000944378.
After 19067 training step(s), loss on training batch is 0.000986593.
After 19068 training step(s), loss on training batch is 0.000910861.
After 19069 training step(s), loss on training batch is 0.000946041.
After 19070 training step(s), loss on training batch is 0.000913048.
After 19071 training step(s), loss on training batch is 0.000865444.
After 19072 training step(s), loss on training batch is 0.000900027.
After 19073 training step(s), loss on training batch is 0.00109796.
After 19074 training step(s), loss on training batch is 0.00105734.
After 19075 training step(s), loss on training batch is 0.00106656.
After 19076 training step(s), loss on training batch is 0.000882006.
After 19077 training step(s), loss on training batch is 0.000876818.
After 19078 training step(s), loss on training batch is 0.000915399.
After 19079 training step(s), loss on training batch is 0.00083789.
After 19080 training step(s), loss on training batch is 0.000825043.
After 19081 training step(s), loss on training batch is 0.000791885.
After 19082 training step(s), loss on training batch is 0.000839095.
After 19083 training step(s), loss on training batch is 0.000894909.
After 19084 training step(s), loss on training batch is 0.000814716.
After 19085 training step(s), loss on training batch is 0.000832463.
After 19086 training step(s), loss on training batch is 0.00101512.
After 19087 training step(s), loss on training batch is 0.00081153.
After 19088 training step(s), loss on training batch is 0.00097659.
After 19089 training step(s), loss on training batch is 0.000830638.
After 19090 training step(s), loss on training batch is 0.000819746.
After 19091 training step(s), loss on training batch is 0.000846888.
After 19092 training step(s), loss on training batch is 0.000836082.
After 19093 training step(s), loss on training batch is 0.000782851.
After 19094 training step(s), loss on training batch is 0.00091086.
After 19095 training step(s), loss on training batch is 0.000899024.
After 19096 training step(s), loss on training batch is 0.00100119.
After 19097 training step(s), loss on training batch is 0.000840772.
After 19098 training step(s), loss on training batch is 0.000841145.
After 19099 training step(s), loss on training batch is 0.000787371.
After 19100 training step(s), loss on training batch is 0.000848705.
After 19101 training step(s), loss on training batch is 0.00075291.
After 19102 training step(s), loss on training batch is 0.000923894.
After 19103 training step(s), loss on training batch is 0.000820584.
After 19104 training step(s), loss on training batch is 0.00100793.
After 19105 training step(s), loss on training batch is 0.00103458.
After 19106 training step(s), loss on training batch is 0.000992651.
After 19107 training step(s), loss on training batch is 0.00118463.
After 19108 training step(s), loss on training batch is 0.00101232.
After 19109 training step(s), loss on training batch is 0.00635978.
After 19110 training step(s), loss on training batch is 0.00140131.
After 19111 training step(s), loss on training batch is 0.00125909.
After 19112 training step(s), loss on training batch is 0.00124856.
After 19113 training step(s), loss on training batch is 0.00119655.
After 19114 training step(s), loss on training batch is 0.00121436.
After 19115 training step(s), loss on training batch is 0.00104355.
After 19116 training step(s), loss on training batch is 0.000480619.
After 19117 training step(s), loss on training batch is 0.000431828.
After 19118 training step(s), loss on training batch is 0.000414754.
After 19119 training step(s), loss on training batch is 0.000430475.
After 19120 training step(s), loss on training batch is 0.000449379.
After 19121 training step(s), loss on training batch is 0.000461008.
After 19122 training step(s), loss on training batch is 0.000469756.
After 19123 training step(s), loss on training batch is 0.000464631.
After 19124 training step(s), loss on training batch is 0.000449504.
After 19125 training step(s), loss on training batch is 0.000411391.
After 19126 training step(s), loss on training batch is 0.000859596.
After 19127 training step(s), loss on training batch is 0.000893169.
After 19128 training step(s), loss on training batch is 0.000963658.
After 19129 training step(s), loss on training batch is 0.000863988.
After 19130 training step(s), loss on training batch is 0.00138022.
After 19131 training step(s), loss on training batch is 0.00124198.
After 19132 training step(s), loss on training batch is 0.00100322.
After 19133 training step(s), loss on training batch is 0.000961435.
After 19134 training step(s), loss on training batch is 0.000892385.
After 19135 training step(s), loss on training batch is 0.000849331.
After 19136 training step(s), loss on training batch is 0.0008796.
After 19137 training step(s), loss on training batch is 0.000896583.
After 19138 training step(s), loss on training batch is 0.00101198.
After 19139 training step(s), loss on training batch is 0.000860411.
After 19140 training step(s), loss on training batch is 0.000852894.
After 19141 training step(s), loss on training batch is 0.000855454.
After 19142 training step(s), loss on training batch is 0.00150722.
After 19143 training step(s), loss on training batch is 0.000303558.
After 19144 training step(s), loss on training batch is 0.000344259.
After 19145 training step(s), loss on training batch is 0.000320805.
After 19146 training step(s), loss on training batch is 0.000405593.
After 19147 training step(s), loss on training batch is 0.00037513.
After 19148 training step(s), loss on training batch is 0.000353613.
After 19149 training step(s), loss on training batch is 0.000314063.
After 19150 training step(s), loss on training batch is 0.000293582.
After 19151 training step(s), loss on training batch is 0.000464803.
After 19152 training step(s), loss on training batch is 0.000369591.
After 19153 training step(s), loss on training batch is 0.000376595.
After 19154 training step(s), loss on training batch is 0.000340038.
After 19155 training step(s), loss on training batch is 0.000452293.
After 19156 training step(s), loss on training batch is 0.000349458.
After 19157 training step(s), loss on training batch is 0.000301211.
After 19158 training step(s), loss on training batch is 0.000307972.
After 19159 training step(s), loss on training batch is 0.000671524.
After 19160 training step(s), loss on training batch is 0.000838468.
After 19161 training step(s), loss on training batch is 0.000437118.
After 19162 training step(s), loss on training batch is 0.000349469.
After 19163 training step(s), loss on training batch is 0.000310422.
After 19164 training step(s), loss on training batch is 0.00039457.
After 19165 training step(s), loss on training batch is 0.00042594.
After 19166 training step(s), loss on training batch is 0.000365696.
After 19167 training step(s), loss on training batch is 0.000340787.
After 19168 training step(s), loss on training batch is 0.000470571.
After 19169 training step(s), loss on training batch is 0.00028917.
After 19170 training step(s), loss on training batch is 0.000359753.
After 19171 training step(s), loss on training batch is 0.000256643.
After 19172 training step(s), loss on training batch is 0.000262297.
After 19173 training step(s), loss on training batch is 0.000287555.
After 19174 training step(s), loss on training batch is 0.000295271.
After 19175 training step(s), loss on training batch is 0.00043274.
After 19176 training step(s), loss on training batch is 0.000373748.
After 19177 training step(s), loss on training batch is 0.000363789.
After 19178 training step(s), loss on training batch is 0.000292464.
After 19179 training step(s), loss on training batch is 0.000328283.
After 19180 training step(s), loss on training batch is 0.000292311.
After 19181 training step(s), loss on training batch is 0.000322996.
After 19182 training step(s), loss on training batch is 0.000295371.
After 19183 training step(s), loss on training batch is 0.000358949.
After 19184 training step(s), loss on training batch is 0.000349786.
After 19185 training step(s), loss on training batch is 0.000372991.
After 19186 training step(s), loss on training batch is 0.000486696.
After 19187 training step(s), loss on training batch is 0.000367439.
After 19188 training step(s), loss on training batch is 0.000303859.
After 19189 training step(s), loss on training batch is 0.000348581.
After 19190 training step(s), loss on training batch is 0.000322246.
After 19191 training step(s), loss on training batch is 0.000294664.
After 19192 training step(s), loss on training batch is 0.000298898.
After 19193 training step(s), loss on training batch is 0.000301612.
After 19194 training step(s), loss on training batch is 0.00025552.
After 19195 training step(s), loss on training batch is 0.000403638.
After 19196 training step(s), loss on training batch is 0.000405915.
After 19197 training step(s), loss on training batch is 0.000232048.
After 19198 training step(s), loss on training batch is 0.000246714.
After 19199 training step(s), loss on training batch is 0.000303546.
After 19200 training step(s), loss on training batch is 0.000317168.
After 19201 training step(s), loss on training batch is 0.000475008.
After 19202 training step(s), loss on training batch is 0.000552956.
After 19203 training step(s), loss on training batch is 0.00035056.
After 19204 training step(s), loss on training batch is 0.000485961.
After 19205 training step(s), loss on training batch is 0.000846882.
After 19206 training step(s), loss on training batch is 0.000609698.
After 19207 training step(s), loss on training batch is 0.000358805.
After 19208 training step(s), loss on training batch is 0.00043928.
After 19209 training step(s), loss on training batch is 0.000510156.
After 19210 training step(s), loss on training batch is 0.000534249.
After 19211 training step(s), loss on training batch is 0.000362985.
After 19212 training step(s), loss on training batch is 0.000296468.
After 19213 training step(s), loss on training batch is 0.000361323.
After 19214 training step(s), loss on training batch is 0.000387614.
After 19215 training step(s), loss on training batch is 0.000364607.
After 19216 training step(s), loss on training batch is 0.000308505.
After 19217 training step(s), loss on training batch is 0.000397273.
After 19218 training step(s), loss on training batch is 0.000323689.
After 19219 training step(s), loss on training batch is 0.000416339.
After 19220 training step(s), loss on training batch is 0.000370997.
After 19221 training step(s), loss on training batch is 0.000279153.
After 19222 training step(s), loss on training batch is 0.000377721.
After 19223 training step(s), loss on training batch is 0.000336985.
After 19224 training step(s), loss on training batch is 0.000291228.
After 19225 training step(s), loss on training batch is 0.000295436.
After 19226 training step(s), loss on training batch is 0.000344586.
After 19227 training step(s), loss on training batch is 0.000286037.
After 19228 training step(s), loss on training batch is 0.000728273.
After 19229 training step(s), loss on training batch is 0.000536951.
After 19230 training step(s), loss on training batch is 0.000489894.
After 19231 training step(s), loss on training batch is 0.000497322.
After 19232 training step(s), loss on training batch is 0.00055627.
After 19233 training step(s), loss on training batch is 0.000623747.
After 19234 training step(s), loss on training batch is 0.000540545.
After 19235 training step(s), loss on training batch is 0.000440552.
After 19236 training step(s), loss on training batch is 0.000623928.
After 19237 training step(s), loss on training batch is 0.000587319.
After 19238 training step(s), loss on training batch is 0.000455464.
After 19239 training step(s), loss on training batch is 0.000449501.
After 19240 training step(s), loss on training batch is 0.000950781.
After 19241 training step(s), loss on training batch is 0.000461299.
After 19242 training step(s), loss on training batch is 0.00052906.
After 19243 training step(s), loss on training batch is 0.000536307.
After 19244 training step(s), loss on training batch is 0.000602171.
After 19245 training step(s), loss on training batch is 0.000461699.
After 19246 training step(s), loss on training batch is 0.000564847.
After 19247 training step(s), loss on training batch is 0.00048205.
After 19248 training step(s), loss on training batch is 0.000516747.
After 19249 training step(s), loss on training batch is 0.000563824.
After 19250 training step(s), loss on training batch is 0.000548062.
After 19251 training step(s), loss on training batch is 0.000540437.
After 19252 training step(s), loss on training batch is 0.000501631.
After 19253 training step(s), loss on training batch is 0.000551051.
After 19254 training step(s), loss on training batch is 0.000405214.
After 19255 training step(s), loss on training batch is 0.000518488.
After 19256 training step(s), loss on training batch is 0.000942699.
After 19257 training step(s), loss on training batch is 0.000812324.
After 19258 training step(s), loss on training batch is 0.000872096.
After 19259 training step(s), loss on training batch is 0.000873864.
After 19260 training step(s), loss on training batch is 0.00077287.
After 19261 training step(s), loss on training batch is 0.000806422.
After 19262 training step(s), loss on training batch is 0.00093433.
After 19263 training step(s), loss on training batch is 0.000838676.
After 19264 training step(s), loss on training batch is 0.000772093.
After 19265 training step(s), loss on training batch is 0.000967063.
After 19266 training step(s), loss on training batch is 0.000765834.
After 19267 training step(s), loss on training batch is 0.000794571.
After 19268 training step(s), loss on training batch is 0.000892494.
After 19269 training step(s), loss on training batch is 0.00129903.
After 19270 training step(s), loss on training batch is 0.00283837.
After 19271 training step(s), loss on training batch is 0.00132753.
After 19272 training step(s), loss on training batch is 0.000904756.
After 19273 training step(s), loss on training batch is 0.00107301.
After 19274 training step(s), loss on training batch is 0.000933111.
After 19275 training step(s), loss on training batch is 0.000884218.
After 19276 training step(s), loss on training batch is 0.000819517.
After 19277 training step(s), loss on training batch is 0.00111717.
After 19278 training step(s), loss on training batch is 0.000818504.
After 19279 training step(s), loss on training batch is 0.000793244.
After 19280 training step(s), loss on training batch is 0.000781128.
After 19281 training step(s), loss on training batch is 0.000563005.
After 19282 training step(s), loss on training batch is 0.00036022.
After 19283 training step(s), loss on training batch is 0.000269834.
After 19284 training step(s), loss on training batch is 0.000317951.
After 19285 training step(s), loss on training batch is 0.000334084.
After 19286 training step(s), loss on training batch is 0.000449165.
After 19287 training step(s), loss on training batch is 0.000533766.
After 19288 training step(s), loss on training batch is 0.000371243.
After 19289 training step(s), loss on training batch is 0.000334949.
After 19290 training step(s), loss on training batch is 0.000297656.
After 19291 training step(s), loss on training batch is 0.000250393.
After 19292 training step(s), loss on training batch is 0.000289272.
After 19293 training step(s), loss on training batch is 0.000291883.
After 19294 training step(s), loss on training batch is 0.00029116.
After 19295 training step(s), loss on training batch is 0.000319646.
After 19296 training step(s), loss on training batch is 0.000330154.
After 19297 training step(s), loss on training batch is 0.000268678.
After 19298 training step(s), loss on training batch is 0.000253074.
After 19299 training step(s), loss on training batch is 0.000265953.
After 19300 training step(s), loss on training batch is 0.000262241.
After 19301 training step(s), loss on training batch is 0.000573947.
After 19302 training step(s), loss on training batch is 0.000424457.
After 19303 training step(s), loss on training batch is 0.000472179.
After 19304 training step(s), loss on training batch is 0.000560288.
After 19305 training step(s), loss on training batch is 0.000539882.
After 19306 training step(s), loss on training batch is 0.000545163.
After 19307 training step(s), loss on training batch is 0.000504973.
After 19308 training step(s), loss on training batch is 0.000305243.
After 19309 training step(s), loss on training batch is 0.000340383.
After 19310 training step(s), loss on training batch is 0.000360055.
After 19311 training step(s), loss on training batch is 0.000419103.
After 19312 training step(s), loss on training batch is 0.000422064.
After 19313 training step(s), loss on training batch is 0.000422506.
After 19314 training step(s), loss on training batch is 0.000497385.
After 19315 training step(s), loss on training batch is 0.00122371.
After 19316 training step(s), loss on training batch is 0.000884378.
After 19317 training step(s), loss on training batch is 0.000716538.
After 19318 training step(s), loss on training batch is 0.000563635.
After 19319 training step(s), loss on training batch is 0.000356797.
After 19320 training step(s), loss on training batch is 0.000349289.
After 19321 training step(s), loss on training batch is 0.000362029.
After 19322 training step(s), loss on training batch is 0.000321465.
After 19323 training step(s), loss on training batch is 0.000501614.
After 19324 training step(s), loss on training batch is 0.000379937.
After 19325 training step(s), loss on training batch is 0.000367752.
After 19326 training step(s), loss on training batch is 0.000365462.
After 19327 training step(s), loss on training batch is 0.000624884.
After 19328 training step(s), loss on training batch is 0.00176246.
After 19329 training step(s), loss on training batch is 0.0005654.
After 19330 training step(s), loss on training batch is 0.000443544.
After 19331 training step(s), loss on training batch is 0.000512578.
After 19332 training step(s), loss on training batch is 0.000463087.
After 19333 training step(s), loss on training batch is 0.000398299.
After 19334 training step(s), loss on training batch is 0.000319901.
After 19335 training step(s), loss on training batch is 0.000293169.
After 19336 training step(s), loss on training batch is 0.000293153.
After 19337 training step(s), loss on training batch is 0.000334302.
After 19338 training step(s), loss on training batch is 0.000332314.
After 19339 training step(s), loss on training batch is 0.000372895.
After 19340 training step(s), loss on training batch is 0.000427092.
After 19341 training step(s), loss on training batch is 0.000342883.
After 19342 training step(s), loss on training batch is 0.000425327.
After 19343 training step(s), loss on training batch is 0.000419126.
After 19344 training step(s), loss on training batch is 0.000472394.
After 19345 training step(s), loss on training batch is 0.00032022.
After 19346 training step(s), loss on training batch is 0.000421754.
After 19347 training step(s), loss on training batch is 0.000328059.
After 19348 training step(s), loss on training batch is 0.000302822.
After 19349 training step(s), loss on training batch is 0.0003485.
After 19350 training step(s), loss on training batch is 0.000410318.
After 19351 training step(s), loss on training batch is 0.000403066.
After 19352 training step(s), loss on training batch is 0.000388128.
After 19353 training step(s), loss on training batch is 0.000384672.
After 19354 training step(s), loss on training batch is 0.00036771.
After 19355 training step(s), loss on training batch is 0.000334278.
After 19356 training step(s), loss on training batch is 0.000432984.
After 19357 training step(s), loss on training batch is 0.000513674.
After 19358 training step(s), loss on training batch is 0.000341688.
After 19359 training step(s), loss on training batch is 0.000372135.
After 19360 training step(s), loss on training batch is 0.000469616.
After 19361 training step(s), loss on training batch is 0.000325275.
After 19362 training step(s), loss on training batch is 0.000299825.
After 19363 training step(s), loss on training batch is 0.000308872.
After 19364 training step(s), loss on training batch is 0.000427049.
After 19365 training step(s), loss on training batch is 0.000382986.
After 19366 training step(s), loss on training batch is 0.000383127.
After 19367 training step(s), loss on training batch is 0.000365462.
After 19368 training step(s), loss on training batch is 0.000346075.
After 19369 training step(s), loss on training batch is 0.000408099.
After 19370 training step(s), loss on training batch is 0.000334326.
After 19371 training step(s), loss on training batch is 0.000324185.
After 19372 training step(s), loss on training batch is 0.000330762.
After 19373 training step(s), loss on training batch is 0.000316501.
After 19374 training step(s), loss on training batch is 0.00029716.
After 19375 training step(s), loss on training batch is 0.000357053.
After 19376 training step(s), loss on training batch is 0.000344599.
After 19377 training step(s), loss on training batch is 0.000451643.
After 19378 training step(s), loss on training batch is 0.000348503.
After 19379 training step(s), loss on training batch is 0.000307401.
After 19380 training step(s), loss on training batch is 0.000337374.
After 19381 training step(s), loss on training batch is 0.000324847.
After 19382 training step(s), loss on training batch is 0.000345537.
After 19383 training step(s), loss on training batch is 0.000598228.
After 19384 training step(s), loss on training batch is 0.00072018.
After 19385 training step(s), loss on training batch is 0.000838876.
After 19386 training step(s), loss on training batch is 0.000564124.
After 19387 training step(s), loss on training batch is 0.000632746.
After 19388 training step(s), loss on training batch is 0.000535632.
After 19389 training step(s), loss on training batch is 0.000560768.
After 19390 training step(s), loss on training batch is 0.000540218.
After 19391 training step(s), loss on training batch is 0.000633332.
After 19392 training step(s), loss on training batch is 0.000659879.
After 19393 training step(s), loss on training batch is 0.000534136.
After 19394 training step(s), loss on training batch is 0.000508085.
After 19395 training step(s), loss on training batch is 0.00063801.
After 19396 training step(s), loss on training batch is 0.000616322.
After 19397 training step(s), loss on training batch is 0.000557008.
After 19398 training step(s), loss on training batch is 0.000602445.
After 19399 training step(s), loss on training batch is 0.000751872.
After 19400 training step(s), loss on training batch is 0.000616046.
After 19401 training step(s), loss on training batch is 0.000538745.
After 19402 training step(s), loss on training batch is 0.000644039.
After 19403 training step(s), loss on training batch is 0.000527253.
After 19404 training step(s), loss on training batch is 0.000523466.
After 19405 training step(s), loss on training batch is 0.000455215.
After 19406 training step(s), loss on training batch is 0.000608923.
After 19407 training step(s), loss on training batch is 0.000723375.
After 19408 training step(s), loss on training batch is 0.000622061.
After 19409 training step(s), loss on training batch is 0.000488914.
After 19410 training step(s), loss on training batch is 0.000703564.
After 19411 training step(s), loss on training batch is 0.000567897.
After 19412 training step(s), loss on training batch is 0.000585784.
After 19413 training step(s), loss on training batch is 0.000444806.
After 19414 training step(s), loss on training batch is 0.000474346.
After 19415 training step(s), loss on training batch is 0.00043401.
After 19416 training step(s), loss on training batch is 0.000467839.
After 19417 training step(s), loss on training batch is 0.000474314.
After 19418 training step(s), loss on training batch is 0.000664096.
After 19419 training step(s), loss on training batch is 0.000903267.
After 19420 training step(s), loss on training batch is 0.000903167.
After 19421 training step(s), loss on training batch is 0.00100223.
After 19422 training step(s), loss on training batch is 0.000536771.
After 19423 training step(s), loss on training batch is 0.000528106.
After 19424 training step(s), loss on training batch is 0.000550811.
After 19425 training step(s), loss on training batch is 0.000593288.
After 19426 training step(s), loss on training batch is 0.000635509.
After 19427 training step(s), loss on training batch is 0.000545254.
After 19428 training step(s), loss on training batch is 0.000579693.
After 19429 training step(s), loss on training batch is 0.000596396.
After 19430 training step(s), loss on training batch is 0.000649837.
After 19431 training step(s), loss on training batch is 0.000702406.
After 19432 training step(s), loss on training batch is 0.000514518.
After 19433 training step(s), loss on training batch is 0.00053236.
After 19434 training step(s), loss on training batch is 0.000489387.
After 19435 training step(s), loss on training batch is 0.000457686.
After 19436 training step(s), loss on training batch is 0.000669256.
After 19437 training step(s), loss on training batch is 0.000958457.
After 19438 training step(s), loss on training batch is 0.000457092.
After 19439 training step(s), loss on training batch is 0.000533632.
After 19440 training step(s), loss on training batch is 0.000548791.
After 19441 training step(s), loss on training batch is 0.000506479.
After 19442 training step(s), loss on training batch is 0.00049428.
After 19443 training step(s), loss on training batch is 0.000560833.
After 19444 training step(s), loss on training batch is 0.00052443.
After 19445 training step(s), loss on training batch is 0.000551503.
After 19446 training step(s), loss on training batch is 0.00069981.
After 19447 training step(s), loss on training batch is 0.000534237.
After 19448 training step(s), loss on training batch is 0.000533126.
After 19449 training step(s), loss on training batch is 0.0005735.
After 19450 training step(s), loss on training batch is 0.000577954.
After 19451 training step(s), loss on training batch is 0.000486278.
After 19452 training step(s), loss on training batch is 0.000463561.
After 19453 training step(s), loss on training batch is 0.000598815.
After 19454 training step(s), loss on training batch is 0.000536632.
After 19455 training step(s), loss on training batch is 0.000594231.
After 19456 training step(s), loss on training batch is 0.00054592.
After 19457 training step(s), loss on training batch is 0.000486328.
After 19458 training step(s), loss on training batch is 0.000592288.
After 19459 training step(s), loss on training batch is 0.000747799.
After 19460 training step(s), loss on training batch is 0.000498438.
After 19461 training step(s), loss on training batch is 0.000460486.
After 19462 training step(s), loss on training batch is 0.000464026.
After 19463 training step(s), loss on training batch is 0.00048523.
After 19464 training step(s), loss on training batch is 0.000451578.
After 19465 training step(s), loss on training batch is 0.00058626.
After 19466 training step(s), loss on training batch is 0.00095212.
After 19467 training step(s), loss on training batch is 0.000985573.
After 19468 training step(s), loss on training batch is 0.000911699.
After 19469 training step(s), loss on training batch is 0.000941609.
After 19470 training step(s), loss on training batch is 0.00090845.
After 19471 training step(s), loss on training batch is 0.000859155.
After 19472 training step(s), loss on training batch is 0.000898653.
After 19473 training step(s), loss on training batch is 0.00109614.
After 19474 training step(s), loss on training batch is 0.00105871.
After 19475 training step(s), loss on training batch is 0.00106624.
After 19476 training step(s), loss on training batch is 0.000874812.
After 19477 training step(s), loss on training batch is 0.000871909.
After 19478 training step(s), loss on training batch is 0.000908436.
After 19479 training step(s), loss on training batch is 0.000794403.
After 19480 training step(s), loss on training batch is 0.000820714.
After 19481 training step(s), loss on training batch is 0.000769044.
After 19482 training step(s), loss on training batch is 0.000812758.
After 19483 training step(s), loss on training batch is 0.000907909.
After 19484 training step(s), loss on training batch is 0.000808949.
After 19485 training step(s), loss on training batch is 0.000816423.
After 19486 training step(s), loss on training batch is 0.0010465.
After 19487 training step(s), loss on training batch is 0.000797297.
After 19488 training step(s), loss on training batch is 0.000975782.
After 19489 training step(s), loss on training batch is 0.000824704.
After 19490 training step(s), loss on training batch is 0.000813766.
After 19491 training step(s), loss on training batch is 0.000842486.
After 19492 training step(s), loss on training batch is 0.000910675.
After 19493 training step(s), loss on training batch is 0.000785559.
After 19494 training step(s), loss on training batch is 0.000913629.
After 19495 training step(s), loss on training batch is 0.000957217.
After 19496 training step(s), loss on training batch is 0.00104675.
After 19497 training step(s), loss on training batch is 0.000825854.
After 19498 training step(s), loss on training batch is 0.000837898.
After 19499 training step(s), loss on training batch is 0.000775939.
After 19500 training step(s), loss on training batch is 0.000839108.
After 19501 training step(s), loss on training batch is 0.000748236.
After 19502 training step(s), loss on training batch is 0.00092171.
After 19503 training step(s), loss on training batch is 0.000819761.
After 19504 training step(s), loss on training batch is 0.00101553.
After 19505 training step(s), loss on training batch is 0.00106266.
After 19506 training step(s), loss on training batch is 0.00101671.
After 19507 training step(s), loss on training batch is 0.00120596.
After 19508 training step(s), loss on training batch is 0.00100521.
After 19509 training step(s), loss on training batch is 0.00670179.
After 19510 training step(s), loss on training batch is 0.00140695.
After 19511 training step(s), loss on training batch is 0.00127148.
After 19512 training step(s), loss on training batch is 0.00123846.
After 19513 training step(s), loss on training batch is 0.00118934.
After 19514 training step(s), loss on training batch is 0.00121781.
After 19515 training step(s), loss on training batch is 0.00105627.
After 19516 training step(s), loss on training batch is 0.000493625.
After 19517 training step(s), loss on training batch is 0.00041559.
After 19518 training step(s), loss on training batch is 0.000400769.
After 19519 training step(s), loss on training batch is 0.000418605.
After 19520 training step(s), loss on training batch is 0.000441428.
After 19521 training step(s), loss on training batch is 0.000449099.
After 19522 training step(s), loss on training batch is 0.000459873.
After 19523 training step(s), loss on training batch is 0.000451412.
After 19524 training step(s), loss on training batch is 0.000438139.
After 19525 training step(s), loss on training batch is 0.000400538.
After 19526 training step(s), loss on training batch is 0.000827731.
After 19527 training step(s), loss on training batch is 0.000862622.
After 19528 training step(s), loss on training batch is 0.000920822.
After 19529 training step(s), loss on training batch is 0.000821058.
After 19530 training step(s), loss on training batch is 0.00142811.
After 19531 training step(s), loss on training batch is 0.00127135.
After 19532 training step(s), loss on training batch is 0.000968119.
After 19533 training step(s), loss on training batch is 0.00098995.
After 19534 training step(s), loss on training batch is 0.000865452.
After 19535 training step(s), loss on training batch is 0.000835124.
After 19536 training step(s), loss on training batch is 0.000850458.
After 19537 training step(s), loss on training batch is 0.000853252.
After 19538 training step(s), loss on training batch is 0.00103642.
After 19539 training step(s), loss on training batch is 0.000850943.
After 19540 training step(s), loss on training batch is 0.000839187.
After 19541 training step(s), loss on training batch is 0.000840331.
After 19542 training step(s), loss on training batch is 0.00155101.
After 19543 training step(s), loss on training batch is 0.000316428.
After 19544 training step(s), loss on training batch is 0.000357181.
After 19545 training step(s), loss on training batch is 0.000333092.
After 19546 training step(s), loss on training batch is 0.000412604.
After 19547 training step(s), loss on training batch is 0.000367143.
After 19548 training step(s), loss on training batch is 0.000363622.
After 19549 training step(s), loss on training batch is 0.000313391.
After 19550 training step(s), loss on training batch is 0.000289635.
After 19551 training step(s), loss on training batch is 0.000469721.
After 19552 training step(s), loss on training batch is 0.000369408.
After 19553 training step(s), loss on training batch is 0.000376599.
After 19554 training step(s), loss on training batch is 0.000342866.
After 19555 training step(s), loss on training batch is 0.000455072.
After 19556 training step(s), loss on training batch is 0.000350717.
After 19557 training step(s), loss on training batch is 0.000302056.
After 19558 training step(s), loss on training batch is 0.000307089.
After 19559 training step(s), loss on training batch is 0.000603244.
After 19560 training step(s), loss on training batch is 0.000777104.
After 19561 training step(s), loss on training batch is 0.000439756.
After 19562 training step(s), loss on training batch is 0.000356189.
After 19563 training step(s), loss on training batch is 0.000319221.
After 19564 training step(s), loss on training batch is 0.000394627.
After 19565 training step(s), loss on training batch is 0.000420936.
After 19566 training step(s), loss on training batch is 0.000368028.
After 19567 training step(s), loss on training batch is 0.000338659.
After 19568 training step(s), loss on training batch is 0.000472649.
After 19569 training step(s), loss on training batch is 0.000287695.
After 19570 training step(s), loss on training batch is 0.000359365.
After 19571 training step(s), loss on training batch is 0.000255737.
After 19572 training step(s), loss on training batch is 0.000259019.
After 19573 training step(s), loss on training batch is 0.000295853.
After 19574 training step(s), loss on training batch is 0.000298844.
After 19575 training step(s), loss on training batch is 0.000411071.
After 19576 training step(s), loss on training batch is 0.000350472.
After 19577 training step(s), loss on training batch is 0.000361716.
After 19578 training step(s), loss on training batch is 0.000297504.
After 19579 training step(s), loss on training batch is 0.000325604.
After 19580 training step(s), loss on training batch is 0.00029503.
After 19581 training step(s), loss on training batch is 0.000320572.
After 19582 training step(s), loss on training batch is 0.00029206.
After 19583 training step(s), loss on training batch is 0.000352987.
After 19584 training step(s), loss on training batch is 0.000349638.
After 19585 training step(s), loss on training batch is 0.000371114.
After 19586 training step(s), loss on training batch is 0.000493197.
After 19587 training step(s), loss on training batch is 0.000364907.
After 19588 training step(s), loss on training batch is 0.000303179.
After 19589 training step(s), loss on training batch is 0.000348308.
After 19590 training step(s), loss on training batch is 0.000321182.
After 19591 training step(s), loss on training batch is 0.000294576.
After 19592 training step(s), loss on training batch is 0.000294734.
After 19593 training step(s), loss on training batch is 0.000300282.
After 19594 training step(s), loss on training batch is 0.000255194.
After 19595 training step(s), loss on training batch is 0.000402045.
After 19596 training step(s), loss on training batch is 0.000407508.
After 19597 training step(s), loss on training batch is 0.0002282.
After 19598 training step(s), loss on training batch is 0.000247437.
After 19599 training step(s), loss on training batch is 0.000308836.
After 19600 training step(s), loss on training batch is 0.000319401.
After 19601 training step(s), loss on training batch is 0.000470405.
After 19602 training step(s), loss on training batch is 0.000546167.
After 19603 training step(s), loss on training batch is 0.000346786.
After 19604 training step(s), loss on training batch is 0.000480941.
After 19605 training step(s), loss on training batch is 0.000834271.
After 19606 training step(s), loss on training batch is 0.000605357.
After 19607 training step(s), loss on training batch is 0.000354941.
After 19608 training step(s), loss on training batch is 0.000431059.
After 19609 training step(s), loss on training batch is 0.000507994.
After 19610 training step(s), loss on training batch is 0.000534882.
After 19611 training step(s), loss on training batch is 0.000359601.
After 19612 training step(s), loss on training batch is 0.000288909.
After 19613 training step(s), loss on training batch is 0.000346328.
After 19614 training step(s), loss on training batch is 0.00036574.
After 19615 training step(s), loss on training batch is 0.000345899.
After 19616 training step(s), loss on training batch is 0.00030217.
After 19617 training step(s), loss on training batch is 0.000375571.
After 19618 training step(s), loss on training batch is 0.000326651.
After 19619 training step(s), loss on training batch is 0.000396197.
After 19620 training step(s), loss on training batch is 0.000363944.
After 19621 training step(s), loss on training batch is 0.000281172.
After 19622 training step(s), loss on training batch is 0.000363002.
After 19623 training step(s), loss on training batch is 0.000343881.
After 19624 training step(s), loss on training batch is 0.000293339.
After 19625 training step(s), loss on training batch is 0.000293851.
After 19626 training step(s), loss on training batch is 0.00034389.
After 19627 training step(s), loss on training batch is 0.000275493.
After 19628 training step(s), loss on training batch is 0.000766299.
After 19629 training step(s), loss on training batch is 0.000561441.
After 19630 training step(s), loss on training batch is 0.000486044.
After 19631 training step(s), loss on training batch is 0.000488712.
After 19632 training step(s), loss on training batch is 0.000561595.
After 19633 training step(s), loss on training batch is 0.000629894.
After 19634 training step(s), loss on training batch is 0.000551149.
After 19635 training step(s), loss on training batch is 0.000434432.
After 19636 training step(s), loss on training batch is 0.000626419.
After 19637 training step(s), loss on training batch is 0.000590856.
After 19638 training step(s), loss on training batch is 0.000464307.
After 19639 training step(s), loss on training batch is 0.000452317.
After 19640 training step(s), loss on training batch is 0.000982475.
After 19641 training step(s), loss on training batch is 0.000448421.
After 19642 training step(s), loss on training batch is 0.000521197.
After 19643 training step(s), loss on training batch is 0.000527512.
After 19644 training step(s), loss on training batch is 0.000601357.
After 19645 training step(s), loss on training batch is 0.000451049.
After 19646 training step(s), loss on training batch is 0.000578483.
After 19647 training step(s), loss on training batch is 0.000477051.
After 19648 training step(s), loss on training batch is 0.000506542.
After 19649 training step(s), loss on training batch is 0.00058272.
After 19650 training step(s), loss on training batch is 0.000554082.
After 19651 training step(s), loss on training batch is 0.000561254.
After 19652 training step(s), loss on training batch is 0.000517792.
After 19653 training step(s), loss on training batch is 0.000558716.
After 19654 training step(s), loss on training batch is 0.000399528.
After 19655 training step(s), loss on training batch is 0.000526032.
After 19656 training step(s), loss on training batch is 0.000928435.
After 19657 training step(s), loss on training batch is 0.000784007.
After 19658 training step(s), loss on training batch is 0.000843471.
After 19659 training step(s), loss on training batch is 0.000857808.
After 19660 training step(s), loss on training batch is 0.00071609.
After 19661 training step(s), loss on training batch is 0.000759549.
After 19662 training step(s), loss on training batch is 0.000945933.
After 19663 training step(s), loss on training batch is 0.000817548.
After 19664 training step(s), loss on training batch is 0.000744175.
After 19665 training step(s), loss on training batch is 0.000976817.
After 19666 training step(s), loss on training batch is 0.000742452.
After 19667 training step(s), loss on training batch is 0.000768787.
After 19668 training step(s), loss on training batch is 0.000876636.
After 19669 training step(s), loss on training batch is 0.00135422.
After 19670 training step(s), loss on training batch is 0.00293159.
After 19671 training step(s), loss on training batch is 0.00130748.
After 19672 training step(s), loss on training batch is 0.000889827.
After 19673 training step(s), loss on training batch is 0.00107159.
After 19674 training step(s), loss on training batch is 0.000940561.
After 19675 training step(s), loss on training batch is 0.000895311.
After 19676 training step(s), loss on training batch is 0.000839003.
After 19677 training step(s), loss on training batch is 0.00108919.
After 19678 training step(s), loss on training batch is 0.000806389.
After 19679 training step(s), loss on training batch is 0.00078161.
After 19680 training step(s), loss on training batch is 0.000754632.
After 19681 training step(s), loss on training batch is 0.000754389.
After 19682 training step(s), loss on training batch is 0.000373033.
After 19683 training step(s), loss on training batch is 0.000252111.
After 19684 training step(s), loss on training batch is 0.000314427.
After 19685 training step(s), loss on training batch is 0.000333763.
After 19686 training step(s), loss on training batch is 0.000469306.
After 19687 training step(s), loss on training batch is 0.000551558.
After 19688 training step(s), loss on training batch is 0.000360355.
After 19689 training step(s), loss on training batch is 0.000321881.
After 19690 training step(s), loss on training batch is 0.000286551.
After 19691 training step(s), loss on training batch is 0.000241811.
After 19692 training step(s), loss on training batch is 0.000291117.
After 19693 training step(s), loss on training batch is 0.000284368.
After 19694 training step(s), loss on training batch is 0.000274861.
After 19695 training step(s), loss on training batch is 0.000304629.
After 19696 training step(s), loss on training batch is 0.000324484.
After 19697 training step(s), loss on training batch is 0.000258714.
After 19698 training step(s), loss on training batch is 0.000244036.
After 19699 training step(s), loss on training batch is 0.000262508.
After 19700 training step(s), loss on training batch is 0.000262196.
After 19701 training step(s), loss on training batch is 0.000574985.
After 19702 training step(s), loss on training batch is 0.000425452.
After 19703 training step(s), loss on training batch is 0.000459967.
After 19704 training step(s), loss on training batch is 0.00054073.
After 19705 training step(s), loss on training batch is 0.000519054.
After 19706 training step(s), loss on training batch is 0.000541062.
After 19707 training step(s), loss on training batch is 0.000512477.
After 19708 training step(s), loss on training batch is 0.000299328.
After 19709 training step(s), loss on training batch is 0.000333926.
After 19710 training step(s), loss on training batch is 0.000355315.
After 19711 training step(s), loss on training batch is 0.000409712.
After 19712 training step(s), loss on training batch is 0.000413603.
After 19713 training step(s), loss on training batch is 0.000432975.
After 19714 training step(s), loss on training batch is 0.000536969.
After 19715 training step(s), loss on training batch is 0.00124653.
After 19716 training step(s), loss on training batch is 0.000903866.
After 19717 training step(s), loss on training batch is 0.000717425.
After 19718 training step(s), loss on training batch is 0.000555679.
After 19719 training step(s), loss on training batch is 0.000360402.
After 19720 training step(s), loss on training batch is 0.000344919.
After 19721 training step(s), loss on training batch is 0.000356841.
After 19722 training step(s), loss on training batch is 0.000314839.
After 19723 training step(s), loss on training batch is 0.000487793.
After 19724 training step(s), loss on training batch is 0.000359152.
After 19725 training step(s), loss on training batch is 0.000348469.
After 19726 training step(s), loss on training batch is 0.000342216.
After 19727 training step(s), loss on training batch is 0.000690276.
After 19728 training step(s), loss on training batch is 0.00160091.
After 19729 training step(s), loss on training batch is 0.000560064.
After 19730 training step(s), loss on training batch is 0.000437849.
After 19731 training step(s), loss on training batch is 0.000543325.
After 19732 training step(s), loss on training batch is 0.000456829.
After 19733 training step(s), loss on training batch is 0.000393997.
After 19734 training step(s), loss on training batch is 0.000320456.
After 19735 training step(s), loss on training batch is 0.000283764.
After 19736 training step(s), loss on training batch is 0.000288825.
After 19737 training step(s), loss on training batch is 0.000328663.
After 19738 training step(s), loss on training batch is 0.000318155.
After 19739 training step(s), loss on training batch is 0.000363247.
After 19740 training step(s), loss on training batch is 0.000409791.
After 19741 training step(s), loss on training batch is 0.000329511.
After 19742 training step(s), loss on training batch is 0.00041233.
After 19743 training step(s), loss on training batch is 0.000403726.
After 19744 training step(s), loss on training batch is 0.000467487.
After 19745 training step(s), loss on training batch is 0.000319315.
After 19746 training step(s), loss on training batch is 0.000413581.
After 19747 training step(s), loss on training batch is 0.00032987.
After 19748 training step(s), loss on training batch is 0.000304186.
After 19749 training step(s), loss on training batch is 0.000347882.
After 19750 training step(s), loss on training batch is 0.000452384.
After 19751 training step(s), loss on training batch is 0.00042222.
After 19752 training step(s), loss on training batch is 0.000376824.
After 19753 training step(s), loss on training batch is 0.000381368.
After 19754 training step(s), loss on training batch is 0.000357106.
After 19755 training step(s), loss on training batch is 0.000333983.
After 19756 training step(s), loss on training batch is 0.000434966.
After 19757 training step(s), loss on training batch is 0.000519504.
After 19758 training step(s), loss on training batch is 0.000343648.
After 19759 training step(s), loss on training batch is 0.000373338.
After 19760 training step(s), loss on training batch is 0.000468173.
After 19761 training step(s), loss on training batch is 0.000325712.
After 19762 training step(s), loss on training batch is 0.00030225.
After 19763 training step(s), loss on training batch is 0.000309187.
After 19764 training step(s), loss on training batch is 0.000424793.
After 19765 training step(s), loss on training batch is 0.000387046.
After 19766 training step(s), loss on training batch is 0.000380937.
After 19767 training step(s), loss on training batch is 0.000366507.
After 19768 training step(s), loss on training batch is 0.000359524.
After 19769 training step(s), loss on training batch is 0.000409948.
After 19770 training step(s), loss on training batch is 0.0003367.
After 19771 training step(s), loss on training batch is 0.000325575.
After 19772 training step(s), loss on training batch is 0.000325337.
After 19773 training step(s), loss on training batch is 0.000311794.
After 19774 training step(s), loss on training batch is 0.000291377.
After 19775 training step(s), loss on training batch is 0.000359724.
After 19776 training step(s), loss on training batch is 0.000351747.
After 19777 training step(s), loss on training batch is 0.00043343.
After 19778 training step(s), loss on training batch is 0.000347774.
After 19779 training step(s), loss on training batch is 0.000313824.
After 19780 training step(s), loss on training batch is 0.000332927.
After 19781 training step(s), loss on training batch is 0.000326246.
After 19782 training step(s), loss on training batch is 0.000335373.
After 19783 training step(s), loss on training batch is 0.00057514.
After 19784 training step(s), loss on training batch is 0.000682209.
After 19785 training step(s), loss on training batch is 0.00080792.
After 19786 training step(s), loss on training batch is 0.000557976.
After 19787 training step(s), loss on training batch is 0.000629156.
After 19788 training step(s), loss on training batch is 0.000538039.
After 19789 training step(s), loss on training batch is 0.000556587.
After 19790 training step(s), loss on training batch is 0.000533589.
After 19791 training step(s), loss on training batch is 0.000627569.
After 19792 training step(s), loss on training batch is 0.000653718.
After 19793 training step(s), loss on training batch is 0.000530695.
After 19794 training step(s), loss on training batch is 0.000511526.
After 19795 training step(s), loss on training batch is 0.000623994.
After 19796 training step(s), loss on training batch is 0.000606112.
After 19797 training step(s), loss on training batch is 0.000547464.
After 19798 training step(s), loss on training batch is 0.000612403.
After 19799 training step(s), loss on training batch is 0.00075403.
After 19800 training step(s), loss on training batch is 0.000641141.
After 19801 training step(s), loss on training batch is 0.000541595.
After 19802 training step(s), loss on training batch is 0.00064666.
After 19803 training step(s), loss on training batch is 0.000527115.
After 19804 training step(s), loss on training batch is 0.000524851.
After 19805 training step(s), loss on training batch is 0.000456614.
After 19806 training step(s), loss on training batch is 0.000604295.
After 19807 training step(s), loss on training batch is 0.000717841.
After 19808 training step(s), loss on training batch is 0.00061606.
After 19809 training step(s), loss on training batch is 0.000486655.
After 19810 training step(s), loss on training batch is 0.000690633.
After 19811 training step(s), loss on training batch is 0.000573242.
After 19812 training step(s), loss on training batch is 0.000556755.
After 19813 training step(s), loss on training batch is 0.000458054.
After 19814 training step(s), loss on training batch is 0.000478224.
After 19815 training step(s), loss on training batch is 0.000449982.
After 19816 training step(s), loss on training batch is 0.00047656.
After 19817 training step(s), loss on training batch is 0.000478067.
After 19818 training step(s), loss on training batch is 0.000629093.
After 19819 training step(s), loss on training batch is 0.00084277.
After 19820 training step(s), loss on training batch is 0.000871454.
After 19821 training step(s), loss on training batch is 0.000924419.
After 19822 training step(s), loss on training batch is 0.000540641.
After 19823 training step(s), loss on training batch is 0.000532705.
After 19824 training step(s), loss on training batch is 0.000543221.
After 19825 training step(s), loss on training batch is 0.000585303.
After 19826 training step(s), loss on training batch is 0.000627987.
After 19827 training step(s), loss on training batch is 0.000534245.
After 19828 training step(s), loss on training batch is 0.000577798.
After 19829 training step(s), loss on training batch is 0.000589319.
After 19830 training step(s), loss on training batch is 0.0006481.
After 19831 training step(s), loss on training batch is 0.000695603.
After 19832 training step(s), loss on training batch is 0.000516804.
After 19833 training step(s), loss on training batch is 0.000545034.
After 19834 training step(s), loss on training batch is 0.000502016.
After 19835 training step(s), loss on training batch is 0.000466557.
After 19836 training step(s), loss on training batch is 0.00066219.
After 19837 training step(s), loss on training batch is 0.000937374.
After 19838 training step(s), loss on training batch is 0.000454932.
After 19839 training step(s), loss on training batch is 0.000533818.
After 19840 training step(s), loss on training batch is 0.000541737.
After 19841 training step(s), loss on training batch is 0.00050874.
After 19842 training step(s), loss on training batch is 0.000491615.
After 19843 training step(s), loss on training batch is 0.000556443.
After 19844 training step(s), loss on training batch is 0.000522257.
After 19845 training step(s), loss on training batch is 0.000549591.
After 19846 training step(s), loss on training batch is 0.000694822.
After 19847 training step(s), loss on training batch is 0.000534016.
After 19848 training step(s), loss on training batch is 0.000532167.
After 19849 training step(s), loss on training batch is 0.000573631.
After 19850 training step(s), loss on training batch is 0.000575747.
After 19851 training step(s), loss on training batch is 0.000491808.
After 19852 training step(s), loss on training batch is 0.000461502.
After 19853 training step(s), loss on training batch is 0.00059352.
After 19854 training step(s), loss on training batch is 0.000540676.
After 19855 training step(s), loss on training batch is 0.000596653.
After 19856 training step(s), loss on training batch is 0.000541142.
After 19857 training step(s), loss on training batch is 0.000482804.
After 19858 training step(s), loss on training batch is 0.000588803.
After 19859 training step(s), loss on training batch is 0.000747113.
After 19860 training step(s), loss on training batch is 0.00049662.
After 19861 training step(s), loss on training batch is 0.000459954.
After 19862 training step(s), loss on training batch is 0.000466321.
After 19863 training step(s), loss on training batch is 0.000486223.
After 19864 training step(s), loss on training batch is 0.00045081.
After 19865 training step(s), loss on training batch is 0.000578428.
After 19866 training step(s), loss on training batch is 0.000947676.
After 19867 training step(s), loss on training batch is 0.000983796.
After 19868 training step(s), loss on training batch is 0.000909669.
After 19869 training step(s), loss on training batch is 0.000926684.
After 19870 training step(s), loss on training batch is 0.000901803.
After 19871 training step(s), loss on training batch is 0.00085565.
After 19872 training step(s), loss on training batch is 0.000896003.
After 19873 training step(s), loss on training batch is 0.00108992.
After 19874 training step(s), loss on training batch is 0.001049.
After 19875 training step(s), loss on training batch is 0.00106159.
After 19876 training step(s), loss on training batch is 0.000875927.
After 19877 training step(s), loss on training batch is 0.00087507.
After 19878 training step(s), loss on training batch is 0.000906191.
After 19879 training step(s), loss on training batch is 0.000840561.
After 19880 training step(s), loss on training batch is 0.000824189.
After 19881 training step(s), loss on training batch is 0.000798371.
After 19882 training step(s), loss on training batch is 0.000843436.
After 19883 training step(s), loss on training batch is 0.000903772.
After 19884 training step(s), loss on training batch is 0.000805795.
After 19885 training step(s), loss on training batch is 0.00081704.
After 19886 training step(s), loss on training batch is 0.00103237.
After 19887 training step(s), loss on training batch is 0.000804958.
After 19888 training step(s), loss on training batch is 0.00096692.
After 19889 training step(s), loss on training batch is 0.000830091.
After 19890 training step(s), loss on training batch is 0.000818246.
After 19891 training step(s), loss on training batch is 0.000838776.
After 19892 training step(s), loss on training batch is 0.00083318.
After 19893 training step(s), loss on training batch is 0.000774524.
After 19894 training step(s), loss on training batch is 0.000905933.
After 19895 training step(s), loss on training batch is 0.000901161.
After 19896 training step(s), loss on training batch is 0.00100635.
After 19897 training step(s), loss on training batch is 0.000839414.
After 19898 training step(s), loss on training batch is 0.000837191.
After 19899 training step(s), loss on training batch is 0.000757564.
After 19900 training step(s), loss on training batch is 0.000827183.
After 19901 training step(s), loss on training batch is 0.000755839.
After 19902 training step(s), loss on training batch is 0.000927428.
After 19903 training step(s), loss on training batch is 0.000807875.
After 19904 training step(s), loss on training batch is 0.00100648.
After 19905 training step(s), loss on training batch is 0.00108315.
After 19906 training step(s), loss on training batch is 0.0010194.
After 19907 training step(s), loss on training batch is 0.00120284.
After 19908 training step(s), loss on training batch is 0.00100119.
After 19909 training step(s), loss on training batch is 0.00645956.
After 19910 training step(s), loss on training batch is 0.001405.
After 19911 training step(s), loss on training batch is 0.00129282.
After 19912 training step(s), loss on training batch is 0.00126987.
After 19913 training step(s), loss on training batch is 0.00120016.
After 19914 training step(s), loss on training batch is 0.00121154.
After 19915 training step(s), loss on training batch is 0.00104695.
After 19916 training step(s), loss on training batch is 0.000493471.
After 19917 training step(s), loss on training batch is 0.00044555.
After 19918 training step(s), loss on training batch is 0.000424992.
After 19919 training step(s), loss on training batch is 0.000435581.
After 19920 training step(s), loss on training batch is 0.000454121.
After 19921 training step(s), loss on training batch is 0.000467027.
After 19922 training step(s), loss on training batch is 0.000481876.
After 19923 training step(s), loss on training batch is 0.000477516.
After 19924 training step(s), loss on training batch is 0.000456011.
After 19925 training step(s), loss on training batch is 0.000414608.
After 19926 training step(s), loss on training batch is 0.000855072.
After 19927 training step(s), loss on training batch is 0.000880114.
After 19928 training step(s), loss on training batch is 0.000952939.
After 19929 training step(s), loss on training batch is 0.000850053.
After 19930 training step(s), loss on training batch is 0.00137445.
After 19931 training step(s), loss on training batch is 0.00123386.
After 19932 training step(s), loss on training batch is 0.000979752.
After 19933 training step(s), loss on training batch is 0.000953306.
After 19934 training step(s), loss on training batch is 0.00087534.
After 19935 training step(s), loss on training batch is 0.000829346.
After 19936 training step(s), loss on training batch is 0.000850799.
After 19937 training step(s), loss on training batch is 0.000866601.
After 19938 training step(s), loss on training batch is 0.000995922.
After 19939 training step(s), loss on training batch is 0.000845977.
After 19940 training step(s), loss on training batch is 0.00084.
After 19941 training step(s), loss on training batch is 0.00083759.
After 19942 training step(s), loss on training batch is 0.00151158.
After 19943 training step(s), loss on training batch is 0.000313441.
After 19944 training step(s), loss on training batch is 0.000351716.
After 19945 training step(s), loss on training batch is 0.000325847.
After 19946 training step(s), loss on training batch is 0.000409517.
After 19947 training step(s), loss on training batch is 0.000363336.
After 19948 training step(s), loss on training batch is 0.000360752.
After 19949 training step(s), loss on training batch is 0.000311459.
After 19950 training step(s), loss on training batch is 0.000290861.
After 19951 training step(s), loss on training batch is 0.000456665.
After 19952 training step(s), loss on training batch is 0.000368322.
After 19953 training step(s), loss on training batch is 0.000377045.
After 19954 training step(s), loss on training batch is 0.000338714.
After 19955 training step(s), loss on training batch is 0.000455998.
After 19956 training step(s), loss on training batch is 0.000339844.
After 19957 training step(s), loss on training batch is 0.000299435.
After 19958 training step(s), loss on training batch is 0.000309143.
After 19959 training step(s), loss on training batch is 0.000649492.
After 19960 training step(s), loss on training batch is 0.000805407.
After 19961 training step(s), loss on training batch is 0.000432626.
After 19962 training step(s), loss on training batch is 0.000350891.
After 19963 training step(s), loss on training batch is 0.000314972.
After 19964 training step(s), loss on training batch is 0.00039172.
After 19965 training step(s), loss on training batch is 0.000418535.
After 19966 training step(s), loss on training batch is 0.000355849.
After 19967 training step(s), loss on training batch is 0.000335678.
After 19968 training step(s), loss on training batch is 0.000461803.
After 19969 training step(s), loss on training batch is 0.000285313.
After 19970 training step(s), loss on training batch is 0.000353909.
After 19971 training step(s), loss on training batch is 0.000252171.
After 19972 training step(s), loss on training batch is 0.000256568.
After 19973 training step(s), loss on training batch is 0.000294091.
After 19974 training step(s), loss on training batch is 0.000299477.
After 19975 training step(s), loss on training batch is 0.000406561.
After 19976 training step(s), loss on training batch is 0.000351262.
After 19977 training step(s), loss on training batch is 0.00036105.
After 19978 training step(s), loss on training batch is 0.000296733.
After 19979 training step(s), loss on training batch is 0.000322675.
After 19980 training step(s), loss on training batch is 0.000293205.
After 19981 training step(s), loss on training batch is 0.000323014.
After 19982 training step(s), loss on training batch is 0.000298273.
After 19983 training step(s), loss on training batch is 0.000364674.
After 19984 training step(s), loss on training batch is 0.000345535.
After 19985 training step(s), loss on training batch is 0.000372977.
After 19986 training step(s), loss on training batch is 0.000480411.
After 19987 training step(s), loss on training batch is 0.000365724.
After 19988 training step(s), loss on training batch is 0.000302482.
After 19989 training step(s), loss on training batch is 0.000345372.
After 19990 training step(s), loss on training batch is 0.000321624.
After 19991 training step(s), loss on training batch is 0.000295338.
After 19992 training step(s), loss on training batch is 0.000298981.
After 19993 training step(s), loss on training batch is 0.00030455.
After 19994 training step(s), loss on training batch is 0.000255416.
After 19995 training step(s), loss on training batch is 0.000393478.
After 19996 training step(s), loss on training batch is 0.000402879.
After 19997 training step(s), loss on training batch is 0.000229298.
After 19998 training step(s), loss on training batch is 0.000245136.
After 19999 training step(s), loss on training batch is 0.000302549.
After 20000 training step(s), loss on training batch is 0.00031511.
2018-03-25 17:07:33.124917: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-03-25 17:07:33.139904: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-03-25 17:07:33.139919: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-03-25 17:07:33.139924: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-03-25 17:07:33.139928: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2018-03-25 17:07:33.328537: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-03-25 17:07:33.328845: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties: 
name: GeForce GTX 1080
major: 6 minor: 1 memoryClockRate (GHz) 1.8475
pciBusID 0000:01:00.0
Total memory: 7.92GiB
Free memory: 7.28GiB
2018-03-25 17:07:33.438523: W tensorflow/stream_executor/cuda/cuda_driver.cc:523] A non-primary context 0x563c3a765180 exists before initializing the StreamExecutor. We haven't verified StreamExecutor works with that.
2018-03-25 17:07:33.438700: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-03-25 17:07:33.439054: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 1 with properties: 
name: GeForce GTX 1080
major: 6 minor: 1 memoryClockRate (GHz) 1.8475
pciBusID 0000:0a:00.0
Total memory: 7.92GiB
Free memory: 7.80GiB
2018-03-25 17:07:33.439473: I tensorflow/core/common_runtime/gpu/gpu_device.cc:847] Peer access not supported between device ordinals 0 and 1
2018-03-25 17:07:33.439486: I tensorflow/core/common_runtime/gpu/gpu_device.cc:847] Peer access not supported between device ordinals 1 and 0
2018-03-25 17:07:33.439500: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0 1 
2018-03-25 17:07:33.439506: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y N 
2018-03-25 17:07:33.439509: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 1:   N Y 
2018-03-25 17:07:33.439516: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0)
2018-03-25 17:07:33.439520: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:1) -> (device: 1, name: GeForce GTX 1080, pci bus id: 0000:0a:00.0)
After 1 training step(s), loss on training batch is 1.3888.
After 2 training step(s), loss on training batch is 1.38881.
After 3 training step(s), loss on training batch is 1.38879.
After 4 training step(s), loss on training batch is 1.38877.
After 5 training step(s), loss on training batch is 1.38874.
After 6 training step(s), loss on training batch is 1.38873.
After 7 training step(s), loss on training batch is 1.3887.
After 8 training step(s), loss on training batch is 1.38869.
After 9 training step(s), loss on training batch is 1.38865.
After 10 training step(s), loss on training batch is 1.38863.
After 11 training step(s), loss on training batch is 1.3886.
After 12 training step(s), loss on training batch is 1.38857.
After 13 training step(s), loss on training batch is 1.38854.
After 14 training step(s), loss on training batch is 1.38849.
After 15 training step(s), loss on training batch is 1.38846.
After 16 training step(s), loss on training batch is 1.38842.
After 17 training step(s), loss on training batch is 1.38838.
After 18 training step(s), loss on training batch is 1.38834.
After 19 training step(s), loss on training batch is 1.3883.
After 20 training step(s), loss on training batch is 1.38829.
After 21 training step(s), loss on training batch is 1.38824.
After 22 training step(s), loss on training batch is 1.38821.
After 23 training step(s), loss on training batch is 1.38816.
After 24 training step(s), loss on training batch is 1.38814.
After 25 training step(s), loss on training batch is 1.38811.
After 26 training step(s), loss on training batch is 1.38806.
After 27 training step(s), loss on training batch is 1.38803.
After 28 training step(s), loss on training batch is 1.388.
After 29 training step(s), loss on training batch is 1.38794.
After 30 training step(s), loss on training batch is 1.38794.
After 31 training step(s), loss on training batch is 1.38789.
After 32 training step(s), loss on training batch is 1.38786.
After 33 training step(s), loss on training batch is 1.38784.
After 34 training step(s), loss on training batch is 1.38779.
After 35 training step(s), loss on training batch is 1.38777.
After 36 training step(s), loss on training batch is 1.38772.
After 37 training step(s), loss on training batch is 1.3877.
After 38 training step(s), loss on training batch is 1.38769.
After 39 training step(s), loss on training batch is 1.38764.
After 40 training step(s), loss on training batch is 1.3876.
After 41 training step(s), loss on training batch is 1.38759.
After 42 training step(s), loss on training batch is 1.38757.
After 43 training step(s), loss on training batch is 1.38753.
After 44 training step(s), loss on training batch is 1.38749.
After 45 training step(s), loss on training batch is 1.38748.
After 46 training step(s), loss on training batch is 1.38747.
After 47 training step(s), loss on training batch is 1.38743.
After 48 training step(s), loss on training batch is 1.38741.
After 49 training step(s), loss on training batch is 1.38737.
After 50 training step(s), loss on training batch is 1.38735.
After 51 training step(s), loss on training batch is 1.38733.
After 52 training step(s), loss on training batch is 1.3873.
After 53 training step(s), loss on training batch is 1.38729.
After 54 training step(s), loss on training batch is 1.38727.
After 55 training step(s), loss on training batch is 1.38724.
After 56 training step(s), loss on training batch is 1.38721.
After 57 training step(s), loss on training batch is 1.3872.
After 58 training step(s), loss on training batch is 1.38718.
After 59 training step(s), loss on training batch is 1.38716.
After 60 training step(s), loss on training batch is 1.38715.
After 61 training step(s), loss on training batch is 1.38713.
After 62 training step(s), loss on training batch is 1.38711.
After 63 training step(s), loss on training batch is 1.38709.
After 64 training step(s), loss on training batch is 1.38707.
After 65 training step(s), loss on training batch is 1.38704.
After 66 training step(s), loss on training batch is 1.38705.
After 67 training step(s), loss on training batch is 1.38703.
After 68 training step(s), loss on training batch is 1.387.
After 69 training step(s), loss on training batch is 1.387.
After 70 training step(s), loss on training batch is 1.38698.
After 71 training step(s), loss on training batch is 1.38696.
After 72 training step(s), loss on training batch is 1.38695.
After 73 training step(s), loss on training batch is 1.38692.
After 74 training step(s), loss on training batch is 1.38691.
After 75 training step(s), loss on training batch is 1.38691.
After 76 training step(s), loss on training batch is 1.38688.
After 77 training step(s), loss on training batch is 1.38688.
After 78 training step(s), loss on training batch is 1.38685.
After 79 training step(s), loss on training batch is 1.38686.
After 80 training step(s), loss on training batch is 1.38683.
After 81 training step(s), loss on training batch is 1.38684.
After 82 training step(s), loss on training batch is 1.38682.
After 83 training step(s), loss on training batch is 1.3868.
After 84 training step(s), loss on training batch is 1.3868.
After 85 training step(s), loss on training batch is 1.3868.
After 86 training step(s), loss on training batch is 1.38676.
After 87 training step(s), loss on training batch is 1.38676.
After 88 training step(s), loss on training batch is 1.38674.
After 89 training step(s), loss on training batch is 1.38674.
After 90 training step(s), loss on training batch is 1.38674.
After 91 training step(s), loss on training batch is 1.38671.
After 92 training step(s), loss on training batch is 1.38671.
After 93 training step(s), loss on training batch is 1.3867.
After 94 training step(s), loss on training batch is 1.38668.
After 95 training step(s), loss on training batch is 1.38667.
After 96 training step(s), loss on training batch is 1.38668.
After 97 training step(s), loss on training batch is 1.38667.
After 98 training step(s), loss on training batch is 1.38667.
After 99 training step(s), loss on training batch is 1.38665.
After 100 training step(s), loss on training batch is 1.38663.
After 101 training step(s), loss on training batch is 1.38664.
After 102 training step(s), loss on training batch is 1.38663.
After 103 training step(s), loss on training batch is 1.38662.
After 104 training step(s), loss on training batch is 1.38661.
After 105 training step(s), loss on training batch is 1.38661.
After 106 training step(s), loss on training batch is 1.38661.
After 107 training step(s), loss on training batch is 1.3866.
After 108 training step(s), loss on training batch is 1.38659.
After 109 training step(s), loss on training batch is 1.38656.
After 110 training step(s), loss on training batch is 1.38657.
After 111 training step(s), loss on training batch is 1.38656.
After 112 training step(s), loss on training batch is 1.38656.
After 113 training step(s), loss on training batch is 1.38656.
After 114 training step(s), loss on training batch is 1.38655.
After 115 training step(s), loss on training batch is 1.38655.
After 116 training step(s), loss on training batch is 1.38653.
After 117 training step(s), loss on training batch is 1.38654.
After 118 training step(s), loss on training batch is 1.38653.
After 119 training step(s), loss on training batch is 1.38652.
After 120 training step(s), loss on training batch is 1.38652.
After 121 training step(s), loss on training batch is 1.38651.
After 122 training step(s), loss on training batch is 1.3865.
After 123 training step(s), loss on training batch is 1.3865.
After 124 training step(s), loss on training batch is 1.38649.
After 125 training step(s), loss on training batch is 1.38649.
After 126 training step(s), loss on training batch is 1.3865.
After 127 training step(s), loss on training batch is 1.3865.
After 128 training step(s), loss on training batch is 1.38649.
After 129 training step(s), loss on training batch is 1.38648.
After 130 training step(s), loss on training batch is 1.38647.
After 131 training step(s), loss on training batch is 1.38646.
After 132 training step(s), loss on training batch is 1.38645.
After 133 training step(s), loss on training batch is 1.38647.
After 134 training step(s), loss on training batch is 1.38646.
After 135 training step(s), loss on training batch is 1.38646.
After 136 training step(s), loss on training batch is 1.38645.
After 137 training step(s), loss on training batch is 1.38644.
After 138 training step(s), loss on training batch is 1.38645.
After 139 training step(s), loss on training batch is 1.38644.
After 140 training step(s), loss on training batch is 1.38644.
After 141 training step(s), loss on training batch is 1.38642.
After 142 training step(s), loss on training batch is 1.38644.
After 143 training step(s), loss on training batch is 1.38644.
After 144 training step(s), loss on training batch is 1.38642.
After 145 training step(s), loss on training batch is 1.38642.
After 146 training step(s), loss on training batch is 1.38642.
After 147 training step(s), loss on training batch is 1.38642.
After 148 training step(s), loss on training batch is 1.38641.
After 149 training step(s), loss on training batch is 1.38642.
After 150 training step(s), loss on training batch is 1.38641.
After 151 training step(s), loss on training batch is 1.38639.
After 152 training step(s), loss on training batch is 1.38641.
After 153 training step(s), loss on training batch is 1.3864.
After 154 training step(s), loss on training batch is 1.3864.
After 155 training step(s), loss on training batch is 1.3864.
After 156 training step(s), loss on training batch is 1.38638.
After 157 training step(s), loss on training batch is 1.3864.
After 158 training step(s), loss on training batch is 1.38639.
After 159 training step(s), loss on training batch is 1.38638.
After 160 training step(s), loss on training batch is 1.38638.
After 161 training step(s), loss on training batch is 1.38639.
After 162 training step(s), loss on training batch is 1.38637.
After 163 training step(s), loss on training batch is 1.38637.
After 164 training step(s), loss on training batch is 1.38637.
After 165 training step(s), loss on training batch is 1.38638.
After 166 training step(s), loss on training batch is 1.38637.
After 167 training step(s), loss on training batch is 1.38637.
After 168 training step(s), loss on training batch is 1.38638.
After 169 training step(s), loss on training batch is 1.38636.
After 170 training step(s), loss on training batch is 1.38637.
After 171 training step(s), loss on training batch is 1.38636.
After 172 training step(s), loss on training batch is 1.38637.
After 173 training step(s), loss on training batch is 1.38636.
After 174 training step(s), loss on training batch is 1.38637.
After 175 training step(s), loss on training batch is 1.38636.
After 176 training step(s), loss on training batch is 1.38637.
After 177 training step(s), loss on training batch is 1.38636.
After 178 training step(s), loss on training batch is 1.38636.
After 179 training step(s), loss on training batch is 1.38635.
After 180 training step(s), loss on training batch is 1.38636.
After 181 training step(s), loss on training batch is 1.38634.
After 182 training step(s), loss on training batch is 1.38636.
After 183 training step(s), loss on training batch is 1.38636.
After 184 training step(s), loss on training batch is 1.38634.
After 185 training step(s), loss on training batch is 1.38635.
After 186 training step(s), loss on training batch is 1.38634.
After 187 training step(s), loss on training batch is 1.38634.
After 188 training step(s), loss on training batch is 1.38635.
After 189 training step(s), loss on training batch is 1.38633.
After 190 training step(s), loss on training batch is 1.38634.
After 191 training step(s), loss on training batch is 1.38633.
After 192 training step(s), loss on training batch is 1.38635.
After 193 training step(s), loss on training batch is 1.38633.
After 194 training step(s), loss on training batch is 1.38633.
After 195 training step(s), loss on training batch is 1.38634.
After 196 training step(s), loss on training batch is 1.38633.
After 197 training step(s), loss on training batch is 1.38634.
After 198 training step(s), loss on training batch is 1.38632.
After 199 training step(s), loss on training batch is 1.38633.
After 200 training step(s), loss on training batch is 1.38634.
After 201 training step(s), loss on training batch is 1.38633.
After 202 training step(s), loss on training batch is 1.38634.
After 203 training step(s), loss on training batch is 1.38634.
After 204 training step(s), loss on training batch is 1.38632.
After 205 training step(s), loss on training batch is 1.38635.
After 206 training step(s), loss on training batch is 1.38632.
After 207 training step(s), loss on training batch is 1.38632.
After 208 training step(s), loss on training batch is 1.38633.
After 209 training step(s), loss on training batch is 1.38632.
After 210 training step(s), loss on training batch is 1.38632.
After 211 training step(s), loss on training batch is 1.38634.
After 212 training step(s), loss on training batch is 1.38633.
After 213 training step(s), loss on training batch is 1.38633.
After 214 training step(s), loss on training batch is 1.38633.
After 215 training step(s), loss on training batch is 1.38631.
After 216 training step(s), loss on training batch is 1.38633.
After 217 training step(s), loss on training batch is 1.38633.
After 218 training step(s), loss on training batch is 1.38631.
After 219 training step(s), loss on training batch is 1.38633.
After 220 training step(s), loss on training batch is 1.38632.
After 221 training step(s), loss on training batch is 1.38632.
After 222 training step(s), loss on training batch is 1.38632.
After 223 training step(s), loss on training batch is 1.38632.
After 224 training step(s), loss on training batch is 1.38632.
After 225 training step(s), loss on training batch is 1.38631.
After 226 training step(s), loss on training batch is 1.38633.
After 227 training step(s), loss on training batch is 1.3863.
After 228 training step(s), loss on training batch is 1.3863.
After 229 training step(s), loss on training batch is 1.38633.
After 230 training step(s), loss on training batch is 1.38632.
After 231 training step(s), loss on training batch is 1.38631.
After 232 training step(s), loss on training batch is 1.38631.
After 233 training step(s), loss on training batch is 1.38634.
After 234 training step(s), loss on training batch is 1.38633.
After 235 training step(s), loss on training batch is 1.3863.
After 236 training step(s), loss on training batch is 1.38632.
After 237 training step(s), loss on training batch is 1.38632.
After 238 training step(s), loss on training batch is 1.38631.
After 239 training step(s), loss on training batch is 1.38631.
After 240 training step(s), loss on training batch is 1.38632.
After 241 training step(s), loss on training batch is 1.38632.
After 242 training step(s), loss on training batch is 1.3863.
After 243 training step(s), loss on training batch is 1.3863.
After 244 training step(s), loss on training batch is 1.38631.
After 245 training step(s), loss on training batch is 1.38631.
After 246 training step(s), loss on training batch is 1.3863.
After 247 training step(s), loss on training batch is 1.38631.
After 248 training step(s), loss on training batch is 1.38631.
After 249 training step(s), loss on training batch is 1.38632.
After 250 training step(s), loss on training batch is 1.38632.
After 251 training step(s), loss on training batch is 1.38631.
After 252 training step(s), loss on training batch is 1.3863.
After 253 training step(s), loss on training batch is 1.38631.
After 254 training step(s), loss on training batch is 1.38632.
After 255 training step(s), loss on training batch is 1.38632.
After 256 training step(s), loss on training batch is 1.3863.
After 257 training step(s), loss on training batch is 1.3863.
After 258 training step(s), loss on training batch is 1.38631.
After 259 training step(s), loss on training batch is 1.38632.
After 260 training step(s), loss on training batch is 1.38632.
After 261 training step(s), loss on training batch is 1.3863.
After 262 training step(s), loss on training batch is 1.3863.
After 263 training step(s), loss on training batch is 1.38631.
After 264 training step(s), loss on training batch is 1.38632.
After 265 training step(s), loss on training batch is 1.38631.
After 266 training step(s), loss on training batch is 1.3863.
After 267 training step(s), loss on training batch is 1.3863.
After 268 training step(s), loss on training batch is 1.38631.
After 269 training step(s), loss on training batch is 1.3863.
After 270 training step(s), loss on training batch is 1.38631.
After 271 training step(s), loss on training batch is 1.3863.
After 272 training step(s), loss on training batch is 1.38629.
After 273 training step(s), loss on training batch is 1.3863.
After 274 training step(s), loss on training batch is 1.38631.
After 275 training step(s), loss on training batch is 1.3863.
After 276 training step(s), loss on training batch is 1.38631.
After 277 training step(s), loss on training batch is 1.3863.
After 278 training step(s), loss on training batch is 1.3863.
After 279 training step(s), loss on training batch is 1.3863.
After 280 training step(s), loss on training batch is 1.38631.
After 281 training step(s), loss on training batch is 1.38631.
After 282 training step(s), loss on training batch is 1.38631.
After 283 training step(s), loss on training batch is 1.38631.
After 284 training step(s), loss on training batch is 1.3863.
After 285 training step(s), loss on training batch is 1.38629.
After 286 training step(s), loss on training batch is 1.3863.
After 287 training step(s), loss on training batch is 1.38631.
After 288 training step(s), loss on training batch is 1.38631.
After 289 training step(s), loss on training batch is 1.38631.
After 290 training step(s), loss on training batch is 1.38632.
After 291 training step(s), loss on training batch is 1.38629.
After 292 training step(s), loss on training batch is 1.38629.
After 293 training step(s), loss on training batch is 1.38629.
After 294 training step(s), loss on training batch is 1.3863.
After 295 training step(s), loss on training batch is 1.38631.
After 296 training step(s), loss on training batch is 1.3863.
After 297 training step(s), loss on training batch is 1.3863.
After 298 training step(s), loss on training batch is 1.38631.
After 299 training step(s), loss on training batch is 1.3863.
After 300 training step(s), loss on training batch is 1.3863.
After 301 training step(s), loss on training batch is 1.38629.
After 302 training step(s), loss on training batch is 1.3863.
After 303 training step(s), loss on training batch is 1.38631.
After 304 training step(s), loss on training batch is 1.38631.
After 305 training step(s), loss on training batch is 1.38631.
After 306 training step(s), loss on training batch is 1.38631.
After 307 training step(s), loss on training batch is 1.38629.
After 308 training step(s), loss on training batch is 1.38629.
After 309 training step(s), loss on training batch is 1.3863.
After 310 training step(s), loss on training batch is 1.38629.
After 311 training step(s), loss on training batch is 1.38629.
After 312 training step(s), loss on training batch is 1.3863.
After 313 training step(s), loss on training batch is 1.38631.
After 314 training step(s), loss on training batch is 1.38631.
After 315 training step(s), loss on training batch is 1.38631.
After 316 training step(s), loss on training batch is 1.38628.
After 317 training step(s), loss on training batch is 1.38629.
After 318 training step(s), loss on training batch is 1.38629.
After 319 training step(s), loss on training batch is 1.38629.
After 320 training step(s), loss on training batch is 1.38629.
After 321 training step(s), loss on training batch is 1.3863.
After 322 training step(s), loss on training batch is 1.38631.
After 323 training step(s), loss on training batch is 1.38631.
After 324 training step(s), loss on training batch is 1.3863.
After 325 training step(s), loss on training batch is 1.38631.
After 326 training step(s), loss on training batch is 1.38631.
After 327 training step(s), loss on training batch is 1.38631.
After 328 training step(s), loss on training batch is 1.38629.
After 329 training step(s), loss on training batch is 1.38629.
After 330 training step(s), loss on training batch is 1.38629.
After 331 training step(s), loss on training batch is 1.38629.
After 332 training step(s), loss on training batch is 1.38629.
After 333 training step(s), loss on training batch is 1.3863.
After 334 training step(s), loss on training batch is 1.38631.
After 335 training step(s), loss on training batch is 1.38631.
After 336 training step(s), loss on training batch is 1.3863.
After 337 training step(s), loss on training batch is 1.3863.
After 338 training step(s), loss on training batch is 1.38631.
After 339 training step(s), loss on training batch is 1.38629.
After 340 training step(s), loss on training batch is 1.38629.
After 341 training step(s), loss on training batch is 1.38629.
After 342 training step(s), loss on training batch is 1.3863.
After 343 training step(s), loss on training batch is 1.3863.
After 344 training step(s), loss on training batch is 1.38631.
After 345 training step(s), loss on training batch is 1.3863.
After 346 training step(s), loss on training batch is 1.38631.
After 347 training step(s), loss on training batch is 1.38631.
After 348 training step(s), loss on training batch is 1.38629.
After 349 training step(s), loss on training batch is 1.38629.
After 350 training step(s), loss on training batch is 1.38629.
After 351 training step(s), loss on training batch is 1.3863.
After 352 training step(s), loss on training batch is 1.38628.
After 353 training step(s), loss on training batch is 1.38629.
After 354 training step(s), loss on training batch is 1.38628.
After 355 training step(s), loss on training batch is 1.3863.
After 356 training step(s), loss on training batch is 1.3863.
After 357 training step(s), loss on training batch is 1.3863.
After 358 training step(s), loss on training batch is 1.38631.
After 359 training step(s), loss on training batch is 1.38631.
After 360 training step(s), loss on training batch is 1.38631.
After 361 training step(s), loss on training batch is 1.38631.
After 362 training step(s), loss on training batch is 1.38631.
After 363 training step(s), loss on training batch is 1.38631.
After 364 training step(s), loss on training batch is 1.38631.
After 365 training step(s), loss on training batch is 1.38632.
After 366 training step(s), loss on training batch is 1.38629.
After 367 training step(s), loss on training batch is 1.38629.
After 368 training step(s), loss on training batch is 1.38629.
After 369 training step(s), loss on training batch is 1.3863.
After 370 training step(s), loss on training batch is 1.38628.
After 371 training step(s), loss on training batch is 1.38628.
After 372 training step(s), loss on training batch is 1.38629.
After 373 training step(s), loss on training batch is 1.38629.
After 374 training step(s), loss on training batch is 1.38629.
After 375 training step(s), loss on training batch is 1.38629.
After 376 training step(s), loss on training batch is 1.3863.
After 377 training step(s), loss on training batch is 1.3863.
After 378 training step(s), loss on training batch is 1.38631.
After 379 training step(s), loss on training batch is 1.38631.
After 380 training step(s), loss on training batch is 1.38632.
After 381 training step(s), loss on training batch is 1.3863.
After 382 training step(s), loss on training batch is 1.38631.
After 383 training step(s), loss on training batch is 1.38631.
After 384 training step(s), loss on training batch is 1.38631.
After 385 training step(s), loss on training batch is 1.38631.
After 386 training step(s), loss on training batch is 1.38631.
After 387 training step(s), loss on training batch is 1.38629.
After 388 training step(s), loss on training batch is 1.38629.
After 389 training step(s), loss on training batch is 1.38629.
After 390 training step(s), loss on training batch is 1.38629.
After 391 training step(s), loss on training batch is 1.38629.
After 392 training step(s), loss on training batch is 1.38629.
After 393 training step(s), loss on training batch is 1.38628.
After 394 training step(s), loss on training batch is 1.38629.
After 395 training step(s), loss on training batch is 1.38628.
After 396 training step(s), loss on training batch is 1.38629.
After 397 training step(s), loss on training batch is 1.38629.
After 398 training step(s), loss on training batch is 1.3863.
After 399 training step(s), loss on training batch is 1.3863.
After 400 training step(s), loss on training batch is 1.3863.
After 401 training step(s), loss on training batch is 1.38629.
After 402 training step(s), loss on training batch is 1.38629.
After 403 training step(s), loss on training batch is 1.38629.
After 404 training step(s), loss on training batch is 1.38629.
After 405 training step(s), loss on training batch is 1.38629.
After 406 training step(s), loss on training batch is 1.3863.
After 407 training step(s), loss on training batch is 1.3863.
After 408 training step(s), loss on training batch is 1.3863.
After 409 training step(s), loss on training batch is 1.38631.
After 410 training step(s), loss on training batch is 1.3863.
After 411 training step(s), loss on training batch is 1.3863.
After 412 training step(s), loss on training batch is 1.3863.
After 413 training step(s), loss on training batch is 1.3863.
After 414 training step(s), loss on training batch is 1.3863.
After 415 training step(s), loss on training batch is 1.3863.
After 416 training step(s), loss on training batch is 1.38628.
After 417 training step(s), loss on training batch is 1.38628.
After 418 training step(s), loss on training batch is 1.38628.
After 419 training step(s), loss on training batch is 1.38631.
After 420 training step(s), loss on training batch is 1.38629.
After 421 training step(s), loss on training batch is 1.38629.
After 422 training step(s), loss on training batch is 1.38629.
After 423 training step(s), loss on training batch is 1.38629.
After 424 training step(s), loss on training batch is 1.38629.
After 425 training step(s), loss on training batch is 1.38629.
After 426 training step(s), loss on training batch is 1.38629.
After 427 training step(s), loss on training batch is 1.3863.
After 428 training step(s), loss on training batch is 1.38628.
After 429 training step(s), loss on training batch is 1.38629.
After 430 training step(s), loss on training batch is 1.38629.
After 431 training step(s), loss on training batch is 1.38629.
After 432 training step(s), loss on training batch is 1.38628.
After 433 training step(s), loss on training batch is 1.38628.
After 434 training step(s), loss on training batch is 1.38629.
After 435 training step(s), loss on training batch is 1.38629.
After 436 training step(s), loss on training batch is 1.38628.
After 437 training step(s), loss on training batch is 1.38629.
After 438 training step(s), loss on training batch is 1.38629.
After 439 training step(s), loss on training batch is 1.3863.
After 440 training step(s), loss on training batch is 1.3863.
After 441 training step(s), loss on training batch is 1.3863.
After 442 training step(s), loss on training batch is 1.3863.
After 443 training step(s), loss on training batch is 1.3863.
After 444 training step(s), loss on training batch is 1.3863.
After 445 training step(s), loss on training batch is 1.38631.
After 446 training step(s), loss on training batch is 1.3863.
After 447 training step(s), loss on training batch is 1.3863.
After 448 training step(s), loss on training batch is 1.3863.
After 449 training step(s), loss on training batch is 1.3863.
After 450 training step(s), loss on training batch is 1.3863.
After 451 training step(s), loss on training batch is 1.3863.
After 452 training step(s), loss on training batch is 1.3863.
After 453 training step(s), loss on training batch is 1.38631.
After 454 training step(s), loss on training batch is 1.3863.
After 455 training step(s), loss on training batch is 1.3863.
After 456 training step(s), loss on training batch is 1.3863.
After 457 training step(s), loss on training batch is 1.38629.
After 458 training step(s), loss on training batch is 1.38629.
After 459 training step(s), loss on training batch is 1.38629.
After 460 training step(s), loss on training batch is 1.38629.
After 461 training step(s), loss on training batch is 1.38629.
After 462 training step(s), loss on training batch is 1.38629.
After 463 training step(s), loss on training batch is 1.38629.
After 464 training step(s), loss on training batch is 1.38629.
After 465 training step(s), loss on training batch is 1.3863.
After 466 training step(s), loss on training batch is 1.38629.
After 467 training step(s), loss on training batch is 1.3863.
After 468 training step(s), loss on training batch is 1.3863.
After 469 training step(s), loss on training batch is 1.3863.
After 470 training step(s), loss on training batch is 1.3863.
After 471 training step(s), loss on training batch is 1.38631.
After 472 training step(s), loss on training batch is 1.3863.
After 473 training step(s), loss on training batch is 1.38631.
After 474 training step(s), loss on training batch is 1.38631.
After 475 training step(s), loss on training batch is 1.3863.
After 476 training step(s), loss on training batch is 1.3863.
After 477 training step(s), loss on training batch is 1.3863.
After 478 training step(s), loss on training batch is 1.3863.
After 479 training step(s), loss on training batch is 1.38631.
After 480 training step(s), loss on training batch is 1.3863.
After 481 training step(s), loss on training batch is 1.38631.
After 482 training step(s), loss on training batch is 1.38628.
After 483 training step(s), loss on training batch is 1.38629.
After 484 training step(s), loss on training batch is 1.38629.
After 485 training step(s), loss on training batch is 1.38628.
After 486 training step(s), loss on training batch is 1.38629.
After 487 training step(s), loss on training batch is 1.38629.
After 488 training step(s), loss on training batch is 1.38629.
After 489 training step(s), loss on training batch is 1.38629.
After 490 training step(s), loss on training batch is 1.3863.
After 491 training step(s), loss on training batch is 1.3863.
After 492 training step(s), loss on training batch is 1.3863.
After 493 training step(s), loss on training batch is 1.3863.
After 494 training step(s), loss on training batch is 1.3863.
After 495 training step(s), loss on training batch is 1.3863.
After 496 training step(s), loss on training batch is 1.3863.
After 497 training step(s), loss on training batch is 1.3863.
After 498 training step(s), loss on training batch is 1.3863.
After 499 training step(s), loss on training batch is 1.3863.
After 500 training step(s), loss on training batch is 1.3863.
After 501 training step(s), loss on training batch is 1.3863.
After 502 training step(s), loss on training batch is 1.38629.
After 503 training step(s), loss on training batch is 1.38629.
After 504 training step(s), loss on training batch is 1.38629.
After 505 training step(s), loss on training batch is 1.38629.
After 506 training step(s), loss on training batch is 1.38629.
After 507 training step(s), loss on training batch is 1.38629.
After 508 training step(s), loss on training batch is 1.3863.
After 509 training step(s), loss on training batch is 1.3863.
After 510 training step(s), loss on training batch is 1.3863.
After 511 training step(s), loss on training batch is 1.38631.
After 512 training step(s), loss on training batch is 1.38631.
After 513 training step(s), loss on training batch is 1.38631.
After 514 training step(s), loss on training batch is 1.38631.
After 515 training step(s), loss on training batch is 1.38632.
After 516 training step(s), loss on training batch is 1.38629.
After 517 training step(s), loss on training batch is 1.38629.
After 518 training step(s), loss on training batch is 1.3863.
After 519 training step(s), loss on training batch is 1.3863.
After 520 training step(s), loss on training batch is 1.38632.
After 521 training step(s), loss on training batch is 1.3863.
After 522 training step(s), loss on training batch is 1.3863.
After 523 training step(s), loss on training batch is 1.3863.
After 524 training step(s), loss on training batch is 1.3863.
After 525 training step(s), loss on training batch is 1.3863.
After 526 training step(s), loss on training batch is 1.3863.
After 527 training step(s), loss on training batch is 1.38632.
After 528 training step(s), loss on training batch is 1.38632.
After 529 training step(s), loss on training batch is 1.38632.
After 530 training step(s), loss on training batch is 1.3863.
After 531 training step(s), loss on training batch is 1.3863.
After 532 training step(s), loss on training batch is 1.3863.
After 533 training step(s), loss on training batch is 1.3863.
After 534 training step(s), loss on training batch is 1.38631.
After 535 training step(s), loss on training batch is 1.38631.
After 536 training step(s), loss on training batch is 1.38631.
After 537 training step(s), loss on training batch is 1.38631.
After 538 training step(s), loss on training batch is 1.38631.
After 539 training step(s), loss on training batch is 1.38631.
After 540 training step(s), loss on training batch is 1.38631.
After 541 training step(s), loss on training batch is 1.38631.
After 542 training step(s), loss on training batch is 1.38631.
After 543 training step(s), loss on training batch is 1.38628.
After 544 training step(s), loss on training batch is 1.38628.
After 545 training step(s), loss on training batch is 1.38628.
After 546 training step(s), loss on training batch is 1.38629.
After 547 training step(s), loss on training batch is 1.38629.
After 548 training step(s), loss on training batch is 1.38629.
After 549 training step(s), loss on training batch is 1.38629.
After 550 training step(s), loss on training batch is 1.38629.
After 551 training step(s), loss on training batch is 1.38629.
After 552 training step(s), loss on training batch is 1.38629.
After 553 training step(s), loss on training batch is 1.38629.
After 554 training step(s), loss on training batch is 1.38629.
After 555 training step(s), loss on training batch is 1.38629.
After 556 training step(s), loss on training batch is 1.38631.
After 557 training step(s), loss on training batch is 1.3863.
After 558 training step(s), loss on training batch is 1.38629.
After 559 training step(s), loss on training batch is 1.38629.
After 560 training step(s), loss on training batch is 1.3863.
After 561 training step(s), loss on training batch is 1.38629.
After 562 training step(s), loss on training batch is 1.38629.
After 563 training step(s), loss on training batch is 1.38629.
After 564 training step(s), loss on training batch is 1.38629.
After 565 training step(s), loss on training batch is 1.38629.
After 566 training step(s), loss on training batch is 1.3863.
After 567 training step(s), loss on training batch is 1.38629.
After 568 training step(s), loss on training batch is 1.38629.
After 569 training step(s), loss on training batch is 1.38629.
After 570 training step(s), loss on training batch is 1.38628.
After 571 training step(s), loss on training batch is 1.38628.
After 572 training step(s), loss on training batch is 1.38628.
After 573 training step(s), loss on training batch is 1.38628.
After 574 training step(s), loss on training batch is 1.38628.
After 575 training step(s), loss on training batch is 1.38628.
After 576 training step(s), loss on training batch is 1.38628.
After 577 training step(s), loss on training batch is 1.38628.
After 578 training step(s), loss on training batch is 1.38628.
After 579 training step(s), loss on training batch is 1.38628.
After 580 training step(s), loss on training batch is 1.38628.
After 581 training step(s), loss on training batch is 1.38628.
After 582 training step(s), loss on training batch is 1.38629.
After 583 training step(s), loss on training batch is 1.38629.
After 584 training step(s), loss on training batch is 1.38628.
After 585 training step(s), loss on training batch is 1.38628.
After 586 training step(s), loss on training batch is 1.38629.
After 587 training step(s), loss on training batch is 1.38629.
After 588 training step(s), loss on training batch is 1.38629.
After 589 training step(s), loss on training batch is 1.38629.
After 590 training step(s), loss on training batch is 1.38629.
After 591 training step(s), loss on training batch is 1.38629.
After 592 training step(s), loss on training batch is 1.38629.
After 593 training step(s), loss on training batch is 1.38629.
After 594 training step(s), loss on training batch is 1.38629.
After 595 training step(s), loss on training batch is 1.38629.
After 596 training step(s), loss on training batch is 1.38629.
After 597 training step(s), loss on training batch is 1.38629.
After 598 training step(s), loss on training batch is 1.38629.
After 599 training step(s), loss on training batch is 1.38629.
After 600 training step(s), loss on training batch is 1.38629.
After 601 training step(s), loss on training batch is 1.38629.
After 602 training step(s), loss on training batch is 1.38629.
After 603 training step(s), loss on training batch is 1.38629.
After 604 training step(s), loss on training batch is 1.38629.
After 605 training step(s), loss on training batch is 1.3863.
After 606 training step(s), loss on training batch is 1.38629.
After 607 training step(s), loss on training batch is 1.38629.
After 608 training step(s), loss on training batch is 1.3863.
After 609 training step(s), loss on training batch is 1.38629.
After 610 training step(s), loss on training batch is 1.38629.
After 611 training step(s), loss on training batch is 1.3863.
After 612 training step(s), loss on training batch is 1.3863.
After 613 training step(s), loss on training batch is 1.3863.
After 614 training step(s), loss on training batch is 1.3863.
After 615 training step(s), loss on training batch is 1.3863.
After 616 training step(s), loss on training batch is 1.3863.
After 617 training step(s), loss on training batch is 1.3863.
After 618 training step(s), loss on training batch is 1.38628.
After 619 training step(s), loss on training batch is 1.3863.
After 620 training step(s), loss on training batch is 1.38628.
After 621 training step(s), loss on training batch is 1.38629.
After 622 training step(s), loss on training batch is 1.3863.
After 623 training step(s), loss on training batch is 1.3863.
After 624 training step(s), loss on training batch is 1.3863.
After 625 training step(s), loss on training batch is 1.3863.
After 626 training step(s), loss on training batch is 1.3863.
After 627 training step(s), loss on training batch is 1.3863.
After 628 training step(s), loss on training batch is 1.38628.
After 629 training step(s), loss on training batch is 1.38628.
After 630 training step(s), loss on training batch is 1.38628.
After 631 training step(s), loss on training batch is 1.38628.
After 632 training step(s), loss on training batch is 1.38628.
After 633 training step(s), loss on training batch is 1.38629.
After 634 training step(s), loss on training batch is 1.38628.
After 635 training step(s), loss on training batch is 1.38629.
After 636 training step(s), loss on training batch is 1.38629.
After 637 training step(s), loss on training batch is 1.38629.
After 638 training step(s), loss on training batch is 1.38628.
After 639 training step(s), loss on training batch is 1.38629.
After 640 training step(s), loss on training batch is 1.38629.
After 641 training step(s), loss on training batch is 1.38629.
After 642 training step(s), loss on training batch is 1.38629.
After 643 training step(s), loss on training batch is 1.38629.
After 644 training step(s), loss on training batch is 1.38629.
After 645 training step(s), loss on training batch is 1.38629.
After 646 training step(s), loss on training batch is 1.38629.
After 647 training step(s), loss on training batch is 1.38629.
After 648 training step(s), loss on training batch is 1.38629.
After 649 training step(s), loss on training batch is 1.38629.
After 650 training step(s), loss on training batch is 1.38629.
After 651 training step(s), loss on training batch is 1.38629.
After 652 training step(s), loss on training batch is 1.38629.
After 653 training step(s), loss on training batch is 1.38629.
After 654 training step(s), loss on training batch is 1.38629.
After 655 training step(s), loss on training batch is 1.38629.
After 656 training step(s), loss on training batch is 1.38629.
After 657 training step(s), loss on training batch is 1.38629.
After 658 training step(s), loss on training batch is 1.38629.
After 659 training step(s), loss on training batch is 1.38629.
After 660 training step(s), loss on training batch is 1.38629.
After 661 training step(s), loss on training batch is 1.38629.
After 662 training step(s), loss on training batch is 1.38629.
After 663 training step(s), loss on training batch is 1.38629.
After 664 training step(s), loss on training batch is 1.38629.
After 665 training step(s), loss on training batch is 1.38629.
After 666 training step(s), loss on training batch is 1.3863.
After 667 training step(s), loss on training batch is 1.3863.
After 668 training step(s), loss on training batch is 1.3863.
After 669 training step(s), loss on training batch is 1.3863.
After 670 training step(s), loss on training batch is 1.3863.
After 671 training step(s), loss on training batch is 1.38631.
After 672 training step(s), loss on training batch is 1.38631.
After 673 training step(s), loss on training batch is 1.3863.
After 674 training step(s), loss on training batch is 1.3863.
After 675 training step(s), loss on training batch is 1.3863.
After 676 training step(s), loss on training batch is 1.3863.
After 677 training step(s), loss on training batch is 1.38631.
After 678 training step(s), loss on training batch is 1.38631.
After 679 training step(s), loss on training batch is 1.3863.
After 680 training step(s), loss on training batch is 1.38631.
After 681 training step(s), loss on training batch is 1.3863.
After 682 training step(s), loss on training batch is 1.3863.
After 683 training step(s), loss on training batch is 1.38631.
After 684 training step(s), loss on training batch is 1.38631.
After 685 training step(s), loss on training batch is 1.38631.
After 686 training step(s), loss on training batch is 1.38631.
After 687 training step(s), loss on training batch is 1.38631.
After 688 training step(s), loss on training batch is 1.38631.
After 689 training step(s), loss on training batch is 1.38631.
After 690 training step(s), loss on training batch is 1.38631.
After 691 training step(s), loss on training batch is 1.38631.
After 692 training step(s), loss on training batch is 1.38631.
After 693 training step(s), loss on training batch is 1.38631.
After 694 training step(s), loss on training batch is 1.3863.
After 695 training step(s), loss on training batch is 1.38631.
After 696 training step(s), loss on training batch is 1.3863.
After 697 training step(s), loss on training batch is 1.3863.
After 698 training step(s), loss on training batch is 1.3863.
After 699 training step(s), loss on training batch is 1.3863.
After 700 training step(s), loss on training batch is 1.3863.
After 701 training step(s), loss on training batch is 1.3863.
After 702 training step(s), loss on training batch is 1.3863.
After 703 training step(s), loss on training batch is 1.3863.
After 704 training step(s), loss on training batch is 1.3863.
After 705 training step(s), loss on training batch is 1.3863.
After 706 training step(s), loss on training batch is 1.3863.
After 707 training step(s), loss on training batch is 1.3863.
After 708 training step(s), loss on training batch is 1.3863.
After 709 training step(s), loss on training batch is 1.3863.
After 710 training step(s), loss on training batch is 1.3863.
After 711 training step(s), loss on training batch is 1.3863.
After 712 training step(s), loss on training batch is 1.3863.
After 713 training step(s), loss on training batch is 1.38631.
After 714 training step(s), loss on training batch is 1.3863.
After 715 training step(s), loss on training batch is 1.3863.
After 716 training step(s), loss on training batch is 1.3863.
After 717 training step(s), loss on training batch is 1.3863.
After 718 training step(s), loss on training batch is 1.3863.
After 719 training step(s), loss on training batch is 1.3863.
After 720 training step(s), loss on training batch is 1.3863.
After 721 training step(s), loss on training batch is 1.3863.
After 722 training step(s), loss on training batch is 1.3863.
After 723 training step(s), loss on training batch is 1.3863.
After 724 training step(s), loss on training batch is 1.3863.
After 725 training step(s), loss on training batch is 1.3863.
After 726 training step(s), loss on training batch is 1.3863.
After 727 training step(s), loss on training batch is 1.3863.
After 728 training step(s), loss on training batch is 1.3863.
After 729 training step(s), loss on training batch is 1.3863.
After 730 training step(s), loss on training batch is 1.3863.
After 731 training step(s), loss on training batch is 1.3863.
After 732 training step(s), loss on training batch is 1.3863.
After 733 training step(s), loss on training batch is 1.3863.
After 734 training step(s), loss on training batch is 1.3863.
After 735 training step(s), loss on training batch is 1.3863.
After 736 training step(s), loss on training batch is 1.3863.
After 737 training step(s), loss on training batch is 1.3863.
After 738 training step(s), loss on training batch is 1.3863.
After 739 training step(s), loss on training batch is 1.3863.
After 740 training step(s), loss on training batch is 1.3863.
After 741 training step(s), loss on training batch is 1.3863.
After 742 training step(s), loss on training batch is 1.3863.
After 743 training step(s), loss on training batch is 1.3863.
After 744 training step(s), loss on training batch is 1.3863.
After 745 training step(s), loss on training batch is 1.3863.
After 746 training step(s), loss on training batch is 1.38628.
After 747 training step(s), loss on training batch is 1.38628.
After 748 training step(s), loss on training batch is 1.38628.
After 749 training step(s), loss on training batch is 1.38628.
After 750 training step(s), loss on training batch is 1.38628.
After 751 training step(s), loss on training batch is 1.38628.
After 752 training step(s), loss on training batch is 1.38628.
After 753 training step(s), loss on training batch is 1.38628.
After 754 training step(s), loss on training batch is 1.38628.
After 755 training step(s), loss on training batch is 1.38629.
After 756 training step(s), loss on training batch is 1.38628.
After 757 training step(s), loss on training batch is 1.38629.
After 758 training step(s), loss on training batch is 1.38628.
After 759 training step(s), loss on training batch is 1.38628.
After 760 training step(s), loss on training batch is 1.38629.
After 761 training step(s), loss on training batch is 1.38628.
After 762 training step(s), loss on training batch is 1.38628.
After 763 training step(s), loss on training batch is 1.38628.
After 764 training step(s), loss on training batch is 1.38629.
After 765 training step(s), loss on training batch is 1.38628.
After 766 training step(s), loss on training batch is 1.38628.
After 767 training step(s), loss on training batch is 1.38628.
After 768 training step(s), loss on training batch is 1.38628.
After 769 training step(s), loss on training batch is 1.38628.
After 770 training step(s), loss on training batch is 1.38628.
After 771 training step(s), loss on training batch is 1.38628.
After 772 training step(s), loss on training batch is 1.38629.
After 773 training step(s), loss on training batch is 1.38628.
After 774 training step(s), loss on training batch is 1.38629.
After 775 training step(s), loss on training batch is 1.38628.
After 776 training step(s), loss on training batch is 1.38629.
After 777 training step(s), loss on training batch is 1.38628.
After 778 training step(s), loss on training batch is 1.38628.
After 779 training step(s), loss on training batch is 1.38629.
After 780 training step(s), loss on training batch is 1.38629.
After 781 training step(s), loss on training batch is 1.38628.
After 782 training step(s), loss on training batch is 1.38628.
After 783 training step(s), loss on training batch is 1.38628.
After 784 training step(s), loss on training batch is 1.38628.
After 785 training step(s), loss on training batch is 1.38628.
After 786 training step(s), loss on training batch is 1.38628.
After 787 training step(s), loss on training batch is 1.38628.
After 788 training step(s), loss on training batch is 1.38629.
After 789 training step(s), loss on training batch is 1.38628.
After 790 training step(s), loss on training batch is 1.38628.
After 791 training step(s), loss on training batch is 1.38628.
After 792 training step(s), loss on training batch is 1.38628.
After 793 training step(s), loss on training batch is 1.38628.
After 794 training step(s), loss on training batch is 1.38628.
After 795 training step(s), loss on training batch is 1.3863.
After 796 training step(s), loss on training batch is 1.38628.
After 797 training step(s), loss on training batch is 1.38628.
After 798 training step(s), loss on training batch is 1.38628.
After 799 training step(s), loss on training batch is 1.38628.
After 800 training step(s), loss on training batch is 1.38628.
After 801 training step(s), loss on training batch is 1.38631.
After 802 training step(s), loss on training batch is 1.3863.
After 803 training step(s), loss on training batch is 1.38628.
After 804 training step(s), loss on training batch is 1.38629.
After 805 training step(s), loss on training batch is 1.3863.
After 806 training step(s), loss on training batch is 1.38631.
After 807 training step(s), loss on training batch is 1.38629.
After 808 training step(s), loss on training batch is 1.38629.
After 809 training step(s), loss on training batch is 1.38628.
After 810 training step(s), loss on training batch is 1.38629.
After 811 training step(s), loss on training batch is 1.3863.
After 812 training step(s), loss on training batch is 1.3863.
After 813 training step(s), loss on training batch is 1.3863.
After 814 training step(s), loss on training batch is 1.3863.
After 815 training step(s), loss on training batch is 1.3863.
After 816 training step(s), loss on training batch is 1.38629.
After 817 training step(s), loss on training batch is 1.3863.
After 818 training step(s), loss on training batch is 1.3863.
After 819 training step(s), loss on training batch is 1.38629.
After 820 training step(s), loss on training batch is 1.38629.
After 821 training step(s), loss on training batch is 1.38629.
After 822 training step(s), loss on training batch is 1.3863.
After 823 training step(s), loss on training batch is 1.3863.
After 824 training step(s), loss on training batch is 1.3863.
After 825 training step(s), loss on training batch is 1.3863.
After 826 training step(s), loss on training batch is 1.3863.
After 827 training step(s), loss on training batch is 1.38629.
After 828 training step(s), loss on training batch is 1.3863.
After 829 training step(s), loss on training batch is 1.3863.
After 830 training step(s), loss on training batch is 1.3863.
After 831 training step(s), loss on training batch is 1.3863.
After 832 training step(s), loss on training batch is 1.3863.
After 833 training step(s), loss on training batch is 1.3863.
After 834 training step(s), loss on training batch is 1.3863.
After 835 training step(s), loss on training batch is 1.3863.
After 836 training step(s), loss on training batch is 1.3863.
After 837 training step(s), loss on training batch is 1.3863.
After 838 training step(s), loss on training batch is 1.3863.
After 839 training step(s), loss on training batch is 1.3863.
After 840 training step(s), loss on training batch is 1.3863.
After 841 training step(s), loss on training batch is 1.3863.
After 842 training step(s), loss on training batch is 1.3863.
After 843 training step(s), loss on training batch is 1.3863.
After 844 training step(s), loss on training batch is 1.3863.
After 845 training step(s), loss on training batch is 1.3863.
After 846 training step(s), loss on training batch is 1.3863.
After 847 training step(s), loss on training batch is 1.3863.
After 848 training step(s), loss on training batch is 1.38631.
After 849 training step(s), loss on training batch is 1.38631.
After 850 training step(s), loss on training batch is 1.38631.
After 851 training step(s), loss on training batch is 1.38631.
After 852 training step(s), loss on training batch is 1.38631.
After 853 training step(s), loss on training batch is 1.38631.
After 854 training step(s), loss on training batch is 1.3863.
After 855 training step(s), loss on training batch is 1.3863.
After 856 training step(s), loss on training batch is 1.38629.
After 857 training step(s), loss on training batch is 1.38629.
After 858 training step(s), loss on training batch is 1.3863.
After 859 training step(s), loss on training batch is 1.3863.
After 860 training step(s), loss on training batch is 1.38629.
After 861 training step(s), loss on training batch is 1.3863.
After 862 training step(s), loss on training batch is 1.38629.
After 863 training step(s), loss on training batch is 1.3863.
After 864 training step(s), loss on training batch is 1.38629.
After 865 training step(s), loss on training batch is 1.38629.
After 866 training step(s), loss on training batch is 1.3863.
After 867 training step(s), loss on training batch is 1.38629.
After 868 training step(s), loss on training batch is 1.38629.
After 869 training step(s), loss on training batch is 1.3863.
After 870 training step(s), loss on training batch is 1.38629.
After 871 training step(s), loss on training batch is 1.38629.
After 872 training step(s), loss on training batch is 1.3863.
After 873 training step(s), loss on training batch is 1.38629.
After 874 training step(s), loss on training batch is 1.38629.
After 875 training step(s), loss on training batch is 1.3863.
After 876 training step(s), loss on training batch is 1.3863.
After 877 training step(s), loss on training batch is 1.38629.
After 878 training step(s), loss on training batch is 1.3863.
After 879 training step(s), loss on training batch is 1.38629.
After 880 training step(s), loss on training batch is 1.3863.
After 881 training step(s), loss on training batch is 1.38628.
After 882 training step(s), loss on training batch is 1.3863.
After 883 training step(s), loss on training batch is 1.3863.
After 884 training step(s), loss on training batch is 1.3863.
After 885 training step(s), loss on training batch is 1.3863.
After 886 training step(s), loss on training batch is 1.38631.
After 887 training step(s), loss on training batch is 1.3863.
After 888 training step(s), loss on training batch is 1.3863.
After 889 training step(s), loss on training batch is 1.3863.
After 890 training step(s), loss on training batch is 1.3863.
After 891 training step(s), loss on training batch is 1.3863.
After 892 training step(s), loss on training batch is 1.3863.
After 893 training step(s), loss on training batch is 1.3863.
After 894 training step(s), loss on training batch is 1.3863.
After 895 training step(s), loss on training batch is 1.3863.
After 896 training step(s), loss on training batch is 1.3863.
After 897 training step(s), loss on training batch is 1.3863.
After 898 training step(s), loss on training batch is 1.3863.
After 899 training step(s), loss on training batch is 1.38631.
After 900 training step(s), loss on training batch is 1.3863.
After 901 training step(s), loss on training batch is 1.38629.
After 902 training step(s), loss on training batch is 1.38629.
After 903 training step(s), loss on training batch is 1.38629.
After 904 training step(s), loss on training batch is 1.38631.
After 905 training step(s), loss on training batch is 1.38631.
After 906 training step(s), loss on training batch is 1.38631.
After 907 training step(s), loss on training batch is 1.38631.
After 908 training step(s), loss on training batch is 1.38629.
After 909 training step(s), loss on training batch is 1.38629.
After 910 training step(s), loss on training batch is 1.38629.
After 911 training step(s), loss on training batch is 1.38629.
After 912 training step(s), loss on training batch is 1.38629.
After 913 training step(s), loss on training batch is 1.38629.
After 914 training step(s), loss on training batch is 1.38629.
After 915 training step(s), loss on training batch is 1.38628.
After 916 training step(s), loss on training batch is 1.3863.
After 917 training step(s), loss on training batch is 1.38628.
After 918 training step(s), loss on training batch is 1.38628.
After 919 training step(s), loss on training batch is 1.38629.
After 920 training step(s), loss on training batch is 1.38629.
After 921 training step(s), loss on training batch is 1.38629.
After 922 training step(s), loss on training batch is 1.38629.
After 923 training step(s), loss on training batch is 1.38629.
After 924 training step(s), loss on training batch is 1.38629.
After 925 training step(s), loss on training batch is 1.38629.
After 926 training step(s), loss on training batch is 1.38629.
After 927 training step(s), loss on training batch is 1.38628.
After 928 training step(s), loss on training batch is 1.38628.
After 929 training step(s), loss on training batch is 1.38627.
After 930 training step(s), loss on training batch is 1.38629.
After 931 training step(s), loss on training batch is 1.38629.
After 932 training step(s), loss on training batch is 1.38629.
After 933 training step(s), loss on training batch is 1.38629.
After 934 training step(s), loss on training batch is 1.38629.
After 935 training step(s), loss on training batch is 1.38629.
After 936 training step(s), loss on training batch is 1.38629.
After 937 training step(s), loss on training batch is 1.38629.
After 938 training step(s), loss on training batch is 1.38629.
After 939 training step(s), loss on training batch is 1.38629.
After 940 training step(s), loss on training batch is 1.38629.
After 941 training step(s), loss on training batch is 1.38629.
After 942 training step(s), loss on training batch is 1.38629.
After 943 training step(s), loss on training batch is 1.38629.
After 944 training step(s), loss on training batch is 1.38629.
After 945 training step(s), loss on training batch is 1.38629.
After 946 training step(s), loss on training batch is 1.38629.
After 947 training step(s), loss on training batch is 1.38629.
After 948 training step(s), loss on training batch is 1.38629.
After 949 training step(s), loss on training batch is 1.38629.
After 950 training step(s), loss on training batch is 1.38629.
After 951 training step(s), loss on training batch is 1.38629.
After 952 training step(s), loss on training batch is 1.38629.
After 953 training step(s), loss on training batch is 1.38629.
After 954 training step(s), loss on training batch is 1.38629.
After 955 training step(s), loss on training batch is 1.38629.
After 956 training step(s), loss on training batch is 1.38629.
After 957 training step(s), loss on training batch is 1.38629.
After 958 training step(s), loss on training batch is 1.38629.
After 959 training step(s), loss on training batch is 1.38629.
After 960 training step(s), loss on training batch is 1.38629.
After 961 training step(s), loss on training batch is 1.38628.
After 962 training step(s), loss on training batch is 1.38629.
After 963 training step(s), loss on training batch is 1.38629.
After 964 training step(s), loss on training batch is 1.38629.
After 965 training step(s), loss on training batch is 1.38629.
After 966 training step(s), loss on training batch is 1.38629.
After 967 training step(s), loss on training batch is 1.38629.
After 968 training step(s), loss on training batch is 1.38629.
After 969 training step(s), loss on training batch is 1.38629.
After 970 training step(s), loss on training batch is 1.38629.
After 971 training step(s), loss on training batch is 1.38629.
After 972 training step(s), loss on training batch is 1.38629.
After 973 training step(s), loss on training batch is 1.38629.
After 974 training step(s), loss on training batch is 1.38629.
After 975 training step(s), loss on training batch is 1.38629.
After 976 training step(s), loss on training batch is 1.38629.
After 977 training step(s), loss on training batch is 1.3863.
After 978 training step(s), loss on training batch is 1.38629.
After 979 training step(s), loss on training batch is 1.38629.
After 980 training step(s), loss on training batch is 1.38629.
After 981 training step(s), loss on training batch is 1.38629.
After 982 training step(s), loss on training batch is 1.3863.
After 983 training step(s), loss on training batch is 1.38629.
After 984 training step(s), loss on training batch is 1.38629.
After 985 training step(s), loss on training batch is 1.38629.
After 986 training step(s), loss on training batch is 1.38629.
After 987 training step(s), loss on training batch is 1.38629.
After 988 training step(s), loss on training batch is 1.38629.
After 989 training step(s), loss on training batch is 1.38629.
After 990 training step(s), loss on training batch is 1.38629.
After 991 training step(s), loss on training batch is 1.38629.
After 992 training step(s), loss on training batch is 1.38629.
After 993 training step(s), loss on training batch is 1.38629.
After 994 training step(s), loss on training batch is 1.38629.
After 995 training step(s), loss on training batch is 1.38629.
After 996 training step(s), loss on training batch is 1.38629.
After 997 training step(s), loss on training batch is 1.38629.
After 998 training step(s), loss on training batch is 1.38629.
After 999 training step(s), loss on training batch is 1.38629.
After 1000 training step(s), loss on training batch is 1.38629.
After 1001 training step(s), loss on training batch is 1.38629.
After 1002 training step(s), loss on training batch is 1.38629.
After 1003 training step(s), loss on training batch is 1.38629.
After 1004 training step(s), loss on training batch is 1.38629.
After 1005 training step(s), loss on training batch is 1.38629.
After 1006 training step(s), loss on training batch is 1.38629.
After 1007 training step(s), loss on training batch is 1.38629.
After 1008 training step(s), loss on training batch is 1.38629.
After 1009 training step(s), loss on training batch is 1.38629.
After 1010 training step(s), loss on training batch is 1.38629.
After 1011 training step(s), loss on training batch is 1.38629.
After 1012 training step(s), loss on training batch is 1.38629.
After 1013 training step(s), loss on training batch is 1.38629.
After 1014 training step(s), loss on training batch is 1.38629.
After 1015 training step(s), loss on training batch is 1.38629.
After 1016 training step(s), loss on training batch is 1.38629.
After 1017 training step(s), loss on training batch is 1.38629.
After 1018 training step(s), loss on training batch is 1.38629.
After 1019 training step(s), loss on training batch is 1.38629.
After 1020 training step(s), loss on training batch is 1.38628.
After 1021 training step(s), loss on training batch is 1.38629.
After 1022 training step(s), loss on training batch is 1.38629.
After 1023 training step(s), loss on training batch is 1.38629.
After 1024 training step(s), loss on training batch is 1.38629.
After 1025 training step(s), loss on training batch is 1.38629.
After 1026 training step(s), loss on training batch is 1.38629.
After 1027 training step(s), loss on training batch is 1.38629.
After 1028 training step(s), loss on training batch is 1.38629.
After 1029 training step(s), loss on training batch is 1.38629.
After 1030 training step(s), loss on training batch is 1.38629.
After 1031 training step(s), loss on training batch is 1.38629.
After 1032 training step(s), loss on training batch is 1.38629.
After 1033 training step(s), loss on training batch is 1.38629.
After 1034 training step(s), loss on training batch is 1.38629.
After 1035 training step(s), loss on training batch is 1.38629.
After 1036 training step(s), loss on training batch is 1.38629.
After 1037 training step(s), loss on training batch is 1.38629.
After 1038 training step(s), loss on training batch is 1.38629.
After 1039 training step(s), loss on training batch is 1.38629.
After 1040 training step(s), loss on training batch is 1.38629.
After 1041 training step(s), loss on training batch is 1.38629.
After 1042 training step(s), loss on training batch is 1.38629.
After 1043 training step(s), loss on training batch is 1.38629.
After 1044 training step(s), loss on training batch is 1.38629.
After 1045 training step(s), loss on training batch is 1.38629.
After 1046 training step(s), loss on training batch is 1.38629.
After 1047 training step(s), loss on training batch is 1.38629.
After 1048 training step(s), loss on training batch is 1.38629.
After 1049 training step(s), loss on training batch is 1.38629.
After 1050 training step(s), loss on training batch is 1.38629.
After 1051 training step(s), loss on training batch is 1.38629.
After 1052 training step(s), loss on training batch is 1.38629.
After 1053 training step(s), loss on training batch is 1.38629.
After 1054 training step(s), loss on training batch is 1.38629.
After 1055 training step(s), loss on training batch is 1.38629.
After 1056 training step(s), loss on training batch is 1.38629.
After 1057 training step(s), loss on training batch is 1.38629.
After 1058 training step(s), loss on training batch is 1.38629.
After 1059 training step(s), loss on training batch is 1.38629.
After 1060 training step(s), loss on training batch is 1.38629.
After 1061 training step(s), loss on training batch is 1.38629.
After 1062 training step(s), loss on training batch is 1.38629.
After 1063 training step(s), loss on training batch is 1.38629.
After 1064 training step(s), loss on training batch is 1.38629.
After 1065 training step(s), loss on training batch is 1.38629.
After 1066 training step(s), loss on training batch is 1.38629.
After 1067 training step(s), loss on training batch is 1.38629.
After 1068 training step(s), loss on training batch is 1.38629.
After 1069 training step(s), loss on training batch is 1.38629.
After 1070 training step(s), loss on training batch is 1.38629.
After 1071 training step(s), loss on training batch is 1.38629.
After 1072 training step(s), loss on training batch is 1.38629.
After 1073 training step(s), loss on training batch is 1.38629.
After 1074 training step(s), loss on training batch is 1.38629.
After 1075 training step(s), loss on training batch is 1.38629.
After 1076 training step(s), loss on training batch is 1.38629.
After 1077 training step(s), loss on training batch is 1.38629.
After 1078 training step(s), loss on training batch is 1.38629.
After 1079 training step(s), loss on training batch is 1.38629.
After 1080 training step(s), loss on training batch is 1.38629.
After 1081 training step(s), loss on training batch is 1.38629.
After 1082 training step(s), loss on training batch is 1.38629.
After 1083 training step(s), loss on training batch is 1.38629.
After 1084 training step(s), loss on training batch is 1.38629.
After 1085 training step(s), loss on training batch is 1.38629.
After 1086 training step(s), loss on training batch is 1.38629.
After 1087 training step(s), loss on training batch is 1.38629.
After 1088 training step(s), loss on training batch is 1.38629.
After 1089 training step(s), loss on training batch is 1.38629.
After 1090 training step(s), loss on training batch is 1.38629.
After 1091 training step(s), loss on training batch is 1.38629.
After 1092 training step(s), loss on training batch is 1.38629.
After 1093 training step(s), loss on training batch is 1.38629.
After 1094 training step(s), loss on training batch is 1.38629.
After 1095 training step(s), loss on training batch is 1.38629.
After 1096 training step(s), loss on training batch is 1.38629.
After 1097 training step(s), loss on training batch is 1.38629.
After 1098 training step(s), loss on training batch is 1.38629.
After 1099 training step(s), loss on training batch is 1.38629.
After 1100 training step(s), loss on training batch is 1.38629.
After 1101 training step(s), loss on training batch is 1.38629.
After 1102 training step(s), loss on training batch is 1.38629.
After 1103 training step(s), loss on training batch is 1.38629.
After 1104 training step(s), loss on training batch is 1.38629.
After 1105 training step(s), loss on training batch is 1.38629.
After 1106 training step(s), loss on training batch is 1.38628.
After 1107 training step(s), loss on training batch is 1.38629.
After 1108 training step(s), loss on training batch is 1.38629.
After 1109 training step(s), loss on training batch is 1.38628.
After 1110 training step(s), loss on training batch is 1.38629.
After 1111 training step(s), loss on training batch is 1.38629.
After 1112 training step(s), loss on training batch is 1.38629.
After 1113 training step(s), loss on training batch is 1.38629.
After 1114 training step(s), loss on training batch is 1.38629.
After 1115 training step(s), loss on training batch is 1.38629.
After 1116 training step(s), loss on training batch is 1.38628.
After 1117 training step(s), loss on training batch is 1.38629.
After 1118 training step(s), loss on training batch is 1.38628.
After 1119 training step(s), loss on training batch is 1.38629.
After 1120 training step(s), loss on training batch is 1.38629.
After 1121 training step(s), loss on training batch is 1.38629.
After 1122 training step(s), loss on training batch is 1.38629.
After 1123 training step(s), loss on training batch is 1.38629.
After 1124 training step(s), loss on training batch is 1.38629.
After 1125 training step(s), loss on training batch is 1.38629.
After 1126 training step(s), loss on training batch is 1.38629.
After 1127 training step(s), loss on training batch is 1.38629.
After 1128 training step(s), loss on training batch is 1.38629.
After 1129 training step(s), loss on training batch is 1.38629.
After 1130 training step(s), loss on training batch is 1.38629.
After 1131 training step(s), loss on training batch is 1.38629.
After 1132 training step(s), loss on training batch is 1.38629.
After 1133 training step(s), loss on training batch is 1.38629.
After 1134 training step(s), loss on training batch is 1.38629.
After 1135 training step(s), loss on training batch is 1.38629.
After 1136 training step(s), loss on training batch is 1.38629.
After 1137 training step(s), loss on training batch is 1.38629.
After 1138 training step(s), loss on training batch is 1.38629.
After 1139 training step(s), loss on training batch is 1.38629.
After 1140 training step(s), loss on training batch is 1.38629.
After 1141 training step(s), loss on training batch is 1.38629.
After 1142 training step(s), loss on training batch is 1.38628.
After 1143 training step(s), loss on training batch is 1.38631.
After 1144 training step(s), loss on training batch is 1.38631.
After 1145 training step(s), loss on training batch is 1.38631.
After 1146 training step(s), loss on training batch is 1.38631.
After 1147 training step(s), loss on training batch is 1.38631.
After 1148 training step(s), loss on training batch is 1.38631.
After 1149 training step(s), loss on training batch is 1.38631.
After 1150 training step(s), loss on training batch is 1.38631.
After 1151 training step(s), loss on training batch is 1.38631.
After 1152 training step(s), loss on training batch is 1.38631.
After 1153 training step(s), loss on training batch is 1.38631.
After 1154 training step(s), loss on training batch is 1.38631.
After 1155 training step(s), loss on training batch is 1.38631.
After 1156 training step(s), loss on training batch is 1.38631.
After 1157 training step(s), loss on training batch is 1.38631.
After 1158 training step(s), loss on training batch is 1.3863.
After 1159 training step(s), loss on training batch is 1.3863.
After 1160 training step(s), loss on training batch is 1.38631.
After 1161 training step(s), loss on training batch is 1.3863.
After 1162 training step(s), loss on training batch is 1.3863.
After 1163 training step(s), loss on training batch is 1.3863.
After 1164 training step(s), loss on training batch is 1.3863.
After 1165 training step(s), loss on training batch is 1.3863.
After 1166 training step(s), loss on training batch is 1.3863.
After 1167 training step(s), loss on training batch is 1.3863.
After 1168 training step(s), loss on training batch is 1.3863.
After 1169 training step(s), loss on training batch is 1.3863.
After 1170 training step(s), loss on training batch is 1.3863.
After 1171 training step(s), loss on training batch is 1.3863.
After 1172 training step(s), loss on training batch is 1.3863.
After 1173 training step(s), loss on training batch is 1.3863.
After 1174 training step(s), loss on training batch is 1.38631.
After 1175 training step(s), loss on training batch is 1.3863.
After 1176 training step(s), loss on training batch is 1.3863.
After 1177 training step(s), loss on training batch is 1.3863.
After 1178 training step(s), loss on training batch is 1.3863.
After 1179 training step(s), loss on training batch is 1.3863.
After 1180 training step(s), loss on training batch is 1.3863.
After 1181 training step(s), loss on training batch is 1.3863.
After 1182 training step(s), loss on training batch is 1.3863.
After 1183 training step(s), loss on training batch is 1.3863.
After 1184 training step(s), loss on training batch is 1.3863.
After 1185 training step(s), loss on training batch is 1.3863.
After 1186 training step(s), loss on training batch is 1.3863.
After 1187 training step(s), loss on training batch is 1.3863.
After 1188 training step(s), loss on training batch is 1.3863.
After 1189 training step(s), loss on training batch is 1.3863.
After 1190 training step(s), loss on training batch is 1.3863.
After 1191 training step(s), loss on training batch is 1.3863.
After 1192 training step(s), loss on training batch is 1.3863.
After 1193 training step(s), loss on training batch is 1.3863.
After 1194 training step(s), loss on training batch is 1.3863.
After 1195 training step(s), loss on training batch is 1.3863.
After 1196 training step(s), loss on training batch is 1.3863.
After 1197 training step(s), loss on training batch is 1.3863.
After 1198 training step(s), loss on training batch is 1.3863.
After 1199 training step(s), loss on training batch is 1.3863.
After 1200 training step(s), loss on training batch is 1.3863.
After 1201 training step(s), loss on training batch is 1.38629.
After 1202 training step(s), loss on training batch is 1.38631.
After 1203 training step(s), loss on training batch is 1.38628.
After 1204 training step(s), loss on training batch is 1.38628.
After 1205 training step(s), loss on training batch is 1.3863.
After 1206 training step(s), loss on training batch is 1.38629.
After 1207 training step(s), loss on training batch is 1.38629.
After 1208 training step(s), loss on training batch is 1.38628.
After 1209 training step(s), loss on training batch is 1.38628.
After 1210 training step(s), loss on training batch is 1.38628.
After 1211 training step(s), loss on training batch is 1.38629.
After 1212 training step(s), loss on training batch is 1.38629.
After 1213 training step(s), loss on training batch is 1.38629.
After 1214 training step(s), loss on training batch is 1.38629.
After 1215 training step(s), loss on training batch is 1.38629.
After 1216 training step(s), loss on training batch is 1.38629.
After 1217 training step(s), loss on training batch is 1.38629.
After 1218 training step(s), loss on training batch is 1.38629.
After 1219 training step(s), loss on training batch is 1.38629.
After 1220 training step(s), loss on training batch is 1.38629.
After 1221 training step(s), loss on training batch is 1.38629.
After 1222 training step(s), loss on training batch is 1.38629.
After 1223 training step(s), loss on training batch is 1.38629.
After 1224 training step(s), loss on training batch is 1.38629.
After 1225 training step(s), loss on training batch is 1.38629.
After 1226 training step(s), loss on training batch is 1.38629.
After 1227 training step(s), loss on training batch is 1.38628.
After 1228 training step(s), loss on training batch is 1.38629.
After 1229 training step(s), loss on training batch is 1.38628.
After 1230 training step(s), loss on training batch is 1.38628.
After 1231 training step(s), loss on training batch is 1.38629.
After 1232 training step(s), loss on training batch is 1.38628.
After 1233 training step(s), loss on training batch is 1.38629.
After 1234 training step(s), loss on training batch is 1.38628.
After 1235 training step(s), loss on training batch is 1.38629.
After 1236 training step(s), loss on training batch is 1.38628.
After 1237 training step(s), loss on training batch is 1.38629.
After 1238 training step(s), loss on training batch is 1.38629.
After 1239 training step(s), loss on training batch is 1.38629.
After 1240 training step(s), loss on training batch is 1.38628.
After 1241 training step(s), loss on training batch is 1.38628.
After 1242 training step(s), loss on training batch is 1.38628.
After 1243 training step(s), loss on training batch is 1.38629.
After 1244 training step(s), loss on training batch is 1.38629.
After 1245 training step(s), loss on training batch is 1.38629.
After 1246 training step(s), loss on training batch is 1.38629.
After 1247 training step(s), loss on training batch is 1.38629.
After 1248 training step(s), loss on training batch is 1.38629.
After 1249 training step(s), loss on training batch is 1.38628.
After 1250 training step(s), loss on training batch is 1.38629.
After 1251 training step(s), loss on training batch is 1.38629.
After 1252 training step(s), loss on training batch is 1.38628.
After 1253 training step(s), loss on training batch is 1.38629.
After 1254 training step(s), loss on training batch is 1.38629.
After 1255 training step(s), loss on training batch is 1.3863.
After 1256 training step(s), loss on training batch is 1.38629.
After 1257 training step(s), loss on training batch is 1.38629.
After 1258 training step(s), loss on training batch is 1.38629.
After 1259 training step(s), loss on training batch is 1.38629.
After 1260 training step(s), loss on training batch is 1.38629.
After 1261 training step(s), loss on training batch is 1.38629.
After 1262 training step(s), loss on training batch is 1.38628.
After 1263 training step(s), loss on training batch is 1.38629.
After 1264 training step(s), loss on training batch is 1.38629.
After 1265 training step(s), loss on training batch is 1.38628.
After 1266 training step(s), loss on training batch is 1.38629.
After 1267 training step(s), loss on training batch is 1.38629.
After 1268 training step(s), loss on training batch is 1.38629.
After 1269 training step(s), loss on training batch is 1.38628.
After 1270 training step(s), loss on training batch is 1.38628.
After 1271 training step(s), loss on training batch is 1.38628.
After 1272 training step(s), loss on training batch is 1.38629.
After 1273 training step(s), loss on training batch is 1.38629.
After 1274 training step(s), loss on training batch is 1.38629.
After 1275 training step(s), loss on training batch is 1.38629.
After 1276 training step(s), loss on training batch is 1.38629.
After 1277 training step(s), loss on training batch is 1.38629.
After 1278 training step(s), loss on training batch is 1.38629.
After 1279 training step(s), loss on training batch is 1.38629.
After 1280 training step(s), loss on training batch is 1.38629.
After 1281 training step(s), loss on training batch is 1.38629.
After 1282 training step(s), loss on training batch is 1.38631.
After 1283 training step(s), loss on training batch is 1.3863.
After 1284 training step(s), loss on training batch is 1.3863.
After 1285 training step(s), loss on training batch is 1.38631.
After 1286 training step(s), loss on training batch is 1.38631.
After 1287 training step(s), loss on training batch is 1.38631.
After 1288 training step(s), loss on training batch is 1.3863.
After 1289 training step(s), loss on training batch is 1.3863.
After 1290 training step(s), loss on training batch is 1.3863.
After 1291 training step(s), loss on training batch is 1.38631.
After 1292 training step(s), loss on training batch is 1.3863.
After 1293 training step(s), loss on training batch is 1.3863.
After 1294 training step(s), loss on training batch is 1.3863.
After 1295 training step(s), loss on training batch is 1.3863.
After 1296 training step(s), loss on training batch is 1.3863.
After 1297 training step(s), loss on training batch is 1.3863.
After 1298 training step(s), loss on training batch is 1.38631.
After 1299 training step(s), loss on training batch is 1.38631.
After 1300 training step(s), loss on training batch is 1.3863.
After 1301 training step(s), loss on training batch is 1.3863.
After 1302 training step(s), loss on training batch is 1.38629.
After 1303 training step(s), loss on training batch is 1.3863.
After 1304 training step(s), loss on training batch is 1.38629.
After 1305 training step(s), loss on training batch is 1.38629.
After 1306 training step(s), loss on training batch is 1.3863.
After 1307 training step(s), loss on training batch is 1.38629.
After 1308 training step(s), loss on training batch is 1.38629.
After 1309 training step(s), loss on training batch is 1.38629.
After 1310 training step(s), loss on training batch is 1.38629.
After 1311 training step(s), loss on training batch is 1.38629.
After 1312 training step(s), loss on training batch is 1.38629.
After 1313 training step(s), loss on training batch is 1.38629.
After 1314 training step(s), loss on training batch is 1.38629.
After 1315 training step(s), loss on training batch is 1.38629.
After 1316 training step(s), loss on training batch is 1.3863.
After 1317 training step(s), loss on training batch is 1.38628.
After 1318 training step(s), loss on training batch is 1.38628.
After 1319 training step(s), loss on training batch is 1.38629.
After 1320 training step(s), loss on training batch is 1.38629.
After 1321 training step(s), loss on training batch is 1.38629.
After 1322 training step(s), loss on training batch is 1.38629.
After 1323 training step(s), loss on training batch is 1.38629.
After 1324 training step(s), loss on training batch is 1.38629.
After 1325 training step(s), loss on training batch is 1.38629.
After 1326 training step(s), loss on training batch is 1.38629.
After 1327 training step(s), loss on training batch is 1.38628.
After 1328 training step(s), loss on training batch is 1.38628.
After 1329 training step(s), loss on training batch is 1.38628.
After 1330 training step(s), loss on training batch is 1.38629.
After 1331 training step(s), loss on training batch is 1.38629.
After 1332 training step(s), loss on training batch is 1.38629.
After 1333 training step(s), loss on training batch is 1.38629.
After 1334 training step(s), loss on training batch is 1.38629.
After 1335 training step(s), loss on training batch is 1.38629.
After 1336 training step(s), loss on training batch is 1.38629.
After 1337 training step(s), loss on training batch is 1.38629.
After 1338 training step(s), loss on training batch is 1.38629.
After 1339 training step(s), loss on training batch is 1.38629.
After 1340 training step(s), loss on training batch is 1.38629.
After 1341 training step(s), loss on training batch is 1.38629.
After 1342 training step(s), loss on training batch is 1.38629.
After 1343 training step(s), loss on training batch is 1.38629.
After 1344 training step(s), loss on training batch is 1.38629.
After 1345 training step(s), loss on training batch is 1.38629.
After 1346 training step(s), loss on training batch is 1.38629.
After 1347 training step(s), loss on training batch is 1.38629.
After 1348 training step(s), loss on training batch is 1.38629.
After 1349 training step(s), loss on training batch is 1.38629.
After 1350 training step(s), loss on training batch is 1.3863.
After 1351 training step(s), loss on training batch is 1.38629.
After 1352 training step(s), loss on training batch is 1.38629.
After 1353 training step(s), loss on training batch is 1.38629.
After 1354 training step(s), loss on training batch is 1.38629.
After 1355 training step(s), loss on training batch is 1.38629.
After 1356 training step(s), loss on training batch is 1.38629.
After 1357 training step(s), loss on training batch is 1.3863.
After 1358 training step(s), loss on training batch is 1.38631.
After 1359 training step(s), loss on training batch is 1.38631.
After 1360 training step(s), loss on training batch is 1.3863.
After 1361 training step(s), loss on training batch is 1.38628.
After 1362 training step(s), loss on training batch is 1.38631.
After 1363 training step(s), loss on training batch is 1.3863.
After 1364 training step(s), loss on training batch is 1.38631.
After 1365 training step(s), loss on training batch is 1.3863.
After 1366 training step(s), loss on training batch is 1.38631.
After 1367 training step(s), loss on training batch is 1.3863.
After 1368 training step(s), loss on training batch is 1.38631.
After 1369 training step(s), loss on training batch is 1.38631.
After 1370 training step(s), loss on training batch is 1.38631.
After 1371 training step(s), loss on training batch is 1.38631.
After 1372 training step(s), loss on training batch is 1.38631.
After 1373 training step(s), loss on training batch is 1.38631.
After 1374 training step(s), loss on training batch is 1.38631.
After 1375 training step(s), loss on training batch is 1.38631.
After 1376 training step(s), loss on training batch is 1.3863.
After 1377 training step(s), loss on training batch is 1.38631.
After 1378 training step(s), loss on training batch is 1.38631.
After 1379 training step(s), loss on training batch is 1.38631.
After 1380 training step(s), loss on training batch is 1.38631.
After 1381 training step(s), loss on training batch is 1.38631.
After 1382 training step(s), loss on training batch is 1.3863.
After 1383 training step(s), loss on training batch is 1.38629.
After 1384 training step(s), loss on training batch is 1.38629.
After 1385 training step(s), loss on training batch is 1.38629.
After 1386 training step(s), loss on training batch is 1.38629.
After 1387 training step(s), loss on training batch is 1.38629.
After 1388 training step(s), loss on training batch is 1.38629.
After 1389 training step(s), loss on training batch is 1.38629.
After 1390 training step(s), loss on training batch is 1.38629.
After 1391 training step(s), loss on training batch is 1.38629.
After 1392 training step(s), loss on training batch is 1.38629.
After 1393 training step(s), loss on training batch is 1.38629.
After 1394 training step(s), loss on training batch is 1.38629.
After 1395 training step(s), loss on training batch is 1.38629.
After 1396 training step(s), loss on training batch is 1.38629.
After 1397 training step(s), loss on training batch is 1.38629.
After 1398 training step(s), loss on training batch is 1.38629.
After 1399 training step(s), loss on training batch is 1.38629.
After 1400 training step(s), loss on training batch is 1.38628.
After 1401 training step(s), loss on training batch is 1.38629.
After 1402 training step(s), loss on training batch is 1.38629.
After 1403 training step(s), loss on training batch is 1.38629.
After 1404 training step(s), loss on training batch is 1.38629.
After 1405 training step(s), loss on training batch is 1.38629.
After 1406 training step(s), loss on training batch is 1.38629.
After 1407 training step(s), loss on training batch is 1.38629.
After 1408 training step(s), loss on training batch is 1.38629.
After 1409 training step(s), loss on training batch is 1.38629.
After 1410 training step(s), loss on training batch is 1.38629.
After 1411 training step(s), loss on training batch is 1.38629.
After 1412 training step(s), loss on training batch is 1.38629.
After 1413 training step(s), loss on training batch is 1.38629.
After 1414 training step(s), loss on training batch is 1.38629.
After 1415 training step(s), loss on training batch is 1.38629.
After 1416 training step(s), loss on training batch is 1.38629.
After 1417 training step(s), loss on training batch is 1.38629.
After 1418 training step(s), loss on training batch is 1.38629.
After 1419 training step(s), loss on training batch is 1.38629.
After 1420 training step(s), loss on training batch is 1.38628.
After 1421 training step(s), loss on training batch is 1.38629.
After 1422 training step(s), loss on training batch is 1.38629.
After 1423 training step(s), loss on training batch is 1.38629.
After 1424 training step(s), loss on training batch is 1.38629.
After 1425 training step(s), loss on training batch is 1.38628.
After 1426 training step(s), loss on training batch is 1.38629.
After 1427 training step(s), loss on training batch is 1.38629.
After 1428 training step(s), loss on training batch is 1.38628.
After 1429 training step(s), loss on training batch is 1.38629.
After 1430 training step(s), loss on training batch is 1.38629.
After 1431 training step(s), loss on training batch is 1.38629.
After 1432 training step(s), loss on training batch is 1.38629.
After 1433 training step(s), loss on training batch is 1.38629.
After 1434 training step(s), loss on training batch is 1.38629.
After 1435 training step(s), loss on training batch is 1.38629.
After 1436 training step(s), loss on training batch is 1.38629.
After 1437 training step(s), loss on training batch is 1.38629.
After 1438 training step(s), loss on training batch is 1.38629.
After 1439 training step(s), loss on training batch is 1.38629.
After 1440 training step(s), loss on training batch is 1.38629.
After 1441 training step(s), loss on training batch is 1.38629.
After 1442 training step(s), loss on training batch is 1.38629.
After 1443 training step(s), loss on training batch is 1.38629.
After 1444 training step(s), loss on training batch is 1.38629.
After 1445 training step(s), loss on training batch is 1.38629.
After 1446 training step(s), loss on training batch is 1.38629.
After 1447 training step(s), loss on training batch is 1.38629.
After 1448 training step(s), loss on training batch is 1.38629.
After 1449 training step(s), loss on training batch is 1.38629.
After 1450 training step(s), loss on training batch is 1.38629.
After 1451 training step(s), loss on training batch is 1.38629.
After 1452 training step(s), loss on training batch is 1.38629.
After 1453 training step(s), loss on training batch is 1.38629.
After 1454 training step(s), loss on training batch is 1.38629.
After 1455 training step(s), loss on training batch is 1.38629.
After 1456 training step(s), loss on training batch is 1.38629.
After 1457 training step(s), loss on training batch is 1.38629.
After 1458 training step(s), loss on training batch is 1.38629.
After 1459 training step(s), loss on training batch is 1.38629.
After 1460 training step(s), loss on training batch is 1.38629.
After 1461 training step(s), loss on training batch is 1.38629.
After 1462 training step(s), loss on training batch is 1.38629.
After 1463 training step(s), loss on training batch is 1.38629.
After 1464 training step(s), loss on training batch is 1.38629.
After 1465 training step(s), loss on training batch is 1.38629.
After 1466 training step(s), loss on training batch is 1.38629.
After 1467 training step(s), loss on training batch is 1.38629.
After 1468 training step(s), loss on training batch is 1.38629.
After 1469 training step(s), loss on training batch is 1.38629.
After 1470 training step(s), loss on training batch is 1.38629.
After 1471 training step(s), loss on training batch is 1.38629.
After 1472 training step(s), loss on training batch is 1.38629.
After 1473 training step(s), loss on training batch is 1.38629.
After 1474 training step(s), loss on training batch is 1.38629.
After 1475 training step(s), loss on training batch is 1.38629.
After 1476 training step(s), loss on training batch is 1.38629.
After 1477 training step(s), loss on training batch is 1.38629.
After 1478 training step(s), loss on training batch is 1.38629.
After 1479 training step(s), loss on training batch is 1.38629.
After 1480 training step(s), loss on training batch is 1.38629.
After 1481 training step(s), loss on training batch is 1.38629.
After 1482 training step(s), loss on training batch is 1.38629.
After 1483 training step(s), loss on training batch is 1.38629.
After 1484 training step(s), loss on training batch is 1.38629.
After 1485 training step(s), loss on training batch is 1.38629.
After 1486 training step(s), loss on training batch is 1.38629.
After 1487 training step(s), loss on training batch is 1.38629.
After 1488 training step(s), loss on training batch is 1.38629.
After 1489 training step(s), loss on training batch is 1.38629.
After 1490 training step(s), loss on training batch is 1.38629.
After 1491 training step(s), loss on training batch is 1.38629.
After 1492 training step(s), loss on training batch is 1.38629.
After 1493 training step(s), loss on training batch is 1.38629.
After 1494 training step(s), loss on training batch is 1.38629.
After 1495 training step(s), loss on training batch is 1.38629.
After 1496 training step(s), loss on training batch is 1.38629.
After 1497 training step(s), loss on training batch is 1.38629.
After 1498 training step(s), loss on training batch is 1.38629.
After 1499 training step(s), loss on training batch is 1.38629.
After 1500 training step(s), loss on training batch is 1.38629.
After 1501 training step(s), loss on training batch is 1.38629.
After 1502 training step(s), loss on training batch is 1.38629.
After 1503 training step(s), loss on training batch is 1.38629.
After 1504 training step(s), loss on training batch is 1.38629.
After 1505 training step(s), loss on training batch is 1.38629.
After 1506 training step(s), loss on training batch is 1.38629.
After 1507 training step(s), loss on training batch is 1.38629.
After 1508 training step(s), loss on training batch is 1.38629.
After 1509 training step(s), loss on training batch is 1.38628.
After 1510 training step(s), loss on training batch is 1.38629.
After 1511 training step(s), loss on training batch is 1.38629.
After 1512 training step(s), loss on training batch is 1.38629.
After 1513 training step(s), loss on training batch is 1.38629.
After 1514 training step(s), loss on training batch is 1.38629.
After 1515 training step(s), loss on training batch is 1.38629.
After 1516 training step(s), loss on training batch is 1.38629.
After 1517 training step(s), loss on training batch is 1.38629.
After 1518 training step(s), loss on training batch is 1.38629.
After 1519 training step(s), loss on training batch is 1.38629.
After 1520 training step(s), loss on training batch is 1.38629.
After 1521 training step(s), loss on training batch is 1.38629.
After 1522 training step(s), loss on training batch is 1.38629.
After 1523 training step(s), loss on training batch is 1.38629.
After 1524 training step(s), loss on training batch is 1.38629.
After 1525 training step(s), loss on training batch is 1.38629.
After 1526 training step(s), loss on training batch is 1.38629.
After 1527 training step(s), loss on training batch is 1.38629.
After 1528 training step(s), loss on training batch is 1.38629.
After 1529 training step(s), loss on training batch is 1.38629.
After 1530 training step(s), loss on training batch is 1.38629.
After 1531 training step(s), loss on training batch is 1.38629.
After 1532 training step(s), loss on training batch is 1.38629.
After 1533 training step(s), loss on training batch is 1.38629.
After 1534 training step(s), loss on training batch is 1.38629.
After 1535 training step(s), loss on training batch is 1.38629.
After 1536 training step(s), loss on training batch is 1.38629.
After 1537 training step(s), loss on training batch is 1.38629.
After 1538 training step(s), loss on training batch is 1.38628.
After 1539 training step(s), loss on training batch is 1.38629.
After 1540 training step(s), loss on training batch is 1.38629.
After 1541 training step(s), loss on training batch is 1.38629.
After 1542 training step(s), loss on training batch is 1.38629.
After 1543 training step(s), loss on training batch is 1.3863.
After 1544 training step(s), loss on training batch is 1.38631.
After 1545 training step(s), loss on training batch is 1.3863.
After 1546 training step(s), loss on training batch is 1.3863.
After 1547 training step(s), loss on training batch is 1.38631.
After 1548 training step(s), loss on training batch is 1.3863.
After 1549 training step(s), loss on training batch is 1.3863.
After 1550 training step(s), loss on training batch is 1.38631.
After 1551 training step(s), loss on training batch is 1.3863.
After 1552 training step(s), loss on training batch is 1.3863.
After 1553 training step(s), loss on training batch is 1.3863.
After 1554 training step(s), loss on training batch is 1.3863.
After 1555 training step(s), loss on training batch is 1.38631.
After 1556 training step(s), loss on training batch is 1.3863.
After 1557 training step(s), loss on training batch is 1.38631.
After 1558 training step(s), loss on training batch is 1.3863.
After 1559 training step(s), loss on training batch is 1.3863.
After 1560 training step(s), loss on training batch is 1.38631.
After 1561 training step(s), loss on training batch is 1.3863.
After 1562 training step(s), loss on training batch is 1.3863.
After 1563 training step(s), loss on training batch is 1.3863.
After 1564 training step(s), loss on training batch is 1.3863.
After 1565 training step(s), loss on training batch is 1.3863.
After 1566 training step(s), loss on training batch is 1.38631.
After 1567 training step(s), loss on training batch is 1.38631.
After 1568 training step(s), loss on training batch is 1.38631.
After 1569 training step(s), loss on training batch is 1.38631.
After 1570 training step(s), loss on training batch is 1.38631.
After 1571 training step(s), loss on training batch is 1.3863.
After 1572 training step(s), loss on training batch is 1.3863.
After 1573 training step(s), loss on training batch is 1.3863.
After 1574 training step(s), loss on training batch is 1.38631.
After 1575 training step(s), loss on training batch is 1.3863.
After 1576 training step(s), loss on training batch is 1.38631.
After 1577 training step(s), loss on training batch is 1.3863.
After 1578 training step(s), loss on training batch is 1.3863.
After 1579 training step(s), loss on training batch is 1.38631.
After 1580 training step(s), loss on training batch is 1.38631.
After 1581 training step(s), loss on training batch is 1.38631.
After 1582 training step(s), loss on training batch is 1.3863.
After 1583 training step(s), loss on training batch is 1.38631.
After 1584 training step(s), loss on training batch is 1.3863.
After 1585 training step(s), loss on training batch is 1.38631.
After 1586 training step(s), loss on training batch is 1.3863.
After 1587 training step(s), loss on training batch is 1.3863.
After 1588 training step(s), loss on training batch is 1.3863.
After 1589 training step(s), loss on training batch is 1.3863.
After 1590 training step(s), loss on training batch is 1.3863.
After 1591 training step(s), loss on training batch is 1.3863.
After 1592 training step(s), loss on training batch is 1.3863.
After 1593 training step(s), loss on training batch is 1.3863.
After 1594 training step(s), loss on training batch is 1.3863.
After 1595 training step(s), loss on training batch is 1.3863.
After 1596 training step(s), loss on training batch is 1.3863.
After 1597 training step(s), loss on training batch is 1.3863.
After 1598 training step(s), loss on training batch is 1.3863.
After 1599 training step(s), loss on training batch is 1.3863.
After 1600 training step(s), loss on training batch is 1.3863.
After 1601 training step(s), loss on training batch is 1.3863.
After 1602 training step(s), loss on training batch is 1.38631.
After 1603 training step(s), loss on training batch is 1.38628.
After 1604 training step(s), loss on training batch is 1.38628.
After 1605 training step(s), loss on training batch is 1.3863.
After 1606 training step(s), loss on training batch is 1.38629.
After 1607 training step(s), loss on training batch is 1.38629.
After 1608 training step(s), loss on training batch is 1.38629.
After 1609 training step(s), loss on training batch is 1.38629.
After 1610 training step(s), loss on training batch is 1.38628.
After 1611 training step(s), loss on training batch is 1.38629.
After 1612 training step(s), loss on training batch is 1.38629.
After 1613 training step(s), loss on training batch is 1.38629.
After 1614 training step(s), loss on training batch is 1.38629.
After 1615 training step(s), loss on training batch is 1.38629.
After 1616 training step(s), loss on training batch is 1.38629.
After 1617 training step(s), loss on training batch is 1.38629.
After 1618 training step(s), loss on training batch is 1.38629.
After 1619 training step(s), loss on training batch is 1.38629.
After 1620 training step(s), loss on training batch is 1.38629.
After 1621 training step(s), loss on training batch is 1.38629.
After 1622 training step(s), loss on training batch is 1.38629.
After 1623 training step(s), loss on training batch is 1.38629.
After 1624 training step(s), loss on training batch is 1.38629.
After 1625 training step(s), loss on training batch is 1.38629.
After 1626 training step(s), loss on training batch is 1.38629.
After 1627 training step(s), loss on training batch is 1.38628.
After 1628 training step(s), loss on training batch is 1.38629.
After 1629 training step(s), loss on training batch is 1.38628.
After 1630 training step(s), loss on training batch is 1.38629.
After 1631 training step(s), loss on training batch is 1.38628.
After 1632 training step(s), loss on training batch is 1.38629.
After 1633 training step(s), loss on training batch is 1.38629.
After 1634 training step(s), loss on training batch is 1.38629.
After 1635 training step(s), loss on training batch is 1.38629.
After 1636 training step(s), loss on training batch is 1.38629.
After 1637 training step(s), loss on training batch is 1.38629.
After 1638 training step(s), loss on training batch is 1.38629.
After 1639 training step(s), loss on training batch is 1.38629.
After 1640 training step(s), loss on training batch is 1.38629.
After 1641 training step(s), loss on training batch is 1.38629.
After 1642 training step(s), loss on training batch is 1.38629.
After 1643 training step(s), loss on training batch is 1.38629.
After 1644 training step(s), loss on training batch is 1.38629.
After 1645 training step(s), loss on training batch is 1.38629.
After 1646 training step(s), loss on training batch is 1.38629.
After 1647 training step(s), loss on training batch is 1.38629.
After 1648 training step(s), loss on training batch is 1.38629.
After 1649 training step(s), loss on training batch is 1.38629.
After 1650 training step(s), loss on training batch is 1.38629.
After 1651 training step(s), loss on training batch is 1.38629.
After 1652 training step(s), loss on training batch is 1.38629.
After 1653 training step(s), loss on training batch is 1.38629.
After 1654 training step(s), loss on training batch is 1.38629.
After 1655 training step(s), loss on training batch is 1.3863.
After 1656 training step(s), loss on training batch is 1.38629.
After 1657 training step(s), loss on training batch is 1.38629.
After 1658 training step(s), loss on training batch is 1.38629.
After 1659 training step(s), loss on training batch is 1.38629.
After 1660 training step(s), loss on training batch is 1.38629.
After 1661 training step(s), loss on training batch is 1.38629.
After 1662 training step(s), loss on training batch is 1.38628.
After 1663 training step(s), loss on training batch is 1.38629.
After 1664 training step(s), loss on training batch is 1.38629.
After 1665 training step(s), loss on training batch is 1.38628.
After 1666 training step(s), loss on training batch is 1.38629.
After 1667 training step(s), loss on training batch is 1.38629.
After 1668 training step(s), loss on training batch is 1.38629.
After 1669 training step(s), loss on training batch is 1.38629.
After 1670 training step(s), loss on training batch is 1.38629.
After 1671 training step(s), loss on training batch is 1.38628.
After 1672 training step(s), loss on training batch is 1.38629.
After 1673 training step(s), loss on training batch is 1.38629.
After 1674 training step(s), loss on training batch is 1.38629.
After 1675 training step(s), loss on training batch is 1.38629.
After 1676 training step(s), loss on training batch is 1.38629.
After 1677 training step(s), loss on training batch is 1.38629.
After 1678 training step(s), loss on training batch is 1.38629.
After 1679 training step(s), loss on training batch is 1.38629.
After 1680 training step(s), loss on training batch is 1.38629.
After 1681 training step(s), loss on training batch is 1.38629.
After 1682 training step(s), loss on training batch is 1.3863.
After 1683 training step(s), loss on training batch is 1.3863.
After 1684 training step(s), loss on training batch is 1.38631.
After 1685 training step(s), loss on training batch is 1.38631.
After 1686 training step(s), loss on training batch is 1.38631.
After 1687 training step(s), loss on training batch is 1.38631.
After 1688 training step(s), loss on training batch is 1.3863.
After 1689 training step(s), loss on training batch is 1.3863.
After 1690 training step(s), loss on training batch is 1.3863.
After 1691 training step(s), loss on training batch is 1.38631.
After 1692 training step(s), loss on training batch is 1.3863.
After 1693 training step(s), loss on training batch is 1.3863.
After 1694 training step(s), loss on training batch is 1.3863.
After 1695 training step(s), loss on training batch is 1.3863.
After 1696 training step(s), loss on training batch is 1.3863.
After 1697 training step(s), loss on training batch is 1.3863.
After 1698 training step(s), loss on training batch is 1.38631.
After 1699 training step(s), loss on training batch is 1.38631.
After 1700 training step(s), loss on training batch is 1.3863.
After 1701 training step(s), loss on training batch is 1.3863.
After 1702 training step(s), loss on training batch is 1.3863.
After 1703 training step(s), loss on training batch is 1.38628.
After 1704 training step(s), loss on training batch is 1.38629.
After 1705 training step(s), loss on training batch is 1.3863.
After 1706 training step(s), loss on training batch is 1.3863.
After 1707 training step(s), loss on training batch is 1.3863.
After 1708 training step(s), loss on training batch is 1.38629.
After 1709 training step(s), loss on training batch is 1.3863.
After 1710 training step(s), loss on training batch is 1.3863.
After 1711 training step(s), loss on training batch is 1.3863.
After 1712 training step(s), loss on training batch is 1.3863.
After 1713 training step(s), loss on training batch is 1.3863.
After 1714 training step(s), loss on training batch is 1.3863.
After 1715 training step(s), loss on training batch is 1.38629.
After 1716 training step(s), loss on training batch is 1.3863.
After 1717 training step(s), loss on training batch is 1.38629.
After 1718 training step(s), loss on training batch is 1.38628.
After 1719 training step(s), loss on training batch is 1.38629.
After 1720 training step(s), loss on training batch is 1.38629.
After 1721 training step(s), loss on training batch is 1.38629.
After 1722 training step(s), loss on training batch is 1.38629.
After 1723 training step(s), loss on training batch is 1.38629.
After 1724 training step(s), loss on training batch is 1.38629.
After 1725 training step(s), loss on training batch is 1.38629.
After 1726 training step(s), loss on training batch is 1.38629.
After 1727 training step(s), loss on training batch is 1.38628.
After 1728 training step(s), loss on training batch is 1.38628.
After 1729 training step(s), loss on training batch is 1.38628.
After 1730 training step(s), loss on training batch is 1.38629.
After 1731 training step(s), loss on training batch is 1.38629.
After 1732 training step(s), loss on training batch is 1.38629.
After 1733 training step(s), loss on training batch is 1.38629.
After 1734 training step(s), loss on training batch is 1.38629.
After 1735 training step(s), loss on training batch is 1.38631.
After 1736 training step(s), loss on training batch is 1.38631.
After 1737 training step(s), loss on training batch is 1.3863.
After 1738 training step(s), loss on training batch is 1.38631.
After 1739 training step(s), loss on training batch is 1.38631.
After 1740 training step(s), loss on training batch is 1.38631.
After 1741 training step(s), loss on training batch is 1.3863.
After 1742 training step(s), loss on training batch is 1.38631.
After 1743 training step(s), loss on training batch is 1.38631.
After 1744 training step(s), loss on training batch is 1.38631.
After 1745 training step(s), loss on training batch is 1.38631.
After 1746 training step(s), loss on training batch is 1.38631.
After 1747 training step(s), loss on training batch is 1.38631.
After 1748 training step(s), loss on training batch is 1.3863.
After 1749 training step(s), loss on training batch is 1.38631.
After 1750 training step(s), loss on training batch is 1.38631.
After 1751 training step(s), loss on training batch is 1.3863.
After 1752 training step(s), loss on training batch is 1.38631.
After 1753 training step(s), loss on training batch is 1.3863.
After 1754 training step(s), loss on training batch is 1.38631.
After 1755 training step(s), loss on training batch is 1.38631.
After 1756 training step(s), loss on training batch is 1.38629.
After 1757 training step(s), loss on training batch is 1.3863.
After 1758 training step(s), loss on training batch is 1.38631.
After 1759 training step(s), loss on training batch is 1.3863.
After 1760 training step(s), loss on training batch is 1.3863.
After 1761 training step(s), loss on training batch is 1.38628.
After 1762 training step(s), loss on training batch is 1.38631.
After 1763 training step(s), loss on training batch is 1.3863.
After 1764 training step(s), loss on training batch is 1.3863.
After 1765 training step(s), loss on training batch is 1.3863.
After 1766 training step(s), loss on training batch is 1.38631.
After 1767 training step(s), loss on training batch is 1.3863.
After 1768 training step(s), loss on training batch is 1.38631.
After 1769 training step(s), loss on training batch is 1.38631.
After 1770 training step(s), loss on training batch is 1.38631.
After 1771 training step(s), loss on training batch is 1.3863.
After 1772 training step(s), loss on training batch is 1.38631.
After 1773 training step(s), loss on training batch is 1.38631.
After 1774 training step(s), loss on training batch is 1.38631.
After 1775 training step(s), loss on training batch is 1.38631.
After 1776 training step(s), loss on training batch is 1.38631.
After 1777 training step(s), loss on training batch is 1.38631.
After 1778 training step(s), loss on training batch is 1.38631.
After 1779 training step(s), loss on training batch is 1.38631.
After 1780 training step(s), loss on training batch is 1.38631.
After 1781 training step(s), loss on training batch is 1.38631.
After 1782 training step(s), loss on training batch is 1.3863.
After 1783 training step(s), loss on training batch is 1.38629.
After 1784 training step(s), loss on training batch is 1.38629.
After 1785 training step(s), loss on training batch is 1.38629.
After 1786 training step(s), loss on training batch is 1.38629.
After 1787 training step(s), loss on training batch is 1.38629.
After 1788 training step(s), loss on training batch is 1.38629.
After 1789 training step(s), loss on training batch is 1.38629.
After 1790 training step(s), loss on training batch is 1.38629.
After 1791 training step(s), loss on training batch is 1.38629.
After 1792 training step(s), loss on training batch is 1.38629.
After 1793 training step(s), loss on training batch is 1.38629.
After 1794 training step(s), loss on training batch is 1.38629.
After 1795 training step(s), loss on training batch is 1.38629.
After 1796 training step(s), loss on training batch is 1.38629.
After 1797 training step(s), loss on training batch is 1.38629.
After 1798 training step(s), loss on training batch is 1.38629.
After 1799 training step(s), loss on training batch is 1.38629.
After 1800 training step(s), loss on training batch is 1.38628.
After 1801 training step(s), loss on training batch is 1.38629.
After 1802 training step(s), loss on training batch is 1.38629.
After 1803 training step(s), loss on training batch is 1.38629.
After 1804 training step(s), loss on training batch is 1.38629.
After 1805 training step(s), loss on training batch is 1.38629.
After 1806 training step(s), loss on training batch is 1.38629.
After 1807 training step(s), loss on training batch is 1.38629.
After 1808 training step(s), loss on training batch is 1.38629.
After 1809 training step(s), loss on training batch is 1.38629.
After 1810 training step(s), loss on training batch is 1.38629.
After 1811 training step(s), loss on training batch is 1.38629.
After 1812 training step(s), loss on training batch is 1.38629.
After 1813 training step(s), loss on training batch is 1.38629.
After 1814 training step(s), loss on training batch is 1.38629.
After 1815 training step(s), loss on training batch is 1.38629.
After 1816 training step(s), loss on training batch is 1.38629.
After 1817 training step(s), loss on training batch is 1.38629.
After 1818 training step(s), loss on training batch is 1.38629.
After 1819 training step(s), loss on training batch is 1.38629.
After 1820 training step(s), loss on training batch is 1.38629.
After 1821 training step(s), loss on training batch is 1.38629.
After 1822 training step(s), loss on training batch is 1.38629.
After 1823 training step(s), loss on training batch is 1.38629.
After 1824 training step(s), loss on training batch is 1.38629.
After 1825 training step(s), loss on training batch is 1.38629.
After 1826 training step(s), loss on training batch is 1.38629.
After 1827 training step(s), loss on training batch is 1.38629.
After 1828 training step(s), loss on training batch is 1.38629.
After 1829 training step(s), loss on training batch is 1.38629.
After 1830 training step(s), loss on training batch is 1.38629.
After 1831 training step(s), loss on training batch is 1.38629.
After 1832 training step(s), loss on training batch is 1.38629.
After 1833 training step(s), loss on training batch is 1.38629.
After 1834 training step(s), loss on training batch is 1.38629.
After 1835 training step(s), loss on training batch is 1.38629.
After 1836 training step(s), loss on training batch is 1.38629.
After 1837 training step(s), loss on training batch is 1.38629.
After 1838 training step(s), loss on training batch is 1.38629.
After 1839 training step(s), loss on training batch is 1.38629.
After 1840 training step(s), loss on training batch is 1.38629.
After 1841 training step(s), loss on training batch is 1.38629.
After 1842 training step(s), loss on training batch is 1.38629.
After 1843 training step(s), loss on training batch is 1.38629.
After 1844 training step(s), loss on training batch is 1.38629.
After 1845 training step(s), loss on training batch is 1.38629.
After 1846 training step(s), loss on training batch is 1.38629.
After 1847 training step(s), loss on training batch is 1.38629.
After 1848 training step(s), loss on training batch is 1.38629.
After 1849 training step(s), loss on training batch is 1.38629.
After 1850 training step(s), loss on training batch is 1.38629.
After 1851 training step(s), loss on training batch is 1.38629.
After 1852 training step(s), loss on training batch is 1.38629.
After 1853 training step(s), loss on training batch is 1.38629.
After 1854 training step(s), loss on training batch is 1.38629.
After 1855 training step(s), loss on training batch is 1.38629.
After 1856 training step(s), loss on training batch is 1.38629.
After 1857 training step(s), loss on training batch is 1.38629.
After 1858 training step(s), loss on training batch is 1.38629.
After 1859 training step(s), loss on training batch is 1.38629.
After 1860 training step(s), loss on training batch is 1.38629.
After 1861 training step(s), loss on training batch is 1.38629.
After 1862 training step(s), loss on training batch is 1.38629.
After 1863 training step(s), loss on training batch is 1.38629.
After 1864 training step(s), loss on training batch is 1.38629.
After 1865 training step(s), loss on training batch is 1.38629.
After 1866 training step(s), loss on training batch is 1.38629.
After 1867 training step(s), loss on training batch is 1.38629.
After 1868 training step(s), loss on training batch is 1.38629.
After 1869 training step(s), loss on training batch is 1.38629.
After 1870 training step(s), loss on training batch is 1.38629.
After 1871 training step(s), loss on training batch is 1.38629.
After 1872 training step(s), loss on training batch is 1.38629.
After 1873 training step(s), loss on training batch is 1.38629.
After 1874 training step(s), loss on training batch is 1.38629.
After 1875 training step(s), loss on training batch is 1.38629.
After 1876 training step(s), loss on training batch is 1.38629.
After 1877 training step(s), loss on training batch is 1.38629.
After 1878 training step(s), loss on training batch is 1.38629.
After 1879 training step(s), loss on training batch is 1.38629.
After 1880 training step(s), loss on training batch is 1.38629.
After 1881 training step(s), loss on training batch is 1.38629.
After 1882 training step(s), loss on training batch is 1.38629.
After 1883 training step(s), loss on training batch is 1.38629.
After 1884 training step(s), loss on training batch is 1.38629.
After 1885 training step(s), loss on training batch is 1.38629.
After 1886 training step(s), loss on training batch is 1.38629.
After 1887 training step(s), loss on training batch is 1.38629.
After 1888 training step(s), loss on training batch is 1.38629.
After 1889 training step(s), loss on training batch is 1.38629.
After 1890 training step(s), loss on training batch is 1.38629.
After 1891 training step(s), loss on training batch is 1.38629.
After 1892 training step(s), loss on training batch is 1.38629.
After 1893 training step(s), loss on training batch is 1.38629.
After 1894 training step(s), loss on training batch is 1.38629.
After 1895 training step(s), loss on training batch is 1.38629.
After 1896 training step(s), loss on training batch is 1.38629.
After 1897 training step(s), loss on training batch is 1.38629.
After 1898 training step(s), loss on training batch is 1.38629.
After 1899 training step(s), loss on training batch is 1.38629.
After 1900 training step(s), loss on training batch is 1.38629.
After 1901 training step(s), loss on training batch is 1.38629.
After 1902 training step(s), loss on training batch is 1.38629.
After 1903 training step(s), loss on training batch is 1.38629.
After 1904 training step(s), loss on training batch is 1.38629.
After 1905 training step(s), loss on training batch is 1.38629.
After 1906 training step(s), loss on training batch is 1.38629.
After 1907 training step(s), loss on training batch is 1.38629.
After 1908 training step(s), loss on training batch is 1.38629.
After 1909 training step(s), loss on training batch is 1.38628.
After 1910 training step(s), loss on training batch is 1.38629.
After 1911 training step(s), loss on training batch is 1.38629.
After 1912 training step(s), loss on training batch is 1.38629.
After 1913 training step(s), loss on training batch is 1.38629.
After 1914 training step(s), loss on training batch is 1.38629.
After 1915 training step(s), loss on training batch is 1.38629.
After 1916 training step(s), loss on training batch is 1.38628.
After 1917 training step(s), loss on training batch is 1.38628.
After 1918 training step(s), loss on training batch is 1.38628.
After 1919 training step(s), loss on training batch is 1.38628.
After 1920 training step(s), loss on training batch is 1.38628.
After 1921 training step(s), loss on training batch is 1.38628.
After 1922 training step(s), loss on training batch is 1.38628.
After 1923 training step(s), loss on training batch is 1.38628.
After 1924 training step(s), loss on training batch is 1.38628.
After 1925 training step(s), loss on training batch is 1.38628.
After 1926 training step(s), loss on training batch is 1.38629.
After 1927 training step(s), loss on training batch is 1.38629.
After 1928 training step(s), loss on training batch is 1.38629.
After 1929 training step(s), loss on training batch is 1.38629.
After 1930 training step(s), loss on training batch is 1.38629.
After 1931 training step(s), loss on training batch is 1.38629.
After 1932 training step(s), loss on training batch is 1.38629.
After 1933 training step(s), loss on training batch is 1.38629.
After 1934 training step(s), loss on training batch is 1.38629.
After 1935 training step(s), loss on training batch is 1.38629.
After 1936 training step(s), loss on training batch is 1.38629.
After 1937 training step(s), loss on training batch is 1.38629.
After 1938 training step(s), loss on training batch is 1.38628.
After 1939 training step(s), loss on training batch is 1.38629.
After 1940 training step(s), loss on training batch is 1.38629.
After 1941 training step(s), loss on training batch is 1.38629.
After 1942 training step(s), loss on training batch is 1.38628.
After 1943 training step(s), loss on training batch is 1.3863.
After 1944 training step(s), loss on training batch is 1.3863.
After 1945 training step(s), loss on training batch is 1.3863.
After 1946 training step(s), loss on training batch is 1.3863.
After 1947 training step(s), loss on training batch is 1.3863.
After 1948 training step(s), loss on training batch is 1.3863.
After 1949 training step(s), loss on training batch is 1.3863.
After 1950 training step(s), loss on training batch is 1.38631.
After 1951 training step(s), loss on training batch is 1.3863.
After 1952 training step(s), loss on training batch is 1.3863.
After 1953 training step(s), loss on training batch is 1.3863.
After 1954 training step(s), loss on training batch is 1.3863.
After 1955 training step(s), loss on training batch is 1.38631.
After 1956 training step(s), loss on training batch is 1.3863.
After 1957 training step(s), loss on training batch is 1.38631.
After 1958 training step(s), loss on training batch is 1.3863.
After 1959 training step(s), loss on training batch is 1.3863.
After 1960 training step(s), loss on training batch is 1.38631.
After 1961 training step(s), loss on training batch is 1.3863.
After 1962 training step(s), loss on training batch is 1.3863.
After 1963 training step(s), loss on training batch is 1.3863.
After 1964 training step(s), loss on training batch is 1.3863.
After 1965 training step(s), loss on training batch is 1.3863.
After 1966 training step(s), loss on training batch is 1.3863.
After 1967 training step(s), loss on training batch is 1.3863.
After 1968 training step(s), loss on training batch is 1.3863.
After 1969 training step(s), loss on training batch is 1.3863.
After 1970 training step(s), loss on training batch is 1.3863.
After 1971 training step(s), loss on training batch is 1.3863.
After 1972 training step(s), loss on training batch is 1.3863.
After 1973 training step(s), loss on training batch is 1.3863.
After 1974 training step(s), loss on training batch is 1.38631.
After 1975 training step(s), loss on training batch is 1.3863.
After 1976 training step(s), loss on training batch is 1.38631.
After 1977 training step(s), loss on training batch is 1.3863.
After 1978 training step(s), loss on training batch is 1.3863.
After 1979 training step(s), loss on training batch is 1.38631.
After 1980 training step(s), loss on training batch is 1.38631.
After 1981 training step(s), loss on training batch is 1.38631.
After 1982 training step(s), loss on training batch is 1.3863.
After 1983 training step(s), loss on training batch is 1.38631.
After 1984 training step(s), loss on training batch is 1.3863.
After 1985 training step(s), loss on training batch is 1.38631.
After 1986 training step(s), loss on training batch is 1.3863.
After 1987 training step(s), loss on training batch is 1.3863.
After 1988 training step(s), loss on training batch is 1.3863.
After 1989 training step(s), loss on training batch is 1.3863.
After 1990 training step(s), loss on training batch is 1.3863.
After 1991 training step(s), loss on training batch is 1.3863.
After 1992 training step(s), loss on training batch is 1.3863.
After 1993 training step(s), loss on training batch is 1.3863.
After 1994 training step(s), loss on training batch is 1.3863.
After 1995 training step(s), loss on training batch is 1.3863.
After 1996 training step(s), loss on training batch is 1.3863.
After 1997 training step(s), loss on training batch is 1.3863.
After 1998 training step(s), loss on training batch is 1.3863.
After 1999 training step(s), loss on training batch is 1.3863.
After 2000 training step(s), loss on training batch is 1.3863.
After 2001 training step(s), loss on training batch is 1.38628.
After 2002 training step(s), loss on training batch is 1.38629.
After 2003 training step(s), loss on training batch is 1.38629.
After 2004 training step(s), loss on training batch is 1.38629.
After 2005 training step(s), loss on training batch is 1.3863.
After 2006 training step(s), loss on training batch is 1.38629.
After 2007 training step(s), loss on training batch is 1.38629.
After 2008 training step(s), loss on training batch is 1.38629.
After 2009 training step(s), loss on training batch is 1.38629.
After 2010 training step(s), loss on training batch is 1.38628.
After 2011 training step(s), loss on training batch is 1.38629.
After 2012 training step(s), loss on training batch is 1.38629.
After 2013 training step(s), loss on training batch is 1.38629.
After 2014 training step(s), loss on training batch is 1.38629.
After 2015 training step(s), loss on training batch is 1.38629.
After 2016 training step(s), loss on training batch is 1.38629.
After 2017 training step(s), loss on training batch is 1.38629.
After 2018 training step(s), loss on training batch is 1.38629.
After 2019 training step(s), loss on training batch is 1.38628.
After 2020 training step(s), loss on training batch is 1.38628.
After 2021 training step(s), loss on training batch is 1.38629.
After 2022 training step(s), loss on training batch is 1.38629.
After 2023 training step(s), loss on training batch is 1.38629.
After 2024 training step(s), loss on training batch is 1.38629.
After 2025 training step(s), loss on training batch is 1.38629.
After 2026 training step(s), loss on training batch is 1.38629.
After 2027 training step(s), loss on training batch is 1.3863.
After 2028 training step(s), loss on training batch is 1.38629.
After 2029 training step(s), loss on training batch is 1.38629.
After 2030 training step(s), loss on training batch is 1.38629.
After 2031 training step(s), loss on training batch is 1.38629.
After 2032 training step(s), loss on training batch is 1.38629.
After 2033 training step(s), loss on training batch is 1.38629.
After 2034 training step(s), loss on training batch is 1.38628.
After 2035 training step(s), loss on training batch is 1.38628.
After 2036 training step(s), loss on training batch is 1.38629.
After 2037 training step(s), loss on training batch is 1.38628.
After 2038 training step(s), loss on training batch is 1.38629.
After 2039 training step(s), loss on training batch is 1.38629.
After 2040 training step(s), loss on training batch is 1.38629.
After 2041 training step(s), loss on training batch is 1.38629.
After 2042 training step(s), loss on training batch is 1.38629.
After 2043 training step(s), loss on training batch is 1.38629.
After 2044 training step(s), loss on training batch is 1.38629.
After 2045 training step(s), loss on training batch is 1.38628.
After 2046 training step(s), loss on training batch is 1.38629.
After 2047 training step(s), loss on training batch is 1.38629.
After 2048 training step(s), loss on training batch is 1.38629.
After 2049 training step(s), loss on training batch is 1.38629.
After 2050 training step(s), loss on training batch is 1.38629.
After 2051 training step(s), loss on training batch is 1.38629.
After 2052 training step(s), loss on training batch is 1.38629.
After 2053 training step(s), loss on training batch is 1.38629.
After 2054 training step(s), loss on training batch is 1.38629.
After 2055 training step(s), loss on training batch is 1.38629.
After 2056 training step(s), loss on training batch is 1.38629.
After 2057 training step(s), loss on training batch is 1.38629.
After 2058 training step(s), loss on training batch is 1.38629.
After 2059 training step(s), loss on training batch is 1.38629.
After 2060 training step(s), loss on training batch is 1.38629.
After 2061 training step(s), loss on training batch is 1.38629.
After 2062 training step(s), loss on training batch is 1.38629.
After 2063 training step(s), loss on training batch is 1.38629.
After 2064 training step(s), loss on training batch is 1.38629.
After 2065 training step(s), loss on training batch is 1.38628.
After 2066 training step(s), loss on training batch is 1.38629.
After 2067 training step(s), loss on training batch is 1.38629.
After 2068 training step(s), loss on training batch is 1.38629.
After 2069 training step(s), loss on training batch is 1.38629.
After 2070 training step(s), loss on training batch is 1.38628.
After 2071 training step(s), loss on training batch is 1.38629.
After 2072 training step(s), loss on training batch is 1.38629.
After 2073 training step(s), loss on training batch is 1.38629.
After 2074 training step(s), loss on training batch is 1.38629.
After 2075 training step(s), loss on training batch is 1.38629.
After 2076 training step(s), loss on training batch is 1.38629.
After 2077 training step(s), loss on training batch is 1.38629.
After 2078 training step(s), loss on training batch is 1.38629.
After 2079 training step(s), loss on training batch is 1.38629.
After 2080 training step(s), loss on training batch is 1.38629.
After 2081 training step(s), loss on training batch is 1.38629.
After 2082 training step(s), loss on training batch is 1.3863.
After 2083 training step(s), loss on training batch is 1.3863.
After 2084 training step(s), loss on training batch is 1.3863.
After 2085 training step(s), loss on training batch is 1.38631.
After 2086 training step(s), loss on training batch is 1.38631.
After 2087 training step(s), loss on training batch is 1.38631.
After 2088 training step(s), loss on training batch is 1.3863.
After 2089 training step(s), loss on training batch is 1.3863.
After 2090 training step(s), loss on training batch is 1.3863.
After 2091 training step(s), loss on training batch is 1.38631.
After 2092 training step(s), loss on training batch is 1.3863.
After 2093 training step(s), loss on training batch is 1.3863.
After 2094 training step(s), loss on training batch is 1.3863.
After 2095 training step(s), loss on training batch is 1.3863.
After 2096 training step(s), loss on training batch is 1.3863.
After 2097 training step(s), loss on training batch is 1.3863.
After 2098 training step(s), loss on training batch is 1.38631.
After 2099 training step(s), loss on training batch is 1.38631.
After 2100 training step(s), loss on training batch is 1.3863.
After 2101 training step(s), loss on training batch is 1.3863.
After 2102 training step(s), loss on training batch is 1.38629.
After 2103 training step(s), loss on training batch is 1.38628.
After 2104 training step(s), loss on training batch is 1.38629.
After 2105 training step(s), loss on training batch is 1.38629.
After 2106 training step(s), loss on training batch is 1.3863.
After 2107 training step(s), loss on training batch is 1.38629.
After 2108 training step(s), loss on training batch is 1.38629.
After 2109 training step(s), loss on training batch is 1.38629.
After 2110 training step(s), loss on training batch is 1.38629.
After 2111 training step(s), loss on training batch is 1.38629.
After 2112 training step(s), loss on training batch is 1.38629.
After 2113 training step(s), loss on training batch is 1.38629.
After 2114 training step(s), loss on training batch is 1.38629.
After 2115 training step(s), loss on training batch is 1.38629.
After 2116 training step(s), loss on training batch is 1.3863.
After 2117 training step(s), loss on training batch is 1.38628.
After 2118 training step(s), loss on training batch is 1.38628.
After 2119 training step(s), loss on training batch is 1.38629.
After 2120 training step(s), loss on training batch is 1.38629.
After 2121 training step(s), loss on training batch is 1.38629.
After 2122 training step(s), loss on training batch is 1.38629.
After 2123 training step(s), loss on training batch is 1.38629.
After 2124 training step(s), loss on training batch is 1.38629.
After 2125 training step(s), loss on training batch is 1.38629.
After 2126 training step(s), loss on training batch is 1.38629.
After 2127 training step(s), loss on training batch is 1.38628.
After 2128 training step(s), loss on training batch is 1.38628.
After 2129 training step(s), loss on training batch is 1.38628.
After 2130 training step(s), loss on training batch is 1.38629.
After 2131 training step(s), loss on training batch is 1.38629.
After 2132 training step(s), loss on training batch is 1.38629.
After 2133 training step(s), loss on training batch is 1.38629.
After 2134 training step(s), loss on training batch is 1.38629.
After 2135 training step(s), loss on training batch is 1.3863.
After 2136 training step(s), loss on training batch is 1.3863.
After 2137 training step(s), loss on training batch is 1.3863.
After 2138 training step(s), loss on training batch is 1.3863.
After 2139 training step(s), loss on training batch is 1.38629.
After 2140 training step(s), loss on training batch is 1.38629.
After 2141 training step(s), loss on training batch is 1.38629.
After 2142 training step(s), loss on training batch is 1.38629.
After 2143 training step(s), loss on training batch is 1.38629.
After 2144 training step(s), loss on training batch is 1.38629.
After 2145 training step(s), loss on training batch is 1.38629.
After 2146 training step(s), loss on training batch is 1.38629.
After 2147 training step(s), loss on training batch is 1.38629.
After 2148 training step(s), loss on training batch is 1.38629.
After 2149 training step(s), loss on training batch is 1.38629.
After 2150 training step(s), loss on training batch is 1.38629.
After 2151 training step(s), loss on training batch is 1.3863.
After 2152 training step(s), loss on training batch is 1.3863.
After 2153 training step(s), loss on training batch is 1.38631.
After 2154 training step(s), loss on training batch is 1.38631.
After 2155 training step(s), loss on training batch is 1.38631.
After 2156 training step(s), loss on training batch is 1.38629.
After 2157 training step(s), loss on training batch is 1.3863.
After 2158 training step(s), loss on training batch is 1.38631.
After 2159 training step(s), loss on training batch is 1.3863.
After 2160 training step(s), loss on training batch is 1.3863.
After 2161 training step(s), loss on training batch is 1.38628.
After 2162 training step(s), loss on training batch is 1.38631.
After 2163 training step(s), loss on training batch is 1.3863.
After 2164 training step(s), loss on training batch is 1.38631.
After 2165 training step(s), loss on training batch is 1.3863.
After 2166 training step(s), loss on training batch is 1.38631.
After 2167 training step(s), loss on training batch is 1.3863.
After 2168 training step(s), loss on training batch is 1.38631.
After 2169 training step(s), loss on training batch is 1.38631.
After 2170 training step(s), loss on training batch is 1.38631.
After 2171 training step(s), loss on training batch is 1.3863.
After 2172 training step(s), loss on training batch is 1.38631.
After 2173 training step(s), loss on training batch is 1.38631.
After 2174 training step(s), loss on training batch is 1.38631.
After 2175 training step(s), loss on training batch is 1.38631.
After 2176 training step(s), loss on training batch is 1.3863.
After 2177 training step(s), loss on training batch is 1.38631.
After 2178 training step(s), loss on training batch is 1.38631.
After 2179 training step(s), loss on training batch is 1.38631.
After 2180 training step(s), loss on training batch is 1.38631.
After 2181 training step(s), loss on training batch is 1.38631.
After 2182 training step(s), loss on training batch is 1.3863.
After 2183 training step(s), loss on training batch is 1.38629.
After 2184 training step(s), loss on training batch is 1.38629.
After 2185 training step(s), loss on training batch is 1.38629.
After 2186 training step(s), loss on training batch is 1.38629.
After 2187 training step(s), loss on training batch is 1.38629.
After 2188 training step(s), loss on training batch is 1.38629.
After 2189 training step(s), loss on training batch is 1.38629.
After 2190 training step(s), loss on training batch is 1.38629.
After 2191 training step(s), loss on training batch is 1.38629.
After 2192 training step(s), loss on training batch is 1.38629.
After 2193 training step(s), loss on training batch is 1.38629.
After 2194 training step(s), loss on training batch is 1.38629.
After 2195 training step(s), loss on training batch is 1.38629.
After 2196 training step(s), loss on training batch is 1.38629.
After 2197 training step(s), loss on training batch is 1.38629.
After 2198 training step(s), loss on training batch is 1.38629.
After 2199 training step(s), loss on training batch is 1.38629.
After 2200 training step(s), loss on training batch is 1.38628.
After 2201 training step(s), loss on training batch is 1.38629.
After 2202 training step(s), loss on training batch is 1.38629.
After 2203 training step(s), loss on training batch is 1.38629.
After 2204 training step(s), loss on training batch is 1.38629.
After 2205 training step(s), loss on training batch is 1.38629.
After 2206 training step(s), loss on training batch is 1.38629.
After 2207 training step(s), loss on training batch is 1.38629.
After 2208 training step(s), loss on training batch is 1.38629.
After 2209 training step(s), loss on training batch is 1.38629.
After 2210 training step(s), loss on training batch is 1.38629.
After 2211 training step(s), loss on training batch is 1.38629.
After 2212 training step(s), loss on training batch is 1.38629.
After 2213 training step(s), loss on training batch is 1.38629.
After 2214 training step(s), loss on training batch is 1.38629.
After 2215 training step(s), loss on training batch is 1.38629.
After 2216 training step(s), loss on training batch is 1.38629.
After 2217 training step(s), loss on training batch is 1.38629.
After 2218 training step(s), loss on training batch is 1.3863.
After 2219 training step(s), loss on training batch is 1.38629.
After 2220 training step(s), loss on training batch is 1.38629.
After 2221 training step(s), loss on training batch is 1.38628.
After 2222 training step(s), loss on training batch is 1.38629.
After 2223 training step(s), loss on training batch is 1.38629.
After 2224 training step(s), loss on training batch is 1.38629.
After 2225 training step(s), loss on training batch is 1.38629.
After 2226 training step(s), loss on training batch is 1.38629.
After 2227 training step(s), loss on training batch is 1.38629.
After 2228 training step(s), loss on training batch is 1.38628.
After 2229 training step(s), loss on training batch is 1.38629.
After 2230 training step(s), loss on training batch is 1.38629.
After 2231 training step(s), loss on training batch is 1.38628.
After 2232 training step(s), loss on training batch is 1.38629.
After 2233 training step(s), loss on training batch is 1.38629.
After 2234 training step(s), loss on training batch is 1.38629.
After 2235 training step(s), loss on training batch is 1.38629.
After 2236 training step(s), loss on training batch is 1.38629.
After 2237 training step(s), loss on training batch is 1.38629.
After 2238 training step(s), loss on training batch is 1.38629.
After 2239 training step(s), loss on training batch is 1.38629.
After 2240 training step(s), loss on training batch is 1.38629.
After 2241 training step(s), loss on training batch is 1.38629.
After 2242 training step(s), loss on training batch is 1.38629.
After 2243 training step(s), loss on training batch is 1.38629.
After 2244 training step(s), loss on training batch is 1.38629.
After 2245 training step(s), loss on training batch is 1.38629.
After 2246 training step(s), loss on training batch is 1.38629.
After 2247 training step(s), loss on training batch is 1.38629.
After 2248 training step(s), loss on training batch is 1.38629.
After 2249 training step(s), loss on training batch is 1.38629.
After 2250 training step(s), loss on training batch is 1.38629.
After 2251 training step(s), loss on training batch is 1.38629.
After 2252 training step(s), loss on training batch is 1.38629.
After 2253 training step(s), loss on training batch is 1.38629.
After 2254 training step(s), loss on training batch is 1.38629.
After 2255 training step(s), loss on training batch is 1.38629.
After 2256 training step(s), loss on training batch is 1.38629.
After 2257 training step(s), loss on training batch is 1.38629.
After 2258 training step(s), loss on training batch is 1.38629.
After 2259 training step(s), loss on training batch is 1.38629.
After 2260 training step(s), loss on training batch is 1.38629.
After 2261 training step(s), loss on training batch is 1.38629.
After 2262 training step(s), loss on training batch is 1.38629.
After 2263 training step(s), loss on training batch is 1.38629.
After 2264 training step(s), loss on training batch is 1.38629.
After 2265 training step(s), loss on training batch is 1.38629.
After 2266 training step(s), loss on training batch is 1.38629.
After 2267 training step(s), loss on training batch is 1.38629.
After 2268 training step(s), loss on training batch is 1.38629.
After 2269 training step(s), loss on training batch is 1.38629.
After 2270 training step(s), loss on training batch is 1.38629.
After 2271 training step(s), loss on training batch is 1.38629.
After 2272 training step(s), loss on training batch is 1.38629.
After 2273 training step(s), loss on training batch is 1.38629.
After 2274 training step(s), loss on training batch is 1.38629.
After 2275 training step(s), loss on training batch is 1.38629.
After 2276 training step(s), loss on training batch is 1.38629.
After 2277 training step(s), loss on training batch is 1.38629.
After 2278 training step(s), loss on training batch is 1.38629.
After 2279 training step(s), loss on training batch is 1.38629.
After 2280 training step(s), loss on training batch is 1.38629.
After 2281 training step(s), loss on training batch is 1.38629.
After 2282 training step(s), loss on training batch is 1.38629.
After 2283 training step(s), loss on training batch is 1.38629.
After 2284 training step(s), loss on training batch is 1.38629.
After 2285 training step(s), loss on training batch is 1.38629.
After 2286 training step(s), loss on training batch is 1.38629.
After 2287 training step(s), loss on training batch is 1.38629.
After 2288 training step(s), loss on training batch is 1.38629.
After 2289 training step(s), loss on training batch is 1.38629.
After 2290 training step(s), loss on training batch is 1.38629.
After 2291 training step(s), loss on training batch is 1.38629.
After 2292 training step(s), loss on training batch is 1.38629.
After 2293 training step(s), loss on training batch is 1.38629.
After 2294 training step(s), loss on training batch is 1.38629.
After 2295 training step(s), loss on training batch is 1.38629.
After 2296 training step(s), loss on training batch is 1.38629.
After 2297 training step(s), loss on training batch is 1.38629.
After 2298 training step(s), loss on training batch is 1.38629.
After 2299 training step(s), loss on training batch is 1.38629.
After 2300 training step(s), loss on training batch is 1.38629.
After 2301 training step(s), loss on training batch is 1.38629.
After 2302 training step(s), loss on training batch is 1.38629.
After 2303 training step(s), loss on training batch is 1.38629.
After 2304 training step(s), loss on training batch is 1.38629.
After 2305 training step(s), loss on training batch is 1.38629.
After 2306 training step(s), loss on training batch is 1.38628.
After 2307 training step(s), loss on training batch is 1.38629.
After 2308 training step(s), loss on training batch is 1.38629.
After 2309 training step(s), loss on training batch is 1.38628.
After 2310 training step(s), loss on training batch is 1.38629.
After 2311 training step(s), loss on training batch is 1.38629.
After 2312 training step(s), loss on training batch is 1.38629.
After 2313 training step(s), loss on training batch is 1.38629.
After 2314 training step(s), loss on training batch is 1.38629.
After 2315 training step(s), loss on training batch is 1.38629.
After 2316 training step(s), loss on training batch is 1.38628.
After 2317 training step(s), loss on training batch is 1.38628.
After 2318 training step(s), loss on training batch is 1.38628.
After 2319 training step(s), loss on training batch is 1.38628.
After 2320 training step(s), loss on training batch is 1.38628.
After 2321 training step(s), loss on training batch is 1.38629.
After 2322 training step(s), loss on training batch is 1.38629.
After 2323 training step(s), loss on training batch is 1.38629.
After 2324 training step(s), loss on training batch is 1.38629.
After 2325 training step(s), loss on training batch is 1.38629.
After 2326 training step(s), loss on training batch is 1.38629.
After 2327 training step(s), loss on training batch is 1.38629.
After 2328 training step(s), loss on training batch is 1.38629.
After 2329 training step(s), loss on training batch is 1.38629.
After 2330 training step(s), loss on training batch is 1.38629.
After 2331 training step(s), loss on training batch is 1.38629.
After 2332 training step(s), loss on training batch is 1.38629.
After 2333 training step(s), loss on training batch is 1.38629.
After 2334 training step(s), loss on training batch is 1.38629.
After 2335 training step(s), loss on training batch is 1.38629.
After 2336 training step(s), loss on training batch is 1.38629.
After 2337 training step(s), loss on training batch is 1.38629.
After 2338 training step(s), loss on training batch is 1.38628.
After 2339 training step(s), loss on training batch is 1.38629.
After 2340 training step(s), loss on training batch is 1.38629.
After 2341 training step(s), loss on training batch is 1.38629.
After 2342 training step(s), loss on training batch is 1.38628.
After 2343 training step(s), loss on training batch is 1.3863.
After 2344 training step(s), loss on training batch is 1.38631.
After 2345 training step(s), loss on training batch is 1.3863.
After 2346 training step(s), loss on training batch is 1.3863.
After 2347 training step(s), loss on training batch is 1.3863.
After 2348 training step(s), loss on training batch is 1.3863.
After 2349 training step(s), loss on training batch is 1.3863.
After 2350 training step(s), loss on training batch is 1.38631.
After 2351 training step(s), loss on training batch is 1.3863.
After 2352 training step(s), loss on training batch is 1.3863.
After 2353 training step(s), loss on training batch is 1.3863.
After 2354 training step(s), loss on training batch is 1.3863.
After 2355 training step(s), loss on training batch is 1.38631.
After 2356 training step(s), loss on training batch is 1.3863.
After 2357 training step(s), loss on training batch is 1.38631.
After 2358 training step(s), loss on training batch is 1.3863.
After 2359 training step(s), loss on training batch is 1.3863.
After 2360 training step(s), loss on training batch is 1.38631.
After 2361 training step(s), loss on training batch is 1.3863.
After 2362 training step(s), loss on training batch is 1.3863.
After 2363 training step(s), loss on training batch is 1.3863.
After 2364 training step(s), loss on training batch is 1.3863.
After 2365 training step(s), loss on training batch is 1.3863.
After 2366 training step(s), loss on training batch is 1.3863.
After 2367 training step(s), loss on training batch is 1.3863.
After 2368 training step(s), loss on training batch is 1.3863.
After 2369 training step(s), loss on training batch is 1.3863.
After 2370 training step(s), loss on training batch is 1.3863.
After 2371 training step(s), loss on training batch is 1.3863.
After 2372 training step(s), loss on training batch is 1.3863.
After 2373 training step(s), loss on training batch is 1.3863.
After 2374 training step(s), loss on training batch is 1.38631.
After 2375 training step(s), loss on training batch is 1.3863.
After 2376 training step(s), loss on training batch is 1.3863.
After 2377 training step(s), loss on training batch is 1.3863.
After 2378 training step(s), loss on training batch is 1.3863.
After 2379 training step(s), loss on training batch is 1.3863.
After 2380 training step(s), loss on training batch is 1.3863.
After 2381 training step(s), loss on training batch is 1.3863.
After 2382 training step(s), loss on training batch is 1.3863.
After 2383 training step(s), loss on training batch is 1.3863.
After 2384 training step(s), loss on training batch is 1.3863.
After 2385 training step(s), loss on training batch is 1.3863.
After 2386 training step(s), loss on training batch is 1.38631.
After 2387 training step(s), loss on training batch is 1.3863.
After 2388 training step(s), loss on training batch is 1.38631.
After 2389 training step(s), loss on training batch is 1.38631.
After 2390 training step(s), loss on training batch is 1.38631.
After 2391 training step(s), loss on training batch is 1.3863.
After 2392 training step(s), loss on training batch is 1.38631.
After 2393 training step(s), loss on training batch is 1.3863.
After 2394 training step(s), loss on training batch is 1.3863.
After 2395 training step(s), loss on training batch is 1.3863.
After 2396 training step(s), loss on training batch is 1.38631.
After 2397 training step(s), loss on training batch is 1.3863.
After 2398 training step(s), loss on training batch is 1.3863.
After 2399 training step(s), loss on training batch is 1.3863.
After 2400 training step(s), loss on training batch is 1.3863.
After 2401 training step(s), loss on training batch is 1.38628.
After 2402 training step(s), loss on training batch is 1.38629.
After 2403 training step(s), loss on training batch is 1.38629.
After 2404 training step(s), loss on training batch is 1.38629.
After 2405 training step(s), loss on training batch is 1.3863.
After 2406 training step(s), loss on training batch is 1.38629.
After 2407 training step(s), loss on training batch is 1.38629.
After 2408 training step(s), loss on training batch is 1.38629.
After 2409 training step(s), loss on training batch is 1.38628.
After 2410 training step(s), loss on training batch is 1.38628.
After 2411 training step(s), loss on training batch is 1.38629.
After 2412 training step(s), loss on training batch is 1.38629.
After 2413 training step(s), loss on training batch is 1.38629.
After 2414 training step(s), loss on training batch is 1.38629.
After 2415 training step(s), loss on training batch is 1.38629.
After 2416 training step(s), loss on training batch is 1.38629.
After 2417 training step(s), loss on training batch is 1.38629.
After 2418 training step(s), loss on training batch is 1.38629.
After 2419 training step(s), loss on training batch is 1.38628.
After 2420 training step(s), loss on training batch is 1.38628.
After 2421 training step(s), loss on training batch is 1.38629.
After 2422 training step(s), loss on training batch is 1.38629.
After 2423 training step(s), loss on training batch is 1.38629.
After 2424 training step(s), loss on training batch is 1.38629.
After 2425 training step(s), loss on training batch is 1.38629.
After 2426 training step(s), loss on training batch is 1.38629.
After 2427 training step(s), loss on training batch is 1.38628.
After 2428 training step(s), loss on training batch is 1.38629.
After 2429 training step(s), loss on training batch is 1.38629.
After 2430 training step(s), loss on training batch is 1.38629.
After 2431 training step(s), loss on training batch is 1.38629.
After 2432 training step(s), loss on training batch is 1.38629.
After 2433 training step(s), loss on training batch is 1.38629.
After 2434 training step(s), loss on training batch is 1.38629.
After 2435 training step(s), loss on training batch is 1.38629.
After 2436 training step(s), loss on training batch is 1.38629.
After 2437 training step(s), loss on training batch is 1.38629.
After 2438 training step(s), loss on training batch is 1.38629.
After 2439 training step(s), loss on training batch is 1.38628.
After 2440 training step(s), loss on training batch is 1.38629.
After 2441 training step(s), loss on training batch is 1.38629.
After 2442 training step(s), loss on training batch is 1.38629.
After 2443 training step(s), loss on training batch is 1.38629.
After 2444 training step(s), loss on training batch is 1.38629.
After 2445 training step(s), loss on training batch is 1.38629.
After 2446 training step(s), loss on training batch is 1.38629.
After 2447 training step(s), loss on training batch is 1.38629.
After 2448 training step(s), loss on training batch is 1.38629.
After 2449 training step(s), loss on training batch is 1.38629.
After 2450 training step(s), loss on training batch is 1.38629.
After 2451 training step(s), loss on training batch is 1.38629.
After 2452 training step(s), loss on training batch is 1.38629.
After 2453 training step(s), loss on training batch is 1.38629.
After 2454 training step(s), loss on training batch is 1.38629.
After 2455 training step(s), loss on training batch is 1.3863.
After 2456 training step(s), loss on training batch is 1.38629.
After 2457 training step(s), loss on training batch is 1.38628.
After 2458 training step(s), loss on training batch is 1.38629.
After 2459 training step(s), loss on training batch is 1.38629.
After 2460 training step(s), loss on training batch is 1.38629.
After 2461 training step(s), loss on training batch is 1.38629.
After 2462 training step(s), loss on training batch is 1.38628.
After 2463 training step(s), loss on training batch is 1.38629.
After 2464 training step(s), loss on training batch is 1.38629.
After 2465 training step(s), loss on training batch is 1.38628.
After 2466 training step(s), loss on training batch is 1.38629.
After 2467 training step(s), loss on training batch is 1.38629.
After 2468 training step(s), loss on training batch is 1.38629.
After 2469 training step(s), loss on training batch is 1.38629.
After 2470 training step(s), loss on training batch is 1.38629.
After 2471 training step(s), loss on training batch is 1.38629.
After 2472 training step(s), loss on training batch is 1.38629.
After 2473 training step(s), loss on training batch is 1.38629.
After 2474 training step(s), loss on training batch is 1.38629.
After 2475 training step(s), loss on training batch is 1.38629.
After 2476 training step(s), loss on training batch is 1.38629.
After 2477 training step(s), loss on training batch is 1.38629.
After 2478 training step(s), loss on training batch is 1.38629.
After 2479 training step(s), loss on training batch is 1.38629.
After 2480 training step(s), loss on training batch is 1.38629.
After 2481 training step(s), loss on training batch is 1.38629.
After 2482 training step(s), loss on training batch is 1.3863.
After 2483 training step(s), loss on training batch is 1.3863.
After 2484 training step(s), loss on training batch is 1.3863.
After 2485 training step(s), loss on training batch is 1.38631.
After 2486 training step(s), loss on training batch is 1.38631.
After 2487 training step(s), loss on training batch is 1.38631.
After 2488 training step(s), loss on training batch is 1.3863.
After 2489 training step(s), loss on training batch is 1.3863.
After 2490 training step(s), loss on training batch is 1.3863.
After 2491 training step(s), loss on training batch is 1.38631.
After 2492 training step(s), loss on training batch is 1.3863.
After 2493 training step(s), loss on training batch is 1.3863.
After 2494 training step(s), loss on training batch is 1.3863.
After 2495 training step(s), loss on training batch is 1.3863.
After 2496 training step(s), loss on training batch is 1.3863.
After 2497 training step(s), loss on training batch is 1.3863.
After 2498 training step(s), loss on training batch is 1.38631.
After 2499 training step(s), loss on training batch is 1.38631.
After 2500 training step(s), loss on training batch is 1.3863.
After 2501 training step(s), loss on training batch is 1.3863.
After 2502 training step(s), loss on training batch is 1.3863.
After 2503 training step(s), loss on training batch is 1.38629.
After 2504 training step(s), loss on training batch is 1.3863.
After 2505 training step(s), loss on training batch is 1.3863.
After 2506 training step(s), loss on training batch is 1.3863.
After 2507 training step(s), loss on training batch is 1.3863.
After 2508 training step(s), loss on training batch is 1.38629.
After 2509 training step(s), loss on training batch is 1.3863.
After 2510 training step(s), loss on training batch is 1.3863.
After 2511 training step(s), loss on training batch is 1.3863.
After 2512 training step(s), loss on training batch is 1.3863.
After 2513 training step(s), loss on training batch is 1.3863.
After 2514 training step(s), loss on training batch is 1.3863.
After 2515 training step(s), loss on training batch is 1.3863.
After 2516 training step(s), loss on training batch is 1.3863.
After 2517 training step(s), loss on training batch is 1.38628.
After 2518 training step(s), loss on training batch is 1.38628.
After 2519 training step(s), loss on training batch is 1.38629.
After 2520 training step(s), loss on training batch is 1.3863.
After 2521 training step(s), loss on training batch is 1.3863.
After 2522 training step(s), loss on training batch is 1.3863.
After 2523 training step(s), loss on training batch is 1.3863.
After 2524 training step(s), loss on training batch is 1.3863.
After 2525 training step(s), loss on training batch is 1.3863.
After 2526 training step(s), loss on training batch is 1.3863.
After 2527 training step(s), loss on training batch is 1.38628.
After 2528 training step(s), loss on training batch is 1.38628.
After 2529 training step(s), loss on training batch is 1.38628.
After 2530 training step(s), loss on training batch is 1.3863.
After 2531 training step(s), loss on training batch is 1.3863.
After 2532 training step(s), loss on training batch is 1.3863.
After 2533 training step(s), loss on training batch is 1.3863.
After 2534 training step(s), loss on training batch is 1.38629.
After 2535 training step(s), loss on training batch is 1.38629.
After 2536 training step(s), loss on training batch is 1.38629.
After 2537 training step(s), loss on training batch is 1.38629.
After 2538 training step(s), loss on training batch is 1.38629.
After 2539 training step(s), loss on training batch is 1.38629.
After 2540 training step(s), loss on training batch is 1.38629.
After 2541 training step(s), loss on training batch is 1.38629.
After 2542 training step(s), loss on training batch is 1.38629.
After 2543 training step(s), loss on training batch is 1.38629.
After 2544 training step(s), loss on training batch is 1.38629.
After 2545 training step(s), loss on training batch is 1.38629.
After 2546 training step(s), loss on training batch is 1.38629.
After 2547 training step(s), loss on training batch is 1.38629.
After 2548 training step(s), loss on training batch is 1.38629.
After 2549 training step(s), loss on training batch is 1.38631.
After 2550 training step(s), loss on training batch is 1.38631.
After 2551 training step(s), loss on training batch is 1.3863.
After 2552 training step(s), loss on training batch is 1.38631.
After 2553 training step(s), loss on training batch is 1.38631.
After 2554 training step(s), loss on training batch is 1.38631.
After 2555 training step(s), loss on training batch is 1.38631.
After 2556 training step(s), loss on training batch is 1.38629.
After 2557 training step(s), loss on training batch is 1.3863.
After 2558 training step(s), loss on training batch is 1.38631.
After 2559 training step(s), loss on training batch is 1.38631.
After 2560 training step(s), loss on training batch is 1.3863.
After 2561 training step(s), loss on training batch is 1.38628.
After 2562 training step(s), loss on training batch is 1.38631.
After 2563 training step(s), loss on training batch is 1.3863.
After 2564 training step(s), loss on training batch is 1.3863.
After 2565 training step(s), loss on training batch is 1.3863.
After 2566 training step(s), loss on training batch is 1.38631.
After 2567 training step(s), loss on training batch is 1.3863.
After 2568 training step(s), loss on training batch is 1.38631.
After 2569 training step(s), loss on training batch is 1.38631.
After 2570 training step(s), loss on training batch is 1.38631.
After 2571 training step(s), loss on training batch is 1.38631.
After 2572 training step(s), loss on training batch is 1.38631.
After 2573 training step(s), loss on training batch is 1.38631.
After 2574 training step(s), loss on training batch is 1.38631.
After 2575 training step(s), loss on training batch is 1.38631.
After 2576 training step(s), loss on training batch is 1.38631.
After 2577 training step(s), loss on training batch is 1.38631.
After 2578 training step(s), loss on training batch is 1.38631.
After 2579 training step(s), loss on training batch is 1.38631.
After 2580 training step(s), loss on training batch is 1.38631.
After 2581 training step(s), loss on training batch is 1.38631.
After 2582 training step(s), loss on training batch is 1.3863.
After 2583 training step(s), loss on training batch is 1.38629.
After 2584 training step(s), loss on training batch is 1.38629.
After 2585 training step(s), loss on training batch is 1.38629.
After 2586 training step(s), loss on training batch is 1.38629.
After 2587 training step(s), loss on training batch is 1.38629.
After 2588 training step(s), loss on training batch is 1.38629.
After 2589 training step(s), loss on training batch is 1.38629.
After 2590 training step(s), loss on training batch is 1.38629.
After 2591 training step(s), loss on training batch is 1.38629.
After 2592 training step(s), loss on training batch is 1.38629.
After 2593 training step(s), loss on training batch is 1.38629.
After 2594 training step(s), loss on training batch is 1.38629.
After 2595 training step(s), loss on training batch is 1.38629.
After 2596 training step(s), loss on training batch is 1.38629.
After 2597 training step(s), loss on training batch is 1.38629.
After 2598 training step(s), loss on training batch is 1.38629.
After 2599 training step(s), loss on training batch is 1.38629.
After 2600 training step(s), loss on training batch is 1.38629.